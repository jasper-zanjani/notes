{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\ud83c\udfe0 Home \u2714\ufe0f TODO C# Working with files and streams in C# Using HttpClient to consume APIs in .NET Core Programming in Microsoft C# - Exam 70-483 udemy.com - Programming in Microsoft C# - Exam 70-483 pytest 2.5-hour Udemy course 6-hour Udemy course Python Testing With Pytest \ud83e\udd47 Certifications Exam Name Links 70-483 \u271d Programming in C# 70-484 \u271d Essentials of developing Windows Store apps in C# 70-485 \u271d Advanced Windows Store app development using C# \ud83d\ude80 Space Missions to look forward to James Webb Space Telescope (JWST) to be launched ca. October 31, 2021 Artemis-1 to be launched in late 2021 , with first scientific observations after 6 months of testing Jupiter Icy Moon Explorer (JUICE) to be launched 2022 and arrive 2029 E-ELT first light expected November 2026","title":"\ud83c\udfe0 Home"},{"location":"#home","text":"","title":"\ud83c\udfe0 Home"},{"location":"#todo","text":"C# Working with files and streams in C# Using HttpClient to consume APIs in .NET Core Programming in Microsoft C# - Exam 70-483 udemy.com - Programming in Microsoft C# - Exam 70-483 pytest 2.5-hour Udemy course 6-hour Udemy course Python Testing With Pytest","title":"\u2714&#xfe0f; TODO"},{"location":"#certifications","text":"Exam Name Links 70-483 \u271d Programming in C# 70-484 \u271d Essentials of developing Windows Store apps in C# 70-485 \u271d Advanced Windows Store app development using C#","title":"\ud83e\udd47 Certifications"},{"location":"#space","text":"Missions to look forward to James Webb Space Telescope (JWST) to be launched ca. October 31, 2021 Artemis-1 to be launched in late 2021 , with first scientific observations after 6 months of testing Jupiter Icy Moon Explorer (JUICE) to be launched 2022 and arrive 2029 E-ELT first light expected November 2026","title":"\ud83d\ude80 Space"},{"location":"Certifications/","text":"\ud83e\udd47 Certifications Exam Name Links 70-483 \u271d Programming in C# 70-484 \u271d Essentials of developing Windows Store apps in C# 70-485 \u271d Advanced Windows Store app development using C#","title":"\ud83e\udd47 Certifications"},{"location":"Certifications/#certifications","text":"Exam Name Links 70-483 \u271d Programming in C# 70-484 \u271d Essentials of developing Windows Store apps in C# 70-485 \u271d Advanced Windows Store app development using C#","title":"\ud83e\udd47 Certifications"},{"location":"Cloud/","text":"\u2601\ufe0f Cloud Compute IaaS Azure VMs EC2 Compute Engine PaaS App Service Elastic Beanstalk App Engine Serverless Functions Lambda Cloud Functions Cloud Run Containers Individual containers ACI ECS Kubernetes AKS EKS GKE Container registry Artifact Registry Storage Archive Glacier Backups Recovery Services Vault Backup Physical media Data Box Import/Export Service Snowball Transfer Appliance Network Private networks VNets VPC VPC Security rules Network Security Group (NSG) Security Group Firewall Rules DNS Azure DNS Route 53 Cloud DNS Development NoSQL Cosmos DB DynamoDB DocumentDB Firestore Spanner CI/CD Azure Devops CodeBuild CodeCommit CodeDeploy CodePipeline Cloud Build Messaging SNS Pub/Sub Computer Vision Computer Vision Rekognition Cloud Vision Big Data Big Data Data Lake Store Redshift Athena BigQuery BigTable Dataprep Streaming data Event Hubs Service Bus Stream Analytics Kinesis Athena DataFlow Batch processing HDInsight Batch EMR Batch DataFlow Dataproc Links TODO: Cloud Storage CloudWatch Azure Functions Glacier Google Cloud Storage (GCS) gcloud gsutil Simple Notification Service \ud83d\udee0\ufe0f Administration \ud83d\udcb0 Cost management Azure quotas apply to subscriptions and are implemented with tags . Resource quotas trigger alarms when resource creation and consumption hit a threshold. These are not to be confused with resource limits which can stop resources from being created, whereas quotas can not. Spending quotas trigger alarms when spending has reached a threshold. Azure budgets can be viewed and administered in the Cost Management + Billing blade. Users must have at least the Reader role at the subscription scope to view, and Contributor to create and manage, budgets. \ud83d\udd12 Locks Azure resource locks are used to apply restrictions across all users and roles and can be applied at subscription, resource group, or resource scopes. CanNotDelete ReadOnly effectively restricts all authorized users to the permissions granted by the Reader role Storage account keys of a locked storage account cannot be listed because the list keys operation is handled through a POST request Visual Studio Server Explorer will not be able to display files for a locked App Service resource, because that interaction requires write access VMs in a locked resource group will not be able to be started or restarted, because those operations require a POST request All child resources of the scope at which a lock is applied inherit the lock. A CanNotDelete lock applied to a DNS A record would also prevent the deletion of the DNS zone that the record resides in, as well as the resource group the zone resides in. Of the builtin roles , only two have access to the Microsoft.Authorization/* or Microsoft.Authorization/locks/* actions required to create or delete locks: Owner User Access Administrator Resource locks apply to the management plane of Azure, specifically operations sent to https://management.azure.com Managed applications create two resource groups to implement locks: One resource group to contain an overview of the service, which isn't locked Another resource group containing the infrastructure for the service, which is locked Sources: Move resources to a new resource group or subscription Some services have limitations or requirements when moving resources between groups ( src ) Source and destination subscriptions must be within the same [AAD][Azure AD] tenant Destination subscription must be registered for the resource provider of the resource being moved Account moving the resources must have at least the following permissions: Microsoft.Resources/subscriptions/resourceGroups/moveResources/action Microsoft.Resources/subscriptions/resourceGroups/write IAM All cloud providers offer Identity and Access Management (IAM) systems that are used to control access to resources. Role-based access control (RBAC) is the universally recommended and modern approach, relying on bundles of specific permissions called roles that can be assigned to principals. All cloud providers offer the ability to define custom roles and come with many ready-to-use role definitions (\" predefined roles \" in GCP or \" built-in roles \" in Azure). Cloud providers also still support legacy IAM systems which are deprecated. Azure classic administrator roles include \"Account Administrator\", \"Service Administrator\" and \"Co-Administrator\" GCP primitive roles \"owner\", \"editor\", and \"viewer\" can still be applied to most GCP resources. Important roles Owner has full access to all resources and can delegate access. Service Administrator and Co-Administrators are assigned this role at the subscription scope. Contributor can create and manage all resources (full read/write privileges), but cannot delegate access. Reader can view resources. Cost Management Contributor Cost Management Reader Resource Policy Contributor User Administrator User Access Administrator Billing Account Administrator Billing Account Creator Billing Account User Billing Account Viewer Compute Engine Admin Compute Engine Network Admin Compute Engine Security Admin Compute Engine Viewer Compute Service Agent Folder Admin Project Creator Shared VPC Admin Infrastructure All cloud providers divide their global services into a hierarchy of geographically defined regions , each of which is in turn divided into availability zones (what AWS calls its Global Infrastructure). Azure datacenters contain multiple availability zones, and every Azure region has at least three availability zones. Azure services are also divided into geographies , generally coterminous with countries. Azure geographies are further divided into regional pairs . Each regional pair receives rolling updates one member at a time. Most services are regionally based, meaning the underlying hardware of that service's instance will exist in only a single Region. Some regions, like AWS GovCloud , have restricted access. Some AWS resources, however, are technically running on hardware that exists in a single Region, but presented as global. Resources Services available on Free Tier \ud83d\udc41\ufe0f Monitoring Azure Monitor Network Watcher CloudWatch Stackdriver Trace Resources Cloud providers exhibit some variety in how resources can be organized. All cloud providers support key-value tags , many of which can be applied to the same resource. Any Azure resource can only exist in a single resource group , which can contain resources from any region or subscription. However, resource groups may not contain other resource groups. GCP projects are equivalent to Azure resource groups, in that they are containers for and direct parents to resources. However, projects can be placed within folders , which do support nested hierarchies. AWS does not have an equivalent method of organizing resources. Azure subscriptions can be organized into Management Groups , and they can be nested in a hierarchy of management groups up to a maximum depth of six levels. In AWS the Organizational Unit (OU) , which can organize user accounts (subscriptions) and the resources they contain in a nested hierarchy, appears to be equivalent. A pattern common to Azure is that of a service being implemented in two resource types, one of which determines important configuration settings shared by all instances of the service which are contained within it. This is the case for storage accounts , App Service , Azure Data Explorer clusters, etc. Description Tenant Organization Organization Corresponds to a company or organization Management group Organizational Unit Logical container for user accounts and the resources created by that user Subscription Member account ? Credential associated with an individual Folder Organize resources and their parents in a nested hierarchy Resource group Project Logical container that is the direct parent to any resource, tied to a Region Tag Tag Label Key-value pairs that are used to organize resources The resource hierarchy organizes GCP resources in 3 levels below Domain Domain Organization corresponds to a company or organization. A single cloud identity is associated with a single organization and can have super admins Billing Account tracks charges and billing account admins can set budgets. Payments Profile is a Google-level resource that is used to pay for all Google services. \ud83d\udee0\ufe0f Support AWS offers various support plan tiers that provide 24/7 email, chat, and phone access to AWS cloud support engineers. Basic Support Plan Developer Support Plan (greater of $29 or 3% of monthly account usage) Business Support Plan Enterprise Support Plan (>$15,000/mo.) offers a Technical Account Manager (TAM) , a dedicated guide and advocate AWS documentation is available in several places: AWS documentation AWS Knowledge Center is a sprawling FAQ AWS security resources AWS forums Professional Services team makes white papers and webinars publicly available Tags Azure tags: Tag names have a limit of 512 characters (128 characters for storage accounts) Tag values have a limit of 256 characters. Resources and resource groups are limited to 15 tags. VMs cannot exceed 2048 characters for all tag names and values combined. Infrastructure as Code All cloud providers support ways of provisioning resources declaratively. Azure [ARM][ARM] templates are JSON, but [Bicep][Bicep] is a domain-specific language and command-line utility that can be used to generate templates from simpler, YAML-like syntax. \ud83d\udda5\ufe0f Compute IaaS All cloud providers offer Infrastructure as a Service (IaaS) , whereby virtual machines can be provisioned with specific compute resources and base operating systems. AWS also offers configuration management services like [OpsWorks][OpsWorks] and [Systems Manager][Systems Manager] GCP virtual machines are referred to as instances , and are available in three general machine family types: general-purpose, memory-optimized, and compute-optimized. Machine type describes the different packaged configurations representing allocated compute resources, or what is called a SKU in Azure. Containers Build and package container artifacts Private container registry Serverless \ud83d\udcbe Storage Archive Backups Azure Backup are integrated into Portal and clickable from the VM blade. You have to specify a Recovery Services vault and a Backup policy . The policy can specify frequency of backups, and other settings. Using Backup service costs $10 per VM plus the cost of used storage. 2 methods to restore data after backing up a VM to Azure Backup: Restore a recovery point as a new VM Restore access to files only Physical media Data Box Import/Export Service Snowball Transfer Appliance Uploading files to GCS \ud83c\udfe2 Networking All cloud providers offer an implementation of software-defined networking (SDN) that allows a logically isolated network to be defined as a block of IP addresses allocated from one of the private ranges (10.0.0.0/8, 192.168.0.0/16, or 172.16.0.0/12). In AWS and GCP, this network is referred to as a Virtual Private Cloud (VPC) , whereas in Azure it is called a Virtual Network (VNet) . In all providers, the network is confined to a single region and must have at least one IP segment called a subnet defined within it which must be a subset of the range used to define the virtual network itself. The smallest possible CIDR range for a subnet in Azure is 29, which provides 3 addresses for use (Azure reserves 5). In AWS, the smallest possible CIDR range is 28. In AWS, VPCs have a default range of 172.31.0.0/16 and subnets have a default subnet mask of /20. In Azure, subnets span Availability Zones, can only be deleted if empty, and their names, which are immutable, must be unique. In AWS, a subnet exists only within a single Availability Zone. VNet peering allows VMs in two separate virtual networks to communicate directly. In all cloud providers, this is a one-way process which must be repeated in both directions in order to have two-way communication. In Azure, before the introduction of peering, virtual networks were connected using S2S VPN or by connecting to the same ExpressRoute circuit. It is not required for the peered networks to be in the same region ( Global VNet peering ), subscription, or tenant, although cross-tenant peering is not available in the Portal but must be configured from the command-line or ARM templates. VNet peering has to be disabled before moving a VNet, and a VNet can only be moved within the same subscription. There is a maximum of 100 peering connections per VNet Peerings cannot be moved to another resource group or subscription, so they must be disabled before moving peered VNets. Service endpoints facilitate restricting traffic from Azure services. Service endpoint policies allow restricting traffic to the granularity of individual Azure service instances. An internet gateway is a VPC resource that allows EC2 instances to obtain a public IP address and access the Internet. In order to access the Internet, instances must be in a public subnet , one that contains a default route to the VPC's internet gateway. ExpressRoute is the main service used to connect Azure to on-premises networks, although P2S and S2S VPNs are also options. Direct Connect provides dedicated network connectivity to an AWS VPC through links offered through APN partners. In GCP, in addition to peering, a shared VPC can be created that is associated with multiple projects. Resources: Migrating to GCP? First Things First: VPCs User-defined routes In Azure, a virtual appliance refers to a VM running a network application like a load-balancer, firewall, or router. Service chaining refers to the process of deploying a network virtual appliance (NVA) into a hub network to route traffic between spokes using user-defined routes (UDR) . This is a method of reducing the complexity of pairing between individual spoke networks in complex hub-and-spoke architectures. AZ-103: 309 In such a deployment, the peerings must be set to Allow Forwarded Traffic . Alternatively, two peered networks can share a single virtual network gateway, say to connect to an external network. The pairing connection to the network that contains the gateway must be set to Use Remote Gateways The pairing connection from the network containing the gateway must be set to Allow Gateway Transit Network security Network Security Group (NSG) Security Group Firewall Rules Azure Network Security Groups (NSGs) are assocated with network interfaces and contain an arbitrary number of security rules . Each rule has the following properties: Name Priority : number between 100 and 4096, lower numbers indicate a higher priority Source or destination : IP address, CIDR block, service tag, or application security group Protocol : TCP , UDP , ICMP , or Any Direction : Inbound or outbound Port range ; Action : allow or deny Service tags represent a group of IP address prefixes managed by Microsoft available for use in NSG rules: VirtualNetwork : all CIDR ranges defined for the virtual network, all connected on-premises address spaces, peered VNets or VNets connected to a VNET gateway AzureLoadBalancer : Virtual IP address of the host where Azure's health probes originate Internet : IP address space that is outside the virtual network AzureCloud* : IP address space for Azure, including all datacenter public IP addresses AzureTrafficManager* : IP address space for the Azure Traffic Manager probe IP addresses Storage : NSG flow logging ,which saves the 5-tuple of all packets, is available as a low-cost way to monitor traffic. Flow logs record all IP flows going in and out of an NSG and are collected per NSG rule. They are charged per GB of logs collected and include a free tier of 5 GB/month. In AWS VPCs, Security Groups are similar to firewall rules that regulate inbound and outbound traffic of an instance. Outbound traffic is unrestricted by default, and every VPC contains a default security group. A network access control lists (NACLs) , also like a firewall, contains inbound and outbound rules but operates on the subnet. By default, a NACL allows all inbound and outbound traffic. In GCP, each VPC has a set of firewall rules that control traffic not only into and out of the VPC, but between instances in the same VPC. Each rule can be tagged, and individual instances with the same tags inherit those rules. Resources: Protect your Google Cloud Instances with Firewall Rules DNS Azure DNS Route 53 Cloud DNS CDN Users can use Azure CDN as a cache, reducing load from website. Content is cached by the CDN until its time-to-live (TTL) elapses, which can be controlled in the HTTP response from the origin server. Permanently removing content from the CDN requires it be first removed from the origin servers, meaning if the content is in a storage account it should be set to private or deleted from the storage, or the container itself should be deleted. Cached copies may remain in the CDN endpoint until the TTL has expired, unless it is purged . There are 4 pricing tiers available within Azure CDN: Azure CDN Standard from Microsoft does not offer dynamic site acceleration (DSA) (cf. Azure Front Door Service) Azure CDN Standard from Akamai Azure CDN Standard from Verizon Azure CDN Premium from Verizon , for which caching is configured using a rules engine. AWS CloudFront GCP CDN Load-balancing Azure Load Balancer Application Gateway AWS Elastic Load Balancer GCP Load balancing \ud83d\udc68\u200d\ud83d\udcbb Development NoSQL NoSQL databases differ from relational databases in that they do not obey the principle of data normalization . That is, the same data can be stored in more than one place. This is an advantage for databases that are optimized for reads as opposed to writes, because fewer queries are needed to retrieve information. However, when changing information that is duplicated in several places, write operations will be more laborious and prone to error. NoSQL databases are also horizontally scalable because the information can be sharded horizontally more easily than relational database, which are only vertically scalable (meaning scaling them requires larger and larger computers) and can only be sharded vertically. ( src ) Big Data History Beginning in 2000, Amazon began developing Merchant.com, a planned e-commerce service that was intended to be the base upon which other enterprises would develop online shopping sites. At the time, Amazon's development environment was a jumbled mess, and in the effort to consolidate and organize the enterprise into a set of well-documented APIs. Despite these changes, software development remained sluggish, and an investigation discovered that individual teams were procuring storage, compute, and database resources independently. AWS originated out of the effort to consolidate these resources across the enterprise and remove this bottleneck. Azure was announced in 2008 and publicly released in 2010 after earlier experiments in cloud computing like Whitehorse and RedDog. In fact, references to the \"classic\" model predating the Azure Resource Manager (ARM) actually refer to RedDog: the \"classic\" portal was also known as \"RedDog Front-End\". \ud83d\udcd8 Glossary Apigee The Apigee API platform is a management service that allows developers to deploy, monitor, and secure their APIs and generates API proxies. APN Amazon Partner Network App Engine App Engine allows developers to deploy applications developed in popular programming languages to a serverless environment. It is available in two environment types: Standard and Flexible. Standard environment is the original App Engine environment, consisting of a preconfigured, language-specific runtime like Java, Python, PHP, Node.js, or Go. Flexible environment is similar to GKE in that it can run a customized container. App Engine is designed to support applications implemented as a microservices architecture. There are four components: The application is the top-level container that houses all other components. Services are versioned and provide a specific function. Versions are produced every time a service is updated. Every version runs on an instance . Each version of a service runs on its own instance, whose size can be determined by specifying the instance class . Instances can be dynamic or resident. Resident instances run continually and can be added or removed manually. Dynamic instances support autoscaling based on load. App Engine has three modes of scaling: Automatic scaling creates an instance with a specified request rate, response latency, and application metrics. Basic scaling creates instances only when requests are received Manual scaling supports operational continuity regardless of load level. App Service An App Service plan resource determines the billable compute resources available for the App Services applications managed by it. A plan acts as a container for multiple web applications sharing the same server farm (\"workers\"), and for this reason Windows and Linux apps can't be mixed in the same App Service plan. \"Web app\" is the legacy name for Azure App Service . App Service SSL certificates need to be deleted from each App Service before moving it to a new resource group. Resources \ud83d\udcb0 Pricing Tutorial: build and run a custom image in Azure App Service Create an App Service app with deployment from GitHub using Azure CLI Create a web app with continuous deployment from GitHub What is Azure Front Door? Create a Front Door - PowerShell Application Gateway Azure Application Gateway is used to load balance a large-scale set using more than 100 instances in place of Azure Load Balancer . AZ-103: p. 223 Application Gateway supports session affinity to save user state using browser cookies. Unlike Azure Load Balancer, which operates at OSI layer 4 and has limited security capabilities, Application Gateway operates at OSI layer 7 and provides Web Application Firewall (WAF) functioanlity to block attacks like SQL injection, cross-site scripting, and header injection. HTTPS is also only available with layer 7 load balancers like Application Gateway. Athena Athena is a serverless AWS service that allows SQL queries to be run against data stored in a [S3][S3] bucket. Athena works closely with [AWS Glue][AWS Glue] to extract schema information and crawl data sources. Before running for the first time, you must provide a path to a S3 bucket to store query results. Sources: How to use SQL to query S3 files with AWS Athena AWS CLI AWS CLI is version 1 is maintained for legacy compatibility purposes. AWS Developer Tools A collection of tools that provide CI services: CodeCommit CodeBuild CodeDeploy CodePipeline AWS Glue AzCopy AzCopy can be used to copy files to File storage. Azure Bastion Azure Bastion is a PaaS service deployed within a VNet that allows connectivity to a VM from the Portal. Once deployed in a VNet, RDP/SSH is available to all VMs in that VNet. This session is streamed to your local device over an HTMLS session using the browser. - It is not deployed per VM, but once per VNet to its own dedicated subnet , at least /27 or larger - No public IP is necessary on the VM, the connection from Bastion to the VM is to the private IP. However, the Bastion itself does require a public IP. - Bastion can now span peered VNets IPv6 support is limited in Azure. IPv6 addresses are not added to VMs by default and must be explicitly defined by adding an endpoint to each VM to be using it. Routing by IPv6 is also not supported, so load balancers have to be deployed. Sources Azure Virtual Network Overview Azure Bastion Introduction to flow logging for NSGs Bicep Project Bicep is a domain-specific language and command-line utility that can be used to generate [ARM][ARM] templates. Project Bicep \u2013 Next generation ARM Templates Azure Container Instances Azure Container Instances (ACI) allows a simpler way of running isolated containers in smaller-scale deployments than Azure Kubernetes Service . The top-level resource in ACI is the container group , a collection of containers that get scheduled on the same host machine. These containers share a lifecycle, resources, local network, and storage volumes, and is equivalent to a Kubernetes pod. Container groups can be deployed to a subnet that already hosts a container group or an empty one, but it may not be deployed to a subnet that already has other resources like VMs. Resources What is Azure Container Instances? Container groups in ACI Azure Data Explorer Azure DevOps Azure DevOps used to be known as Visual Studio Team Services and Team Foundation Server . Sources DevOps Training Workshop YouTube Install DevOps CLI az extension add --name azure-devops Azure DNS Azure DNS supports private zones, which provide name resolution for VMs on a VNet and between VNets without having to create a custom DNS solution. Time-to-live for DNS record sets is provided in seconds. Azure DNS alias records allow other Azure resources to be referenced from the DNS zone, rather than static IP addresses or domain names. This allows these records to be automatically updated or deleted when the underlying Azure resource is changed. An A alias record set is a special type of record set that allows you to create an alternative name for a record set in your domain zone or for resources in your subscription. A CNAME alias record set can only point to another CNAME record set. Custom domains can be used by implementing CNAME DNS records, which are used in DNS to map alias domain names to the \"canonical\" name. Sources Azure DNS alias records overview Azure File Service Azure File Service allows you to create one or more file shares in the cloud (up to 5 TB per share), similar to a regular Windows File Server. It supports the SMB protocol, so you can connect directly to a file share from outside of Azure, if traffic to port 445 is allowed through the LAN and ISP. It can also be mapped within Windows. A clever use of a file share is as persistent storage for the Azure Cloud Shell. src Azure File Sync Azure File Sync extends Azure File Service to allow on-premises file services to be extended to Azure while maintaining performance and compatibility, communicating over TCP 443 over SSL, and not IPSec. Use cases include: Replace on-premises file servers Easily replicate data on-premises to make it available during lift-and-shift migrations Simply cloud development and management Azure File Sync works using an Azure File Sync agent , available as an MSI package for Windows Server 2012R2, 2016, and 2019, to register file servers as endpoints to an Azure File Sync Group . After installation, Azure credentials for a subscription must be provided. AZ-103: 153 In order to create an Azure File Sync, first a Storage Sync Service resource must be created, which works like a container to hold one or more sync groups . Every sync group has only a single cloud endpoint , referring to a storage account, but can have more than one server endpoint . Any server can only be registered to a single Storage Sync Service, and servers synced to different Storage Sync Service resources cannot sync with each other. Cloud tiering is an optional feature in Azure File Sync in which frequently accessed files are cached in the on-prem file servers, while less commonly accessed files are tiered to Azure Files. This is done by enabling Cloud Tiering, then selecting a free space policy , a percentage which indicates the amount of free space to maintain on the server endpoint's volume. When a user does access one of these tiered files, that file is downloaded to the on-prem cache and made available locally from that point on. This frees up local storage. Cloud tiering cannot be used with server endpoints on the system volume Although server endpoints can be configured with different free space policies, the most restrictive setting takes effect For tiered files, the file will be partially downloaded as needed Although a mount point can be a server endpoint, there can be no mount points inside a server endpoint When a filename collision occurs between the file share and file server, the file on the server has its filename appended with the server's name. Azure Policy Azure Policy is a service that can create, assign, and manage policies to enforce governance. Policy definitions, authored in JSON, implement policy by describing desired behavior for Azure resources when they are created or updated. AZ-103: p. 72 To implement policy, a policy definition is created first, then a policy assignment assigns it to a scope. Policy definitions can be packaged together using initiative definitions and applied to a scope using initiative assignments RBAC roles deny by default and allow explicitly . But Azure Policy allows by default and denies explicitly Policies can be applied at the management group , subscription , or resource group scope, with all child resources and resource groups being affected. Every policy definition has a single effect , which includes: Audit : create a warning event in the log Modify : used to add, update, or remove properties or tags on a resource during creation or update. Append AuditIfNotExists Deny DeployIfNotExists Disabled The order of evaluation of effects is: Disabled, Append, Deny, Audit (\"DADA\") Sources What is Azure Policy? Azure Policy Samples Understand Azure Policy effects Azure VMs Virtual Machines represent Azure's IaaS offering. A dedicated host group has to be created and placed in a resource group and associated with a location and availability zone and assigned a fault domain. A host then has to be created, a size specified, and associated with a host group. Any VM intended to run on the host has to be created in the same location and availability zone and associated with the host in the Advanced tab. Azure spot instances are available at deep discounts. 3 types of disk are available to Azure VMs: Operating System Disk (OS Disk) Temporary Disk Data Disk Azure VM image types include: Managed images (recommended), which remove the dependency of the VM to the image, at least within the same region. Copying a VM to another region still requires the managed image to be copied first. Unmanaged images, which required the VM to be created in the same storage account as that of the image. VM copies required the image to be copies first. VM images are captured from an existing VM that has been generalized (prepared), removing unique settings (hostname, security IDs, personal information, user accounts, domain join information, etc) but not customizations (software installations, patches, additional files, folders), using sysprep.exe for Windows machines or Microsoft Azure Linux Agent ( waagent ) for Linux machines. VM images in AWS are called Amazon Machine Images (AMI) . Azure VPN Virtual network gateways in Azure are of two types: VPN gateways and ExpressRoute gateways . Any virtual network can have only a single gateway of each type. VPN gateways send encrypted traffic between the virtual network and an on-premises location. VPN Gateways must be deployed into their own dedicated subnet (named \"GatewaySubnet\" ) with a minimum size of CIDR /29, although a CIDR /27 address block is recommended. VPN connections between an on-premises network and a VNet are only possible if the network ranges do not overlap. VPN gateways can be classified by the topology of the connection: Site-to-Site (S2S) connections require an on-premises VPN device associated with a public IP address. Multi-Site connections require a RouteBased VPN type. Point-to-Site (P2S) allows individual computers to securely connect to a VNet without need for a VPN device, which is useful for telecommuting, and can use SSTP, OpenVPN, or IKEv2. There are several authentication considerations. VNet-to-VNet connections are also possible, but VNet peering may be preferable if the virtual networks meet certain requirements. Site-to-Site Multi-Site Point-to-Site VNet-to-VNet VPN gateways can also be classified on VPN type . Route-based VPNs (previously called \"dynamic routing gateways\") require routes to be defined in a routing table to direct packets into tunnel interfaces. Policy-based VPNs (previously called \"static routing gateways\" in the classic deployment model) can only be used on the Basic gateway SKU and offer only a single S2S tunnel. Route-based Policy-based There is a profusion of Gateway SKUs that determine the maximum connections, throughput, and availability of other features like BGP and zone-redundancy available for each topology. Every Azure VPN gateway consists of two instances in an active-standby configuration. During failover, a brief interruption of 10-15 seconds for planned maintenance or up to 60-90 seconds in the case of unplanned disruption, may occur. But the gateway can be configured to be active-active , which will establish S2S VPN tunnels to both gateway instances with traffic being routed through both tunnels simultaneously. There will still be only a single connection resource, but the on-premises VPN device must be configured to establish both of these tunnels. The most highly available arrangement would use multiple VPN devices with the VPN gateway in active-active configuration, creating 4 IPsec tunnels that evenly carry Azure traffic. Active-Standby Active-Active Dual redundancy Bigquery Petabyte-scale analytics database service for data warehousing. BigQuery can be executed using the bq command-line utility. BigTable Cloud BigTable is a GCP realtime database used for Big Data. BigTable can be executed using the cbt command-line utility. BigTable evolved out of Google's need to ensure access to petabytes of data in its web search business line. It was described in a 2006 research paper that ended up launching the entire NoSQL industry. In 2015 it was made available as a service to cloud customers. YouTube BigTable doesn't support secondary indexes. Resources The Right Bigtable Index Makes All the Difference The Right Bigtable Index Makes All the Difference Bigtable vs. Firestore Billing Account Administrator GCP predefined role that grants permissions to manage self-service accounts but not to create new ones. Billing Account Creator Predefined GCP role that grants permissions to create new self-service accounts. Billing Account User GCP predefined role that enables user to link projects to a billing account. Billing Account Viewer GCP predefined role that grants permissions to view transactional and billing data associated to a GCP account. Cloud AutoML Cloud AutoML is a GCP service that allows developers without machine learning experience to develop machine learning models. Cloud Device Administrator Azure built-in role that grants users full access to manage devices in Azure AD. Cloud Functions Cloud Functions is GCP's serverless compute offering and is suited to running short-running logic, such as calling other APIs in response to an event. Resources: Cloud Functions in a minute Quickstart Cloud Machine Learning Engine Platform for building and deploying scalable machine learning systems to production. Cloud Natural Language Processing GCP tool for analyzing human languages and extracting information from text. Cloud Run Google Cloud Run is built on a native open standard that will allow using the same container on other cloud providers. It bills down to the nearest 100 ms interval. Cloud Run provides an HTTPS endpoint to the container. Cloud Run can also run on your own K8S cluster running on GKE , recommended for workloads that have a consistently high level of traffic, since you are billed for the provisioned cluster resources. However, resources like CPU, GPU, and other items can be customized. Resources: Cloud Run in a minute Differences between Cloud Run and Cloud Run on GKE Cloud Run: deploy a prebuilt container Build and deploy with Cloud Run Cloud Vision Image analysis platform for annotating images with metadata, extracting text, or filtering content. Sources: Getting started: Image recognition and classification CloudFormation CloudFormation is AWS's declarative automation service, which can use JSON or YAML-format templates. These resources are placed into a named stack , a container that organizes the resources described by the template, and the stack name must be unique to the account. This allows provisioned resources to be easily managed, since the stack contains a record of events, and to be quickly destroyed by deleting the stack. CloudFormation Designer allows templates to be viewed as a diagram of resources. CloudFront Amazon CloudFront is a CDN that helps deliver static and dynamic content worldwide. CloudFront caches content in edge locations , of which there are more than 150 spread out across 6 continents. Edge locations may not be chosen arbitrarily, rather there are three options: US, Canada, and Europe US, Canada, Europe, Asia, and Africa All edge locations In order to make content available on CloudFront, you must create a distribution , which defines the type and origin of the content to cache. There are two types of distribution: A Web distribution is used for static and dynamic content, including streaming video, accessible via HTTP or HTTPS. Its origin can be a web server or a public S3 bucket. Real-Time Messaging Protocol (RTMP) distribution delivers streaming audio or video. The media player and media files must be stored in S3 buckets. CloudTrail AWS CloudTrail keeps event logs of actions that occur against AWS resources. These events are divided into API and non-API actions. API actions include creating, modifying, or deleting resources. Non-API actions include everything else, like logging into the management console. Events are also classified as management events and data events Management events (also control plane operations ) are operations that a principal attempts to execute against an AWS resource. Data events are S3 object-level activity and Lambda function executions. These are treated separately from management events because they tend to be higher volume. Resources \ufe0f How can I use CloudTrail to review what API calls and actions have occurred in my AWS account? CloudWatch Amazon CloudWatch collects logs, metrics, and events from AWS resources and non-AWS on-premises servers and presents a dashboard for visual analysis. All AWS resources automatically send their metrics to CloudWatch Metrics, which stores the data for up to 15 months. CloudWatch alarms can be configured for single metrics. Applications and AWS services have to be configured to send log events to CloudWatch Logs, and they are stored indefinitely by default although retention settings can be configured. Log events from the same source are organized into a log stream. Log streams are then organized into log groups. Metric filters extract metric data from log events. CloudWatch Events is a feature that monitors for changes in AWS resources as a result of API operations. Cloudyn Although the Cloudyn service, which had been purchased by Microsoft, was being offered as a standalone service, it has now been deprecated because its functionality has been incorporated natively into other sections of the Cost Management + Billing blade. CodeCommit CodeCommit is the AWS private git repo service. CodeDeploy CodeDeploy is an AWS service for automatically deploying applications to AWS compute resources or on-prem servers. CodeDeploy can pull source code from [S3][S3] and repos from GitHub or Bitbucket but notably not CodeCommit (ref. CodePipeline ). CodePipeline CodePipeline is an AWS service for orchestrating and automating every stage of software development. It defines a series of stages, two of which are required - source and deployment - but other stages like testing or approval can be incorporated. Cognito Cognito is an AWS service that integrates with identity providers like Amazon, Google, Microsoft, and Facebook to add user access control to an application. Compute Engine Compute Engine is GCP's IaaS offering. An instance group is a collection of VM instances that you can manage as a single entity. Two types: Managed instance groups operate applications like web front-ends across a group of identical VMs created with a template. They provide high availability, healing, scaling, and automatic updates. Unmanaged instance groups allow you to manually load balance a group of VMs. VMs can be added or removed at will. Getting started with GCE Compute Engine Admin Predefined GCP role that grants full control of Compute Engine resources. Compute Engine Network Admin Predefined GCP role that grants full control of Compute Engine networking resources. Compute Engine Security Admin Predefined GCP role that grants full control of Compute Engine security resources. Compute Engine Viewer Predefined GCP role that grans read-only access to all Compute Engine resources, but exclusive of data stored on disks, images, and snapshots. Compute Service Agent Predefined GCP role that grants Compute Engine Service Account access to assert service account authority. Computer Vision Computer vision is a subfield of artificial intelligence concerned with developing the capability of computers to recognize objects in images and to understand visual information. Container Instances Azure Container Instances (ACI) is a PaaS service that facilitates deployment of individual containers. CosmosDB Cosmos DB started as Project Florence in 2010 to address shortcomings with SQL Server in supporting highly available services like Xbox. In 2015 the product was relaunched as Document DB, then renamed Cosmos DB in 2017. An emulator is available for Cosmos DB here . A Cosmos DB account can be used for free for 30 days, and does not require an Azure subscription. Throughput is measured and billed in Request Units (RU) per second. The minimum manually provisioned throughput level is 400 RU/sec. There are three throughput provisioning offers: Manual , where a static throughput level is provisioned. This is best for highly predictable workloads. Autoscaling , where Azure will automatically scale throughput based on usage, reducing it down to a minimum of 10% of the provisioned throughput. Serverless , where you pay for only the RUs you need. This throughput provisioning model is ideal for small demonstration projects. This feature is forthcoming. The cost of using a CosmosDB database can be approximated using the Capacity calculator . In general, these are reasonable back-of-hand estimates for common operations to estimate costs: Read item: 1 RU SQL query: ~2.8 RU Create item: 10 RU There are various choices of API for Cosmos DB accounts which affect the data model used for databases. SQL API is the Core API, and works off JSON documents and SQL query syntax MongoDB API uses BSON documents (binary encoded JSON) and MongoDB query syntax Table API and uses Key-Value database design reflects API of Azure Table Storage Gremlin API is a graph database using a flat store of vertices and edges Cassandra API is columnar, and unlike most NoSQL databases does specify a schema Consistency , in a distributed NoSQL database like Cosmos DB, describes the uniformity of data across replicas. Consistency levels describe how and when data is replicated to provide varying consistency guarantees. Strong consistency is the strongest consistency model and requires synchronous replication after every change to database, increasing latency for each write. Session consistency is unique, in that it offers consistent prefix to databases that support a single session or an application with a single token. Eventual consistency is the weakest consistency model and provides no ordering guarantees. Consistent prefix offers read throughput, availability, and write latency comparable to eventual consistency while guaranteeing global order. Bounded staleness implies asynchronous replication and offers guarantees on the number of versions ( K ) or time interval ( T ) reads lag behind writes, referred to as the staleness window . As the staleness window approaches, Azure will delay writes by providing back pressure on writes. Outside the staleness window, data is guaranteed to be globally consistent. Outside the region in which the writes were made, Azure guarantees total global order or consistent prefix , which means, the global order is maintained. Strong Session Eventual Bounded staleness Horizontal partitioning is what allows Cosmos DB to scale-out massively to provide high availability and elasticity. Partitions can be thought of as physical fixed-capacity data buckets that back every container. A partition split occurs when a new physical partition is brought online, resulting in half of the documents existing on a previously existing partition being moved to the new one. Cosmos DB automatically and transparently splits horizontal partitions to achieve elasticity. Logical partitions , determined by the partition key which is set at container creation, group individual documents in ways that are kept on the same physical partition. It is recommended to have a high number of logical partitions, so that CosmosDB has greater flexibility partitioning documents. The partition key is immutable, so the correct choice of partition key is an important architectural consideration. Even distribution of documents is ideal to avoid hot partitions , where some partitions have much greater activity than others, due to uneven distribution of documents. Any partition may not be greater than 20 GB in size. Physical partitions have 4 replicas within a region. There are several common partitioning patterns: Partitioning on /id , which results in every document existing in its own logical partition. This pattern is write-optimized and ideal for IoT applications. Any SQL query for more than one document would be cross-partition by necessity, so direct reads using the /id value would be far more economical. Partitioning small lookup lists on a /type property. This will keep lists of related items used for lookups in the same partition. Optimizing for queries by organizing multiple types of document according to a key data-point. For example, customer data could be kept in the same partition as that customer's orders, avoiding cross-partition queries. Sources: Learning Azure Cosmos DB Cost Management Contributor Azure built-in role that grants access to the Cost Management blade. Cost Management Reader Azure built-in role that grants access to the Cost Management blade. Data Box Azure Data Box is a Microsoft-provided appliance that allows for the transfer of large volumes of data to Azure. These services are only available for EA, CSP, and Microsoft Partner Network Sponsorship offer types. Offering Capacity Storage saccounts Data Box Disk 35 TB 1 Data Box 100 TB 10 Data Box Heavy 1,000 TB 10 Workflow Order: Use Portal to order a data box by creating a Data Box resource Receive: Connect Data Box to network Copy data: Mount file shares and copy data to the device. Return: to Microsoft Upload: Microsoft will upload the data and securely erase it from the device Dataflow Cloud Dataflow is a GCP streaming data framework for defining both batch and stream processing pipelines. Resources What is Dataflow? Dataprep Dataprep is a GCP managed service that allows analysts to visually explore, clean, and prepare data for later analysis. Resources: Dataprep: Qwik Start - Qwiklabs Preview Dataproc Dataproc is a GCP service that manages the creation of data science clusters and data analysis jobs. Resources: Dataproc in a minute DynamoDB DynamoDB is a NoSQL database known for fast (1-9 ms) query times. DynamoDB measures capacity in Read Capacity Units (RCU) and Write Capacity Units (WCU) . 1 RCU = 1 record at most 4 KB in size 1 WCU = 1 record at most 1 KB in size DynamoDB offers the choice between strongly consistent and eventually consistent (half the cost) reads. DynamoDB offers two types of indexes: Global Secondary Index allows you to create a completely new aggregation of data. GSI updates are eventually consistent , with asynchronous updates populated after an update response is passed to the client. Local Secondary Index (LSI) alternate sort key attribute that allows only sorting DynamoDB Streams (changelog for the DynamoDB table) interfaces with AWS Lambda to implement complex queries , computed values like sum, average, maximum, etc. These are implemented in a different processing space than the DynamoDB table itself, so that it does not affect the table. AWS Lambda has an invocation role which defines what Lambda can see (triggered upon a change to the table as reported in DynamoDB Streams) and an execution role which defines what it can do . ECS EKS Elastic Beanstalk AWS Elastic Beanstalk is a PaaS offering. Elastic File System Elastic File System (EFS) is a scalable file system for AWS Linux instances. Multiple instances can be attached to a single EFS volume to share files. EFS volumes are highly available, spanning multiple Availability Zones in a single VPC. Elastic MapReduce Elastic MapReduce (EMR) is AWS's managed big data analysis service, supporting Apache Hadoop, Apache Spark, HBase, Presto, and Flink. Enterprise Agreement Azure customers on an Enterprise Agreement can add up-front commitments to Azure then be billed annually. If the committed spend is exceeded, the overage is billed at the same EA rate. EA customers can create spending quotas and set notification thresholds through the EA Portal. 3 portals used to manage Azure subscriptions EA Portal (ea.azure.com) available only to customers with an Enterprise Agreement Account Portal Azure Portal, includes Azure Cost Management ExpressRoute There are four main architectures used with ExpressRoute Any-to-any connection is used to integrate on-premises WANs using IPVPN. Co-location with cloud exchange is used to order virtual cross-connections to the Azure cloud through the co-location provider's Ethernet exchange. Point-to-point Ethernet connection is used to configure on-premises data center connectivity to Azure through individual point-to-point links Firebase Firebase is a completely unstructured NoSQL database that is known for its client libraries. Firebase Auth offers a free user interface for applications, Firebase UI . Firestore Cloud Firestore was released from beta in early 2019 and combines and improves upon functionality of previous products named Cloud Datastore and Firebase Realtime Database . Firestore is organized into documents , which consist of key-value pairs and are similar to JSON objects, and collections . JSON-like objects are called maps and keys are called fields in Firestore. Collections can contain only documents, but documents can contain sub-collections. Root can only contain collections. So navigating deeper and deeper into the information store will involve alternating between collections and documents. Firestore features a compatibility mode that emulates the behavior of Datastore in accessing Firestore's storage layer while removing some of Datastore's limitations. Queries in Firestore can only be used to find documents stored in one specific collection or sub-collection. However a collection group query , meaning one that spans multiple collections, began to be supported in 2019. Complex relational queries are not possible (in a single query), and query results are usually returned based on equality or greater-than/less-than comparisons. The field has to be specified as having a scope of \"Collection group\" within GCP, and there is a limit of (about) 200 for these queries. An index is created for every field in every document added to a collection, which results in very fast query times that are proportional to the number of results , not records searched. This structure ensures that equality searches are highly performant, as are comparison searches using greater-than or less-than. But this implementation creates bizarre limitations to Firestore's querying capabilities: There is no native way to perform wildcard searches or OR queries. For common instances of such queries, Google recommends adding a field that contains the value for each record Inequality searches present a challenge for Firestore. For some queries that combine conditions on more than one field (i.e. restaurants within a certain range of a location), Firebase will create a \"composite index\" (only within the index, the document itself is not affected) automatically to facilitate searches on those fields. Unlike Firebase , which charges based on the volume of data stored, Firestore charges based on number of operations performed and records returned. Sources: What is a NoSQL Database? Introducing Firebase Realtime database Firebase web application tutorial Firebase Realtime DB vs Firestore How queries work in Firestore Cloud Firestore vs the Realtime Database: Which one do I use? Firebase blog Folder Admin Predefined GCP role that allows folders to be created at an Organization. Front Door Azure Front Door works like Azure Load Balancer for web apps. Resources Pricing gcloud app ? compute ? container ? functions ? run ? services ? source Google Cloud Source Control repositories gcloud source repos clone gcloud source repos create Sources: Google Command Line for beginners Glacier S3 Glacier offers long-term archival at low cost. One or more files are stored in an archive, typically a .zip or .tar file containing multiple files. Archives can range from 1 B to 40 TB in size. Archives are stored in a Glacier vault , a region-specific container analogous to S3 buckets. Vaults must have regionally unique names, but there is no need for a globally unique name. Glacier vaults can be created and deleted using the Glacier service console. But uploading, downloading, or deleting archives must be done through the AWS CLI or an application using the SDK. Some third-party applications can also interact with Glacier. Google Cloud Identity Google's Identity as a Service (IDaaS) provider. Import/Export Service Azure Import/Export service allows the physical shipment of disks procured by the user to Azure for import. This data can be placed into blob or file storage. This service requires the use of a Windows computer with BitLocker and .NET Framework and is dependent on the WAImportExport.exe utility. Procure 2.5-inch or 3.5-inch SATA (not SAS) disks Connect the disks to a Windows machine. Create a volume and encrypt it using BitLocker Install the Azure Import/Export tool (WAImportExport.exe) on the disks. Copy files Create an import job in the Azure Portal Kinesis Kinesis is an AWS service for ingestion and processing of streaming data, such as access logs, video, audio, and telemetry. Kubeflow Kubeflow is a cloud-native platform for machine learning based on Google\u2019s internal machine learning pipelines. Resources: Kubeflow 101 (playlist) Talk - Kubeflow at Spotify Talk - Kubeflow on Kubernetes Kusto Kusto is a case-sensitive query language developed by Microsoft and used in several Azure services: Azure Data Explorer Log Analytics Sentinel Application Insights Microsoft Defender ATP Lightsail Amazon Lightsail offers blueprints that will automatically provision all compute, storage, database, and network resources needed for a deployment. Macie Macie is an AWS service that automatically finds and classifies sensitive data stored in AWS using machine learning to recognize sensitive data such as PII or trade secrets. MARS The Microsoft Azure Recovery Services (MARS) agent is for backing up Windows machines only, but can be installed on VMs on other cloud providers like AWS. MARS can be configured to protect the entire system, volumes, or individual files and folders. Monitor Resources: Dashboards with Azure Monitor Data Neptune Neptune is an AWS graph database. NPM Network Performance Monitor is a Log Analytics network monitoring solution for hybrid networks, providing 3 services: Performance Monitor monitors connectivity between various points in both Azure and on-prem networks Service Connectivity Monitor monitors outbound connectivity from network nodes to external TCP services, monitoring performance metrics like latency, response time, and packet loss ExpressRoute monitors end-to-end connectivity between on-prem network and Azure over ExpressRoute Network Watcher Network Watcher appears like a normal resource in a resource group, but it is deployed as a single instance per Azure region. Network Watcher monitoring and diagnostic tools: IP Flow Verify Next Hop Packet Captures link a Network Watcher resource, a target VM, a storage account, and a filter that specifies the characteristics of network traffic (source and destination IP addresses and ports as well as protocol) to capture, as well as a time limit. Network Topology OpsWorks OpsWorks is AWS's declarative configuration management service that uses the Chef and Puppet configuration management platforms and comes in three varieties: OpsWorks for Puppet Enterprise OpsWorks for Chef Automate OpsWorks Stacks Project A project is the direct parent of all other GCP resources, and always consists of a project name, project ID, and project number. Project Creator Predefined GCP role given to all users currently assigned to a project. Pub/Sub Cloud Pub/Sub is GCP's messaging service, allowing services and applications to communicate. Resources: Cloud Pub/Sub Overview - ep. 1 Recovery Services Vault A Recovery Services Vault is an Azure resource used to centrally manage the backup and recovery of Azure resources, and the centerpiece of any backup strategy. A Backup protection policy defines how a backup plan is implemented. These are most easily created through the Portal. A vault can only back up data from other resources that exist in its region. Rekognition AWS deep learning-based image recognition service. Resource Policy Contributor Azure built-in role that includes access to most Policy operations and should be considered privileged. Route 53 Route 53 is AWS's managed DNS service. Like any other DNS system, it relies on resource records defined in a zone . Route 53 can also provide name resolution for private domain names , used on private networks. Private hosted zones provide DNS resolution for a single domain name within multiple VPCs. But when a resource record must be changed dynamically to work around failures or route users to an underutilized server, routing policies can be used. Simple policy is the default for new resource records and maps a domain name to a single value (i.e. an IP address). Weighted policy distributes traffic across multiple resources according to a predefined ratio. Latency policy sends users to resources in their closest Region. Failover policy allows a secondary resource to be marked for routing when the primary resource is unavailable. Geolocation policy routes users based on their specific continent, country, or state. Multivalue answer policy allows even distribution of traffic across multiple resources by randomizing the order of returned records. All routing policies except Simple can use health checks to modulate routing action. All health checks occur every 10 or 30 seconds and can check one of three resources: Endpoint makes a test connection to a TCP port CloudWatch alarm can be set off in case of high latency or other metrics. Calculated monitors the status of other health checks. Route 53 also offers the Route 53 Traffic Flow visual editor that allows you to create a diagram to represent the desired routing. The diagram isn't translated to individual resource records but rather represents a single policy record which costs 50 USD/month each. In addition to the routing policies above, Traffic Flow also offers the Geoproximity routing policy that directs users to a geographic location based on how close they are. S3 Simple Storage Service (S3) is Amazon's data storage service. S3 stores objects in a container called a bucket . Each bucket must have a globally unique name and exposes a HTTP endpoint (at https://$BUCKET.s3.amazonaws.com/) Each object is associated with a key . Keys are equivalent to filenames, and the bucket is equivalent to a flat filesystem. However, directories can be simulated by placing slashes in the key. Bucket policies (applied to buckets) and user policies (applied to IAM principals) can be used to modulate accessibility. Public or anonymous access to an object can only be granted by bucket policies. Bucket and object ACLs are legacy access control methods that are still usable. S3 buckets store data unencrypted, although encryption at rest is available in two options: Server-side encryption: S3 encrypts uploaded objects before storing them, and decrypts it again before delivery. Client-side encryption: User must encrypt data prior to uploading and decrypt it after downloading. S3 offers various storage classes that differ in their availability and durability , the percent likelihood that an object within it will not be lost over the course of a year. Frequently accessed objects: STANDARD REDUCED_REDUNDANCY Infrequently accessed objects STANDARD_IA ONEZONE_IA GLACIER INTELLIGENT_TIERING automatically moves objects to the most cost-effective storage tier based on access patterns. Storage Gateway is an on-premises VM that provides a connection to S3 for on-premises infrastructure. File gateway lets you use NFS and SMB file shares to transfer data to S3. Data is stored in S3 and cached locally. Volume gateway can be used as an iSCSI target by on-premises servers. Two configuration variants exist: Stored volumes : All data is stored locally and asynchronously backed up to S3 as EBS snapshots. A stored volume can range from 1 GB to 16 TB in size. Cached volumes : Data is stored in S3 and frequently used data is cached locally. A cached volume can range from 1 GB to 32 TB in size. Tape gateway is configured as an iSCSI target by a backup application. Virtual tapes range from 100 GB to 2.5 TB in size. These tapes are asynchronously transferred to a virtual tape library (VTL) backed by S3 and removed when the upload is complete. Recovery requires downloading the virtual tape again. Cloud Storage in Minutes with AWS Storage Gateway Sentinel Azure Sentinel is a cloud-native SIEM and SOAR soluation that can collect data from many sources and present it to security analysts, who can run Kusto queries against the dataset. Azure Sentinel can ingest data from on-premises devices using one of several types of connector , categorized by the type of data ingestion: Native connectors integrate directly with other Microsoft security products, like Azure AD, M365, and Azure Security Center Direct connectors are configured from their source location, such as AWS CloudTrail, Azure Firewall, and Azure Front Door API connectors are implemented by security providers, like Azure Information Protection (AIP), Barracuda Web Application Firewall (WAF), and Microsoft WAF Agent-based connectors , using the Log Analytics agent, make it possible to ingest data from any source that can stream logs in Common Event Format (CEF), such as Windows and Linux machines. Analytic rules are rules that users create to help detect threats and anomalies in an environment: Scheduled rules run on a predetermined schedule Microsoft Security Machine learning behavior analytic rules can (currently) only be created from templates provided by Microsoft using proprietary ML algorithms Simple Queue Service Simple Queue Service (SQS) is an AWS service that can broker messages between components of highly decoupled applications. Snowball AWS Snowball is a physical appliance designed to move large amounts of data to the cloud. The largest Snowball device can store 72 TB of information. Snowball Edge refers to a family of options similar to Snowball but with compute power to run EC2 instances and Lambda functions locally. All Snowball Edge options feature a QSFP+ network interface that is capable of speeds up to 100 Gbps. Snowball Edge devices can also be clustered together. Storage Optimized provides up to 80 TB of storage and 24 vCPUs. Compute Optimized provides up to 39.5 TB of storage and 52 vCPUs. Compute Optimized with GPU is similar to Compute Optimized but includes an NVIDIA GPU, making it useful for ML and HPC applications. Spanner GCP managed scaleable database service. Stackdriver GCP service that collects metrics, logs, and event data from applications and infrastructure and integrates the data so DevOps engineers can monitor, assess, and diagnose operational problems. Super administrator Unique GCP role associated with the root Organization which has powers that exceed that of other administrative users. Storage accounts Azure storage accounts are managed through [Azure Resource Manager][ARM] and management operations are authenticated and authorized using [Azure Active Directory][Azure AD]. There are four services provided within each storage account: Blobs provides a highly scalable service for storing arbitrary data objects, such as text or binary data. There can be multiple containers within a storage account, and a container can have its own folder structure. There are three types of blob: page , block , and append blobs. Tables provides a NoSQL-style store for storing structured data. Tables in Azure storage do not require a fixed schema, thus different entries in the same table can have different fields Queues provides reliable message queueing between application components Files provides managed file shares that can be used by VMs or on-premises servers Options that must be selected when creating a storage account: Performance tier Standard supports all storage services and uses magnetic disks to provide cost-efficient and reliable storage Premium only supports page blobs with the locally-redundant (LRS) replication option, uses high-performance SSD disks Account kind General-purpose V2: only kind to support ZRS General-purpose V1: does not support various access tiers. Blob storage: specialized storage account used to store block and append blobs Replication mode : Storage accounts can be freely moved between the following replication modes, except ZRS, in which case it is recommended to copy data to a new account. Locally-redundant storage (LRS): makes 3 local sychronous within the same Azure facility (zone) Zone-redundant storage (ZRS): makes 3 synchronous copies across multiple availability zones; available for general-purpose v2 storage accounts at Standard performance tier only. Geographically-redundant storage (GRS): makes 3 local synchronous copies plus 3 additional asynchronous copies (typically within 15 minutes, but no SLA) to a second data center far away from the primary region Read-access geographically redundant storage (RA-GRS): makes 3 local synchronous copies plus 3 additional asynchronous copies to a second data center far away from the primary region, which has only read-only access Access tier : Both Blob and StorageV1 can be upgraded to StorageV2, a process which is irreversible. Hot blob storage access tier optimized for the frequent access of objects in the storage account Cool blob storage access tier optimized for storing large amounts of data that is infrequently accessed and stored for at least 30 days Archive blob storage access tier designed for long-term storage of infrequently-used data that can tolerate several hours of retrieval latency, remaining in the Archive tier for at least 180 days. It is stored offline and can take up to 15 hours for it to be \"rehydrated\" to the Cool or Hot tier before it can be accessed. Premium providing high-performance access for frequently-used data on SSD, only available from the Block Blob storage account type. Every storage account service exposes its own Internet-facing endpoint, which must be secured in one of several ways. A firewall can be implemented by using network rules to limit traffic to particular networks. The storage firewall controls IP addresses and VNets can access the storage account and applies to all storage account services. Access can be restricted to specific VNets by creating a Virtual Network Service Endpoint , however this still uses the public IP address. Private Link allows similar functionality using private IPs. MS Docs Public access to blobs can be restricted at the container level on container creation. By default, no public read access is enabled for anonymous users, but users with RBAC rights or with the storage account name and key can have access. This can be done through ARM APIs, the Portal, or Azure Storage Explorer. Container access levels: No public read access: container and blobs can only be accessed by storage account owner (default for new containers) Public read-only access for blobs only (container data is not available, and anonymous clients cannot enumerate the blobs within the container) Full public read-only access: all container and blob data can be read by anonymous requests: Access can also be switched between Shared Key -based authentication (relying on storage account keys) and Azure AD authentication , where a RBAC role determines access to a Container. Authorize access to blobs and queues using Azure Active Directory Access keys grant full access to all data in all services of a storage account and represent the simplest and most powerful control over access. Access keys are typically used by applications for access to Azure storage, either through a Shared Access Signature (SAS) token or directly accessing the storage itself with the name and key. Storage account keys were implemented early in the history of Azure and grant full access to the entire storage account. However, it is considered an anti-pattern to distribute this key; a SAS token should be generated for every stored item to be distributed. Because storage account keys provide write access, a storage account with a ReadOnly resource lock will not enumerate its storage account keys, and users with Read permission will not be able to retrieve the keys either. Systems Manager Systems Manager is an AWS service for imperative configuration management. Systems Manager relies on several types of script: Command documents use normal shell commands and can be run periodically or on a trigger, so long as the instance to be managed has the required agent. Automation documents allow administration of AWS resources, similar in effect to using the Management Console or the AWS CLI . Trusted Advisor Trusted Advisor is an AWS service allows a visual check of resource configurations to ensure compliance with best practices, and is available only to Business and Enterprise Support plans . It offers several categories ( src ) Cost optimization Performance Security Fault tolerance Service limits User Access Administrator Azure built-in role that grants the permissions necessary to assign a user administrative access at the subscription scope. Permissions: Microsoft.Authorization/roleAssignments/write Microsoft.Authorization/roleAssignments/delete User Administrator Azure built-in role that grants the power to manage all aspects of users and groups, including resetting passwords for limited admins. VM Agent Microsoft Azure Virtual Machine Agent (VM Agent) manages VM interaction with the Azure Fabric Controller and comes preinstalled with Windows images from the Marketplace. It can also be installed on a custom image. VM Agent supports the VMSnapshot extension, which is added when backups are enabled. This extension takes a snapshot of the storage at the block level and sends it to the RSV configured. For Windows VMs, this extension leverable the Volume Shadow Copy service. VMSnapshot Volume Shadow Copy waagent Microsoft Azure Linux Agent (waagent) manage VM interaction with the Azure Fabric Controller on Linux VMs. WAImportExport WAImportExport.exe is a CLI tool associated with Azure Import/Export service . It requires a Windows computer with .NET Framework and BitLocker. There are two versions: Version 1 is recommended for blob storage Version 2 is recommended for files storage . Check disks required for selected blobs WAImportExport.exe PreviewExport /sn:<Storage account name> /sk:<Storage account key> /ExportBlobListFile:<Path to XML blob list file> /DriveSize:<Size of drives used> Various flags in WAImportExport.exe allow an XML-format \"blob list\" file to be used to specify files, or as output. All Export all blobs in the storage account <?xml version=\"1.0\" encoding=\"utf-8\"?> <BlobList> <BlobPath> / </BlobPath> </BlobList> Root Export all blobs in the root container <?xml version=\"1.0\" encoding=\"utf-8\"?> <BlobList> <BlobPath> /$root </BlobPath> </BlobList> Blob in root Export blob \"logo.bmp\" in the root container <?xml version=\"1.0\" encoding=\"utf-8\"?> <BlobList> <BlobPath> $root/logo.bmp </BlobPath> </BlobList> Containers Export all blobs in container \"music\" <?xml version=\"1.0\" encoding=\"utf-8\"?> <BlobList> <BlobPath> /music/ </BlobPath> </BlobList> Pattern Export all blobs in any container that begins with \"book\" <?xml version=\"1.0\" encoding=\"utf-8\"?> <BlobList> <BlobPath> /book </BlobPath> </BlobList> Pattern in container Export all blobs in container \"music\" that begin with prefix \"love\" <?xml version=\"1.0\" encoding=\"utf-8\"?> <BlobList> <BlobPath> /music/love </BlobPath> </BlobList> WebJobs WebJobs is a feature of Azure App Service that enables you to run a program or script in the same instance as a web app, API app, or mobile app, at no additional cost and supported only on Windows App Service plans . There are two types: Continuous webjobs default to running on all instances of the linked web app (although it can be configured to run on only one) Triggered webjobs run only when triggered or on a schedule and on only a single instance of the linked web app selected by Azure.","title":"\u2601&#xfe0f; Cloud"},{"location":"Cloud/#cloud","text":"Compute IaaS Azure VMs EC2 Compute Engine PaaS App Service Elastic Beanstalk App Engine Serverless Functions Lambda Cloud Functions Cloud Run Containers Individual containers ACI ECS Kubernetes AKS EKS GKE Container registry Artifact Registry Storage Archive Glacier Backups Recovery Services Vault Backup Physical media Data Box Import/Export Service Snowball Transfer Appliance Network Private networks VNets VPC VPC Security rules Network Security Group (NSG) Security Group Firewall Rules DNS Azure DNS Route 53 Cloud DNS Development NoSQL Cosmos DB DynamoDB DocumentDB Firestore Spanner CI/CD Azure Devops CodeBuild CodeCommit CodeDeploy CodePipeline Cloud Build Messaging SNS Pub/Sub Computer Vision Computer Vision Rekognition Cloud Vision Big Data Big Data Data Lake Store Redshift Athena BigQuery BigTable Dataprep Streaming data Event Hubs Service Bus Stream Analytics Kinesis Athena DataFlow Batch processing HDInsight Batch EMR Batch DataFlow Dataproc Links TODO: Cloud Storage CloudWatch Azure Functions Glacier Google Cloud Storage (GCS) gcloud gsutil Simple Notification Service","title":"\u2601&#xfe0f; Cloud"},{"location":"Cloud/#administration","text":"","title":"\ud83d\udee0&#xfe0f; Administration"},{"location":"Cloud/#cost-management","text":"Azure quotas apply to subscriptions and are implemented with tags . Resource quotas trigger alarms when resource creation and consumption hit a threshold. These are not to be confused with resource limits which can stop resources from being created, whereas quotas can not. Spending quotas trigger alarms when spending has reached a threshold. Azure budgets can be viewed and administered in the Cost Management + Billing blade. Users must have at least the Reader role at the subscription scope to view, and Contributor to create and manage, budgets.","title":"\ud83d\udcb0 Cost management"},{"location":"Cloud/#locks","text":"Azure resource locks are used to apply restrictions across all users and roles and can be applied at subscription, resource group, or resource scopes. CanNotDelete ReadOnly effectively restricts all authorized users to the permissions granted by the Reader role Storage account keys of a locked storage account cannot be listed because the list keys operation is handled through a POST request Visual Studio Server Explorer will not be able to display files for a locked App Service resource, because that interaction requires write access VMs in a locked resource group will not be able to be started or restarted, because those operations require a POST request All child resources of the scope at which a lock is applied inherit the lock. A CanNotDelete lock applied to a DNS A record would also prevent the deletion of the DNS zone that the record resides in, as well as the resource group the zone resides in. Of the builtin roles , only two have access to the Microsoft.Authorization/* or Microsoft.Authorization/locks/* actions required to create or delete locks: Owner User Access Administrator Resource locks apply to the management plane of Azure, specifically operations sent to https://management.azure.com Managed applications create two resource groups to implement locks: One resource group to contain an overview of the service, which isn't locked Another resource group containing the infrastructure for the service, which is locked Sources: Move resources to a new resource group or subscription Some services have limitations or requirements when moving resources between groups ( src ) Source and destination subscriptions must be within the same [AAD][Azure AD] tenant Destination subscription must be registered for the resource provider of the resource being moved Account moving the resources must have at least the following permissions: Microsoft.Resources/subscriptions/resourceGroups/moveResources/action Microsoft.Resources/subscriptions/resourceGroups/write","title":"\ud83d\udd12 Locks"},{"location":"Cloud/#iam","text":"All cloud providers offer Identity and Access Management (IAM) systems that are used to control access to resources. Role-based access control (RBAC) is the universally recommended and modern approach, relying on bundles of specific permissions called roles that can be assigned to principals. All cloud providers offer the ability to define custom roles and come with many ready-to-use role definitions (\" predefined roles \" in GCP or \" built-in roles \" in Azure). Cloud providers also still support legacy IAM systems which are deprecated. Azure classic administrator roles include \"Account Administrator\", \"Service Administrator\" and \"Co-Administrator\" GCP primitive roles \"owner\", \"editor\", and \"viewer\" can still be applied to most GCP resources. Important roles Owner has full access to all resources and can delegate access. Service Administrator and Co-Administrators are assigned this role at the subscription scope. Contributor can create and manage all resources (full read/write privileges), but cannot delegate access. Reader can view resources. Cost Management Contributor Cost Management Reader Resource Policy Contributor User Administrator User Access Administrator Billing Account Administrator Billing Account Creator Billing Account User Billing Account Viewer Compute Engine Admin Compute Engine Network Admin Compute Engine Security Admin Compute Engine Viewer Compute Service Agent Folder Admin Project Creator Shared VPC Admin","title":"IAM"},{"location":"Cloud/#infrastructure","text":"All cloud providers divide their global services into a hierarchy of geographically defined regions , each of which is in turn divided into availability zones (what AWS calls its Global Infrastructure). Azure datacenters contain multiple availability zones, and every Azure region has at least three availability zones. Azure services are also divided into geographies , generally coterminous with countries. Azure geographies are further divided into regional pairs . Each regional pair receives rolling updates one member at a time. Most services are regionally based, meaning the underlying hardware of that service's instance will exist in only a single Region. Some regions, like AWS GovCloud , have restricted access. Some AWS resources, however, are technically running on hardware that exists in a single Region, but presented as global. Resources Services available on Free Tier","title":"Infrastructure"},{"location":"Cloud/#monitoring","text":"Azure Monitor Network Watcher CloudWatch Stackdriver Trace","title":"\ud83d\udc41&#xfe0f; Monitoring"},{"location":"Cloud/#resources","text":"Cloud providers exhibit some variety in how resources can be organized. All cloud providers support key-value tags , many of which can be applied to the same resource. Any Azure resource can only exist in a single resource group , which can contain resources from any region or subscription. However, resource groups may not contain other resource groups. GCP projects are equivalent to Azure resource groups, in that they are containers for and direct parents to resources. However, projects can be placed within folders , which do support nested hierarchies. AWS does not have an equivalent method of organizing resources. Azure subscriptions can be organized into Management Groups , and they can be nested in a hierarchy of management groups up to a maximum depth of six levels. In AWS the Organizational Unit (OU) , which can organize user accounts (subscriptions) and the resources they contain in a nested hierarchy, appears to be equivalent. A pattern common to Azure is that of a service being implemented in two resource types, one of which determines important configuration settings shared by all instances of the service which are contained within it. This is the case for storage accounts , App Service , Azure Data Explorer clusters, etc. Description Tenant Organization Organization Corresponds to a company or organization Management group Organizational Unit Logical container for user accounts and the resources created by that user Subscription Member account ? Credential associated with an individual Folder Organize resources and their parents in a nested hierarchy Resource group Project Logical container that is the direct parent to any resource, tied to a Region Tag Tag Label Key-value pairs that are used to organize resources The resource hierarchy organizes GCP resources in 3 levels below Domain Domain Organization corresponds to a company or organization. A single cloud identity is associated with a single organization and can have super admins Billing Account tracks charges and billing account admins can set budgets. Payments Profile is a Google-level resource that is used to pay for all Google services.","title":"Resources"},{"location":"Cloud/#support","text":"AWS offers various support plan tiers that provide 24/7 email, chat, and phone access to AWS cloud support engineers. Basic Support Plan Developer Support Plan (greater of $29 or 3% of monthly account usage) Business Support Plan Enterprise Support Plan (>$15,000/mo.) offers a Technical Account Manager (TAM) , a dedicated guide and advocate AWS documentation is available in several places: AWS documentation AWS Knowledge Center is a sprawling FAQ AWS security resources AWS forums Professional Services team makes white papers and webinars publicly available","title":"\ud83d\udee0&#xfe0f; Support"},{"location":"Cloud/#tags","text":"Azure tags: Tag names have a limit of 512 characters (128 characters for storage accounts) Tag values have a limit of 256 characters. Resources and resource groups are limited to 15 tags. VMs cannot exceed 2048 characters for all tag names and values combined.","title":"Tags"},{"location":"Cloud/#infrastructure-as-code","text":"All cloud providers support ways of provisioning resources declaratively. Azure [ARM][ARM] templates are JSON, but [Bicep][Bicep] is a domain-specific language and command-line utility that can be used to generate templates from simpler, YAML-like syntax.","title":"Infrastructure as Code"},{"location":"Cloud/#compute","text":"","title":"\ud83d\udda5&#xfe0f; Compute"},{"location":"Cloud/#iaas","text":"All cloud providers offer Infrastructure as a Service (IaaS) , whereby virtual machines can be provisioned with specific compute resources and base operating systems. AWS also offers configuration management services like [OpsWorks][OpsWorks] and [Systems Manager][Systems Manager] GCP virtual machines are referred to as instances , and are available in three general machine family types: general-purpose, memory-optimized, and compute-optimized. Machine type describes the different packaged configurations representing allocated compute resources, or what is called a SKU in Azure.","title":"IaaS"},{"location":"Cloud/#containers","text":"Build and package container artifacts Private container registry","title":"Containers"},{"location":"Cloud/#serverless","text":"","title":"Serverless"},{"location":"Cloud/#storage","text":"","title":"\ud83d\udcbe Storage"},{"location":"Cloud/#archive","text":"","title":"Archive"},{"location":"Cloud/#backups","text":"Azure Backup are integrated into Portal and clickable from the VM blade. You have to specify a Recovery Services vault and a Backup policy . The policy can specify frequency of backups, and other settings. Using Backup service costs $10 per VM plus the cost of used storage. 2 methods to restore data after backing up a VM to Azure Backup: Restore a recovery point as a new VM Restore access to files only","title":"Backups"},{"location":"Cloud/#physical-media","text":"Data Box Import/Export Service Snowball Transfer Appliance Uploading files to GCS","title":"Physical media"},{"location":"Cloud/#networking","text":"All cloud providers offer an implementation of software-defined networking (SDN) that allows a logically isolated network to be defined as a block of IP addresses allocated from one of the private ranges (10.0.0.0/8, 192.168.0.0/16, or 172.16.0.0/12). In AWS and GCP, this network is referred to as a Virtual Private Cloud (VPC) , whereas in Azure it is called a Virtual Network (VNet) . In all providers, the network is confined to a single region and must have at least one IP segment called a subnet defined within it which must be a subset of the range used to define the virtual network itself. The smallest possible CIDR range for a subnet in Azure is 29, which provides 3 addresses for use (Azure reserves 5). In AWS, the smallest possible CIDR range is 28. In AWS, VPCs have a default range of 172.31.0.0/16 and subnets have a default subnet mask of /20. In Azure, subnets span Availability Zones, can only be deleted if empty, and their names, which are immutable, must be unique. In AWS, a subnet exists only within a single Availability Zone. VNet peering allows VMs in two separate virtual networks to communicate directly. In all cloud providers, this is a one-way process which must be repeated in both directions in order to have two-way communication. In Azure, before the introduction of peering, virtual networks were connected using S2S VPN or by connecting to the same ExpressRoute circuit. It is not required for the peered networks to be in the same region ( Global VNet peering ), subscription, or tenant, although cross-tenant peering is not available in the Portal but must be configured from the command-line or ARM templates. VNet peering has to be disabled before moving a VNet, and a VNet can only be moved within the same subscription. There is a maximum of 100 peering connections per VNet Peerings cannot be moved to another resource group or subscription, so they must be disabled before moving peered VNets. Service endpoints facilitate restricting traffic from Azure services. Service endpoint policies allow restricting traffic to the granularity of individual Azure service instances. An internet gateway is a VPC resource that allows EC2 instances to obtain a public IP address and access the Internet. In order to access the Internet, instances must be in a public subnet , one that contains a default route to the VPC's internet gateway. ExpressRoute is the main service used to connect Azure to on-premises networks, although P2S and S2S VPNs are also options. Direct Connect provides dedicated network connectivity to an AWS VPC through links offered through APN partners. In GCP, in addition to peering, a shared VPC can be created that is associated with multiple projects. Resources: Migrating to GCP? First Things First: VPCs","title":"\ud83c\udfe2 Networking"},{"location":"Cloud/#user-defined-routes","text":"In Azure, a virtual appliance refers to a VM running a network application like a load-balancer, firewall, or router. Service chaining refers to the process of deploying a network virtual appliance (NVA) into a hub network to route traffic between spokes using user-defined routes (UDR) . This is a method of reducing the complexity of pairing between individual spoke networks in complex hub-and-spoke architectures. AZ-103: 309 In such a deployment, the peerings must be set to Allow Forwarded Traffic . Alternatively, two peered networks can share a single virtual network gateway, say to connect to an external network. The pairing connection to the network that contains the gateway must be set to Use Remote Gateways The pairing connection from the network containing the gateway must be set to Allow Gateway Transit","title":"User-defined routes"},{"location":"Cloud/#network-security","text":"Network Security Group (NSG) Security Group Firewall Rules Azure Network Security Groups (NSGs) are assocated with network interfaces and contain an arbitrary number of security rules . Each rule has the following properties: Name Priority : number between 100 and 4096, lower numbers indicate a higher priority Source or destination : IP address, CIDR block, service tag, or application security group Protocol : TCP , UDP , ICMP , or Any Direction : Inbound or outbound Port range ; Action : allow or deny Service tags represent a group of IP address prefixes managed by Microsoft available for use in NSG rules: VirtualNetwork : all CIDR ranges defined for the virtual network, all connected on-premises address spaces, peered VNets or VNets connected to a VNET gateway AzureLoadBalancer : Virtual IP address of the host where Azure's health probes originate Internet : IP address space that is outside the virtual network AzureCloud* : IP address space for Azure, including all datacenter public IP addresses AzureTrafficManager* : IP address space for the Azure Traffic Manager probe IP addresses Storage : NSG flow logging ,which saves the 5-tuple of all packets, is available as a low-cost way to monitor traffic. Flow logs record all IP flows going in and out of an NSG and are collected per NSG rule. They are charged per GB of logs collected and include a free tier of 5 GB/month. In AWS VPCs, Security Groups are similar to firewall rules that regulate inbound and outbound traffic of an instance. Outbound traffic is unrestricted by default, and every VPC contains a default security group. A network access control lists (NACLs) , also like a firewall, contains inbound and outbound rules but operates on the subnet. By default, a NACL allows all inbound and outbound traffic. In GCP, each VPC has a set of firewall rules that control traffic not only into and out of the VPC, but between instances in the same VPC. Each rule can be tagged, and individual instances with the same tags inherit those rules. Resources: Protect your Google Cloud Instances with Firewall Rules","title":"Network security"},{"location":"Cloud/#dns","text":"Azure DNS Route 53 Cloud DNS","title":"DNS"},{"location":"Cloud/#cdn","text":"Users can use Azure CDN as a cache, reducing load from website. Content is cached by the CDN until its time-to-live (TTL) elapses, which can be controlled in the HTTP response from the origin server. Permanently removing content from the CDN requires it be first removed from the origin servers, meaning if the content is in a storage account it should be set to private or deleted from the storage, or the container itself should be deleted. Cached copies may remain in the CDN endpoint until the TTL has expired, unless it is purged . There are 4 pricing tiers available within Azure CDN: Azure CDN Standard from Microsoft does not offer dynamic site acceleration (DSA) (cf. Azure Front Door Service) Azure CDN Standard from Akamai Azure CDN Standard from Verizon Azure CDN Premium from Verizon , for which caching is configured using a rules engine. AWS CloudFront GCP CDN","title":"CDN"},{"location":"Cloud/#load-balancing","text":"Azure Load Balancer Application Gateway AWS Elastic Load Balancer GCP Load balancing","title":"Load-balancing"},{"location":"Cloud/#development","text":"","title":"\ud83d\udc68\u200d\ud83d\udcbb Development"},{"location":"Cloud/#nosql","text":"NoSQL databases differ from relational databases in that they do not obey the principle of data normalization . That is, the same data can be stored in more than one place. This is an advantage for databases that are optimized for reads as opposed to writes, because fewer queries are needed to retrieve information. However, when changing information that is duplicated in several places, write operations will be more laborious and prone to error. NoSQL databases are also horizontally scalable because the information can be sharded horizontally more easily than relational database, which are only vertically scalable (meaning scaling them requires larger and larger computers) and can only be sharded vertically. ( src )","title":"NoSQL"},{"location":"Cloud/#big-data","text":"","title":"Big Data"},{"location":"Cloud/#history","text":"Beginning in 2000, Amazon began developing Merchant.com, a planned e-commerce service that was intended to be the base upon which other enterprises would develop online shopping sites. At the time, Amazon's development environment was a jumbled mess, and in the effort to consolidate and organize the enterprise into a set of well-documented APIs. Despite these changes, software development remained sluggish, and an investigation discovered that individual teams were procuring storage, compute, and database resources independently. AWS originated out of the effort to consolidate these resources across the enterprise and remove this bottleneck. Azure was announced in 2008 and publicly released in 2010 after earlier experiments in cloud computing like Whitehorse and RedDog. In fact, references to the \"classic\" model predating the Azure Resource Manager (ARM) actually refer to RedDog: the \"classic\" portal was also known as \"RedDog Front-End\".","title":"History"},{"location":"Cloud/#glossary","text":"","title":"\ud83d\udcd8 Glossary"},{"location":"Cloud/#apigee","text":"The Apigee API platform is a management service that allows developers to deploy, monitor, and secure their APIs and generates API proxies.","title":"Apigee"},{"location":"Cloud/#apn","text":"Amazon Partner Network","title":"APN"},{"location":"Cloud/#app-engine","text":"App Engine allows developers to deploy applications developed in popular programming languages to a serverless environment. It is available in two environment types: Standard and Flexible. Standard environment is the original App Engine environment, consisting of a preconfigured, language-specific runtime like Java, Python, PHP, Node.js, or Go. Flexible environment is similar to GKE in that it can run a customized container. App Engine is designed to support applications implemented as a microservices architecture. There are four components: The application is the top-level container that houses all other components. Services are versioned and provide a specific function. Versions are produced every time a service is updated. Every version runs on an instance . Each version of a service runs on its own instance, whose size can be determined by specifying the instance class . Instances can be dynamic or resident. Resident instances run continually and can be added or removed manually. Dynamic instances support autoscaling based on load. App Engine has three modes of scaling: Automatic scaling creates an instance with a specified request rate, response latency, and application metrics. Basic scaling creates instances only when requests are received Manual scaling supports operational continuity regardless of load level.","title":"App Engine"},{"location":"Cloud/#app-service","text":"An App Service plan resource determines the billable compute resources available for the App Services applications managed by it. A plan acts as a container for multiple web applications sharing the same server farm (\"workers\"), and for this reason Windows and Linux apps can't be mixed in the same App Service plan. \"Web app\" is the legacy name for Azure App Service . App Service SSL certificates need to be deleted from each App Service before moving it to a new resource group. Resources \ud83d\udcb0 Pricing Tutorial: build and run a custom image in Azure App Service Create an App Service app with deployment from GitHub using Azure CLI Create a web app with continuous deployment from GitHub What is Azure Front Door? Create a Front Door - PowerShell","title":"App Service"},{"location":"Cloud/#application-gateway","text":"Azure Application Gateway is used to load balance a large-scale set using more than 100 instances in place of Azure Load Balancer . AZ-103: p. 223 Application Gateway supports session affinity to save user state using browser cookies. Unlike Azure Load Balancer, which operates at OSI layer 4 and has limited security capabilities, Application Gateway operates at OSI layer 7 and provides Web Application Firewall (WAF) functioanlity to block attacks like SQL injection, cross-site scripting, and header injection. HTTPS is also only available with layer 7 load balancers like Application Gateway.","title":"Application Gateway"},{"location":"Cloud/#athena","text":"Athena is a serverless AWS service that allows SQL queries to be run against data stored in a [S3][S3] bucket. Athena works closely with [AWS Glue][AWS Glue] to extract schema information and crawl data sources. Before running for the first time, you must provide a path to a S3 bucket to store query results. Sources: How to use SQL to query S3 files with AWS Athena","title":"Athena"},{"location":"Cloud/#aws-cli","text":"AWS CLI is version 1 is maintained for legacy compatibility purposes.","title":"AWS CLI"},{"location":"Cloud/#aws-developer-tools","text":"A collection of tools that provide CI services: CodeCommit CodeBuild CodeDeploy CodePipeline","title":"AWS Developer Tools"},{"location":"Cloud/#aws-glue","text":"","title":"AWS Glue"},{"location":"Cloud/#azcopy","text":"AzCopy can be used to copy files to File storage.","title":"AzCopy"},{"location":"Cloud/#azure-bastion","text":"Azure Bastion is a PaaS service deployed within a VNet that allows connectivity to a VM from the Portal. Once deployed in a VNet, RDP/SSH is available to all VMs in that VNet. This session is streamed to your local device over an HTMLS session using the browser. - It is not deployed per VM, but once per VNet to its own dedicated subnet , at least /27 or larger - No public IP is necessary on the VM, the connection from Bastion to the VM is to the private IP. However, the Bastion itself does require a public IP. - Bastion can now span peered VNets IPv6 support is limited in Azure. IPv6 addresses are not added to VMs by default and must be explicitly defined by adding an endpoint to each VM to be using it. Routing by IPv6 is also not supported, so load balancers have to be deployed. Sources Azure Virtual Network Overview Azure Bastion Introduction to flow logging for NSGs","title":"Azure Bastion"},{"location":"Cloud/#bicep","text":"Project Bicep is a domain-specific language and command-line utility that can be used to generate [ARM][ARM] templates. Project Bicep \u2013 Next generation ARM Templates","title":"Bicep "},{"location":"Cloud/#azure-container-instances","text":"Azure Container Instances (ACI) allows a simpler way of running isolated containers in smaller-scale deployments than Azure Kubernetes Service . The top-level resource in ACI is the container group , a collection of containers that get scheduled on the same host machine. These containers share a lifecycle, resources, local network, and storage volumes, and is equivalent to a Kubernetes pod. Container groups can be deployed to a subnet that already hosts a container group or an empty one, but it may not be deployed to a subnet that already has other resources like VMs. Resources What is Azure Container Instances? Container groups in ACI","title":"Azure Container Instances"},{"location":"Cloud/#azure-data-explorer","text":"","title":"Azure Data Explorer"},{"location":"Cloud/#azure-devops","text":"Azure DevOps used to be known as Visual Studio Team Services and Team Foundation Server . Sources DevOps Training Workshop YouTube Install DevOps CLI az extension add --name azure-devops","title":"Azure DevOps"},{"location":"Cloud/#azure-dns","text":"Azure DNS supports private zones, which provide name resolution for VMs on a VNet and between VNets without having to create a custom DNS solution. Time-to-live for DNS record sets is provided in seconds. Azure DNS alias records allow other Azure resources to be referenced from the DNS zone, rather than static IP addresses or domain names. This allows these records to be automatically updated or deleted when the underlying Azure resource is changed. An A alias record set is a special type of record set that allows you to create an alternative name for a record set in your domain zone or for resources in your subscription. A CNAME alias record set can only point to another CNAME record set. Custom domains can be used by implementing CNAME DNS records, which are used in DNS to map alias domain names to the \"canonical\" name. Sources Azure DNS alias records overview","title":"Azure DNS"},{"location":"Cloud/#azure-file-service","text":"Azure File Service allows you to create one or more file shares in the cloud (up to 5 TB per share), similar to a regular Windows File Server. It supports the SMB protocol, so you can connect directly to a file share from outside of Azure, if traffic to port 445 is allowed through the LAN and ISP. It can also be mapped within Windows. A clever use of a file share is as persistent storage for the Azure Cloud Shell. src","title":"Azure File Service"},{"location":"Cloud/#azure-file-sync","text":"Azure File Sync extends Azure File Service to allow on-premises file services to be extended to Azure while maintaining performance and compatibility, communicating over TCP 443 over SSL, and not IPSec. Use cases include: Replace on-premises file servers Easily replicate data on-premises to make it available during lift-and-shift migrations Simply cloud development and management Azure File Sync works using an Azure File Sync agent , available as an MSI package for Windows Server 2012R2, 2016, and 2019, to register file servers as endpoints to an Azure File Sync Group . After installation, Azure credentials for a subscription must be provided. AZ-103: 153 In order to create an Azure File Sync, first a Storage Sync Service resource must be created, which works like a container to hold one or more sync groups . Every sync group has only a single cloud endpoint , referring to a storage account, but can have more than one server endpoint . Any server can only be registered to a single Storage Sync Service, and servers synced to different Storage Sync Service resources cannot sync with each other. Cloud tiering is an optional feature in Azure File Sync in which frequently accessed files are cached in the on-prem file servers, while less commonly accessed files are tiered to Azure Files. This is done by enabling Cloud Tiering, then selecting a free space policy , a percentage which indicates the amount of free space to maintain on the server endpoint's volume. When a user does access one of these tiered files, that file is downloaded to the on-prem cache and made available locally from that point on. This frees up local storage. Cloud tiering cannot be used with server endpoints on the system volume Although server endpoints can be configured with different free space policies, the most restrictive setting takes effect For tiered files, the file will be partially downloaded as needed Although a mount point can be a server endpoint, there can be no mount points inside a server endpoint When a filename collision occurs between the file share and file server, the file on the server has its filename appended with the server's name.","title":"Azure File Sync"},{"location":"Cloud/#azure-policy","text":"Azure Policy is a service that can create, assign, and manage policies to enforce governance. Policy definitions, authored in JSON, implement policy by describing desired behavior for Azure resources when they are created or updated. AZ-103: p. 72 To implement policy, a policy definition is created first, then a policy assignment assigns it to a scope. Policy definitions can be packaged together using initiative definitions and applied to a scope using initiative assignments RBAC roles deny by default and allow explicitly . But Azure Policy allows by default and denies explicitly Policies can be applied at the management group , subscription , or resource group scope, with all child resources and resource groups being affected. Every policy definition has a single effect , which includes: Audit : create a warning event in the log Modify : used to add, update, or remove properties or tags on a resource during creation or update. Append AuditIfNotExists Deny DeployIfNotExists Disabled The order of evaluation of effects is: Disabled, Append, Deny, Audit (\"DADA\") Sources What is Azure Policy? Azure Policy Samples Understand Azure Policy effects","title":"Azure Policy"},{"location":"Cloud/#azure-vms","text":"Virtual Machines represent Azure's IaaS offering. A dedicated host group has to be created and placed in a resource group and associated with a location and availability zone and assigned a fault domain. A host then has to be created, a size specified, and associated with a host group. Any VM intended to run on the host has to be created in the same location and availability zone and associated with the host in the Advanced tab. Azure spot instances are available at deep discounts. 3 types of disk are available to Azure VMs: Operating System Disk (OS Disk) Temporary Disk Data Disk Azure VM image types include: Managed images (recommended), which remove the dependency of the VM to the image, at least within the same region. Copying a VM to another region still requires the managed image to be copied first. Unmanaged images, which required the VM to be created in the same storage account as that of the image. VM copies required the image to be copies first. VM images are captured from an existing VM that has been generalized (prepared), removing unique settings (hostname, security IDs, personal information, user accounts, domain join information, etc) but not customizations (software installations, patches, additional files, folders), using sysprep.exe for Windows machines or Microsoft Azure Linux Agent ( waagent ) for Linux machines. VM images in AWS are called Amazon Machine Images (AMI) .","title":"Azure VMs"},{"location":"Cloud/#azure-vpn","text":"Virtual network gateways in Azure are of two types: VPN gateways and ExpressRoute gateways . Any virtual network can have only a single gateway of each type. VPN gateways send encrypted traffic between the virtual network and an on-premises location. VPN Gateways must be deployed into their own dedicated subnet (named \"GatewaySubnet\" ) with a minimum size of CIDR /29, although a CIDR /27 address block is recommended. VPN connections between an on-premises network and a VNet are only possible if the network ranges do not overlap. VPN gateways can be classified by the topology of the connection: Site-to-Site (S2S) connections require an on-premises VPN device associated with a public IP address. Multi-Site connections require a RouteBased VPN type. Point-to-Site (P2S) allows individual computers to securely connect to a VNet without need for a VPN device, which is useful for telecommuting, and can use SSTP, OpenVPN, or IKEv2. There are several authentication considerations. VNet-to-VNet connections are also possible, but VNet peering may be preferable if the virtual networks meet certain requirements. Site-to-Site Multi-Site Point-to-Site VNet-to-VNet VPN gateways can also be classified on VPN type . Route-based VPNs (previously called \"dynamic routing gateways\") require routes to be defined in a routing table to direct packets into tunnel interfaces. Policy-based VPNs (previously called \"static routing gateways\" in the classic deployment model) can only be used on the Basic gateway SKU and offer only a single S2S tunnel. Route-based Policy-based There is a profusion of Gateway SKUs that determine the maximum connections, throughput, and availability of other features like BGP and zone-redundancy available for each topology. Every Azure VPN gateway consists of two instances in an active-standby configuration. During failover, a brief interruption of 10-15 seconds for planned maintenance or up to 60-90 seconds in the case of unplanned disruption, may occur. But the gateway can be configured to be active-active , which will establish S2S VPN tunnels to both gateway instances with traffic being routed through both tunnels simultaneously. There will still be only a single connection resource, but the on-premises VPN device must be configured to establish both of these tunnels. The most highly available arrangement would use multiple VPN devices with the VPN gateway in active-active configuration, creating 4 IPsec tunnels that evenly carry Azure traffic. Active-Standby Active-Active Dual redundancy","title":"Azure VPN"},{"location":"Cloud/#bigquery","text":"Petabyte-scale analytics database service for data warehousing. BigQuery can be executed using the bq command-line utility.","title":"Bigquery"},{"location":"Cloud/#bigtable","text":"Cloud BigTable is a GCP realtime database used for Big Data. BigTable can be executed using the cbt command-line utility. BigTable evolved out of Google's need to ensure access to petabytes of data in its web search business line. It was described in a 2006 research paper that ended up launching the entire NoSQL industry. In 2015 it was made available as a service to cloud customers. YouTube BigTable doesn't support secondary indexes. Resources The Right Bigtable Index Makes All the Difference The Right Bigtable Index Makes All the Difference Bigtable vs. Firestore","title":"BigTable"},{"location":"Cloud/#billing-account-administrator","text":"GCP predefined role that grants permissions to manage self-service accounts but not to create new ones.","title":"Billing Account Administrator"},{"location":"Cloud/#billing-account-creator","text":"Predefined GCP role that grants permissions to create new self-service accounts.","title":"Billing Account Creator"},{"location":"Cloud/#billing-account-user","text":"GCP predefined role that enables user to link projects to a billing account.","title":"Billing Account User"},{"location":"Cloud/#billing-account-viewer","text":"GCP predefined role that grants permissions to view transactional and billing data associated to a GCP account.","title":"Billing Account Viewer"},{"location":"Cloud/#cloud-automl","text":"Cloud AutoML is a GCP service that allows developers without machine learning experience to develop machine learning models.","title":"Cloud AutoML"},{"location":"Cloud/#cloud-device-administrator","text":"Azure built-in role that grants users full access to manage devices in Azure AD.","title":"Cloud Device Administrator "},{"location":"Cloud/#cloud-functions","text":"Cloud Functions is GCP's serverless compute offering and is suited to running short-running logic, such as calling other APIs in response to an event. Resources: Cloud Functions in a minute Quickstart","title":"Cloud Functions"},{"location":"Cloud/#cloud-machine-learning-engine","text":"Platform for building and deploying scalable machine learning systems to production.","title":"Cloud Machine Learning Engine"},{"location":"Cloud/#cloud-natural-language-processing","text":"GCP tool for analyzing human languages and extracting information from text.","title":"Cloud Natural Language Processing"},{"location":"Cloud/#cloud-run","text":"Google Cloud Run is built on a native open standard that will allow using the same container on other cloud providers. It bills down to the nearest 100 ms interval. Cloud Run provides an HTTPS endpoint to the container. Cloud Run can also run on your own K8S cluster running on GKE , recommended for workloads that have a consistently high level of traffic, since you are billed for the provisioned cluster resources. However, resources like CPU, GPU, and other items can be customized. Resources: Cloud Run in a minute Differences between Cloud Run and Cloud Run on GKE Cloud Run: deploy a prebuilt container Build and deploy with Cloud Run","title":"Cloud Run"},{"location":"Cloud/#cloud-vision","text":"Image analysis platform for annotating images with metadata, extracting text, or filtering content. Sources: Getting started: Image recognition and classification","title":"Cloud Vision"},{"location":"Cloud/#cloudformation","text":"CloudFormation is AWS's declarative automation service, which can use JSON or YAML-format templates. These resources are placed into a named stack , a container that organizes the resources described by the template, and the stack name must be unique to the account. This allows provisioned resources to be easily managed, since the stack contains a record of events, and to be quickly destroyed by deleting the stack. CloudFormation Designer allows templates to be viewed as a diagram of resources.","title":"CloudFormation"},{"location":"Cloud/#cloudfront","text":"Amazon CloudFront is a CDN that helps deliver static and dynamic content worldwide. CloudFront caches content in edge locations , of which there are more than 150 spread out across 6 continents. Edge locations may not be chosen arbitrarily, rather there are three options: US, Canada, and Europe US, Canada, Europe, Asia, and Africa All edge locations In order to make content available on CloudFront, you must create a distribution , which defines the type and origin of the content to cache. There are two types of distribution: A Web distribution is used for static and dynamic content, including streaming video, accessible via HTTP or HTTPS. Its origin can be a web server or a public S3 bucket. Real-Time Messaging Protocol (RTMP) distribution delivers streaming audio or video. The media player and media files must be stored in S3 buckets.","title":"CloudFront"},{"location":"Cloud/#cloudtrail","text":"AWS CloudTrail keeps event logs of actions that occur against AWS resources. These events are divided into API and non-API actions. API actions include creating, modifying, or deleting resources. Non-API actions include everything else, like logging into the management console. Events are also classified as management events and data events Management events (also control plane operations ) are operations that a principal attempts to execute against an AWS resource. Data events are S3 object-level activity and Lambda function executions. These are treated separately from management events because they tend to be higher volume. Resources \ufe0f How can I use CloudTrail to review what API calls and actions have occurred in my AWS account?","title":"CloudTrail"},{"location":"Cloud/#cloudwatch","text":"Amazon CloudWatch collects logs, metrics, and events from AWS resources and non-AWS on-premises servers and presents a dashboard for visual analysis. All AWS resources automatically send their metrics to CloudWatch Metrics, which stores the data for up to 15 months. CloudWatch alarms can be configured for single metrics. Applications and AWS services have to be configured to send log events to CloudWatch Logs, and they are stored indefinitely by default although retention settings can be configured. Log events from the same source are organized into a log stream. Log streams are then organized into log groups. Metric filters extract metric data from log events. CloudWatch Events is a feature that monitors for changes in AWS resources as a result of API operations.","title":"CloudWatch"},{"location":"Cloud/#cloudyn","text":"Although the Cloudyn service, which had been purchased by Microsoft, was being offered as a standalone service, it has now been deprecated because its functionality has been incorporated natively into other sections of the Cost Management + Billing blade.","title":"Cloudyn"},{"location":"Cloud/#codecommit","text":"CodeCommit is the AWS private git repo service.","title":"CodeCommit"},{"location":"Cloud/#codedeploy","text":"CodeDeploy is an AWS service for automatically deploying applications to AWS compute resources or on-prem servers. CodeDeploy can pull source code from [S3][S3] and repos from GitHub or Bitbucket but notably not CodeCommit (ref. CodePipeline ).","title":"CodeDeploy"},{"location":"Cloud/#codepipeline","text":"CodePipeline is an AWS service for orchestrating and automating every stage of software development. It defines a series of stages, two of which are required - source and deployment - but other stages like testing or approval can be incorporated.","title":"CodePipeline"},{"location":"Cloud/#cognito","text":"Cognito is an AWS service that integrates with identity providers like Amazon, Google, Microsoft, and Facebook to add user access control to an application.","title":"Cognito"},{"location":"Cloud/#compute-engine","text":"Compute Engine is GCP's IaaS offering. An instance group is a collection of VM instances that you can manage as a single entity. Two types: Managed instance groups operate applications like web front-ends across a group of identical VMs created with a template. They provide high availability, healing, scaling, and automatic updates. Unmanaged instance groups allow you to manually load balance a group of VMs. VMs can be added or removed at will. Getting started with GCE","title":"Compute Engine"},{"location":"Cloud/#compute-engine-admin","text":"Predefined GCP role that grants full control of Compute Engine resources.","title":"Compute Engine Admin"},{"location":"Cloud/#compute-engine-network-admin","text":"Predefined GCP role that grants full control of Compute Engine networking resources.","title":"Compute Engine Network Admin"},{"location":"Cloud/#compute-engine-security-admin","text":"Predefined GCP role that grants full control of Compute Engine security resources.","title":"Compute Engine Security Admin"},{"location":"Cloud/#compute-engine-viewer","text":"Predefined GCP role that grans read-only access to all Compute Engine resources, but exclusive of data stored on disks, images, and snapshots.","title":"Compute Engine Viewer"},{"location":"Cloud/#compute-service-agent","text":"Predefined GCP role that grants Compute Engine Service Account access to assert service account authority.","title":"Compute Service Agent"},{"location":"Cloud/#computer-vision","text":"Computer vision is a subfield of artificial intelligence concerned with developing the capability of computers to recognize objects in images and to understand visual information.","title":"Computer Vision"},{"location":"Cloud/#container-instances","text":"Azure Container Instances (ACI) is a PaaS service that facilitates deployment of individual containers.","title":"Container Instances"},{"location":"Cloud/#cosmosdb","text":"Cosmos DB started as Project Florence in 2010 to address shortcomings with SQL Server in supporting highly available services like Xbox. In 2015 the product was relaunched as Document DB, then renamed Cosmos DB in 2017. An emulator is available for Cosmos DB here . A Cosmos DB account can be used for free for 30 days, and does not require an Azure subscription. Throughput is measured and billed in Request Units (RU) per second. The minimum manually provisioned throughput level is 400 RU/sec. There are three throughput provisioning offers: Manual , where a static throughput level is provisioned. This is best for highly predictable workloads. Autoscaling , where Azure will automatically scale throughput based on usage, reducing it down to a minimum of 10% of the provisioned throughput. Serverless , where you pay for only the RUs you need. This throughput provisioning model is ideal for small demonstration projects. This feature is forthcoming. The cost of using a CosmosDB database can be approximated using the Capacity calculator . In general, these are reasonable back-of-hand estimates for common operations to estimate costs: Read item: 1 RU SQL query: ~2.8 RU Create item: 10 RU There are various choices of API for Cosmos DB accounts which affect the data model used for databases. SQL API is the Core API, and works off JSON documents and SQL query syntax MongoDB API uses BSON documents (binary encoded JSON) and MongoDB query syntax Table API and uses Key-Value database design reflects API of Azure Table Storage Gremlin API is a graph database using a flat store of vertices and edges Cassandra API is columnar, and unlike most NoSQL databases does specify a schema Consistency , in a distributed NoSQL database like Cosmos DB, describes the uniformity of data across replicas. Consistency levels describe how and when data is replicated to provide varying consistency guarantees. Strong consistency is the strongest consistency model and requires synchronous replication after every change to database, increasing latency for each write. Session consistency is unique, in that it offers consistent prefix to databases that support a single session or an application with a single token. Eventual consistency is the weakest consistency model and provides no ordering guarantees. Consistent prefix offers read throughput, availability, and write latency comparable to eventual consistency while guaranteeing global order. Bounded staleness implies asynchronous replication and offers guarantees on the number of versions ( K ) or time interval ( T ) reads lag behind writes, referred to as the staleness window . As the staleness window approaches, Azure will delay writes by providing back pressure on writes. Outside the staleness window, data is guaranteed to be globally consistent. Outside the region in which the writes were made, Azure guarantees total global order or consistent prefix , which means, the global order is maintained. Strong Session Eventual Bounded staleness Horizontal partitioning is what allows Cosmos DB to scale-out massively to provide high availability and elasticity. Partitions can be thought of as physical fixed-capacity data buckets that back every container. A partition split occurs when a new physical partition is brought online, resulting in half of the documents existing on a previously existing partition being moved to the new one. Cosmos DB automatically and transparently splits horizontal partitions to achieve elasticity. Logical partitions , determined by the partition key which is set at container creation, group individual documents in ways that are kept on the same physical partition. It is recommended to have a high number of logical partitions, so that CosmosDB has greater flexibility partitioning documents. The partition key is immutable, so the correct choice of partition key is an important architectural consideration. Even distribution of documents is ideal to avoid hot partitions , where some partitions have much greater activity than others, due to uneven distribution of documents. Any partition may not be greater than 20 GB in size. Physical partitions have 4 replicas within a region. There are several common partitioning patterns: Partitioning on /id , which results in every document existing in its own logical partition. This pattern is write-optimized and ideal for IoT applications. Any SQL query for more than one document would be cross-partition by necessity, so direct reads using the /id value would be far more economical. Partitioning small lookup lists on a /type property. This will keep lists of related items used for lookups in the same partition. Optimizing for queries by organizing multiple types of document according to a key data-point. For example, customer data could be kept in the same partition as that customer's orders, avoiding cross-partition queries. Sources: Learning Azure Cosmos DB","title":"CosmosDB"},{"location":"Cloud/#cost-management-contributor","text":"Azure built-in role that grants access to the Cost Management blade.","title":"Cost Management Contributor"},{"location":"Cloud/#cost-management-reader","text":"Azure built-in role that grants access to the Cost Management blade.","title":"Cost Management Reader"},{"location":"Cloud/#data-box","text":"Azure Data Box is a Microsoft-provided appliance that allows for the transfer of large volumes of data to Azure. These services are only available for EA, CSP, and Microsoft Partner Network Sponsorship offer types. Offering Capacity Storage saccounts Data Box Disk 35 TB 1 Data Box 100 TB 10 Data Box Heavy 1,000 TB 10 Workflow Order: Use Portal to order a data box by creating a Data Box resource Receive: Connect Data Box to network Copy data: Mount file shares and copy data to the device. Return: to Microsoft Upload: Microsoft will upload the data and securely erase it from the device","title":"Data Box"},{"location":"Cloud/#dataflow","text":"Cloud Dataflow is a GCP streaming data framework for defining both batch and stream processing pipelines. Resources What is Dataflow?","title":"Dataflow"},{"location":"Cloud/#dataprep","text":"Dataprep is a GCP managed service that allows analysts to visually explore, clean, and prepare data for later analysis. Resources: Dataprep: Qwik Start - Qwiklabs Preview","title":"Dataprep"},{"location":"Cloud/#dataproc","text":"Dataproc is a GCP service that manages the creation of data science clusters and data analysis jobs. Resources: Dataproc in a minute","title":"Dataproc"},{"location":"Cloud/#dynamodb","text":"DynamoDB is a NoSQL database known for fast (1-9 ms) query times. DynamoDB measures capacity in Read Capacity Units (RCU) and Write Capacity Units (WCU) . 1 RCU = 1 record at most 4 KB in size 1 WCU = 1 record at most 1 KB in size DynamoDB offers the choice between strongly consistent and eventually consistent (half the cost) reads. DynamoDB offers two types of indexes: Global Secondary Index allows you to create a completely new aggregation of data. GSI updates are eventually consistent , with asynchronous updates populated after an update response is passed to the client. Local Secondary Index (LSI) alternate sort key attribute that allows only sorting DynamoDB Streams (changelog for the DynamoDB table) interfaces with AWS Lambda to implement complex queries , computed values like sum, average, maximum, etc. These are implemented in a different processing space than the DynamoDB table itself, so that it does not affect the table. AWS Lambda has an invocation role which defines what Lambda can see (triggered upon a change to the table as reported in DynamoDB Streams) and an execution role which defines what it can do .","title":"DynamoDB"},{"location":"Cloud/#ecs","text":"","title":"ECS"},{"location":"Cloud/#eks","text":"","title":"EKS"},{"location":"Cloud/#elastic-beanstalk","text":"AWS Elastic Beanstalk is a PaaS offering.","title":"Elastic Beanstalk"},{"location":"Cloud/#elastic-file-system","text":"Elastic File System (EFS) is a scalable file system for AWS Linux instances. Multiple instances can be attached to a single EFS volume to share files. EFS volumes are highly available, spanning multiple Availability Zones in a single VPC.","title":"Elastic File System"},{"location":"Cloud/#elastic-mapreduce","text":"Elastic MapReduce (EMR) is AWS's managed big data analysis service, supporting Apache Hadoop, Apache Spark, HBase, Presto, and Flink.","title":"Elastic MapReduce"},{"location":"Cloud/#enterprise-agreement","text":"Azure customers on an Enterprise Agreement can add up-front commitments to Azure then be billed annually. If the committed spend is exceeded, the overage is billed at the same EA rate. EA customers can create spending quotas and set notification thresholds through the EA Portal. 3 portals used to manage Azure subscriptions EA Portal (ea.azure.com) available only to customers with an Enterprise Agreement Account Portal Azure Portal, includes Azure Cost Management","title":"Enterprise Agreement"},{"location":"Cloud/#expressroute","text":"There are four main architectures used with ExpressRoute Any-to-any connection is used to integrate on-premises WANs using IPVPN. Co-location with cloud exchange is used to order virtual cross-connections to the Azure cloud through the co-location provider's Ethernet exchange. Point-to-point Ethernet connection is used to configure on-premises data center connectivity to Azure through individual point-to-point links","title":"ExpressRoute"},{"location":"Cloud/#firebase","text":"Firebase is a completely unstructured NoSQL database that is known for its client libraries. Firebase Auth offers a free user interface for applications, Firebase UI .","title":"Firebase"},{"location":"Cloud/#firestore","text":"Cloud Firestore was released from beta in early 2019 and combines and improves upon functionality of previous products named Cloud Datastore and Firebase Realtime Database . Firestore is organized into documents , which consist of key-value pairs and are similar to JSON objects, and collections . JSON-like objects are called maps and keys are called fields in Firestore. Collections can contain only documents, but documents can contain sub-collections. Root can only contain collections. So navigating deeper and deeper into the information store will involve alternating between collections and documents. Firestore features a compatibility mode that emulates the behavior of Datastore in accessing Firestore's storage layer while removing some of Datastore's limitations. Queries in Firestore can only be used to find documents stored in one specific collection or sub-collection. However a collection group query , meaning one that spans multiple collections, began to be supported in 2019. Complex relational queries are not possible (in a single query), and query results are usually returned based on equality or greater-than/less-than comparisons. The field has to be specified as having a scope of \"Collection group\" within GCP, and there is a limit of (about) 200 for these queries. An index is created for every field in every document added to a collection, which results in very fast query times that are proportional to the number of results , not records searched. This structure ensures that equality searches are highly performant, as are comparison searches using greater-than or less-than. But this implementation creates bizarre limitations to Firestore's querying capabilities: There is no native way to perform wildcard searches or OR queries. For common instances of such queries, Google recommends adding a field that contains the value for each record Inequality searches present a challenge for Firestore. For some queries that combine conditions on more than one field (i.e. restaurants within a certain range of a location), Firebase will create a \"composite index\" (only within the index, the document itself is not affected) automatically to facilitate searches on those fields. Unlike Firebase , which charges based on the volume of data stored, Firestore charges based on number of operations performed and records returned. Sources: What is a NoSQL Database? Introducing Firebase Realtime database Firebase web application tutorial Firebase Realtime DB vs Firestore How queries work in Firestore Cloud Firestore vs the Realtime Database: Which one do I use? Firebase blog","title":"Firestore"},{"location":"Cloud/#folder-admin","text":"Predefined GCP role that allows folders to be created at an Organization.","title":"Folder Admin"},{"location":"Cloud/#front-door","text":"Azure Front Door works like Azure Load Balancer for web apps. Resources Pricing","title":"Front Door"},{"location":"Cloud/#gcloud","text":"app ? compute ? container ? functions ? run ? services ? source Google Cloud Source Control repositories gcloud source repos clone gcloud source repos create Sources: Google Command Line for beginners","title":"gcloud"},{"location":"Cloud/#glacier","text":"S3 Glacier offers long-term archival at low cost. One or more files are stored in an archive, typically a .zip or .tar file containing multiple files. Archives can range from 1 B to 40 TB in size. Archives are stored in a Glacier vault , a region-specific container analogous to S3 buckets. Vaults must have regionally unique names, but there is no need for a globally unique name. Glacier vaults can be created and deleted using the Glacier service console. But uploading, downloading, or deleting archives must be done through the AWS CLI or an application using the SDK. Some third-party applications can also interact with Glacier.","title":"Glacier"},{"location":"Cloud/#google-cloud-identity","text":"Google's Identity as a Service (IDaaS) provider.","title":"Google Cloud Identity"},{"location":"Cloud/#importexport-service","text":"Azure Import/Export service allows the physical shipment of disks procured by the user to Azure for import. This data can be placed into blob or file storage. This service requires the use of a Windows computer with BitLocker and .NET Framework and is dependent on the WAImportExport.exe utility. Procure 2.5-inch or 3.5-inch SATA (not SAS) disks Connect the disks to a Windows machine. Create a volume and encrypt it using BitLocker Install the Azure Import/Export tool (WAImportExport.exe) on the disks. Copy files Create an import job in the Azure Portal","title":"Import/Export Service"},{"location":"Cloud/#kinesis","text":"Kinesis is an AWS service for ingestion and processing of streaming data, such as access logs, video, audio, and telemetry.","title":"Kinesis"},{"location":"Cloud/#kubeflow","text":"Kubeflow is a cloud-native platform for machine learning based on Google\u2019s internal machine learning pipelines. Resources: Kubeflow 101 (playlist) Talk - Kubeflow at Spotify Talk - Kubeflow on Kubernetes","title":"Kubeflow"},{"location":"Cloud/#kusto","text":"Kusto is a case-sensitive query language developed by Microsoft and used in several Azure services: Azure Data Explorer Log Analytics Sentinel Application Insights Microsoft Defender ATP","title":"Kusto"},{"location":"Cloud/#lightsail","text":"Amazon Lightsail offers blueprints that will automatically provision all compute, storage, database, and network resources needed for a deployment.","title":"Lightsail"},{"location":"Cloud/#macie","text":"Macie is an AWS service that automatically finds and classifies sensitive data stored in AWS using machine learning to recognize sensitive data such as PII or trade secrets.","title":"Macie"},{"location":"Cloud/#mars","text":"The Microsoft Azure Recovery Services (MARS) agent is for backing up Windows machines only, but can be installed on VMs on other cloud providers like AWS. MARS can be configured to protect the entire system, volumes, or individual files and folders.","title":"MARS"},{"location":"Cloud/#monitor","text":"Resources: Dashboards with Azure Monitor Data","title":"Monitor"},{"location":"Cloud/#neptune","text":"Neptune is an AWS graph database.","title":"Neptune"},{"location":"Cloud/#npm","text":"Network Performance Monitor is a Log Analytics network monitoring solution for hybrid networks, providing 3 services: Performance Monitor monitors connectivity between various points in both Azure and on-prem networks Service Connectivity Monitor monitors outbound connectivity from network nodes to external TCP services, monitoring performance metrics like latency, response time, and packet loss ExpressRoute monitors end-to-end connectivity between on-prem network and Azure over ExpressRoute","title":"NPM"},{"location":"Cloud/#network-watcher","text":"Network Watcher appears like a normal resource in a resource group, but it is deployed as a single instance per Azure region. Network Watcher monitoring and diagnostic tools: IP Flow Verify Next Hop Packet Captures link a Network Watcher resource, a target VM, a storage account, and a filter that specifies the characteristics of network traffic (source and destination IP addresses and ports as well as protocol) to capture, as well as a time limit. Network Topology","title":"Network Watcher"},{"location":"Cloud/#opsworks","text":"OpsWorks is AWS's declarative configuration management service that uses the Chef and Puppet configuration management platforms and comes in three varieties: OpsWorks for Puppet Enterprise OpsWorks for Chef Automate OpsWorks Stacks","title":"OpsWorks"},{"location":"Cloud/#project","text":"A project is the direct parent of all other GCP resources, and always consists of a project name, project ID, and project number.","title":"Project"},{"location":"Cloud/#project-creator","text":"Predefined GCP role given to all users currently assigned to a project.","title":"Project Creator"},{"location":"Cloud/#pubsub","text":"Cloud Pub/Sub is GCP's messaging service, allowing services and applications to communicate. Resources: Cloud Pub/Sub Overview - ep. 1","title":"Pub/Sub"},{"location":"Cloud/#recovery-services-vault","text":"A Recovery Services Vault is an Azure resource used to centrally manage the backup and recovery of Azure resources, and the centerpiece of any backup strategy. A Backup protection policy defines how a backup plan is implemented. These are most easily created through the Portal. A vault can only back up data from other resources that exist in its region.","title":"Recovery Services Vault"},{"location":"Cloud/#rekognition","text":"AWS deep learning-based image recognition service.","title":"Rekognition"},{"location":"Cloud/#resource-policy-contributor","text":"Azure built-in role that includes access to most Policy operations and should be considered privileged.","title":"Resource Policy Contributor"},{"location":"Cloud/#route-53","text":"Route 53 is AWS's managed DNS service. Like any other DNS system, it relies on resource records defined in a zone . Route 53 can also provide name resolution for private domain names , used on private networks. Private hosted zones provide DNS resolution for a single domain name within multiple VPCs. But when a resource record must be changed dynamically to work around failures or route users to an underutilized server, routing policies can be used. Simple policy is the default for new resource records and maps a domain name to a single value (i.e. an IP address). Weighted policy distributes traffic across multiple resources according to a predefined ratio. Latency policy sends users to resources in their closest Region. Failover policy allows a secondary resource to be marked for routing when the primary resource is unavailable. Geolocation policy routes users based on their specific continent, country, or state. Multivalue answer policy allows even distribution of traffic across multiple resources by randomizing the order of returned records. All routing policies except Simple can use health checks to modulate routing action. All health checks occur every 10 or 30 seconds and can check one of three resources: Endpoint makes a test connection to a TCP port CloudWatch alarm can be set off in case of high latency or other metrics. Calculated monitors the status of other health checks. Route 53 also offers the Route 53 Traffic Flow visual editor that allows you to create a diagram to represent the desired routing. The diagram isn't translated to individual resource records but rather represents a single policy record which costs 50 USD/month each. In addition to the routing policies above, Traffic Flow also offers the Geoproximity routing policy that directs users to a geographic location based on how close they are.","title":"Route 53"},{"location":"Cloud/#s3","text":"Simple Storage Service (S3) is Amazon's data storage service. S3 stores objects in a container called a bucket . Each bucket must have a globally unique name and exposes a HTTP endpoint (at https://$BUCKET.s3.amazonaws.com/) Each object is associated with a key . Keys are equivalent to filenames, and the bucket is equivalent to a flat filesystem. However, directories can be simulated by placing slashes in the key. Bucket policies (applied to buckets) and user policies (applied to IAM principals) can be used to modulate accessibility. Public or anonymous access to an object can only be granted by bucket policies. Bucket and object ACLs are legacy access control methods that are still usable. S3 buckets store data unencrypted, although encryption at rest is available in two options: Server-side encryption: S3 encrypts uploaded objects before storing them, and decrypts it again before delivery. Client-side encryption: User must encrypt data prior to uploading and decrypt it after downloading. S3 offers various storage classes that differ in their availability and durability , the percent likelihood that an object within it will not be lost over the course of a year. Frequently accessed objects: STANDARD REDUCED_REDUNDANCY Infrequently accessed objects STANDARD_IA ONEZONE_IA GLACIER INTELLIGENT_TIERING automatically moves objects to the most cost-effective storage tier based on access patterns. Storage Gateway is an on-premises VM that provides a connection to S3 for on-premises infrastructure. File gateway lets you use NFS and SMB file shares to transfer data to S3. Data is stored in S3 and cached locally. Volume gateway can be used as an iSCSI target by on-premises servers. Two configuration variants exist: Stored volumes : All data is stored locally and asynchronously backed up to S3 as EBS snapshots. A stored volume can range from 1 GB to 16 TB in size. Cached volumes : Data is stored in S3 and frequently used data is cached locally. A cached volume can range from 1 GB to 32 TB in size. Tape gateway is configured as an iSCSI target by a backup application. Virtual tapes range from 100 GB to 2.5 TB in size. These tapes are asynchronously transferred to a virtual tape library (VTL) backed by S3 and removed when the upload is complete. Recovery requires downloading the virtual tape again. Cloud Storage in Minutes with AWS Storage Gateway","title":"S3"},{"location":"Cloud/#sentinel","text":"Azure Sentinel is a cloud-native SIEM and SOAR soluation that can collect data from many sources and present it to security analysts, who can run Kusto queries against the dataset. Azure Sentinel can ingest data from on-premises devices using one of several types of connector , categorized by the type of data ingestion: Native connectors integrate directly with other Microsoft security products, like Azure AD, M365, and Azure Security Center Direct connectors are configured from their source location, such as AWS CloudTrail, Azure Firewall, and Azure Front Door API connectors are implemented by security providers, like Azure Information Protection (AIP), Barracuda Web Application Firewall (WAF), and Microsoft WAF Agent-based connectors , using the Log Analytics agent, make it possible to ingest data from any source that can stream logs in Common Event Format (CEF), such as Windows and Linux machines. Analytic rules are rules that users create to help detect threats and anomalies in an environment: Scheduled rules run on a predetermined schedule Microsoft Security Machine learning behavior analytic rules can (currently) only be created from templates provided by Microsoft using proprietary ML algorithms","title":"Sentinel"},{"location":"Cloud/#simple-queue-service","text":"Simple Queue Service (SQS) is an AWS service that can broker messages between components of highly decoupled applications.","title":"Simple Queue Service"},{"location":"Cloud/#snowball","text":"AWS Snowball is a physical appliance designed to move large amounts of data to the cloud. The largest Snowball device can store 72 TB of information. Snowball Edge refers to a family of options similar to Snowball but with compute power to run EC2 instances and Lambda functions locally. All Snowball Edge options feature a QSFP+ network interface that is capable of speeds up to 100 Gbps. Snowball Edge devices can also be clustered together. Storage Optimized provides up to 80 TB of storage and 24 vCPUs. Compute Optimized provides up to 39.5 TB of storage and 52 vCPUs. Compute Optimized with GPU is similar to Compute Optimized but includes an NVIDIA GPU, making it useful for ML and HPC applications.","title":"Snowball"},{"location":"Cloud/#spanner","text":"GCP managed scaleable database service.","title":"Spanner"},{"location":"Cloud/#stackdriver","text":"GCP service that collects metrics, logs, and event data from applications and infrastructure and integrates the data so DevOps engineers can monitor, assess, and diagnose operational problems.","title":"Stackdriver"},{"location":"Cloud/#super-administrator","text":"Unique GCP role associated with the root Organization which has powers that exceed that of other administrative users.","title":"Super administrator"},{"location":"Cloud/#storage-accounts","text":"Azure storage accounts are managed through [Azure Resource Manager][ARM] and management operations are authenticated and authorized using [Azure Active Directory][Azure AD]. There are four services provided within each storage account: Blobs provides a highly scalable service for storing arbitrary data objects, such as text or binary data. There can be multiple containers within a storage account, and a container can have its own folder structure. There are three types of blob: page , block , and append blobs. Tables provides a NoSQL-style store for storing structured data. Tables in Azure storage do not require a fixed schema, thus different entries in the same table can have different fields Queues provides reliable message queueing between application components Files provides managed file shares that can be used by VMs or on-premises servers Options that must be selected when creating a storage account: Performance tier Standard supports all storage services and uses magnetic disks to provide cost-efficient and reliable storage Premium only supports page blobs with the locally-redundant (LRS) replication option, uses high-performance SSD disks Account kind General-purpose V2: only kind to support ZRS General-purpose V1: does not support various access tiers. Blob storage: specialized storage account used to store block and append blobs Replication mode : Storage accounts can be freely moved between the following replication modes, except ZRS, in which case it is recommended to copy data to a new account. Locally-redundant storage (LRS): makes 3 local sychronous within the same Azure facility (zone) Zone-redundant storage (ZRS): makes 3 synchronous copies across multiple availability zones; available for general-purpose v2 storage accounts at Standard performance tier only. Geographically-redundant storage (GRS): makes 3 local synchronous copies plus 3 additional asynchronous copies (typically within 15 minutes, but no SLA) to a second data center far away from the primary region Read-access geographically redundant storage (RA-GRS): makes 3 local synchronous copies plus 3 additional asynchronous copies to a second data center far away from the primary region, which has only read-only access Access tier : Both Blob and StorageV1 can be upgraded to StorageV2, a process which is irreversible. Hot blob storage access tier optimized for the frequent access of objects in the storage account Cool blob storage access tier optimized for storing large amounts of data that is infrequently accessed and stored for at least 30 days Archive blob storage access tier designed for long-term storage of infrequently-used data that can tolerate several hours of retrieval latency, remaining in the Archive tier for at least 180 days. It is stored offline and can take up to 15 hours for it to be \"rehydrated\" to the Cool or Hot tier before it can be accessed. Premium providing high-performance access for frequently-used data on SSD, only available from the Block Blob storage account type. Every storage account service exposes its own Internet-facing endpoint, which must be secured in one of several ways. A firewall can be implemented by using network rules to limit traffic to particular networks. The storage firewall controls IP addresses and VNets can access the storage account and applies to all storage account services. Access can be restricted to specific VNets by creating a Virtual Network Service Endpoint , however this still uses the public IP address. Private Link allows similar functionality using private IPs. MS Docs Public access to blobs can be restricted at the container level on container creation. By default, no public read access is enabled for anonymous users, but users with RBAC rights or with the storage account name and key can have access. This can be done through ARM APIs, the Portal, or Azure Storage Explorer. Container access levels: No public read access: container and blobs can only be accessed by storage account owner (default for new containers) Public read-only access for blobs only (container data is not available, and anonymous clients cannot enumerate the blobs within the container) Full public read-only access: all container and blob data can be read by anonymous requests: Access can also be switched between Shared Key -based authentication (relying on storage account keys) and Azure AD authentication , where a RBAC role determines access to a Container. Authorize access to blobs and queues using Azure Active Directory Access keys grant full access to all data in all services of a storage account and represent the simplest and most powerful control over access. Access keys are typically used by applications for access to Azure storage, either through a Shared Access Signature (SAS) token or directly accessing the storage itself with the name and key. Storage account keys were implemented early in the history of Azure and grant full access to the entire storage account. However, it is considered an anti-pattern to distribute this key; a SAS token should be generated for every stored item to be distributed. Because storage account keys provide write access, a storage account with a ReadOnly resource lock will not enumerate its storage account keys, and users with Read permission will not be able to retrieve the keys either.","title":"Storage accounts"},{"location":"Cloud/#systems-manager","text":"Systems Manager is an AWS service for imperative configuration management. Systems Manager relies on several types of script: Command documents use normal shell commands and can be run periodically or on a trigger, so long as the instance to be managed has the required agent. Automation documents allow administration of AWS resources, similar in effect to using the Management Console or the AWS CLI .","title":"Systems Manager"},{"location":"Cloud/#trusted-advisor","text":"Trusted Advisor is an AWS service allows a visual check of resource configurations to ensure compliance with best practices, and is available only to Business and Enterprise Support plans . It offers several categories ( src ) Cost optimization Performance Security Fault tolerance Service limits","title":"Trusted Advisor"},{"location":"Cloud/#user-access-administrator","text":"Azure built-in role that grants the permissions necessary to assign a user administrative access at the subscription scope. Permissions: Microsoft.Authorization/roleAssignments/write Microsoft.Authorization/roleAssignments/delete","title":"User Access Administrator"},{"location":"Cloud/#user-administrator","text":"Azure built-in role that grants the power to manage all aspects of users and groups, including resetting passwords for limited admins.","title":"User Administrator "},{"location":"Cloud/#vm-agent","text":"Microsoft Azure Virtual Machine Agent (VM Agent) manages VM interaction with the Azure Fabric Controller and comes preinstalled with Windows images from the Marketplace. It can also be installed on a custom image. VM Agent supports the VMSnapshot extension, which is added when backups are enabled. This extension takes a snapshot of the storage at the block level and sends it to the RSV configured. For Windows VMs, this extension leverable the Volume Shadow Copy service.","title":"VM Agent"},{"location":"Cloud/#vmsnapshot","text":"","title":"VMSnapshot"},{"location":"Cloud/#volume-shadow-copy","text":"","title":"Volume Shadow Copy"},{"location":"Cloud/#waagent","text":"Microsoft Azure Linux Agent (waagent) manage VM interaction with the Azure Fabric Controller on Linux VMs.","title":"waagent"},{"location":"Cloud/#waimportexport","text":"WAImportExport.exe is a CLI tool associated with Azure Import/Export service . It requires a Windows computer with .NET Framework and BitLocker. There are two versions: Version 1 is recommended for blob storage Version 2 is recommended for files storage . Check disks required for selected blobs WAImportExport.exe PreviewExport /sn:<Storage account name> /sk:<Storage account key> /ExportBlobListFile:<Path to XML blob list file> /DriveSize:<Size of drives used> Various flags in WAImportExport.exe allow an XML-format \"blob list\" file to be used to specify files, or as output. All Export all blobs in the storage account <?xml version=\"1.0\" encoding=\"utf-8\"?> <BlobList> <BlobPath> / </BlobPath> </BlobList> Root Export all blobs in the root container <?xml version=\"1.0\" encoding=\"utf-8\"?> <BlobList> <BlobPath> /$root </BlobPath> </BlobList> Blob in root Export blob \"logo.bmp\" in the root container <?xml version=\"1.0\" encoding=\"utf-8\"?> <BlobList> <BlobPath> $root/logo.bmp </BlobPath> </BlobList> Containers Export all blobs in container \"music\" <?xml version=\"1.0\" encoding=\"utf-8\"?> <BlobList> <BlobPath> /music/ </BlobPath> </BlobList> Pattern Export all blobs in any container that begins with \"book\" <?xml version=\"1.0\" encoding=\"utf-8\"?> <BlobList> <BlobPath> /book </BlobPath> </BlobList> Pattern in container Export all blobs in container \"music\" that begin with prefix \"love\" <?xml version=\"1.0\" encoding=\"utf-8\"?> <BlobList> <BlobPath> /music/love </BlobPath> </BlobList>","title":"WAImportExport"},{"location":"Cloud/#webjobs","text":"WebJobs is a feature of Azure App Service that enables you to run a program or script in the same instance as a web app, API app, or mobile app, at no additional cost and supported only on Windows App Service plans . There are two types: Continuous webjobs default to running on all instances of the linked web app (although it can be configured to run on only one) Triggered webjobs run only when triggered or on a schedule and on only a single instance of the linked web app selected by Azure.","title":"WebJobs"},{"location":"Coding/","text":"\ud83d\udc68\u200d\ud83d\udcbb Coding Simple structures Hello, world! using System ; class Program { static void Main () { Console . WriteLine ( \"Hello World!\" ); } } def main (): \"Hello, world!\" ) if __name__ == \"__main__\" : main () Dates and times Parse a date string string rawDate = \"07/04/1776\" ; try { DateTime parsedDate = DateTime . Parse ( rawDate ); } catch ( FormatException ) { Console . WriteLine ( \"Unparsable!\" ) } // Alternatively: DateTime . TryParse ( rawDate , out parsedDate ); Console . WriteLine ( parsedDate . ToLongDateString ()); // => \"July 4, 1776\" File operations Write text file using ( StreamWriter writer = File . CreateText ( \"test.txt\" )) { writer . WriteLine ( \"Hello, world!\" ); } with open ( 'text' , 'w' ) as f : f . write ( 'Hello, world!' ) Read text file using System.IO ; string raven = File . ReadAllText ( \"raven\" ); using System.IO ; string [] raven = File . ReadAllLines ( \"raven\" ); using System.IO ; using ( StreamReader reader = File . OpenText ( \"raven\" )) { reader . ReadToEnd (); } using System.IO ; using ( StreamReader reader = File . OpenText ( \"raven\" )) { string s ; while (( s = sr . ReadLine ()) != null ) { Console . WriteLine ( s ); } } with open ( 'raven' ) as f : f . readlines () Copy file using System.IO ; File . Copy ( ' raven ' , ' raven . bak ' ) import shutil shutil . copyfile ( 'raven' , 'raven.bak' ) cp ./raven { ,.bak } Copy-Item .\\ raven .\\ raven . bak Copy file, overwriting using System.IO ; File . Copy ( ' raven ' , ' raven . bak ' , true ) import shutil shutil . copyfile ( 'raven' , 'raven.bak' ) cp -f ./raven { ,.bak } Copy-Item -Force .\\ raven .\\ raven . bak Move file using System.IO ; File . Move ( ' raven ' , ' raven . bak ' ); CSV using System ; using System.IO ; using CsvHelper ; struct Greek { public string name { get ; set ; } public string city { get ; set ; } public string dob { get ; set ; } } class Program { static void Main ( string [] args ) { using ( StreamReader reader = new StreamReader ( \"greeks.csv\" )) { CsvReader csvreader = new CsvReader ( reader , System . Globalization . CultureInfo . InvariantCulture ); var data = csvreader . GetRecords < Greek >(); foreach ( Greek item in data ) { Console . WriteLine ( $ \"{item.name,-15} {item.city,-15} {item.dob,-15}\" ); } } } } import csv with open ( \"greeks.csv\" ) as f : r = csv . reader ( f ) headers = next ( r ) data = [ row for row in r ] JSON using ( var stream = await storageFile . OpenAsync ( FileAccessMode . Read )) { using ( var dataReader = new DataReader ( stream )) { await dataReader . LoadAsync (( uint ) stream . Size ); var json = dataReader . ReadString (( uint ) stream . Size ); customerList = JsonConvert . DeserializeObject < List < Customer >>( json ); } } Random numbers Integer Random r = new System . Random (); int result = r . Next ( 1 , 6 ); import random random . randrange ( 1 , 6 ) Real number Random r = new System . Random (); int result = r . NextDouble (); import random random . random () Double array primes = [ 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 ] double = lambda x : 2 * x list ( map ( double , primes )) # Alternatively: [ 2 * el for el in primes ] String formatting Desired output (10 spaces per column, with the third right-justified): Socrates Athens 470 BC Plato Athens 428 BC Aristotle Stagira 384 BC Euclid Alexandria 325 BC Pythagoras Samos 570 BC for ( int i = 0 ; i <= greeks . GetUpperBound ( 0 ); i ++) { Console . WriteLine ( \"{0,-10} {1,-10} {2,10}\" , greeks [ i , 0 ], greeks [ i , 1 ], greeks [ i , 2 ]); } for r in greeks : print ( \" {0:10} {1:10} {2:>10} \" . format ( r [ 0 ], r [ 1 ], r [ 2 ])) In C#, multidimensional arrays cannot be traversed with the foreach loops which appear to flatten its structure. Currency formatting Console . WriteLine ( $ \"{123456.789:C }\" ); // $123,456.79 Console . WriteLine ( 123456.789d . ToString ( \"C\" )); // $123,456.79 f-string f \"$ { 123456.789 : ,.2f } \" # $123,456.79 locale module import locale def main (): \"\"\" Formatting a number in currency requires use of the **locale** module, and for the locale environment variables to be set. \"\"\" locale . setlocale ( locale . LC_ALL , 'en_US.UTF-8' ) locale . currency ( 123456.789 ) # $123456.79 if __name__ == \"__main__\" : main () TUI input validation loop Such a loop will continuously prompt for valid input, in this case an integer. Parse while ( true ): { try { int option = Int32 . Parse ( Console . ReadLine ()); } catch ( FormatException ) { // Input was not an integer } catch ( OverflowException ) { // Number was too big } } // Alternatively: while ( true ): { int option ; int . TryParse ( Console . ReadLine (), out option ); if ( option != null ) { // ... } else { break ;} } while True : try : option = int ( input ()) except ValueError : # Integer was not able to be parsed Terminal Color output Console . Color = ConsoleColor . Red ; Console . WriteLine ( \"Red!\" ) Console . ResetColor (); Console . Color = ConsoleColor . Green ; Console . WriteLine ( \"Green!\" ) Console . ResetColor (); Console . Color = ConsoleColor . Yellow ; Console . WriteLine ( \"Yellow!\" ) Console . ResetColor (); Console . Color = ConsoleColor . Blue ; Console . WriteLine ( \"Blue!\" ) Console . ResetColor (); Console . Color = ConsoleColor . Magenta ; Console . WriteLine ( \"Magenta!\" ) Console . ResetColor (); colorama print ( f \" { colorama . Fore . RED } Red! { colorama . Style . RESET_ALL } \" ) print ( f \" { colorama . Fore . GREEN } Green! { colorama . Style . RESET_ALL } \" ) print ( f \" { colorama . Fore . YELLOW } Yellow! { colorama . Style . RESET_ALL } \" ) print ( f \" { colorama . Fore . BLUE } Blue! { colorama . Style . RESET_ALL } \" ) print ( f \" { colorama . Fore . MAGENTA } Magenta! { colorama . Style . RESET_ALL } \" ) Parameterized \"Hello, world!\" using System.CommandLine ; using System.CommandLine.Invocation ; static int Main ( string [] args ) { var cmd = new RootCommand { new Argument < string >( \"name\" ), //, \"Your name\"), new Option < string? >( new [] { \"--greeting\" , \"-g\" }, \"The greeting to use\" ), }; cmd . Handler = CommandHandler . Create < string , string? >( HandleGreeting ); return cmd . Invoke ( args ); } static void HandleGreeting ( string? greeting , string name ) { Console . WriteLine ( $ \"{greeting}, {name}\" ); } import argparse def get_args (): parser = argparse . ArgumentParser ( description = \"Say hello\" ) parser . add_argument ( dest = \"name\" , metavar = \"name\" , default = \"World\" , help = \"Name to greet\" ) parser . add_argument ( \"--greeting\" , \"-g\" , dest = \"greeting\" , default = \"Hello\" , help = \"Greeting to use\" ) return parser . parse_args () def main (): args = get_args () print ( f \" { args . greeting } , { args . name } !\" ) if __name__ == \"__main__\" : main () Oxford comma using System.CommandLine ; using System.CommandLine.Invocation ; using System.Linq ; static int Main ( string [] args ) { var cmd = new RootCommand { new Argument < string []>( \"names\" ) }; cmd . Handler = CommandHandler . Create < string []>( Handler ); return cmd . Invoke ( args ); } static void Handler ( string [] names ) { Console . WriteLine ( $ \"{String.Join(\" , \", names.Take(names.Length -1))}, and {names.Last<string>()}\" ); } #!/usr/bin/env python3 import argparse def get_args (): p = argparse . ArgumentParser ( description = \"Listing args with Oxford comma\" ) p . add_argument ( \"words\" , nargs = \"+\" , help = \"Words to concatenate using Oxford comma\" ) return p . parse_args () def oxford_commafy ( words ): l = len ( words ) if l > 2 : words [ - 1 ] = f \"and { words [ - 1 ] } \" print ( \", \" . join ( words )) elif l == 2 : print ( f \" { words [ 0 ] } and { words [ 1 ] } \" ) else : print ( words [ 0 ]) def main (): args = get_args () . words oxford_commafy ( args ) if __name__ == \"__main__\" : main () Calculator (argparse) import argparse def get_args (): parser = argparse . ArgumentParser () parser . add_argument ( \"operand1\" , type = float ) parser . add_argument ( \"operand2\" , type = float ) op = parser . add_mutually_exclusive_group () op . add_argument ( \"-a\" , \"--add\" , dest = \"add\" , action = \"store_true\" ) op . add_argument ( \"-s\" , \"--subtract\" , action = \"store_true\" ) op . add_argument ( \"-d\" , \"--divide\" , action = \"store_true\" ) op . add_argument ( \"-m\" , \"--multiply\" , action = \"store_true\" ) return parser . parse_args () def main (): args = get_args () if args . add : print ( \"Adding\" ) print ( args . operand1 , \" + \" , args . operand2 , \" = \" , args . operand1 + args . operand2 ) elif args . subtract : print ( \"Subtracting\" ) print ( args . operand1 , \" - \" , args . operand2 , \" = \" , args . operand1 - args . operand2 ) elif args . divide : print ( \"Dividing\" ) print ( args . operand1 , \" / \" , args . operand2 , \" = \" , args . operand1 / args . operand2 ) elif args . multiply : print ( \"Multiplying\" ) print ( args . operand1 , \" * \" , args . operand2 , \" = \" , args . operand1 * args . operand2 ) else : print ( \"Unknown operation!\" ) if __name__ == \"__main__\" : main () Input validation string input ; int inputParsed ; while ( true ) { input = System . Console . ReadLine (); try { inputParsed = int . Parse ( input ); break ; } catch { System . Console . WriteLine ( \"Please input a number...\" ); } } System . Console . WriteLine ( $ \"Number provided: {inputParsed}\" ); Subcommands ./app command subcommand argument static int Main ( string [] args ) { var rootCommand = new RootCommand ( \"command\" ); var command = new Command ( \"subcommand\" ) { new Argument < string >( \"argument\" ); }; command . Handler = new CommandHandler . Create < string >( argumentHandler ); rootCommand . Add ( command ); rootCommand . Invoke ( args ); } private static void argumentHandler ( string argument ) { /* ... */ } Object-oriented programming DnD character Constructor class Character { private int StrengthAbility ; private int DexterityAbility ; private int ConstitutionAbility ; private int IntelligenceAbility ; private int WisdomAbility ; private int CharismaAbility ; private Race Race { get ; } public Character ( Race race ) { this . StrengthAbility = AbilityRoll (); this . DexterityAbility = AbilityRoll (); this . ConstitutionAbility = AbilityRoll (); this . IntelligenceAbility = AbilityRoll (); this . WisdomAbility = AbilityRoll (); this . CharismaAbility = AbilityRoll (); this . Race = race ; } public Character () : this ( Race . HUMAN ) { } } Properties public int Strength { get { return StrengthAbility + GetModifier ( StrengthAbility ) + GetRaceModifier ( Abilities . STRENGTH ); } } public int Dexterity { get { return DexterityAbility + GetModifier ( DexterityAbility ) + GetRaceModifier ( Abilities . DEXTERITY ); } } public int Constitution { get { return ConstitutionAbility + GetModifier ( ConstitutionAbility ) + GetRaceModifier ( Abilities . CONSTITUTION ); } } public int Intelligence { get { return IntelligenceAbility + GetModifier ( IntelligenceAbility ) + GetRaceModifier ( Abilities . INTELLIGENCE ); } } public int Wisdom { get { return WisdomAbility + GetModifier ( WisdomAbility ) + GetRaceModifier ( Abilities . WISDOM ); } } public int Charisma { get { return CharismaAbility + GetModifier ( CharismaAbility ) + GetRaceModifier ( Abilities . CHARISMA ); } } Methods public void Report () { Console . Write ( $ \"Strength: {Strength,2}\" ); Console . Write ( $ \"Dexterity: {Dexterity,2}\" ); Console . Write ( $ \"Constitution: {Constitution,2}\" ); Console . Write ( $ \"Intelligence: {Intelligence,2}\" ); Console . Write ( $ \"Wisdom: {Wisdom,2}\" ); Console . Write ( $ \"Charisma: {Charisma,2}\" ); } static int Roll ( int ceiling ) { Random rng = new Random (); return rng . Next ( 1 , ceiling ); } static int AbilityRoll () { List < int > rolls = new List < int > { Roll ( 6 ), Roll ( 6 ), Roll ( 6 ), Roll ( 6 ) }; rolls . Remove ( rolls . Min ()); return rolls . Sum (); } public static int GetModifier ( int ability ) { return ( int ) System . Math . Floor ((( double ) ability - 10 ) / 2 ); } Constructor class Character : def __init__ ( self , race : Race = Race . HUMAN ): self . _strength_ability = self . ability_roll () self . _dexterity_ability = self . ability_roll () self . _constitution_ability = self . ability_roll () self . _intelligence_ability = self . ability_roll () self . _wisdom_ability = self . ability_roll () self . _charisma_ability = self . ability_roll () self . _race = race Properties @property def Strength ( self ): return self . _strength_ability + self . get_modifier ( self . _strength_ability ) @property def Dexterity ( self ): return self . _dexterity_ability + self . get_modifier ( self . _dexterity_ability ) @property def Constitution ( self ): return self . _constitution_ability + self . get_modifier ( self . _constitution_ability ) @property def Intelligence ( self ): return self . _intelligence_ability + self . get_modifier ( self . _intelligence_ability ) @property def Wisdom ( self ): return self . _wisdom_ability + self . get_modifier ( self . _wisdom_ability ) @property def Charisma ( self ): return self . _charisma_ability + self . get_modifier ( self . _charisma_ability ) Methods @staticmethod def Roll ( range : int = 6 ): return random . randrange ( range ) + 1 @staticmethod def get_modifier ( score : int ): return math . floor (( score - 10 ) / 2 ) @classmethod def ability_roll ( cls ): rolls = [ cls . Roll (), cls . Roll (), cls . Roll (), cls . Roll ()] rolls . remove ( min ( rolls )) return sum ( rolls ) def report ( self ): print ( f \"Strength: { self . Strength } \" ) print ( f \"Dexterity: { self . Dexterity } \" ) print ( f \"Constitution: { self . Constitution } \" ) print ( f \"Intelligence: { self . Intelligence } \" ) print ( f \"Wisdom: { self . Wisdom } \" ) print ( f \"Charisma: { self . Charisma } \" ) RPG character generator Player class #include <string> class Player { protected : std :: string _name { \"Johnny Bravo\" }; Race _race { Race :: HUMAN }; int _hp { 100 }; int _mp { 100 }; public : Player ( std :: string n , Race r , int hp , int mp ) : _name { n }, _race { r }, _hp ( hp ), _mp ( mp ) {} virtual std :: string attack () = 0 ; int getHp () { return _hp ; } int getMp () { return _mp ; } std :: string getRace () { switch ( _race ) { case 0 : return \"human\" ; break ; case 1 : return \"elf\" ; break ; case 2 : return \"dwarf\" ; break ; default : return \"none\" ; break ; } } std :: string getName () { return _name ; } void setHp ( int n ) { _hp = n ; } void setMp ( int n ) { _mp = n ; } void setName ( std :: string s ) { _name = s ; } void setRace ( Race r ) { _race = r ;} }; class Player (): def __init__ ( self , name : str , race : Race , hp : int , mp : int ): self . _name = name self . _race = race self . _hp = hp self . _mp = mp @property def getName ( self ): return self . _name @property def getRace ( self ): return self . _race @property def getHp ( self ): return self . _hp @property def getMp ( self ): return self . _mp def attack ( self ): return \"Have at thee!\" Subclasses class Warrior : public Player { public : Warrior ( std :: string n , Race r ) : Player ( n , r , 200 , 0 ) {} std :: string attack () { return \"I will destroy you with my sword, foul demon!\" ;} }; class Priest : public Player { public : Priest ( std :: string n , Race r ) : Player ( n , r , 100 , 200 ) {} std :: string attack () { return \"Taste the wrath of the Two True Gods!\" ;} }; class Mage : public Player { public : Mage ( std :: string n , Race r ) : Player ( n , r , 150 , 150 ) {} std :: string attack () { return \"You are overmatched by my esoteric artifices!\" ;} }; class Warrior ( Player ): def __init__ ( self , name : str , race : Race ): super () . __init__ ( name , race , 200 , 0 ) def attack ( self ): return \"I will destroy with my sword, foul demon!\" class Priest ( Player ): def __init__ ( self , name : str , race : Race ): super () . __init__ ( name , race , 100 , 200 ) def attack ( self ): return \"Taste the wrath of the Two True Gods!\" class Mage ( Player ): def __init__ ( self , name : str , race : Race ): super () . __init__ ( name , race , 150 , 150 ) def attack ( self ): return \"You are overmatched by my esoteric artifices!\" Race enum Race { HUMAN , ELF , DWARF }; import enum class Race ( enum . Enum ): HUMAN = enum . auto (), ELF = enum . auto (), DWARF = enum . auto () Starships This demonstration project provides a scenario for implementing OOP and TDD principles in a variety of languages and implementations. Oficer and Starship are simple classes with intuitive properties and fields. The StarshipClass enum defines the available starship types. Fleet serves as a container for Starships. Officer public interface IOfficer { string FirstName { get ; set ; } string LastName { get ; set ; } DateTime BirthDate { get ; set ; } char Grade { get ; set ; } string Name { get ; } } public class Officer : IOfficer { public string FirstName { get ; set ; } public string LastName { get ; set ; } public DateTime BirthDate { get ; set ; } public string Name { get { return $ \"{FirstName} {LastName}\" ; } } public char Grade { get ; set ; } } class Captain { [string] $FirstName [string] $LastName [DateTime] $BirthDate Captain ( [string] $n1 [string] $n2 ) { $this . FirstName = $n1 $this . LastName = $n2 } [string] Name () { return \" $( $this . FirstName ) $( $this . LastName ) \" } } #include <string> class Officer { private : std :: string _firstName {}; std :: string _lastName {}; std :: string Name () { return _firstName + _lastName ; } } Starship public interface IStarship { string Name { get ; set ; } string Registry { get ; set ; } int Crew { get ; set ; } StarshipClass StarshipClass { get ; set ; } IOfficer Captain { get ; set ; } } public class Starship : IStarship { public string Name { get ; set ; } public string Registry { get ; set ; } public int Crew { get ; set ; } public StarshipClass StarshipClass { get ; set ; } public IOfficer Captain { get ; set ; } } class Starship { private : std :: string _name {}; std :: string _reg {}; StarshipClass _starshipclass {}; int _crew {}; public : Starship () = default ; Starship ( std :: string n , std :: string r , StarshipClass cls ) : _name { n }, _reg { r }, _starshipclass { cls } {}; ~ Starship () = default ; int getCrew () { return _crew ; } std :: string getName () { return _name ; } std :: string getRegistry () { return _reg ; } std :: string getClass (); }; std :: string Starship :: getClass () { switch ( _starshipclass ) { case 2 : return \"Constitution\" ; break ; default : return \"Other\" ; break ; } } #endif // STARSHIP_H class Starship : def __init__ ( self , name = None , starshipclass : StarshipClass = StarshipClass . NX , registry = None , crew = 0 , ): self . name = name self . registry = registry self . _crew = crew self . crew_on_leave = 0 self . _starshipclass = starshipclass @property def crew ( self ): return self . _crew @crew . setter def crew ( self , crew : int ): if crew < 0 : raise Exception else : self . _crew = crew @property def starshipclass ( self ): return self . _starshipclass @starshipclass . setter def starshipclass ( self , starshipclass : StarshipClass ): if starshipclass not in StarshipClass : raise Exception else : self . _starshipclass = starshipclass StarshipClass public enum StarshipClass { NX , GALAXY , CONSTITUTION , SOVEREIGN , DEFIANT , INTREPID , MIRANDA } enum StarshipClass { NX = 0 GALAXY = 1 , CONSTITUTION = 2 , SOVEREIGN = 3 , DEFIANT = 4 INTREPID = 5 MIRANDA = 6 }; from enum import Enum class StarshipClass ( Enum ): NX = 'NX' GALAXY = 'Galaxy' CONSTITUTION = 'Constitution' SOVEREIGN = 'Sovereign' DEFIANT = 'Defiant' INTREPID = 'Intrepid' MIRANDA = 'Miranda' A Captain is paired with a Starship to form a StarshipDeployment . And the CaptainSelector class is passed to StarshipDeployment by dependency injection. CaptainSelector evaluates whether the Officer provided has what it takes to ply the inky black. This boils down to a check on the Grade property, which is simple to test in testing frameworks where a mocked Officer object can be set up with unsatisfactory Grade values. public class CaptainSelector { public IOfficer Officer { get ; set ; } public CaptainSelector ( IOfficer officer ) { Officer = officer ; } public bool Evaluate () { return Officer . Grade == 'A' ? true : false ; } } StarshipValidator makes a few key checks on the Starship object that is passed in upon instantiation. These checks provide opportunities to mock Starship and Officer objects. IsCaptained() checks if the Starship has a Captain assigned ValidateRegistry() makes sure the Starship's registry number begins with NCC or NX Evaluate() runs all the other methods in the class and returns True only if all checks pass. public class StarshipValidator : IStarshipValidator { public IStarship Starship { get ; set ; } public bool IsCaptained () { return Starship . Captain != null ? true : false ; } public bool ValidateRegistry () { return Starship . Registry . StartsWith ( \"NCC\" ) || Starship . Registry . StartsWith ( \"NX\" ) ? true : false ; } public bool Evaluate () { return ValidateRegistry () && IsCaptained (); } } StarshipDeployment takes a StarshipValidator object by dependency injection, which it uses to perform checks on a given Starship upon invocation of Deploy , which returns a StarshipMission object. This provides the opportunity to test a mocked validator for invocation of the Evaluate() method. public class StarshipDeployment { public IStarshipValidator StarshipValidator { get ; set ; } public StarshipDeployment ( IStarshipValidator validator ) { StarshipValidator = validator ?? throw new ArgumentNullException ( nameof ( validator )); } public bool ValidateDestination ( string destination ) { return destination . Length > 1 ? true : false ; } public StarshipMission Deploy ( Starship starship , string destination ) { bool destinationValidated = ValidateDestination ( destination ); bool starshipValidated = StarshipValidator . Evaluate (); return destinationValidated && starshipValidated ? new StarshipMission { Starship = starship as Starship , Destination = destination } : throw new ArgumentException (); } } Glossary C \"A programming language is low level when its programs require attention to the irrelevant.\" -Alan Perlis Despite C's reputation as a low-level programming language, in fact it merely emulates the ancient PDP-11, which is the only machine for which its abstract machine can be described as \"close to the metal\". In the age of parallel processes, C's serial nature... Sources: C is not a low-level programming language Enumeration In C# the term enumeration refers to the process of successively returning individual values. In Python, the term iteration is used to refer to the same thing, and iterable refers to an object that can be iterated, or parsed out into sub-elements. In Python, any object that exposes the __iter__() and __next__() dunder methods are iterable. In C#, IEnumerable The IEnumerable interface implements enumeration. Both languages feature a keyword that allows a subclass to access its direct parent. Whereas in Python the terms superclass and subclass are used, in C# the terms base class and derived class are preferred. Go Golang or \" Go \" is a fast, high-performance, open-source, statically-typed compiled programming language. It was designed at Google by Rob Pike, Robert Giesemer, and Ken Thompson and first appeared in late 2009. Go has a syntax similar to C, but offers additional features such as memory safety, garbage collection, and others. [ 2 ]\\ Hello world package main import \"fmt\" func main () { fmt . Println ( \"Hello, Go World!\" ) } Compile code [ 3 ] go build script.go # compiles to a binary executable in the same directory named \"script\" go run script.go # compiles and runs the program Mathematical function [ 3 ] package main import ( \"fmt\" \"math\" ) func main () { fmt . Println ( math . Max ( 9 , 5 )) } Get a GitHub package named $REPO by $AUTHOR go get github.com/ $AUTHOR / $REPO go get gopkg.in/kyokomi/emoji.v1 # Emoji support Evaluate type of data package main import ( \"fmt\" \"reflect\" ) func main () { fmt . Println ( reflect . TypeOf ( 1 )) // => int fmt . Println ( reflect . TypeOf ( 9.5 )) // => float64 fmt . Println ( reflect . TypeOf ( \"string\" )) // => string fmt . Println ( reflect . TypeOf ( true )) // => bool } Sources: Go language for beginners in 16 parts Loop unswitching One of the core optimizations that a C compiler performs; transforms a loop containing a conditional into a conditional with a loop in both parts, which changes flow control Register rename engine Component of modern high-end cores which is one of the largest consumers of die area and power Ruby REPL shell irb Begin a function definition def End a function definition end Import {package}, or 'gem' require package Write given objects to ios ; writes newline after any that do not already have one puts ( * obj ) Write given objects to ios , with no newline print ( * obj ) Sort in-place array . sort () Rust cargo Rust's compilation manager, package manager, and general-purpose tool rustc Rust compiler, usually invoked by cargo rustdoc Rust documentation tool Start a new package directory hello cargo new --bin hello Load the locally-stored Rust book \"The Rust Programming Language\" rustup doc --book -> precedes return data type mut mutable, when preceding variable identifiers, allows their values to be changed Data types include: - u64 unsigned 64-bit integer - i32 signed 32-bit integer - u8 unsigned 8-bit integer (byte values) - f32 single-precision floating point - f64 double-precision floating point Scalar Replacement Of Aggregates Scalar Replacement Of Aggregates (SROA) is one of the core optimizations that a C compiler performs; attempts to replace struct s and arrays with fixed lengths with individual variables, which allows the compiler to treat accesses as independent and elide operations entirely if it can prove the results are never visible, which also deletes padding sometimes. Segmented architecture Pointers might be segment IDs and an offset","title":"\ud83d\udc68\u200d\ud83d\udcbb Coding"},{"location":"Coding/#coding","text":"","title":"\ud83d\udc68\u200d\ud83d\udcbb Coding"},{"location":"Coding/#simple-structures","text":"","title":"Simple structures"},{"location":"Coding/#hello-world","text":"using System ; class Program { static void Main () { Console . WriteLine ( \"Hello World!\" ); } } def main (): \"Hello, world!\" ) if __name__ == \"__main__\" : main ()","title":"Hello, world!"},{"location":"Coding/#dates-and-times","text":"Parse a date string string rawDate = \"07/04/1776\" ; try { DateTime parsedDate = DateTime . Parse ( rawDate ); } catch ( FormatException ) { Console . WriteLine ( \"Unparsable!\" ) } // Alternatively: DateTime . TryParse ( rawDate , out parsedDate ); Console . WriteLine ( parsedDate . ToLongDateString ()); // => \"July 4, 1776\"","title":"Dates and times"},{"location":"Coding/#file-operations","text":"","title":"File operations"},{"location":"Coding/#write-text-file","text":"using ( StreamWriter writer = File . CreateText ( \"test.txt\" )) { writer . WriteLine ( \"Hello, world!\" ); } with open ( 'text' , 'w' ) as f : f . write ( 'Hello, world!' )","title":"Write text file"},{"location":"Coding/#read-text-file","text":"using System.IO ; string raven = File . ReadAllText ( \"raven\" ); using System.IO ; string [] raven = File . ReadAllLines ( \"raven\" ); using System.IO ; using ( StreamReader reader = File . OpenText ( \"raven\" )) { reader . ReadToEnd (); } using System.IO ; using ( StreamReader reader = File . OpenText ( \"raven\" )) { string s ; while (( s = sr . ReadLine ()) != null ) { Console . WriteLine ( s ); } } with open ( 'raven' ) as f : f . readlines ()","title":"Read text file"},{"location":"Coding/#copy-file","text":"using System.IO ; File . Copy ( ' raven ' , ' raven . bak ' ) import shutil shutil . copyfile ( 'raven' , 'raven.bak' ) cp ./raven { ,.bak } Copy-Item .\\ raven .\\ raven . bak","title":"Copy file"},{"location":"Coding/#copy-file-overwriting","text":"using System.IO ; File . Copy ( ' raven ' , ' raven . bak ' , true ) import shutil shutil . copyfile ( 'raven' , 'raven.bak' ) cp -f ./raven { ,.bak } Copy-Item -Force .\\ raven .\\ raven . bak","title":"Copy file, overwriting"},{"location":"Coding/#move-file","text":"using System.IO ; File . Move ( ' raven ' , ' raven . bak ' );","title":"Move file"},{"location":"Coding/#csv","text":"using System ; using System.IO ; using CsvHelper ; struct Greek { public string name { get ; set ; } public string city { get ; set ; } public string dob { get ; set ; } } class Program { static void Main ( string [] args ) { using ( StreamReader reader = new StreamReader ( \"greeks.csv\" )) { CsvReader csvreader = new CsvReader ( reader , System . Globalization . CultureInfo . InvariantCulture ); var data = csvreader . GetRecords < Greek >(); foreach ( Greek item in data ) { Console . WriteLine ( $ \"{item.name,-15} {item.city,-15} {item.dob,-15}\" ); } } } } import csv with open ( \"greeks.csv\" ) as f : r = csv . reader ( f ) headers = next ( r ) data = [ row for row in r ]","title":"CSV"},{"location":"Coding/#json","text":"using ( var stream = await storageFile . OpenAsync ( FileAccessMode . Read )) { using ( var dataReader = new DataReader ( stream )) { await dataReader . LoadAsync (( uint ) stream . Size ); var json = dataReader . ReadString (( uint ) stream . Size ); customerList = JsonConvert . DeserializeObject < List < Customer >>( json ); } }","title":"JSON"},{"location":"Coding/#random-numbers","text":"Integer Random r = new System . Random (); int result = r . Next ( 1 , 6 ); import random random . randrange ( 1 , 6 ) Real number Random r = new System . Random (); int result = r . NextDouble (); import random random . random ()","title":"Random numbers"},{"location":"Coding/#double-array","text":"primes = [ 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 ] double = lambda x : 2 * x list ( map ( double , primes )) # Alternatively: [ 2 * el for el in primes ]","title":"Double array"},{"location":"Coding/#string-formatting","text":"Desired output (10 spaces per column, with the third right-justified): Socrates Athens 470 BC Plato Athens 428 BC Aristotle Stagira 384 BC Euclid Alexandria 325 BC Pythagoras Samos 570 BC for ( int i = 0 ; i <= greeks . GetUpperBound ( 0 ); i ++) { Console . WriteLine ( \"{0,-10} {1,-10} {2,10}\" , greeks [ i , 0 ], greeks [ i , 1 ], greeks [ i , 2 ]); } for r in greeks : print ( \" {0:10} {1:10} {2:>10} \" . format ( r [ 0 ], r [ 1 ], r [ 2 ])) In C#, multidimensional arrays cannot be traversed with the foreach loops which appear to flatten its structure. Currency formatting Console . WriteLine ( $ \"{123456.789:C }\" ); // $123,456.79 Console . WriteLine ( 123456.789d . ToString ( \"C\" )); // $123,456.79 f-string f \"$ { 123456.789 : ,.2f } \" # $123,456.79 locale module import locale def main (): \"\"\" Formatting a number in currency requires use of the **locale** module, and for the locale environment variables to be set. \"\"\" locale . setlocale ( locale . LC_ALL , 'en_US.UTF-8' ) locale . currency ( 123456.789 ) # $123456.79 if __name__ == \"__main__\" : main ()","title":"String formatting"},{"location":"Coding/#tui-input-validation-loop","text":"Such a loop will continuously prompt for valid input, in this case an integer. Parse while ( true ): { try { int option = Int32 . Parse ( Console . ReadLine ()); } catch ( FormatException ) { // Input was not an integer } catch ( OverflowException ) { // Number was too big } } // Alternatively: while ( true ): { int option ; int . TryParse ( Console . ReadLine (), out option ); if ( option != null ) { // ... } else { break ;} } while True : try : option = int ( input ()) except ValueError : # Integer was not able to be parsed","title":"TUI input validation loop"},{"location":"Coding/#terminal","text":"","title":"Terminal"},{"location":"Coding/#color-output","text":"Console . Color = ConsoleColor . Red ; Console . WriteLine ( \"Red!\" ) Console . ResetColor (); Console . Color = ConsoleColor . Green ; Console . WriteLine ( \"Green!\" ) Console . ResetColor (); Console . Color = ConsoleColor . Yellow ; Console . WriteLine ( \"Yellow!\" ) Console . ResetColor (); Console . Color = ConsoleColor . Blue ; Console . WriteLine ( \"Blue!\" ) Console . ResetColor (); Console . Color = ConsoleColor . Magenta ; Console . WriteLine ( \"Magenta!\" ) Console . ResetColor (); colorama print ( f \" { colorama . Fore . RED } Red! { colorama . Style . RESET_ALL } \" ) print ( f \" { colorama . Fore . GREEN } Green! { colorama . Style . RESET_ALL } \" ) print ( f \" { colorama . Fore . YELLOW } Yellow! { colorama . Style . RESET_ALL } \" ) print ( f \" { colorama . Fore . BLUE } Blue! { colorama . Style . RESET_ALL } \" ) print ( f \" { colorama . Fore . MAGENTA } Magenta! { colorama . Style . RESET_ALL } \" )","title":"Color output"},{"location":"Coding/#parameterized-hello-world","text":"using System.CommandLine ; using System.CommandLine.Invocation ; static int Main ( string [] args ) { var cmd = new RootCommand { new Argument < string >( \"name\" ), //, \"Your name\"), new Option < string? >( new [] { \"--greeting\" , \"-g\" }, \"The greeting to use\" ), }; cmd . Handler = CommandHandler . Create < string , string? >( HandleGreeting ); return cmd . Invoke ( args ); } static void HandleGreeting ( string? greeting , string name ) { Console . WriteLine ( $ \"{greeting}, {name}\" ); } import argparse def get_args (): parser = argparse . ArgumentParser ( description = \"Say hello\" ) parser . add_argument ( dest = \"name\" , metavar = \"name\" , default = \"World\" , help = \"Name to greet\" ) parser . add_argument ( \"--greeting\" , \"-g\" , dest = \"greeting\" , default = \"Hello\" , help = \"Greeting to use\" ) return parser . parse_args () def main (): args = get_args () print ( f \" { args . greeting } , { args . name } !\" ) if __name__ == \"__main__\" : main ()","title":"Parameterized \"Hello, world!\""},{"location":"Coding/#oxford-comma","text":"using System.CommandLine ; using System.CommandLine.Invocation ; using System.Linq ; static int Main ( string [] args ) { var cmd = new RootCommand { new Argument < string []>( \"names\" ) }; cmd . Handler = CommandHandler . Create < string []>( Handler ); return cmd . Invoke ( args ); } static void Handler ( string [] names ) { Console . WriteLine ( $ \"{String.Join(\" , \", names.Take(names.Length -1))}, and {names.Last<string>()}\" ); } #!/usr/bin/env python3 import argparse def get_args (): p = argparse . ArgumentParser ( description = \"Listing args with Oxford comma\" ) p . add_argument ( \"words\" , nargs = \"+\" , help = \"Words to concatenate using Oxford comma\" ) return p . parse_args () def oxford_commafy ( words ): l = len ( words ) if l > 2 : words [ - 1 ] = f \"and { words [ - 1 ] } \" print ( \", \" . join ( words )) elif l == 2 : print ( f \" { words [ 0 ] } and { words [ 1 ] } \" ) else : print ( words [ 0 ]) def main (): args = get_args () . words oxford_commafy ( args ) if __name__ == \"__main__\" : main ()","title":"Oxford comma"},{"location":"Coding/#calculator","text":"(argparse) import argparse def get_args (): parser = argparse . ArgumentParser () parser . add_argument ( \"operand1\" , type = float ) parser . add_argument ( \"operand2\" , type = float ) op = parser . add_mutually_exclusive_group () op . add_argument ( \"-a\" , \"--add\" , dest = \"add\" , action = \"store_true\" ) op . add_argument ( \"-s\" , \"--subtract\" , action = \"store_true\" ) op . add_argument ( \"-d\" , \"--divide\" , action = \"store_true\" ) op . add_argument ( \"-m\" , \"--multiply\" , action = \"store_true\" ) return parser . parse_args () def main (): args = get_args () if args . add : print ( \"Adding\" ) print ( args . operand1 , \" + \" , args . operand2 , \" = \" , args . operand1 + args . operand2 ) elif args . subtract : print ( \"Subtracting\" ) print ( args . operand1 , \" - \" , args . operand2 , \" = \" , args . operand1 - args . operand2 ) elif args . divide : print ( \"Dividing\" ) print ( args . operand1 , \" / \" , args . operand2 , \" = \" , args . operand1 / args . operand2 ) elif args . multiply : print ( \"Multiplying\" ) print ( args . operand1 , \" * \" , args . operand2 , \" = \" , args . operand1 * args . operand2 ) else : print ( \"Unknown operation!\" ) if __name__ == \"__main__\" : main ()","title":"Calculator"},{"location":"Coding/#input-validation","text":"string input ; int inputParsed ; while ( true ) { input = System . Console . ReadLine (); try { inputParsed = int . Parse ( input ); break ; } catch { System . Console . WriteLine ( \"Please input a number...\" ); } } System . Console . WriteLine ( $ \"Number provided: {inputParsed}\" );","title":"Input validation"},{"location":"Coding/#subcommands","text":"./app command subcommand argument static int Main ( string [] args ) { var rootCommand = new RootCommand ( \"command\" ); var command = new Command ( \"subcommand\" ) { new Argument < string >( \"argument\" ); }; command . Handler = new CommandHandler . Create < string >( argumentHandler ); rootCommand . Add ( command ); rootCommand . Invoke ( args ); } private static void argumentHandler ( string argument ) { /* ... */ }","title":"Subcommands"},{"location":"Coding/#object-oriented-programming","text":"","title":"Object-oriented programming"},{"location":"Coding/#dnd-character","text":"Constructor class Character { private int StrengthAbility ; private int DexterityAbility ; private int ConstitutionAbility ; private int IntelligenceAbility ; private int WisdomAbility ; private int CharismaAbility ; private Race Race { get ; } public Character ( Race race ) { this . StrengthAbility = AbilityRoll (); this . DexterityAbility = AbilityRoll (); this . ConstitutionAbility = AbilityRoll (); this . IntelligenceAbility = AbilityRoll (); this . WisdomAbility = AbilityRoll (); this . CharismaAbility = AbilityRoll (); this . Race = race ; } public Character () : this ( Race . HUMAN ) { } } Properties public int Strength { get { return StrengthAbility + GetModifier ( StrengthAbility ) + GetRaceModifier ( Abilities . STRENGTH ); } } public int Dexterity { get { return DexterityAbility + GetModifier ( DexterityAbility ) + GetRaceModifier ( Abilities . DEXTERITY ); } } public int Constitution { get { return ConstitutionAbility + GetModifier ( ConstitutionAbility ) + GetRaceModifier ( Abilities . CONSTITUTION ); } } public int Intelligence { get { return IntelligenceAbility + GetModifier ( IntelligenceAbility ) + GetRaceModifier ( Abilities . INTELLIGENCE ); } } public int Wisdom { get { return WisdomAbility + GetModifier ( WisdomAbility ) + GetRaceModifier ( Abilities . WISDOM ); } } public int Charisma { get { return CharismaAbility + GetModifier ( CharismaAbility ) + GetRaceModifier ( Abilities . CHARISMA ); } } Methods public void Report () { Console . Write ( $ \"Strength: {Strength,2}\" ); Console . Write ( $ \"Dexterity: {Dexterity,2}\" ); Console . Write ( $ \"Constitution: {Constitution,2}\" ); Console . Write ( $ \"Intelligence: {Intelligence,2}\" ); Console . Write ( $ \"Wisdom: {Wisdom,2}\" ); Console . Write ( $ \"Charisma: {Charisma,2}\" ); } static int Roll ( int ceiling ) { Random rng = new Random (); return rng . Next ( 1 , ceiling ); } static int AbilityRoll () { List < int > rolls = new List < int > { Roll ( 6 ), Roll ( 6 ), Roll ( 6 ), Roll ( 6 ) }; rolls . Remove ( rolls . Min ()); return rolls . Sum (); } public static int GetModifier ( int ability ) { return ( int ) System . Math . Floor ((( double ) ability - 10 ) / 2 ); } Constructor class Character : def __init__ ( self , race : Race = Race . HUMAN ): self . _strength_ability = self . ability_roll () self . _dexterity_ability = self . ability_roll () self . _constitution_ability = self . ability_roll () self . _intelligence_ability = self . ability_roll () self . _wisdom_ability = self . ability_roll () self . _charisma_ability = self . ability_roll () self . _race = race Properties @property def Strength ( self ): return self . _strength_ability + self . get_modifier ( self . _strength_ability ) @property def Dexterity ( self ): return self . _dexterity_ability + self . get_modifier ( self . _dexterity_ability ) @property def Constitution ( self ): return self . _constitution_ability + self . get_modifier ( self . _constitution_ability ) @property def Intelligence ( self ): return self . _intelligence_ability + self . get_modifier ( self . _intelligence_ability ) @property def Wisdom ( self ): return self . _wisdom_ability + self . get_modifier ( self . _wisdom_ability ) @property def Charisma ( self ): return self . _charisma_ability + self . get_modifier ( self . _charisma_ability ) Methods @staticmethod def Roll ( range : int = 6 ): return random . randrange ( range ) + 1 @staticmethod def get_modifier ( score : int ): return math . floor (( score - 10 ) / 2 ) @classmethod def ability_roll ( cls ): rolls = [ cls . Roll (), cls . Roll (), cls . Roll (), cls . Roll ()] rolls . remove ( min ( rolls )) return sum ( rolls ) def report ( self ): print ( f \"Strength: { self . Strength } \" ) print ( f \"Dexterity: { self . Dexterity } \" ) print ( f \"Constitution: { self . Constitution } \" ) print ( f \"Intelligence: { self . Intelligence } \" ) print ( f \"Wisdom: { self . Wisdom } \" ) print ( f \"Charisma: { self . Charisma } \" )","title":"DnD character"},{"location":"Coding/#rpg-character-generator","text":"Player class #include <string> class Player { protected : std :: string _name { \"Johnny Bravo\" }; Race _race { Race :: HUMAN }; int _hp { 100 }; int _mp { 100 }; public : Player ( std :: string n , Race r , int hp , int mp ) : _name { n }, _race { r }, _hp ( hp ), _mp ( mp ) {} virtual std :: string attack () = 0 ; int getHp () { return _hp ; } int getMp () { return _mp ; } std :: string getRace () { switch ( _race ) { case 0 : return \"human\" ; break ; case 1 : return \"elf\" ; break ; case 2 : return \"dwarf\" ; break ; default : return \"none\" ; break ; } } std :: string getName () { return _name ; } void setHp ( int n ) { _hp = n ; } void setMp ( int n ) { _mp = n ; } void setName ( std :: string s ) { _name = s ; } void setRace ( Race r ) { _race = r ;} }; class Player (): def __init__ ( self , name : str , race : Race , hp : int , mp : int ): self . _name = name self . _race = race self . _hp = hp self . _mp = mp @property def getName ( self ): return self . _name @property def getRace ( self ): return self . _race @property def getHp ( self ): return self . _hp @property def getMp ( self ): return self . _mp def attack ( self ): return \"Have at thee!\" Subclasses class Warrior : public Player { public : Warrior ( std :: string n , Race r ) : Player ( n , r , 200 , 0 ) {} std :: string attack () { return \"I will destroy you with my sword, foul demon!\" ;} }; class Priest : public Player { public : Priest ( std :: string n , Race r ) : Player ( n , r , 100 , 200 ) {} std :: string attack () { return \"Taste the wrath of the Two True Gods!\" ;} }; class Mage : public Player { public : Mage ( std :: string n , Race r ) : Player ( n , r , 150 , 150 ) {} std :: string attack () { return \"You are overmatched by my esoteric artifices!\" ;} }; class Warrior ( Player ): def __init__ ( self , name : str , race : Race ): super () . __init__ ( name , race , 200 , 0 ) def attack ( self ): return \"I will destroy with my sword, foul demon!\" class Priest ( Player ): def __init__ ( self , name : str , race : Race ): super () . __init__ ( name , race , 100 , 200 ) def attack ( self ): return \"Taste the wrath of the Two True Gods!\" class Mage ( Player ): def __init__ ( self , name : str , race : Race ): super () . __init__ ( name , race , 150 , 150 ) def attack ( self ): return \"You are overmatched by my esoteric artifices!\" Race enum Race { HUMAN , ELF , DWARF }; import enum class Race ( enum . Enum ): HUMAN = enum . auto (), ELF = enum . auto (), DWARF = enum . auto ()","title":"RPG character generator"},{"location":"Coding/#starships","text":"This demonstration project provides a scenario for implementing OOP and TDD principles in a variety of languages and implementations. Oficer and Starship are simple classes with intuitive properties and fields. The StarshipClass enum defines the available starship types. Fleet serves as a container for Starships. Officer public interface IOfficer { string FirstName { get ; set ; } string LastName { get ; set ; } DateTime BirthDate { get ; set ; } char Grade { get ; set ; } string Name { get ; } } public class Officer : IOfficer { public string FirstName { get ; set ; } public string LastName { get ; set ; } public DateTime BirthDate { get ; set ; } public string Name { get { return $ \"{FirstName} {LastName}\" ; } } public char Grade { get ; set ; } } class Captain { [string] $FirstName [string] $LastName [DateTime] $BirthDate Captain ( [string] $n1 [string] $n2 ) { $this . FirstName = $n1 $this . LastName = $n2 } [string] Name () { return \" $( $this . FirstName ) $( $this . LastName ) \" } } #include <string> class Officer { private : std :: string _firstName {}; std :: string _lastName {}; std :: string Name () { return _firstName + _lastName ; } } Starship public interface IStarship { string Name { get ; set ; } string Registry { get ; set ; } int Crew { get ; set ; } StarshipClass StarshipClass { get ; set ; } IOfficer Captain { get ; set ; } } public class Starship : IStarship { public string Name { get ; set ; } public string Registry { get ; set ; } public int Crew { get ; set ; } public StarshipClass StarshipClass { get ; set ; } public IOfficer Captain { get ; set ; } } class Starship { private : std :: string _name {}; std :: string _reg {}; StarshipClass _starshipclass {}; int _crew {}; public : Starship () = default ; Starship ( std :: string n , std :: string r , StarshipClass cls ) : _name { n }, _reg { r }, _starshipclass { cls } {}; ~ Starship () = default ; int getCrew () { return _crew ; } std :: string getName () { return _name ; } std :: string getRegistry () { return _reg ; } std :: string getClass (); }; std :: string Starship :: getClass () { switch ( _starshipclass ) { case 2 : return \"Constitution\" ; break ; default : return \"Other\" ; break ; } } #endif // STARSHIP_H class Starship : def __init__ ( self , name = None , starshipclass : StarshipClass = StarshipClass . NX , registry = None , crew = 0 , ): self . name = name self . registry = registry self . _crew = crew self . crew_on_leave = 0 self . _starshipclass = starshipclass @property def crew ( self ): return self . _crew @crew . setter def crew ( self , crew : int ): if crew < 0 : raise Exception else : self . _crew = crew @property def starshipclass ( self ): return self . _starshipclass @starshipclass . setter def starshipclass ( self , starshipclass : StarshipClass ): if starshipclass not in StarshipClass : raise Exception else : self . _starshipclass = starshipclass StarshipClass public enum StarshipClass { NX , GALAXY , CONSTITUTION , SOVEREIGN , DEFIANT , INTREPID , MIRANDA } enum StarshipClass { NX = 0 GALAXY = 1 , CONSTITUTION = 2 , SOVEREIGN = 3 , DEFIANT = 4 INTREPID = 5 MIRANDA = 6 }; from enum import Enum class StarshipClass ( Enum ): NX = 'NX' GALAXY = 'Galaxy' CONSTITUTION = 'Constitution' SOVEREIGN = 'Sovereign' DEFIANT = 'Defiant' INTREPID = 'Intrepid' MIRANDA = 'Miranda' A Captain is paired with a Starship to form a StarshipDeployment . And the CaptainSelector class is passed to StarshipDeployment by dependency injection. CaptainSelector evaluates whether the Officer provided has what it takes to ply the inky black. This boils down to a check on the Grade property, which is simple to test in testing frameworks where a mocked Officer object can be set up with unsatisfactory Grade values. public class CaptainSelector { public IOfficer Officer { get ; set ; } public CaptainSelector ( IOfficer officer ) { Officer = officer ; } public bool Evaluate () { return Officer . Grade == 'A' ? true : false ; } } StarshipValidator makes a few key checks on the Starship object that is passed in upon instantiation. These checks provide opportunities to mock Starship and Officer objects. IsCaptained() checks if the Starship has a Captain assigned ValidateRegistry() makes sure the Starship's registry number begins with NCC or NX Evaluate() runs all the other methods in the class and returns True only if all checks pass. public class StarshipValidator : IStarshipValidator { public IStarship Starship { get ; set ; } public bool IsCaptained () { return Starship . Captain != null ? true : false ; } public bool ValidateRegistry () { return Starship . Registry . StartsWith ( \"NCC\" ) || Starship . Registry . StartsWith ( \"NX\" ) ? true : false ; } public bool Evaluate () { return ValidateRegistry () && IsCaptained (); } } StarshipDeployment takes a StarshipValidator object by dependency injection, which it uses to perform checks on a given Starship upon invocation of Deploy , which returns a StarshipMission object. This provides the opportunity to test a mocked validator for invocation of the Evaluate() method. public class StarshipDeployment { public IStarshipValidator StarshipValidator { get ; set ; } public StarshipDeployment ( IStarshipValidator validator ) { StarshipValidator = validator ?? throw new ArgumentNullException ( nameof ( validator )); } public bool ValidateDestination ( string destination ) { return destination . Length > 1 ? true : false ; } public StarshipMission Deploy ( Starship starship , string destination ) { bool destinationValidated = ValidateDestination ( destination ); bool starshipValidated = StarshipValidator . Evaluate (); return destinationValidated && starshipValidated ? new StarshipMission { Starship = starship as Starship , Destination = destination } : throw new ArgumentException (); } }","title":"Starships"},{"location":"Coding/#glossary","text":"","title":"Glossary"},{"location":"Coding/#c","text":"\"A programming language is low level when its programs require attention to the irrelevant.\" -Alan Perlis Despite C's reputation as a low-level programming language, in fact it merely emulates the ancient PDP-11, which is the only machine for which its abstract machine can be described as \"close to the metal\". In the age of parallel processes, C's serial nature... Sources: C is not a low-level programming language","title":"C"},{"location":"Coding/#enumeration","text":"In C# the term enumeration refers to the process of successively returning individual values. In Python, the term iteration is used to refer to the same thing, and iterable refers to an object that can be iterated, or parsed out into sub-elements. In Python, any object that exposes the __iter__() and __next__() dunder methods are iterable. In C#, IEnumerable The IEnumerable interface implements enumeration. Both languages feature a keyword that allows a subclass to access its direct parent. Whereas in Python the terms superclass and subclass are used, in C# the terms base class and derived class are preferred.","title":"Enumeration"},{"location":"Coding/#go","text":"Golang or \" Go \" is a fast, high-performance, open-source, statically-typed compiled programming language. It was designed at Google by Rob Pike, Robert Giesemer, and Ken Thompson and first appeared in late 2009. Go has a syntax similar to C, but offers additional features such as memory safety, garbage collection, and others. [ 2 ]\\ Hello world package main import \"fmt\" func main () { fmt . Println ( \"Hello, Go World!\" ) } Compile code [ 3 ] go build script.go # compiles to a binary executable in the same directory named \"script\" go run script.go # compiles and runs the program Mathematical function [ 3 ] package main import ( \"fmt\" \"math\" ) func main () { fmt . Println ( math . Max ( 9 , 5 )) } Get a GitHub package named $REPO by $AUTHOR go get github.com/ $AUTHOR / $REPO go get gopkg.in/kyokomi/emoji.v1 # Emoji support Evaluate type of data package main import ( \"fmt\" \"reflect\" ) func main () { fmt . Println ( reflect . TypeOf ( 1 )) // => int fmt . Println ( reflect . TypeOf ( 9.5 )) // => float64 fmt . Println ( reflect . TypeOf ( \"string\" )) // => string fmt . Println ( reflect . TypeOf ( true )) // => bool } Sources: Go language for beginners in 16 parts","title":"Go"},{"location":"Coding/#loop-unswitching","text":"One of the core optimizations that a C compiler performs; transforms a loop containing a conditional into a conditional with a loop in both parts, which changes flow control","title":"Loop unswitching"},{"location":"Coding/#register-rename-engine","text":"Component of modern high-end cores which is one of the largest consumers of die area and power","title":"Register rename engine"},{"location":"Coding/#ruby","text":"REPL shell irb Begin a function definition def End a function definition end Import {package}, or 'gem' require package Write given objects to ios ; writes newline after any that do not already have one puts ( * obj ) Write given objects to ios , with no newline print ( * obj ) Sort in-place array . sort ()","title":"Ruby"},{"location":"Coding/#rust","text":"cargo Rust's compilation manager, package manager, and general-purpose tool rustc Rust compiler, usually invoked by cargo rustdoc Rust documentation tool Start a new package directory hello cargo new --bin hello Load the locally-stored Rust book \"The Rust Programming Language\" rustup doc --book -> precedes return data type mut mutable, when preceding variable identifiers, allows their values to be changed Data types include: - u64 unsigned 64-bit integer - i32 signed 32-bit integer - u8 unsigned 8-bit integer (byte values) - f32 single-precision floating point - f64 double-precision floating point","title":"Rust"},{"location":"Coding/#scalar-replacement-of-aggregates","text":"Scalar Replacement Of Aggregates (SROA) is one of the core optimizations that a C compiler performs; attempts to replace struct s and arrays with fixed lengths with individual variables, which allows the compiler to treat accesses as independent and elide operations entirely if it can prove the results are never visible, which also deletes padding sometimes.","title":"Scalar Replacement Of Aggregates"},{"location":"Coding/#segmented-architecture","text":"Pointers might be segment IDs and an offset","title":"Segmented architecture"},{"location":"Containers/","text":"Containers Docker Docker repositories are associated with a single image, various versions of which can be specified with a tag . Docker Desktop is Docker's runtime for Windows which Docker integrates with WSL 2 since June 2019. Docker traditionally distributed its own Linux kernel to use with Docker Desktop . Connect to a session on a running container ( src ) docker attach $CONTID Build a Docker image from a Dockerfile in the present working directory ( src ) docker build -t web . Save changes made to a modified container to a new image ( Zacker ) docker commit $CONTID $USER / $CONT separate binary coded in Python, available through pip can be used from the CLI as well as from YAML files (\"compose file\") can be used as a replacement for the normal docker CLI intended for use in developer workflows to avoid shell scripts that would typically be used to facilitate long command-line docker invocations not intended for production, for which Docker Swarm is preferable v2 is focused on development or testing with a single node v3 is focused on multi-node orchestration features Simple Docker compose file version : '2.0' services : web : image : sample-01 build : . ports : - '3000:3000' docker-compose up -d Bring everything down docker-compose down -v container commit diff ls image build tag Create docker image named \"hello\" with tag \"v0.1\" from contents of current directory docker image build -t hello:v0.1 . | network connect create disconnect inspect ls prune rm Container networks can use various drivers, which are associated with specific key-value pairs in daemon.json : - NAT : containers reside in their own network and the host acts as gateway and set with { \"fixed-cidr\" : \"10.0.0.0/24\" } . - Transparent : containers are assigned IP addresses on the same physical network to which the host belongs (similar to External virtual switches in Hyper-V), set with { \"bridge\": \"none\" } . Create a new network using the transparent driver Zacker : 285 docker network create -d transparent $NETWORKNAME Create a transparent network with static IP addresses docker network create -d transparent --subnet = 10 .0.0.0/24 --gateway = 10 .0.0.1 $NETWORK ps Display a list of all running containers Zacker : 278 docker ps -a rm Remove a container completely (must be stopped, unless -f is used) Zacker : 279 docker rm $CONTID rmi Can be used to remove extraneous tags run d i m p t v \\ cpu-percent network volumes-from Create a Hyper-V container Zacker : 275 docker run -it --isolation = hyperv microsoft/windowsservercore powershell Bind port 80 of the container to port 8080 of the host Zacker : 284 docker run -it -p 8080 :80 microsoft/windowsservercore powershell Create a container with a static IP address on the network you created docker run -it --network = $NETWORK --ip = 10 .0.0.16 --dns = 10 .0.0.10 microsoft/windowsservercore powershell To detach from a running container use the keyboard shortcuts Ctrl P Ctrl Q start Start a stopped container Zacker : 278 docker start $CONTID stop Stop a container Zacker : 278 docker stop $CONTID tag docker tag can be used to rename images and to prepare them to be [pushed][docker push] to a repository. Tag an image on local container host Zacker : 272 docker tag $USERNAME / $IMAGENAME : $TAG volume Docker has several options for containers to store files in a persistent manner: - Volumes are stored in a part of the host filesystem which is managed by Docker (/var/lib/docker/volumes/ on Docker). - Bind mounts may be stored anywhere on the host system and are specified by docker run --volume . - tmpfs mounts are stored in the host system's memory only, and are available only on Linux. Display data volumes PluralSight docker volume ls Dockerfile A Docker image consists of read-only layers , each of which represents an instruction that incrementally the changes the image being built up. Using [docker build][docker build], Dockerfiles can be used to construct new images. The build process can be optimized by placing multiple commands in the same RUN instruction. Dockerfiles are named simply \"Dockerfile\" with no extension or variation. FROM alpine RUN apk update && apk add nodejs COPY . /app WORKDIR /app CMD [ \"node\" , \"index.js\" ] Zacker : 289 FROM microsoft/windowsservercore RUN powershell -command install-windowsfeature dhcp -includemanagementtools RUN powershell -configurationname microsoft.powershell -command add-dhcpserverv4scope -state active -activatepolicies $true -name scopetest -startrange 10 .0.0.100 -endrange 10 .0.0.200 -subnetmask 255 .255.255.0 RUN md boot COPY ./bootfile.wim c:/boot/ CMD powershell PluralSight FROM microsoft/windowsservercore MAINTAINER @mike_pfeiffer RUN powershell.exe -Command Install-WindowsFeature Web-Server COPY ./websrc c:/inetpub/wwwroot CMD [ \"powershell\" ] Kubernetes \ud83d\udcd8 Glossary Deployment Pod ReplicaSet","title":"Containers"},{"location":"Containers/#containers","text":"","title":"Containers"},{"location":"Containers/#docker","text":"Docker repositories are associated with a single image, various versions of which can be specified with a tag . Docker Desktop is Docker's runtime for Windows which Docker integrates with WSL 2 since June 2019. Docker traditionally distributed its own Linux kernel to use with Docker Desktop . Connect to a session on a running container ( src ) docker attach $CONTID Build a Docker image from a Dockerfile in the present working directory ( src ) docker build -t web . Save changes made to a modified container to a new image ( Zacker ) docker commit $CONTID $USER / $CONT separate binary coded in Python, available through pip can be used from the CLI as well as from YAML files (\"compose file\") can be used as a replacement for the normal docker CLI intended for use in developer workflows to avoid shell scripts that would typically be used to facilitate long command-line docker invocations not intended for production, for which Docker Swarm is preferable v2 is focused on development or testing with a single node v3 is focused on multi-node orchestration features Simple Docker compose file version : '2.0' services : web : image : sample-01 build : . ports : - '3000:3000' docker-compose up -d Bring everything down docker-compose down -v","title":"Docker"},{"location":"Containers/#container","text":"commit diff ls","title":"container"},{"location":"Containers/#image","text":"build tag Create docker image named \"hello\" with tag \"v0.1\" from contents of current directory docker image build -t hello:v0.1 . |","title":"image"},{"location":"Containers/#network","text":"connect create disconnect inspect ls prune rm Container networks can use various drivers, which are associated with specific key-value pairs in daemon.json : - NAT : containers reside in their own network and the host acts as gateway and set with { \"fixed-cidr\" : \"10.0.0.0/24\" } . - Transparent : containers are assigned IP addresses on the same physical network to which the host belongs (similar to External virtual switches in Hyper-V), set with { \"bridge\": \"none\" } . Create a new network using the transparent driver Zacker : 285 docker network create -d transparent $NETWORKNAME Create a transparent network with static IP addresses docker network create -d transparent --subnet = 10 .0.0.0/24 --gateway = 10 .0.0.1 $NETWORK","title":"network"},{"location":"Containers/#ps","text":"Display a list of all running containers Zacker : 278 docker ps -a","title":"ps"},{"location":"Containers/#rm","text":"Remove a container completely (must be stopped, unless -f is used) Zacker : 279 docker rm $CONTID","title":"rm"},{"location":"Containers/#rmi","text":"Can be used to remove extraneous tags","title":"rmi"},{"location":"Containers/#run","text":"d i m p t v \\ cpu-percent network volumes-from Create a Hyper-V container Zacker : 275 docker run -it --isolation = hyperv microsoft/windowsservercore powershell Bind port 80 of the container to port 8080 of the host Zacker : 284 docker run -it -p 8080 :80 microsoft/windowsservercore powershell Create a container with a static IP address on the network you created docker run -it --network = $NETWORK --ip = 10 .0.0.16 --dns = 10 .0.0.10 microsoft/windowsservercore powershell To detach from a running container use the keyboard shortcuts Ctrl P Ctrl Q","title":"run"},{"location":"Containers/#start","text":"Start a stopped container Zacker : 278 docker start $CONTID","title":"start"},{"location":"Containers/#stop","text":"Stop a container Zacker : 278 docker stop $CONTID","title":"stop"},{"location":"Containers/#tag","text":"docker tag can be used to rename images and to prepare them to be [pushed][docker push] to a repository. Tag an image on local container host Zacker : 272 docker tag $USERNAME / $IMAGENAME : $TAG","title":"tag"},{"location":"Containers/#volume","text":"Docker has several options for containers to store files in a persistent manner: - Volumes are stored in a part of the host filesystem which is managed by Docker (/var/lib/docker/volumes/ on Docker). - Bind mounts may be stored anywhere on the host system and are specified by docker run --volume . - tmpfs mounts are stored in the host system's memory only, and are available only on Linux. Display data volumes PluralSight docker volume ls","title":"volume"},{"location":"Containers/#dockerfile","text":"A Docker image consists of read-only layers , each of which represents an instruction that incrementally the changes the image being built up. Using [docker build][docker build], Dockerfiles can be used to construct new images. The build process can be optimized by placing multiple commands in the same RUN instruction. Dockerfiles are named simply \"Dockerfile\" with no extension or variation. FROM alpine RUN apk update && apk add nodejs COPY . /app WORKDIR /app CMD [ \"node\" , \"index.js\" ] Zacker : 289 FROM microsoft/windowsservercore RUN powershell -command install-windowsfeature dhcp -includemanagementtools RUN powershell -configurationname microsoft.powershell -command add-dhcpserverv4scope -state active -activatepolicies $true -name scopetest -startrange 10 .0.0.100 -endrange 10 .0.0.200 -subnetmask 255 .255.255.0 RUN md boot COPY ./bootfile.wim c:/boot/ CMD powershell PluralSight FROM microsoft/windowsservercore MAINTAINER @mike_pfeiffer RUN powershell.exe -Command Install-WindowsFeature Web-Server COPY ./websrc c:/inetpub/wwwroot CMD [ \"powershell\" ]","title":"Dockerfile"},{"location":"Containers/#kubernetes","text":"","title":"Kubernetes"},{"location":"Containers/#glossary","text":"","title":"\ud83d\udcd8 Glossary"},{"location":"Containers/#deployment","text":"","title":"Deployment"},{"location":"Containers/#pod","text":"","title":"Pod"},{"location":"Containers/#replicaset","text":"","title":"ReplicaSet"},{"location":"Health/","text":"\ud83d\udc8a Health Substance Effects Lowest cost Ashwagandha < 0.01 Alpha GPC 0.50 Bacopa Monnieri 0.10 Curcumin 0.13 D-aspartic acid 0.15 GABA Kanna 0.40 L-Carnitine 0.08 L-Tyrosine 0.07 Lion's Mane 0.17 Quercetin Resveratrol 0.12 Rhodiola 0.05 Rosemary 0.09 Taurine Ubiquinol 0.11 \u2714\ufe0f\ufe0f TODO Develop a dosage of adaptogens to use when sleep is disturbed. Incorporate notes and research from Reddit \ud83d\udcb8 Sale Product Discount \ud83e\udd57 Dietary deficiencies Substance Dosage (mg) Cheapest cost per dose Calcium 1,000 0.02 Magnesium 200 0.02 Potassium 2,000 0.01 \ud83e\udde0 Nootropics Supplement Dosage (g) Cost ($) Cost (sale) Alpha GPC 0.6 0.71 0.50 Bacopa 0.4 0.19 0.10 DMAE 0.8 0.23 0.23 Huperzine A 0.2 0.33 0.20 L-Theanine 0.2 0.30 0.22 L-Tyrosine 1.0 0.16 0.07 Lion's Mane 1.0 0.36 0.17 Kanna 0.5 0.80 0.40 Total 3.23 2.04 Gorilla Mind Smooth 2.93 Alpha GPC Bacopa monnieri Curcumin DMAE Huperzine A L-Theanine L-Tyrosine Lions Mane Rhodiola Rosemary Saffron Vitamin B12 \ud83d\udc74 Antioxidants In recent years, the mitochondrial free radical theory has emerged to explain the aging process. In this theory, oxidative stress from free radicals, which many studies suggest cause a variety of health issues, cause mitochondrial dynsfunction and eventually aging. Supplementation with antioxidants has been demonstrated to reduce these free radicals and improve health markers. Oxidative stress is a medical concept defined by the state of excess reactive oxygen species (ROS) and reactive nitrogen species (RNS) . These are endogenously produced but harmful to mitochondria because they react with lipids, proteins, and nucleic acids, progressively degrading cellular functions. or a reduction in antioxidants which detoxify them, resulting in generalized cellular damage. Oxidative stress figures prominently in the free radical theory of aging and has been linked to neurodegenerative diseases like Alzheimer's... Long-term effects on DNA from oxidative stress are similar to those caused by radiation exposure. Reactive oxygen species (ROS) include free radicals and peroxides that damage all components of the cell and can disrupt normal cellular signalling. These are produced by disturbances in the normal reduction-oxidation (redox) state of cells Alpha lipoic acid CoQ10 Curcumin Resveratrol Vitamin C Vitamin D3 Vitamin E \ud83d\udc68\u200d\ud83e\uddb2 Hair loss prevention Dutasteride Finasteride Minoxidil \ud83d\ude34 Sleep deprivation In a double-blind placebo-controlled trial, 60 mg/kg bw of caffeine was enough to overcome the effects of 24 hours of sleep deprivation in a battery of anaerobic performance tests. ( 2018 ) Subjects given 5g of creatine four times a day for seven days experienced less of a decline in performance after 24 hours of sleep deprivation in a variety of cognitive performance measures. ( 2006 ) After six weeks of administration with l-theanine (400 mg/day), boys diagnosed with ADHD experienced significantly improved sleep. ( 2011 ) Supplementation with magnesium (100 mg/day) was enough to eliminate the difference between the sleep-deprived arm and the control arm in subjects who were sleep-deprived for a month. ( 1998 ) Substances Acetylcholine Acetylcholine (ACh) is an important neurotransmitter. It is broken down by the cholinergic enzyme Acetylcholinesterase (AChE). Alpha-GPC L-alpha-glycerolphosphorylcholine (Alpha GPC) is the highest quality and most bioavailable form of choline . Unlike Alpha GPC, other forms of choline were found not to be able to cross the blood-brain barrier , preventing enhancement of cognitive performance. It is not yet officially defined as one of the B vitamins, even though it is associated with them. An injectable prescription-only form of alpha-GPC exists called Delecit that is not available in the US. Product $ mg $/600 mg Swanson Alpha-GPC 300 mg x 60 capsules (Swanson sale) $14.99 18,000 0.50 Swanson Alpha-GPC 300 mg x 60 capsules (Swanson) $19.99 18,000 0.67 NOW Supplements Alpha GPC 300mg x 60 capsules $21.44 18,000 0.71 Bulk Supplements Alpha-GPC 129.96 500,000 0.15 Bulk Supplements Alpha-GPC 84.96 250,000 0.20 Bulk Supplements Alpha-GPC 64.96 100,000 0.39 Alpha lipoic acid Alpha-lipoic acid (ALA or LA, also thioctic acid ) is a powerful antioxidant that can replenish both vitamins C and E. Vitamin E requires vitamin C or coenzyme Q to regenerate after being used up in the free radical reaction . A 2011 study found that obese subjects given doses of 1,000-1,800 mg for up to 20 weeks experienced a loss of around 3% of body weight. A 2015 literature review found that ALA is only marginally effective in combating obesity in conjunction with good diet and exercise. ALA exists in two enantiomeric forms: R and S. A 2002 study found that R-ALA has greater biopotency in several metabolic pathways. Product $ mg $/300 mg BulkSupplements R-Alpha lipoic acid 100 g 111.96 100,000 0.33 BulkSupplements R-Alpha lipoic acid 500 g 481.96 500,000 0.29 Swanson R-Fraction ALA 300 mg x 30 capsules (Swanson sale) (sale) 20.49 12.29 9,000 0.68 0.41 Swanson R-Fraction ALA 300 mg x 30 capsules (Amazon) 20.98 9,000 0.70 Doctor's Best R-Lipoic Acid 100 mg x 60 capsules 15.70 6,000 0.79 Nutricost ALA 300 mg x 240 capsules 19.89 72,000 0.08 Ashwagandha Withania somnifera or Ashwagandha is a traditional Ayurvedic herb used as a memory aid. It has been shown to increase serum testosterone in infertile men. ( 2009 ) In a double-blind, placebo-controlled crossover study , healthy men administered Ashwagandha (500 mg/day) experienced improved cognitive performance outcomes. ( 2014 ) A study that compared the difference in effect of 250 and 600 mg/day found that the higher dosage was associated with significantly greater stress and anxiety reduction and increased sleep quality. ( 2019 ) Product $ mg $/250 mg Bulk Supplements Ashwagandha 1 kg 34.96 1,000,000 <0.01 Swanson Ultimate Ashwagandha 250mg x 60 capsules 7.99 3.99 15,000 0.13 0.07 NOW Supplements Ashwagandha 450 mg x 180 capsules 28.99 81,000 0.09 NOW supplements Ashwagandha 450 mg x 90 capsules 17.90 40,500 0.11 Astragalus Product $ mg $/500 mg Gorilla Mind Astragalus extract 750mg x 90 capsules 20.00 67500 0.14 Swanson Full Spectrum Astragalus 470 mg x 100 capsules (sale) 2.59 47,000 0.03 Swanson Full Spectrum Astragalus 470 mg x 100 capsules 3.99 47,000 0.04 Bacopa Bacopa monnieri is an Ayurvedic herbal medicine. Studies have substantiated its use in enhancing memory by reducing inflammation in the brain. The main nootropic constituents of Bacopa are believed to be dammarane types of triterpenoid saponins known as bacosides . Most clinical studies focus on memory, to the omission of other facets of cognition like fluid intelligence, typically lasting 12 weeks. The long-term effect of Bacopa is unknown, but animal models suggest protection from age-related neurodegeneration. ( 2013 ) There are several putative mechanisms of action: Anti-oxidant/neuroprotection Acetylcholinesterase inhbition Choline acetyltransferase activation Beta-amyloid reduction Increased cerebral blood flow Monoamine potentiation and modulation A mix of Bacopa monnieri , Lycopene , Astaxanthin , and vitamin B12 ) administered orally once a day to subjects aged 60 years old or more resulted in improved cognitive performance after 8 weeks. ( 2020 ) A neuropharmacological review found that the maximum efficacious dosage was 200 mg/kg. ( 2013 ) Product $ mg $/400 mg Bulk Supplements Bacopa 1 kg 212.96 1,000,000 0.09 Swanson Bacopa Monnieri 250 mg x 90 capsules sale 9.29 5.11 22,500 0.17 0.09 NOW Supplements Bacoba Extract 450 mg x 90 capsules 18.78 40,500 0.19 Bioperine Product $ mg $/10 mg Swanson Bioperine 10 mg x 60 capsules (35% off sale) 2.59 600 0.04 Swanson Bioperine 10 mg x 60 capsules 3.99 600 0.07 Swanson Bioperine 10 mg x 60 capsules 9.86 600 0.16 Calcium Your dietary intake of calcium is probably 300 mg with the largest provider being 4 eggs providing 100 mg (target is 1,000 mg) Most food sources of calcium are dairy. A cup of edamame may contain about 100 mg An ounce of almonds may contain 367 mg Product $ g $/1,000 mg BulkSupplements Calcium citrate powder (1 kg) 17.96 1,000.0 0.02 Swanson Calcium Citrate & Vitamin D 315 mg x 250 tablets 6.59 10.99 78.8 0.08 0.14 Swanson Liquid Calcium & Magnesium 300 mg x 100 softgels 3.89 6.49 30.0 0.13 0.21 Swanson Calcium Citrate Plus Magnesium 150 mg x 150 capsules 3.81 4.49 22.5 0.17 0.20 Amazon Elements Calcium plus Vitamin D 500 mg x 65 tablets 5.99 10.99 32.5 0.19 0.34 Blue Diamond Almonds (BOGO sale) 9.00 2.5 3.52 Swanson Calcium Carbonate, Aspartate & Citrate 500 mg x 100 tablets 2.74 50.0 0.05 Choline Choline is a precursor to the neurotransmitter acetylcholine . It is endogenously produced and found in food. Many supplements contain choline because it is cheaply produced, but studies indicate it may not be able to cross the blood-brain barrier. Another product of choline is betaine . For individuals who have genetic polymorphisms like MTHFR and are thus far less able to produce B12 and folate, the pathway producing betaine compensates for this shortcoming, which results in deprivation of acetylcholine for cognition. Citrullus lanatus A late 2020 study found that mice given Cucumis melo and watermelon ( Citrullus lanatus ) seed extract experienced enhancement of memory and cognition. Citrus Bergamot Improves lipid profile . CoQ10 Ubiquinone, also called Coenzyme Q, is a coenzyme family ubiquitous to animals and bacteria. Coenzyme Q 10 (CoQ10) is the most commonly found form in humans, an endogeneously produced lipid-soluble antioxidant that metabolizes into ubiquinol . It is one of the most popular supplements and renowned for being a powerful antioxidant. Doses are usually 100-200 mg/day, although there is no established upper limit for tolerance. A meta-analysis of 13 randomized controlled trials found that CoQ10 supplementation increased superoxide dismutase (SOD) and catalase (CAT) and decreased malondialdehyce (MDA) and diene. ( 2019 ) A meta-analysis of 19 randomized controlled trials found that CoQ10 supplementation significantly increased total antioxidant capacity (TAC), SOD, CAT, and glutathione peroxidase (GPx), as well as significantly decreased MDA. ( 2020 ) A 2021 study found that horses who were fed CoQ10 experienced persistent improvement in semen quality. Patients receiving from 300-1200 mg/day exhibited no difference in incidence of drug-related toxicities between placebo and treatment arms. ( 2002 ) Creatine Creatine monohydrate A 2018 review of six studies found that creatine can improve short-term memory and intelligence/reasoning in healthy individuals. Product $ g $/5g BulkSupplements Creatine Monohydrate powder (1 kg) 19.96 1,000 0.10 Optimum Nutrition Creatine powder 4.41 lbs 42.49 2,000 0.11 Optimum Nutrition Creatine powder 1.32 lbs 15.74 600 0.13 Optimum Nutrition Creatine powder 10.5 oz 10.53 300 0.18 Swanson Creatine 1g x 180 capsules 2-pack 17.49 360 0.25 Optimum Nutrition Creatine capsules 150 x 2.5 g servings 26.13 375 0.35 Optimum Nutrition Creatine powder 2.64 lbs 35.94 1,200 0.15 Cucumis melo Muskmelon is a species of melon that includes honeydew and cantaloupe. A late 2020 study found that mice given Cucumis melo and Citrullus lanatus seed extract experienced enhancement of memory and cognition. Curcumin Curcumin ( diferuloylmethane ) is the principal biologically active polyphenolic constituent of turmeric ( Curcuma longa ). It is a powerful antioxidant binding to COX-2 and 5-LOX and has been found to increased brain-derived neurotrophic factor (BDNF). Curcumin supplementation increases serum brain-derived neurotrophic fator (BDNF) ( 2020 ). A literature review found that ( 2020 ): Curcumin can act as a neuroprotectant antioxidant by binding to COX-2 and 5-LOX. Curcumin inhibits the activation of microglial cells and protects dopaminergic neurons against microglia-mediated neurotoxicity. Curcumin inhibits the caspase-3 apoptosis mediator, suggesting that it is an anti-apoptotic agent that would be useful in treating neurodegenerative diseases. A meta-analysis found that curcumin can improve therapy for people with symptoms of depression and anxiety ( 2019 ). Curcumin has very poor bioavailability without co-administration of piperine. ( 1998 ) A systematic review and meta-analysis found that curcumin administration was associated with a significant reduction only in systeolic blood pressure, but not in diastolic, in studies with supplementation exceeding 12 weeks. ( 2019 ) Product $ mg $/300 mg BulkSupplements Curcumin 1 kg 249.96 1,000,000 0.07 Swanson Curcumin Complex 350 mg x 120 capsules (sale) (sale) 18.99 14.24 42,000 0.13 0.10 Amazon Elements turmeric Complex 316 mg x 65 capsules 14.99 20,540 0.22 D-Aspartic acid D-Aspartic acid or D-Aspartate is an amino acid found in various tissues, including in the axon terminals and synaptic vesicles of neuronal tissue and endocrine glands. It is known to induce testosterone synthesis in the testis. A literature review concluded that exogenous D-Asp increases testosterone in animals. However, human studies which demonstrated conflicting results were noted for their short-term generation, small sample size, and other problems. ( 2017 ) Most studies used dosages of 2,600-3,000 mg/day ( Healthline ) Product $ mg $/3g Swanson D-Aspartic Acid 100 g powder (Swanson sale) 4.99 100 0.15 DMAE Dimethylaminoethanol (marketed as DMAE but appearing more commonly as deanol in scientific literature) is not a precursor to acetylcholine as is commonly asserted. However it is hypothesized that it may increase acetylcholine levels by inhibiting choline metabolism. In a double-blind, placebo-controlled trial, children given 40 mg of Ritalin or 500 mg of deanol experienced similar improvements in psychometric tests. Deanol appeared to be more effective for those with learning disabilities. ( 1975 ) Product $ mg $/750 mg BulkSupplements DMAE-bitartrate powder (500 g) 20.96 500,000 0.03 NOW Supplements DMAE 250 mg x 100 capsules 7.51 25,000 0.225 Dutasteride Finasteride Propecia is the commercial brand name for finasteride. GABA Gamma aminobutyric acid (GABA) is considered an inhibitory neurotransmitter because it blocks certain brain signals and decreases activity in the nervous system. GABAergic receptors are targeted by clinically important drugs that treat anxiety, epilepsy, insomnia, and other pathophysiological disorders. ( 2015 ) Glutathione Glutathione is involved in many body processes and is a common therapy for patients with various ailments, including cancer, HIV, etc. It is endogenously produced in the liver and can also be found in various foods. Huperzine A Huperzine A is a lycopodium alkaloid derived from Huperzia serrata (toothed clubmoss or firmoss), which has been used therapeutically for neurological disorders. It suppresses the acetylcholinesterase enzyme which breaks down acetylcholine Huperzine A promoted neurogenesis in the hippocampus of mice in vitro and in vivo ( 2013 ) Huperzine A directly increased neurotrophic activity of rat astrocytes in vitro ( 2005 ) Product $ mcg $/200 mcg BulkSupplements Huperzine A 10 g 29.96 100,000 0.06 BulkSupplements Huperzine A 100 g 325.96 1,000,000 0.07 Swanson Huperzine A 200 mcg x 30 capsules (Swanson) 9.89 5.93 6,000 0.33 0.20 Kanna Kanna (Sceletium tortuosum) increases seratonin in the brain and improves mood. Product $ mg $/200 mg Swanson Sceletium Tortuosum 50 mg x 60 capsules (Swanson BOGO sale) 11.99 6,000 0.40 L-Arginine L-Arginine is an amino acid widely used in pre-workout supplements because it enhances blood flow. It works by providing nitrogren to the nitric oxide synthase (NOS) enzyme to produce nitric oxide (NO), being metabolized to L-citrulline in the process. L-citrulline, in turn, can be reconverted to L-arginine in the kidneys. Both L-citrulline and L-arginine regulate nitrogen and ammonia in the blood. Bioavailability of l-arginine can vary up to 70%, but excessive dosages can cause gastrointestinal issues. L-citrulline, in contrast, has a bioavailability of practically 100%, which is why it is favored now. L-Carnitine Several forms of L-carnitine are available: L-carnitine L-tartrate (LCLT) is a salt of L-carnitine that increases androgen receptor uptake. Resistance-trained men given LCLT experienced an increase in AR content ( 2006 ) It is absorbed faster than other L-carnitine esters ( 2005 ) Acetyl L-carnitine (ALCAR) is an ester of L-carnitine that can pass through the blood-brain barrier, and its acetyl group ensures that it is active in the central nervous system. It is a \"substrate\" for acetylcholine . L-carnitine fumarate is marketed as a weight loss booster under the name Carnishield Product $ mg $/4000 mg Swanson L-Carnitine 500 mg x 100 tabs 7.48 50,000 0.60 Swanson Acetyl L-carnitine 500 mg x 240 caps (Swanson sale) 16.24 120,000 0.54 Swanson Acetyl L-carnitine 500 mg x 100 caps (Swanson sale) 7.69 50,000 0.62 Jarrow LCLT 500 mg x 100 capsules 19.39 50,000 1.55 Swanson L-Carnitine Fumarate 450 mg x 60 caps 10.99 27,000 1.63 NOW Foods Acetyl-L-Carnitine 500 mg x 50 caps 10.38 25,000 1.66 Oral supplementation of carnitine is problematic because of the production of TMAO which has been associated with cardiovascular disease. Some studies indicate that dietary allicin from garlic can reduce the production of TMAO. ( 2015 ) Product $ mg $/24 mg Swanson Allicin 12 mg x 100 tabs 8.60 1,200 0.17 L-Leucine Leucine is a dietary amino acid marketed as directly stimulating muscle growth. Untrained peri- and postmenopausal women performing resistance training experienced no ergogenic effects supplementing with leucine. ( 2020 ) Young adult men experienced no ergogenic effects supplementing with leucine. ( 2019 ) Elderly men experienced no ergogenic effects from ong-term leucine supplementation (7.5g/d) ( 2009 ) L-Theanine An amino acid found in green tea, alone it can increase the feeling of well-being , but in combination with caffeine generally increases cognitive performance. Product $ mg $/200 mg BulkSupplements L-Theanine powder (100 g) 17.96 100,000 0.04 Amazon Elements L-Theanine 200 mg x 60 capsules (sale) 10.22 12,000 0.17 Amazon Elements L-Theanine 200 mg x 60 capsules 13.49 12,000 0.22 Swanson Suntheanine L-Theanine 100 mg x 60 capsules (Swanson sale) 6.74 6,000 0.22 Swanson Suntheanine L-Theanine 100 mg x 60 capsules (Swanson) 8.99 6,000 0.30 L-Tyrosine L-Tyrosine is the most bioavailable form of Tyrosine, especially in comparison with N-acetyl-L-Tyrosine, and a precursor to the neurotransmitter dopamine , thyroxine, adrenaline and noradrenaline, and the catecholamines. It is converted by the enzyme tyrosine hydroxylase. L-Tyrosine is commonly believed to reduce and relieve stress and enhance cognitive performance. Tyrosine abolished fear expression, apparently because of its effect on the catecholaminergic system. ( 2019 ) A literature review of 15 studies found that tyrosine acutely counteracts decrements in working memory and information processing caused by cognitive load or bad weather. ( 2015 ) Product Cost mg $/1000 mg Swanson L-Tyrosine 500 mg x 100 capsules (Swanson sale) 3.59 50,000 0.07 Swanson L-Tyrosine 500 mg x 100 capsules (Swanson) 5.99 50,000 0.12 NOW Supplements L-Tyrosine 750 mg x 90 capsules 10.73 67,500 0.16 Lecithin Lecithin is a generic term designating any yellow-brownish fatty substances occurring in animl and plant tissues that are amphiphilic. Soy lecithin contains various compounds that reduce stress, including phosphatidylserine . Lecithin is widely assumed to be a provider of choline , although as such it is almost certainly inferior to Alpha GPC . Lions Mane A 2021 study found that compounds extracted from Hericium erinaceus demonstrated neurotrophic effects in isolation. In particular isohericerinol A stimulated neurogenesis by increasing Nerve Growth Factor (NGF) production. Product $ mg $/1000 mg Swanson Lion's Mane 500 mg x 60 capsules (Swanson sale) 5.24 30,000 0.17 Swanson Lion's Mane 500 mg x 60 capsules (Swanson) 10.49 30,000 0.35 Swanson Lion's Mane 500 mg x 60 capsules (Amazon) 10.76 30,000 0.36 Lutein Lutein is a carotenoid closely related to beta-carotene, which gives carrots and pumpkins their orange color. Maca Maca root ( Lepidium meyenii ) is a Peruvian vegetable with a long history in South American cuisine, and as an herb it is traditionally used to enhance fertility and sex drive. Some studies suggest an ergogenic benefit to endurance activities. Maca, in particular black maca, improved learning and memory in rodents with memory impairment. A review found that it can improve sperm volume, and motility and the volume of semen in infertile and healthy men. Another review found that maca can help alleviate hot flashes and anxiety in menopausal women. ( src ) Dosage varies 1.5-5.0 g/day Product $ m $/500 mg BulkSupplements Maca 1 kg 29.96 1,000 0.02 Swanson Maca 500 mg x 100 capsules 2.50 50 0.03 Magnesium Recommended daily intake of magnesium is 420 mg, and you are short by approximately 150 mg. Product $ mg $/200 mg BulkSupplements Magnesium Citrate powder (250 g) 11.96 250,000 0.010 Swanson Triple Magnesium Complex 400 mg x 300 caps (Amazon) 9.99 120,000 0.017 Swanson Triple Magnesium Complex 400 mg x 300 caps 11.99 120,000 0.020 Minoxidil Minoxidil is a potassium channel opener originally designed to lower blood pressure. Rogaine is one of many brand names offering 5% topical minoxidil. Modafinil Modafinil is a heavily abused sleep suppressant typically prescribed to narcolepsy patients but also said to be used by fighter pilots. Phosphatidylcholine Phosphatidylcholine is a phospholipid that incorporates choline as a headgroup. Humans with cognitive disorders who were fed two forms of phosphatidylcholine (~100 mg) after breakfast experienced an improvement in their MMSE scores so significant that their new scores rose above the diagnostic threshold for cognitive disorder. ( 2011 ) Administration of phosphatidylcholine to mice with impaired memory due to inbreeding raised their brain acetylcholine concentration to the level of unimpaired mice and resolved their poor memory function. However, phosphatidylcholine treatment did not affect memory or acetylcholine concentrations in normal mice. ( 1995 ) During a randomized, placebo-controlled clinical trial, prenatal supplementation of phosphatidylcholine resulted in improvement of cognitive performance of fetus (P50 inhibition). Delay in development as measured by this test is associated with schizophrenia and developmental problems. ( 2013 ) This finding confirmed earlier research on the long-term benefit of prenatal choline supplementation in animals. Phosphatidylserine Phosphatidylserine (PtdSer or PS) is a phospholipid that supports a gamut of cognitive functions. It is found in the inner leaflet of neural cell membranes where it regulates the release of neurotransmitters acetylcholine, dopamine, and noradrenaline. Two sources of phosphatidylserine were available: bovine-cortex (BC-PtdSer) which has fallen out of favor due to the risk of transferring infections from prion-infected brains, and soy (S-PtdSer). In a randomized, double-blind trial, children diagnosed with ADHD but never having received drug treatment related to it experienced significantly improved ADHD symptoms and short-term memory after 2 months of phosphatidylserine supplementation (200mg/day). Exogenous dosages of 300-800 mg/d are absorbed efficiently, cross the blood-brain barrier, and reverses biochemical deterioration in nerve cells as a result of aging. ( 2015 ) Product $ mg $/200 mg Swanson Phosphatidylserine 100 mg x 90 softgels (sale) 20.89 13.57 9,000 0.46 0.30 Potassium A systematic review and meta-analysis found that intervening to a point where potassium was 30 mmol/day produced the greatest decrease in systolic and diastolic blood pressure. ( 2020 ) Looks like your daily dietary intake is only 1600 mg (target is 3400 mg ) A banana may have 450 mg 100g of broccoli may have 250 mg A cup of edamame (155 g) might have >600 mg 100g of apricot (~15 pieces) provide almost 1,162 mg (or ~80 mg a piece) Might need to supplement with >1,000 mg of potassium Product $ mg $/2,000 mg BulkSupplements Potassium Citrate powder (1 kg) 18.96 1,000,000 0.04 Swanson Potassium Citrate 99 mg x 120 capsules 2.59 12,000 0.43 NOW Supplements Potassium 99 mg x 180 capsules 5.68 18,000 0.63 Swanson Albion Potassium 99mg x 90 capsules (2-pack) 5.97 18,000 0.66 Swanson Potassium Aspartate 99 mg x 60 capsules (BOGO sale) 6.99 12,000 1.16 Swanson Potassium Orotate 99 mg x 60 capsules 3.82 6,000 1.27 Banana 0.45 500 1.92 Apricot 4.49 2,266 3.96 Pygeum Product $ g $/500mg BulkSupplements pygeum 1 kg 36.96 1,000 0.02 Swanson Pygeum 125 mg x 100 capsules (sale) 5.79 2.90 12.5 0.23 0.12 Quercetin Quercetin is a polyphenolic antioxidant ubiquitous in plant food sources. A systematic review and meta-analysis found that taking at least 500 mg/day resulted in significant reduction of blood pressure. ( Serban et al. 2016 ) Product $ mg $/500 mg Swanson Quercetin 475 mg x 60 capsules (25% off sale) 9.74 28500 0.17 Swanson Quercetin 475 mg x 60 capsules 12.99 28500 0.23 Jarrow Quercetin 500 mg x 100 capsules 17.99 50,000 0.18 Jarrow Quercetin 500 mg x 200 capsules 66.95 100,000 0.34 Resveratrol Resveratrol is a polyphenol and estrogenic antioxidant found in grape skin, peanuts, and other foods. Red wine contains less than 13 mg of resveratrol per liter. ( src ) It is confirmed as a powerful antioxidant and may potentially enhance cognitive performance. A systematic review found that supplementation with resveratrol does not have an anti-obesity effect. ( 2021 ) In contrast, an earlier systematic review cited in this one did find that resveratrol supplementation had a positive effect on weight loss. ( 2018 ) A comprehensive review found that even moderate daily supplementation (0.5-1 g) is enough to inhibit estrogen metabolism and increase SHBG. ( 2020 ) Resveratrol has poor bio-availability and a short half-life (1.5 hours). ( 2019 ). Resveratrol upregulates SHBG expression in the liver. ( 2017 ) A literature review found that resveratrol intervention lowered total cholesterol, systolic blood pressure, and fasting glucose with more significant reductions when doses were higher than 300mg/day. ( 2016 ) A double-blind, randomized, placebo-controlled crossover study found that administration of resveratrol (150 mg/day) caused no changes in metabolic risk markers in overweight and obese subjects after 4 weeks with a 4-week washout period. ( 2015 ) Rats force-fed resveratrol at 20mg/kg daily experienced increased serum concentration of gonadotrophins and testosterone by stimulating the HPG axis . Product $ mg $/100 mg BulkSupplements 100g 85.73 100,000 0.09 BulkSupplements 500g 323.48 500,000 0.06 Swanson Resveratrol 500 mg x 30 capsules (BOGO) 32.99 30,000 0.11 Swanson Resveratrol 250 mg x 30 capsules (Swanson BOGO sale) 17.99 15,000 0.12 Swanson Resveratrol 100 mg x 30 capsules (Swanson BOGO sale) 8.99 6,000 0.15 Rhodiola Rhodiola rosea is an adaptogenic herb used in traditional medicine in Europe and Asia. A 2018 meta study examined 36 animal studies and concluded that rhodiola can improve learning and memory function, despite an earlier 2012 review that found that methodological problems with earlier experiments brought these findings into question. A clinical trial in 2000 found that young physicians experienced improvement in cognitive performance on a battery of cognitive tests called the Fatigue Test. The study concluded that rhodiola could reduce general fatigue under stressful conditions. A similar 2003 study found that young military cadets were able to resist fatigue better with rhodiola. Product $ mg $/400 mg Swanson Rhodiola 400 mg x 100 capsules (Swanson sale) 4.79 40,000 0.05 Swanson Rhodiola 400 mg x 100 capsules (Swanson) 7.99 40,000 0.08 Swanson Rhodiola 400 mg x 100 capsules (Amazon) 10.89 40,000 0.11 Now Foods Rhodiola 500 mg x 60 capsules 18.25 30,000 0.24 Ritalin Ritalin is the brand name for the stimulant methylphenidate. Rosemary Mark Moss with Northumbria University has published several studies on the cognitive benefits of various herbs, including peppermint, chamomile, rosemary, and lavender. The studies on rosemary in particular were motivated by traditional associations of rosemary with memory. Aromatherapy with rosemary oil resulted in improvement in cognitive performance ( 2017 ). Drinking rosemary water produced a small benefit to memory ( 2018 ) Cognitive benefit of rosemary aromatherapy was associated with concentration of 1,8-cineole. ( 2012 ) An interesting study contrasted the impact of aromatherapies of lavender and rosemary oils. Rosemary enhanced performance for overall quality of memory, but also impaired speed of memory compared to control. Lavender actually impaired performance of working memory. Rosemary boosted alertness in comparison to both control and lavender. Both lavender and rosemary enhanced contentment. ( 2003 ) Product $ mg $/500 mg Swanson Rosemary Extract 500 mg x 60 capules (Swanson sale) 5.13 30,000 0.09 Swanson Rosemary Extract 500 mg x 60 capules (Swanson) 7.19 30,000 0.12 Swanson Rosemary Extract 500 mg x 60 capules (Amazon) 9.99 30,000 0.17 Saffron Saffron ( Crocus sativus L. ) A 2020 review found that there was some evidence that associated saffron with improvement in cognitive performance, especially in subjects with neurodegenerative disease. Saw palmetto Saw palmetto ( Serenoa repens ) is a dwarf palm tree native to southeast North America that has long been used as a medicinal herb by Native Americans. Studies suggest that it blocks the conversion of testosterone to DHT , and may be effective as a treatment for androgenic alopecia. ( src ) Dosage may be 160-320 mg Product $ g $/500mg BulkSupplements Saw Palmetto 1 kg 43.96 1,000 0.02 Swanson Saw Palmetto 540 mg x 250 capsules sale 15.99 5.69 135 0.06 0.02 Swanson Saw Palmetto 540 mg x 100 capsules sale 4.59 2.75 54 0.04 0.03 Taurine Dosages of taurine in studies are commonly 500 - 2,000 mg /day, however higher doses appear to be well-tolerated. Tryptophan Tryptophan is a precursor of the neurotransmitters serotonin and melatonin. Ubiquinol Ubiquinol (QH) is the fully reduced form of ubiquinone . Intense exercise depletes CoQ10, but supplementation with ubiquinol prevents this deprivation. Ubuiquinol is more bioavailable than CoQ10 by an order of magnitude. ( 2013 ) Like vitamin E , ubiquinol is a lipid-soluble antioxidant, which gives it the special task of protecting sensitive cell membranes. Ubiquinol is depleted before vitamin E because it reacts with radicals first, and it can also replenish depleted vitamins E and C. ( 2013 ) Healthy and well-trained fireman given 200mg/day ubiquinol for two weeks experienced increases in the biomarkers of bone formation and energy mobilization, suggesting ergogenic effects. ( 2020 ) CoQ10 was positively associated with antioxidant capacity, muscle mass, muscle strength, and muscle endurance in patients with osteoarthritis. ( 2020 ) Subjects with mild cognitive impairment who received 200 mg/day ubiquinol for a year experienced improved cerebral vasoreactivity compared to baseline and placebo, but no significant neurological improvement. ( 2021 ) In a double-blind, placebo-controlled study, ubiquinol supplementation increased physical perforrmance. ( 2013 ) Ubiquinol supplementation to mice reduced a variety of markers associated with fatigue and increased muscle and liver glycogen content, which provide energy during exercise. ( 2019 ) A systematic review and meta-analysis found that long-term supplementation with CoQ10 (most studies were 2-4 months while one was 13 months) at dosages varying from 34-225 mg/day can significantly reduce blood pressue. ( 2007 ) Product $ mg $/100 mg Jarrow QH-absorb (Publix sale) 6.71 6,000 0.11 NOW Supplements Ubiquinol 100 mg x 120 softgels 38.69 12,000 0.32 Swanson Ubiquinol 100 mg x 120 softgels (2-pack) 79.99 24,000 0.33 Swanson Ubiquinol 100 mg x 120 softgels 42.99 12,000 0.36 Vitamin B12 Vitamin B12 (cobalamin, methylcobalamin) is a cofactor and precursor of neurotransmitters acetylcholine, dopamine, GABA, norepinephrine, and serotonin. Even mild B12 deficiency was associated with cognitive decline over 8 years. The BLAtwelve Study tested the effects of Bacopa monnieri , Lycopene , Astaxanthin , and Vitamin B12, finding that a mix of the four compounds administered orally once a day to subjects aged 60 years old or more experienced improved cognitive performance after 8 weeks. Product $ mg $/5,000 mcg Amazon Elements B12 5000 mcg x 65 lozenges 12.99 325 0.20 Vitamin C Vitamin C ( \"ascorbic acid\") is a water-soluble antioxidant that can interact directly with free radicals. Its efficacy as an antioxidant has been closely associated with that of Vitamin E , which it may regenerate by reducing the Vitamin E radical. A 2018 study suggested that Vitamin C could inhibit visceral adipocyte hypertrophy A long-term study published in 2008 found that there was no significant effect of vitamin C on cardiovascular health. The body tightly controls serum concentration of vitamin C. Dosages of 200-300 mg/day result in concentrations of 70 micromol/L, whereas dosages of 1.25 g/day produce concentrations of only 135 micromol/L. Product $ M $/1,000 mg Swanson Vitamin C 1,000 mg x 250 capsules 8.04 11.49 250 0.032 0.045 Swanson Vitamin C 500 mg x 400 capsules 7.69 10.99 200 0.039 0.055 Amazon Elements Vitamin C 1000 mg x 300 tablets 17.99 300 0.06 Vitamin D3 Vitamin D3 (cholecalciferol) can regulate lipid peroxidation as a form of neuroprotection by inducing the synthesis of parvalbumin , a protein that binds to Ca 2+ ( 2020 ). Claims that Vitamin D3 can boost testosterone are unfounded ( 2017 , 2019 ) Product $ IU $/5,000 IU Swanson D3 5000 IU x 250 softgels (Swanson sale) 8.24 1,250,000 IU 0.03 Swanson D3 5000 IU x 250 softgels (Swanson) 10.99 1,250,000 IU 0.04 Amazon Elements D3 5000 IU x 180 softgels 9.34 900,000 IU 0.05 Vitamin E Vitamin E (alpha-tocopherol) is a lipid soluble antioxidant that interferes with the propagation of lipid radicals. Its efficacy as an antioxidant has been closely associated with that of Vitamin C . Zinc Men who received 30 mg of zinc a day showed increased levels of free testosterone . The tolerable upper intake level for adult men is 40 mg/day . Product $ M $/30 mg Swanson Zinc Gluconate 30 mg a 250 tabs 3.14 4.49 7.5 0.01 0.02 \ud83d\udcd8 Glossary anxiolytic apoptosis programmed or controlled cell death; cf. necrosis Ayurveda a natural system of medicine originating in India (ref Bacopa ) cholinergic relating to or denoting nerve cells in which acetylcholine acts as a neurotransmitter crossover trial A trial where subjects are randomly assigned to study arms where each arm consists of two consecutive bouts of treatment separated by a washout period. A concern with crossover trials is that there is a chance that subsequent rounds of treatment may be influenced by earlier ones. DHT Dihydrotestosterone is produced after testosterone is aromatized by 5-alpha-reductase. enantiomer molecules that are mirror images of one another endothelium cells that line the interior surface of blood vessels and lymphatic vessels epithelium one of the four basic types of animal tissue ergogenic intended to enhance physical performance, stamina, or recovery free radical reaction gonadotropin-releasing hormone (GnRH) hepatic related to the liver hirsutism A condition causing male-pattern hair growth in women. Hypothalamic-pituitary-gonadal (HPG) axis An endocrine control mechanism involved in the regulation of testosterone in males and estrogen in females. In this concept, the hypothalamus produces GnRH which binds to secretory cells of the anterior pituitary. Binding of GnRH stimulates these cells to produce LH and FSH, which then produce different effects in the sexes: production of estrogen and inhibin in the ovaries of the female and testosterone and sperm in the testes of the male. follicle-stimualting hormone (FSH) International Unit (IU) a unit of measurement for the amount of a substance that varies based on the substance being measured. isomer each of several compounds with the same formula but a different arrangement of atoms and different properties lipid profile Ratio between HDL (good cholesterol) and LDL (bad cholesterol) lipolysis The breakdown of fats and other lipids by hydrolysis to release fatty acids. The process of mobilizing stored energy during fasting or exercise. The metabolic pathway through which lipid triglycerides are hydrolyzed into a glycerol and three fatty acids. luteinizing hormone (LH) Mini Mental State Examination (MMSE) scoring 20 or above corresponds to normal cognitive functions myopathy any disease that affects muscle tissue necrosis uncontrolled cell death as a result of shock; cf. apoptosis nerve Growth Factor (NGF) neurodegenerative disease diseases that result in progressive degeneration or death of neuronal cells phenol An aromatic organic compound with the molecular formula C 6 H 5 OH. ( wiki ) pleiotropic having more than one effect, especially having multiple phenotypic expressions polyphenol A large family of naturally occurring organic compounds composed of multiple phenol units. sarcopenia age-related loss of muscle mass and function squamous referring to the shape of cells that are wide and flat, such as those found in the lining of the mouth, esophagus, and blood vessels (cf. cuboidal and columnar) statin a family of drugs used for treating hyperlipidaemia with a recognized capacity to prevent cardiovascular disease event vascular resistance the resistance that must be overcome to push blood through the circulatory system vasoreactivity a >= 30% decrease in pulmonary vascular resistance (PVR) with vasodilator compared to baseline VAT Visceral adipose tissue (VAT) is fat surrounding the intra-abdominal organs which has been associated with various medical pathologies; alongside subcutaneous adipose tissue (SAT) , one of the two types of body fat tissue visceral obesity abnormally high deposition of visceral adipose tissue (VAT)","title":"\ud83d\udc8a Health"},{"location":"Health/#health","text":"Substance Effects Lowest cost Ashwagandha < 0.01 Alpha GPC 0.50 Bacopa Monnieri 0.10 Curcumin 0.13 D-aspartic acid 0.15 GABA Kanna 0.40 L-Carnitine 0.08 L-Tyrosine 0.07 Lion's Mane 0.17 Quercetin Resveratrol 0.12 Rhodiola 0.05 Rosemary 0.09 Taurine Ubiquinol 0.11","title":"\ud83d\udc8a Health"},{"location":"Health/#todo","text":"Develop a dosage of adaptogens to use when sleep is disturbed. Incorporate notes and research from Reddit","title":"\u2714\ufe0f&#xfe0f; TODO"},{"location":"Health/#sale","text":"Product Discount","title":"\ud83d\udcb8 Sale"},{"location":"Health/#dietary-deficiencies","text":"Substance Dosage (mg) Cheapest cost per dose Calcium 1,000 0.02 Magnesium 200 0.02 Potassium 2,000 0.01","title":"\ud83e\udd57 Dietary deficiencies"},{"location":"Health/#nootropics","text":"Supplement Dosage (g) Cost ($) Cost (sale) Alpha GPC 0.6 0.71 0.50 Bacopa 0.4 0.19 0.10 DMAE 0.8 0.23 0.23 Huperzine A 0.2 0.33 0.20 L-Theanine 0.2 0.30 0.22 L-Tyrosine 1.0 0.16 0.07 Lion's Mane 1.0 0.36 0.17 Kanna 0.5 0.80 0.40 Total 3.23 2.04 Gorilla Mind Smooth 2.93 Alpha GPC Bacopa monnieri Curcumin DMAE Huperzine A L-Theanine L-Tyrosine Lions Mane Rhodiola Rosemary Saffron Vitamin B12","title":"\ud83e\udde0 Nootropics"},{"location":"Health/#antioxidants","text":"In recent years, the mitochondrial free radical theory has emerged to explain the aging process. In this theory, oxidative stress from free radicals, which many studies suggest cause a variety of health issues, cause mitochondrial dynsfunction and eventually aging. Supplementation with antioxidants has been demonstrated to reduce these free radicals and improve health markers. Oxidative stress is a medical concept defined by the state of excess reactive oxygen species (ROS) and reactive nitrogen species (RNS) . These are endogenously produced but harmful to mitochondria because they react with lipids, proteins, and nucleic acids, progressively degrading cellular functions. or a reduction in antioxidants which detoxify them, resulting in generalized cellular damage. Oxidative stress figures prominently in the free radical theory of aging and has been linked to neurodegenerative diseases like Alzheimer's... Long-term effects on DNA from oxidative stress are similar to those caused by radiation exposure. Reactive oxygen species (ROS) include free radicals and peroxides that damage all components of the cell and can disrupt normal cellular signalling. These are produced by disturbances in the normal reduction-oxidation (redox) state of cells Alpha lipoic acid CoQ10 Curcumin Resveratrol Vitamin C Vitamin D3 Vitamin E","title":"\ud83d\udc74 Antioxidants"},{"location":"Health/#hair-loss-prevention","text":"Dutasteride Finasteride Minoxidil","title":"\ud83d\udc68\u200d\ud83e\uddb2 Hair loss prevention"},{"location":"Health/#sleep-deprivation","text":"In a double-blind placebo-controlled trial, 60 mg/kg bw of caffeine was enough to overcome the effects of 24 hours of sleep deprivation in a battery of anaerobic performance tests. ( 2018 ) Subjects given 5g of creatine four times a day for seven days experienced less of a decline in performance after 24 hours of sleep deprivation in a variety of cognitive performance measures. ( 2006 ) After six weeks of administration with l-theanine (400 mg/day), boys diagnosed with ADHD experienced significantly improved sleep. ( 2011 ) Supplementation with magnesium (100 mg/day) was enough to eliminate the difference between the sleep-deprived arm and the control arm in subjects who were sleep-deprived for a month. ( 1998 )","title":"\ud83d\ude34 Sleep deprivation"},{"location":"Health/#substances","text":"","title":"Substances"},{"location":"Health/#acetylcholine","text":"Acetylcholine (ACh) is an important neurotransmitter. It is broken down by the cholinergic enzyme Acetylcholinesterase (AChE).","title":"Acetylcholine"},{"location":"Health/#alpha-gpc","text":"L-alpha-glycerolphosphorylcholine (Alpha GPC) is the highest quality and most bioavailable form of choline . Unlike Alpha GPC, other forms of choline were found not to be able to cross the blood-brain barrier , preventing enhancement of cognitive performance. It is not yet officially defined as one of the B vitamins, even though it is associated with them. An injectable prescription-only form of alpha-GPC exists called Delecit that is not available in the US. Product $ mg $/600 mg Swanson Alpha-GPC 300 mg x 60 capsules (Swanson sale) $14.99 18,000 0.50 Swanson Alpha-GPC 300 mg x 60 capsules (Swanson) $19.99 18,000 0.67 NOW Supplements Alpha GPC 300mg x 60 capsules $21.44 18,000 0.71 Bulk Supplements Alpha-GPC 129.96 500,000 0.15 Bulk Supplements Alpha-GPC 84.96 250,000 0.20 Bulk Supplements Alpha-GPC 64.96 100,000 0.39","title":"Alpha-GPC"},{"location":"Health/#alpha-lipoic-acid","text":"Alpha-lipoic acid (ALA or LA, also thioctic acid ) is a powerful antioxidant that can replenish both vitamins C and E. Vitamin E requires vitamin C or coenzyme Q to regenerate after being used up in the free radical reaction . A 2011 study found that obese subjects given doses of 1,000-1,800 mg for up to 20 weeks experienced a loss of around 3% of body weight. A 2015 literature review found that ALA is only marginally effective in combating obesity in conjunction with good diet and exercise. ALA exists in two enantiomeric forms: R and S. A 2002 study found that R-ALA has greater biopotency in several metabolic pathways. Product $ mg $/300 mg BulkSupplements R-Alpha lipoic acid 100 g 111.96 100,000 0.33 BulkSupplements R-Alpha lipoic acid 500 g 481.96 500,000 0.29 Swanson R-Fraction ALA 300 mg x 30 capsules (Swanson sale) (sale) 20.49 12.29 9,000 0.68 0.41 Swanson R-Fraction ALA 300 mg x 30 capsules (Amazon) 20.98 9,000 0.70 Doctor's Best R-Lipoic Acid 100 mg x 60 capsules 15.70 6,000 0.79 Nutricost ALA 300 mg x 240 capsules 19.89 72,000 0.08","title":"Alpha lipoic acid"},{"location":"Health/#ashwagandha","text":"Withania somnifera or Ashwagandha is a traditional Ayurvedic herb used as a memory aid. It has been shown to increase serum testosterone in infertile men. ( 2009 ) In a double-blind, placebo-controlled crossover study , healthy men administered Ashwagandha (500 mg/day) experienced improved cognitive performance outcomes. ( 2014 ) A study that compared the difference in effect of 250 and 600 mg/day found that the higher dosage was associated with significantly greater stress and anxiety reduction and increased sleep quality. ( 2019 ) Product $ mg $/250 mg Bulk Supplements Ashwagandha 1 kg 34.96 1,000,000 <0.01 Swanson Ultimate Ashwagandha 250mg x 60 capsules 7.99 3.99 15,000 0.13 0.07 NOW Supplements Ashwagandha 450 mg x 180 capsules 28.99 81,000 0.09 NOW supplements Ashwagandha 450 mg x 90 capsules 17.90 40,500 0.11","title":"Ashwagandha"},{"location":"Health/#astragalus","text":"Product $ mg $/500 mg Gorilla Mind Astragalus extract 750mg x 90 capsules 20.00 67500 0.14 Swanson Full Spectrum Astragalus 470 mg x 100 capsules (sale) 2.59 47,000 0.03 Swanson Full Spectrum Astragalus 470 mg x 100 capsules 3.99 47,000 0.04","title":"Astragalus"},{"location":"Health/#bacopa","text":"Bacopa monnieri is an Ayurvedic herbal medicine. Studies have substantiated its use in enhancing memory by reducing inflammation in the brain. The main nootropic constituents of Bacopa are believed to be dammarane types of triterpenoid saponins known as bacosides . Most clinical studies focus on memory, to the omission of other facets of cognition like fluid intelligence, typically lasting 12 weeks. The long-term effect of Bacopa is unknown, but animal models suggest protection from age-related neurodegeneration. ( 2013 ) There are several putative mechanisms of action: Anti-oxidant/neuroprotection Acetylcholinesterase inhbition Choline acetyltransferase activation Beta-amyloid reduction Increased cerebral blood flow Monoamine potentiation and modulation A mix of Bacopa monnieri , Lycopene , Astaxanthin , and vitamin B12 ) administered orally once a day to subjects aged 60 years old or more resulted in improved cognitive performance after 8 weeks. ( 2020 ) A neuropharmacological review found that the maximum efficacious dosage was 200 mg/kg. ( 2013 ) Product $ mg $/400 mg Bulk Supplements Bacopa 1 kg 212.96 1,000,000 0.09 Swanson Bacopa Monnieri 250 mg x 90 capsules sale 9.29 5.11 22,500 0.17 0.09 NOW Supplements Bacoba Extract 450 mg x 90 capsules 18.78 40,500 0.19","title":"Bacopa"},{"location":"Health/#bioperine","text":"Product $ mg $/10 mg Swanson Bioperine 10 mg x 60 capsules (35% off sale) 2.59 600 0.04 Swanson Bioperine 10 mg x 60 capsules 3.99 600 0.07 Swanson Bioperine 10 mg x 60 capsules 9.86 600 0.16","title":"Bioperine"},{"location":"Health/#calcium","text":"Your dietary intake of calcium is probably 300 mg with the largest provider being 4 eggs providing 100 mg (target is 1,000 mg) Most food sources of calcium are dairy. A cup of edamame may contain about 100 mg An ounce of almonds may contain 367 mg Product $ g $/1,000 mg BulkSupplements Calcium citrate powder (1 kg) 17.96 1,000.0 0.02 Swanson Calcium Citrate & Vitamin D 315 mg x 250 tablets 6.59 10.99 78.8 0.08 0.14 Swanson Liquid Calcium & Magnesium 300 mg x 100 softgels 3.89 6.49 30.0 0.13 0.21 Swanson Calcium Citrate Plus Magnesium 150 mg x 150 capsules 3.81 4.49 22.5 0.17 0.20 Amazon Elements Calcium plus Vitamin D 500 mg x 65 tablets 5.99 10.99 32.5 0.19 0.34 Blue Diamond Almonds (BOGO sale) 9.00 2.5 3.52 Swanson Calcium Carbonate, Aspartate & Citrate 500 mg x 100 tablets 2.74 50.0 0.05","title":"Calcium"},{"location":"Health/#choline","text":"Choline is a precursor to the neurotransmitter acetylcholine . It is endogenously produced and found in food. Many supplements contain choline because it is cheaply produced, but studies indicate it may not be able to cross the blood-brain barrier. Another product of choline is betaine . For individuals who have genetic polymorphisms like MTHFR and are thus far less able to produce B12 and folate, the pathway producing betaine compensates for this shortcoming, which results in deprivation of acetylcholine for cognition.","title":"Choline"},{"location":"Health/#citrullus-lanatus","text":"A late 2020 study found that mice given Cucumis melo and watermelon ( Citrullus lanatus ) seed extract experienced enhancement of memory and cognition.","title":"Citrullus lanatus"},{"location":"Health/#citrus-bergamot","text":"Improves lipid profile .","title":"Citrus Bergamot"},{"location":"Health/#coq10","text":"Ubiquinone, also called Coenzyme Q, is a coenzyme family ubiquitous to animals and bacteria. Coenzyme Q 10 (CoQ10) is the most commonly found form in humans, an endogeneously produced lipid-soluble antioxidant that metabolizes into ubiquinol . It is one of the most popular supplements and renowned for being a powerful antioxidant. Doses are usually 100-200 mg/day, although there is no established upper limit for tolerance. A meta-analysis of 13 randomized controlled trials found that CoQ10 supplementation increased superoxide dismutase (SOD) and catalase (CAT) and decreased malondialdehyce (MDA) and diene. ( 2019 ) A meta-analysis of 19 randomized controlled trials found that CoQ10 supplementation significantly increased total antioxidant capacity (TAC), SOD, CAT, and glutathione peroxidase (GPx), as well as significantly decreased MDA. ( 2020 ) A 2021 study found that horses who were fed CoQ10 experienced persistent improvement in semen quality. Patients receiving from 300-1200 mg/day exhibited no difference in incidence of drug-related toxicities between placebo and treatment arms. ( 2002 )","title":"CoQ10"},{"location":"Health/#creatine","text":"Creatine monohydrate A 2018 review of six studies found that creatine can improve short-term memory and intelligence/reasoning in healthy individuals. Product $ g $/5g BulkSupplements Creatine Monohydrate powder (1 kg) 19.96 1,000 0.10 Optimum Nutrition Creatine powder 4.41 lbs 42.49 2,000 0.11 Optimum Nutrition Creatine powder 1.32 lbs 15.74 600 0.13 Optimum Nutrition Creatine powder 10.5 oz 10.53 300 0.18 Swanson Creatine 1g x 180 capsules 2-pack 17.49 360 0.25 Optimum Nutrition Creatine capsules 150 x 2.5 g servings 26.13 375 0.35 Optimum Nutrition Creatine powder 2.64 lbs 35.94 1,200 0.15","title":"Creatine"},{"location":"Health/#cucumis-melo","text":"Muskmelon is a species of melon that includes honeydew and cantaloupe. A late 2020 study found that mice given Cucumis melo and Citrullus lanatus seed extract experienced enhancement of memory and cognition.","title":"Cucumis melo"},{"location":"Health/#curcumin","text":"Curcumin ( diferuloylmethane ) is the principal biologically active polyphenolic constituent of turmeric ( Curcuma longa ). It is a powerful antioxidant binding to COX-2 and 5-LOX and has been found to increased brain-derived neurotrophic factor (BDNF). Curcumin supplementation increases serum brain-derived neurotrophic fator (BDNF) ( 2020 ). A literature review found that ( 2020 ): Curcumin can act as a neuroprotectant antioxidant by binding to COX-2 and 5-LOX. Curcumin inhibits the activation of microglial cells and protects dopaminergic neurons against microglia-mediated neurotoxicity. Curcumin inhibits the caspase-3 apoptosis mediator, suggesting that it is an anti-apoptotic agent that would be useful in treating neurodegenerative diseases. A meta-analysis found that curcumin can improve therapy for people with symptoms of depression and anxiety ( 2019 ). Curcumin has very poor bioavailability without co-administration of piperine. ( 1998 ) A systematic review and meta-analysis found that curcumin administration was associated with a significant reduction only in systeolic blood pressure, but not in diastolic, in studies with supplementation exceeding 12 weeks. ( 2019 ) Product $ mg $/300 mg BulkSupplements Curcumin 1 kg 249.96 1,000,000 0.07 Swanson Curcumin Complex 350 mg x 120 capsules (sale) (sale) 18.99 14.24 42,000 0.13 0.10 Amazon Elements turmeric Complex 316 mg x 65 capsules 14.99 20,540 0.22","title":"Curcumin"},{"location":"Health/#d-aspartic-acid","text":"D-Aspartic acid or D-Aspartate is an amino acid found in various tissues, including in the axon terminals and synaptic vesicles of neuronal tissue and endocrine glands. It is known to induce testosterone synthesis in the testis. A literature review concluded that exogenous D-Asp increases testosterone in animals. However, human studies which demonstrated conflicting results were noted for their short-term generation, small sample size, and other problems. ( 2017 ) Most studies used dosages of 2,600-3,000 mg/day ( Healthline ) Product $ mg $/3g Swanson D-Aspartic Acid 100 g powder (Swanson sale) 4.99 100 0.15","title":"D-Aspartic acid"},{"location":"Health/#dmae","text":"Dimethylaminoethanol (marketed as DMAE but appearing more commonly as deanol in scientific literature) is not a precursor to acetylcholine as is commonly asserted. However it is hypothesized that it may increase acetylcholine levels by inhibiting choline metabolism. In a double-blind, placebo-controlled trial, children given 40 mg of Ritalin or 500 mg of deanol experienced similar improvements in psychometric tests. Deanol appeared to be more effective for those with learning disabilities. ( 1975 ) Product $ mg $/750 mg BulkSupplements DMAE-bitartrate powder (500 g) 20.96 500,000 0.03 NOW Supplements DMAE 250 mg x 100 capsules 7.51 25,000 0.225","title":"DMAE"},{"location":"Health/#dutasteride","text":"","title":"Dutasteride"},{"location":"Health/#finasteride","text":"Propecia is the commercial brand name for finasteride.","title":"Finasteride"},{"location":"Health/#gaba","text":"Gamma aminobutyric acid (GABA) is considered an inhibitory neurotransmitter because it blocks certain brain signals and decreases activity in the nervous system. GABAergic receptors are targeted by clinically important drugs that treat anxiety, epilepsy, insomnia, and other pathophysiological disorders. ( 2015 )","title":"GABA"},{"location":"Health/#glutathione","text":"Glutathione is involved in many body processes and is a common therapy for patients with various ailments, including cancer, HIV, etc. It is endogenously produced in the liver and can also be found in various foods.","title":"Glutathione"},{"location":"Health/#huperzine-a","text":"Huperzine A is a lycopodium alkaloid derived from Huperzia serrata (toothed clubmoss or firmoss), which has been used therapeutically for neurological disorders. It suppresses the acetylcholinesterase enzyme which breaks down acetylcholine Huperzine A promoted neurogenesis in the hippocampus of mice in vitro and in vivo ( 2013 ) Huperzine A directly increased neurotrophic activity of rat astrocytes in vitro ( 2005 ) Product $ mcg $/200 mcg BulkSupplements Huperzine A 10 g 29.96 100,000 0.06 BulkSupplements Huperzine A 100 g 325.96 1,000,000 0.07 Swanson Huperzine A 200 mcg x 30 capsules (Swanson) 9.89 5.93 6,000 0.33 0.20","title":"Huperzine A"},{"location":"Health/#kanna","text":"Kanna (Sceletium tortuosum) increases seratonin in the brain and improves mood. Product $ mg $/200 mg Swanson Sceletium Tortuosum 50 mg x 60 capsules (Swanson BOGO sale) 11.99 6,000 0.40","title":"Kanna"},{"location":"Health/#l-arginine","text":"L-Arginine is an amino acid widely used in pre-workout supplements because it enhances blood flow. It works by providing nitrogren to the nitric oxide synthase (NOS) enzyme to produce nitric oxide (NO), being metabolized to L-citrulline in the process. L-citrulline, in turn, can be reconverted to L-arginine in the kidneys. Both L-citrulline and L-arginine regulate nitrogen and ammonia in the blood. Bioavailability of l-arginine can vary up to 70%, but excessive dosages can cause gastrointestinal issues. L-citrulline, in contrast, has a bioavailability of practically 100%, which is why it is favored now.","title":"L-Arginine"},{"location":"Health/#l-carnitine","text":"Several forms of L-carnitine are available: L-carnitine L-tartrate (LCLT) is a salt of L-carnitine that increases androgen receptor uptake. Resistance-trained men given LCLT experienced an increase in AR content ( 2006 ) It is absorbed faster than other L-carnitine esters ( 2005 ) Acetyl L-carnitine (ALCAR) is an ester of L-carnitine that can pass through the blood-brain barrier, and its acetyl group ensures that it is active in the central nervous system. It is a \"substrate\" for acetylcholine . L-carnitine fumarate is marketed as a weight loss booster under the name Carnishield Product $ mg $/4000 mg Swanson L-Carnitine 500 mg x 100 tabs 7.48 50,000 0.60 Swanson Acetyl L-carnitine 500 mg x 240 caps (Swanson sale) 16.24 120,000 0.54 Swanson Acetyl L-carnitine 500 mg x 100 caps (Swanson sale) 7.69 50,000 0.62 Jarrow LCLT 500 mg x 100 capsules 19.39 50,000 1.55 Swanson L-Carnitine Fumarate 450 mg x 60 caps 10.99 27,000 1.63 NOW Foods Acetyl-L-Carnitine 500 mg x 50 caps 10.38 25,000 1.66 Oral supplementation of carnitine is problematic because of the production of TMAO which has been associated with cardiovascular disease. Some studies indicate that dietary allicin from garlic can reduce the production of TMAO. ( 2015 ) Product $ mg $/24 mg Swanson Allicin 12 mg x 100 tabs 8.60 1,200 0.17","title":"L-Carnitine"},{"location":"Health/#l-leucine","text":"Leucine is a dietary amino acid marketed as directly stimulating muscle growth. Untrained peri- and postmenopausal women performing resistance training experienced no ergogenic effects supplementing with leucine. ( 2020 ) Young adult men experienced no ergogenic effects supplementing with leucine. ( 2019 ) Elderly men experienced no ergogenic effects from ong-term leucine supplementation (7.5g/d) ( 2009 )","title":"L-Leucine"},{"location":"Health/#l-theanine","text":"An amino acid found in green tea, alone it can increase the feeling of well-being , but in combination with caffeine generally increases cognitive performance. Product $ mg $/200 mg BulkSupplements L-Theanine powder (100 g) 17.96 100,000 0.04 Amazon Elements L-Theanine 200 mg x 60 capsules (sale) 10.22 12,000 0.17 Amazon Elements L-Theanine 200 mg x 60 capsules 13.49 12,000 0.22 Swanson Suntheanine L-Theanine 100 mg x 60 capsules (Swanson sale) 6.74 6,000 0.22 Swanson Suntheanine L-Theanine 100 mg x 60 capsules (Swanson) 8.99 6,000 0.30","title":"L-Theanine"},{"location":"Health/#l-tyrosine","text":"L-Tyrosine is the most bioavailable form of Tyrosine, especially in comparison with N-acetyl-L-Tyrosine, and a precursor to the neurotransmitter dopamine , thyroxine, adrenaline and noradrenaline, and the catecholamines. It is converted by the enzyme tyrosine hydroxylase. L-Tyrosine is commonly believed to reduce and relieve stress and enhance cognitive performance. Tyrosine abolished fear expression, apparently because of its effect on the catecholaminergic system. ( 2019 ) A literature review of 15 studies found that tyrosine acutely counteracts decrements in working memory and information processing caused by cognitive load or bad weather. ( 2015 ) Product Cost mg $/1000 mg Swanson L-Tyrosine 500 mg x 100 capsules (Swanson sale) 3.59 50,000 0.07 Swanson L-Tyrosine 500 mg x 100 capsules (Swanson) 5.99 50,000 0.12 NOW Supplements L-Tyrosine 750 mg x 90 capsules 10.73 67,500 0.16","title":"L-Tyrosine"},{"location":"Health/#lecithin","text":"Lecithin is a generic term designating any yellow-brownish fatty substances occurring in animl and plant tissues that are amphiphilic. Soy lecithin contains various compounds that reduce stress, including phosphatidylserine . Lecithin is widely assumed to be a provider of choline , although as such it is almost certainly inferior to Alpha GPC .","title":"Lecithin"},{"location":"Health/#lions-mane","text":"A 2021 study found that compounds extracted from Hericium erinaceus demonstrated neurotrophic effects in isolation. In particular isohericerinol A stimulated neurogenesis by increasing Nerve Growth Factor (NGF) production. Product $ mg $/1000 mg Swanson Lion's Mane 500 mg x 60 capsules (Swanson sale) 5.24 30,000 0.17 Swanson Lion's Mane 500 mg x 60 capsules (Swanson) 10.49 30,000 0.35 Swanson Lion's Mane 500 mg x 60 capsules (Amazon) 10.76 30,000 0.36","title":"Lions Mane"},{"location":"Health/#lutein","text":"Lutein is a carotenoid closely related to beta-carotene, which gives carrots and pumpkins their orange color.","title":"Lutein"},{"location":"Health/#maca","text":"Maca root ( Lepidium meyenii ) is a Peruvian vegetable with a long history in South American cuisine, and as an herb it is traditionally used to enhance fertility and sex drive. Some studies suggest an ergogenic benefit to endurance activities. Maca, in particular black maca, improved learning and memory in rodents with memory impairment. A review found that it can improve sperm volume, and motility and the volume of semen in infertile and healthy men. Another review found that maca can help alleviate hot flashes and anxiety in menopausal women. ( src ) Dosage varies 1.5-5.0 g/day Product $ m $/500 mg BulkSupplements Maca 1 kg 29.96 1,000 0.02 Swanson Maca 500 mg x 100 capsules 2.50 50 0.03","title":"Maca"},{"location":"Health/#magnesium","text":"Recommended daily intake of magnesium is 420 mg, and you are short by approximately 150 mg. Product $ mg $/200 mg BulkSupplements Magnesium Citrate powder (250 g) 11.96 250,000 0.010 Swanson Triple Magnesium Complex 400 mg x 300 caps (Amazon) 9.99 120,000 0.017 Swanson Triple Magnesium Complex 400 mg x 300 caps 11.99 120,000 0.020","title":"Magnesium"},{"location":"Health/#minoxidil","text":"Minoxidil is a potassium channel opener originally designed to lower blood pressure. Rogaine is one of many brand names offering 5% topical minoxidil.","title":"Minoxidil"},{"location":"Health/#modafinil","text":"Modafinil is a heavily abused sleep suppressant typically prescribed to narcolepsy patients but also said to be used by fighter pilots.","title":"Modafinil"},{"location":"Health/#phosphatidylcholine","text":"Phosphatidylcholine is a phospholipid that incorporates choline as a headgroup. Humans with cognitive disorders who were fed two forms of phosphatidylcholine (~100 mg) after breakfast experienced an improvement in their MMSE scores so significant that their new scores rose above the diagnostic threshold for cognitive disorder. ( 2011 ) Administration of phosphatidylcholine to mice with impaired memory due to inbreeding raised their brain acetylcholine concentration to the level of unimpaired mice and resolved their poor memory function. However, phosphatidylcholine treatment did not affect memory or acetylcholine concentrations in normal mice. ( 1995 ) During a randomized, placebo-controlled clinical trial, prenatal supplementation of phosphatidylcholine resulted in improvement of cognitive performance of fetus (P50 inhibition). Delay in development as measured by this test is associated with schizophrenia and developmental problems. ( 2013 ) This finding confirmed earlier research on the long-term benefit of prenatal choline supplementation in animals.","title":"Phosphatidylcholine"},{"location":"Health/#phosphatidylserine","text":"Phosphatidylserine (PtdSer or PS) is a phospholipid that supports a gamut of cognitive functions. It is found in the inner leaflet of neural cell membranes where it regulates the release of neurotransmitters acetylcholine, dopamine, and noradrenaline. Two sources of phosphatidylserine were available: bovine-cortex (BC-PtdSer) which has fallen out of favor due to the risk of transferring infections from prion-infected brains, and soy (S-PtdSer). In a randomized, double-blind trial, children diagnosed with ADHD but never having received drug treatment related to it experienced significantly improved ADHD symptoms and short-term memory after 2 months of phosphatidylserine supplementation (200mg/day). Exogenous dosages of 300-800 mg/d are absorbed efficiently, cross the blood-brain barrier, and reverses biochemical deterioration in nerve cells as a result of aging. ( 2015 ) Product $ mg $/200 mg Swanson Phosphatidylserine 100 mg x 90 softgels (sale) 20.89 13.57 9,000 0.46 0.30","title":"Phosphatidylserine"},{"location":"Health/#potassium","text":"A systematic review and meta-analysis found that intervening to a point where potassium was 30 mmol/day produced the greatest decrease in systolic and diastolic blood pressure. ( 2020 ) Looks like your daily dietary intake is only 1600 mg (target is 3400 mg ) A banana may have 450 mg 100g of broccoli may have 250 mg A cup of edamame (155 g) might have >600 mg 100g of apricot (~15 pieces) provide almost 1,162 mg (or ~80 mg a piece) Might need to supplement with >1,000 mg of potassium Product $ mg $/2,000 mg BulkSupplements Potassium Citrate powder (1 kg) 18.96 1,000,000 0.04 Swanson Potassium Citrate 99 mg x 120 capsules 2.59 12,000 0.43 NOW Supplements Potassium 99 mg x 180 capsules 5.68 18,000 0.63 Swanson Albion Potassium 99mg x 90 capsules (2-pack) 5.97 18,000 0.66 Swanson Potassium Aspartate 99 mg x 60 capsules (BOGO sale) 6.99 12,000 1.16 Swanson Potassium Orotate 99 mg x 60 capsules 3.82 6,000 1.27 Banana 0.45 500 1.92 Apricot 4.49 2,266 3.96","title":"Potassium"},{"location":"Health/#pygeum","text":"Product $ g $/500mg BulkSupplements pygeum 1 kg 36.96 1,000 0.02 Swanson Pygeum 125 mg x 100 capsules (sale) 5.79 2.90 12.5 0.23 0.12","title":"Pygeum"},{"location":"Health/#quercetin","text":"Quercetin is a polyphenolic antioxidant ubiquitous in plant food sources. A systematic review and meta-analysis found that taking at least 500 mg/day resulted in significant reduction of blood pressure. ( Serban et al. 2016 ) Product $ mg $/500 mg Swanson Quercetin 475 mg x 60 capsules (25% off sale) 9.74 28500 0.17 Swanson Quercetin 475 mg x 60 capsules 12.99 28500 0.23 Jarrow Quercetin 500 mg x 100 capsules 17.99 50,000 0.18 Jarrow Quercetin 500 mg x 200 capsules 66.95 100,000 0.34","title":"Quercetin"},{"location":"Health/#resveratrol","text":"Resveratrol is a polyphenol and estrogenic antioxidant found in grape skin, peanuts, and other foods. Red wine contains less than 13 mg of resveratrol per liter. ( src ) It is confirmed as a powerful antioxidant and may potentially enhance cognitive performance. A systematic review found that supplementation with resveratrol does not have an anti-obesity effect. ( 2021 ) In contrast, an earlier systematic review cited in this one did find that resveratrol supplementation had a positive effect on weight loss. ( 2018 ) A comprehensive review found that even moderate daily supplementation (0.5-1 g) is enough to inhibit estrogen metabolism and increase SHBG. ( 2020 ) Resveratrol has poor bio-availability and a short half-life (1.5 hours). ( 2019 ). Resveratrol upregulates SHBG expression in the liver. ( 2017 ) A literature review found that resveratrol intervention lowered total cholesterol, systolic blood pressure, and fasting glucose with more significant reductions when doses were higher than 300mg/day. ( 2016 ) A double-blind, randomized, placebo-controlled crossover study found that administration of resveratrol (150 mg/day) caused no changes in metabolic risk markers in overweight and obese subjects after 4 weeks with a 4-week washout period. ( 2015 ) Rats force-fed resveratrol at 20mg/kg daily experienced increased serum concentration of gonadotrophins and testosterone by stimulating the HPG axis . Product $ mg $/100 mg BulkSupplements 100g 85.73 100,000 0.09 BulkSupplements 500g 323.48 500,000 0.06 Swanson Resveratrol 500 mg x 30 capsules (BOGO) 32.99 30,000 0.11 Swanson Resveratrol 250 mg x 30 capsules (Swanson BOGO sale) 17.99 15,000 0.12 Swanson Resveratrol 100 mg x 30 capsules (Swanson BOGO sale) 8.99 6,000 0.15","title":"Resveratrol"},{"location":"Health/#rhodiola","text":"Rhodiola rosea is an adaptogenic herb used in traditional medicine in Europe and Asia. A 2018 meta study examined 36 animal studies and concluded that rhodiola can improve learning and memory function, despite an earlier 2012 review that found that methodological problems with earlier experiments brought these findings into question. A clinical trial in 2000 found that young physicians experienced improvement in cognitive performance on a battery of cognitive tests called the Fatigue Test. The study concluded that rhodiola could reduce general fatigue under stressful conditions. A similar 2003 study found that young military cadets were able to resist fatigue better with rhodiola. Product $ mg $/400 mg Swanson Rhodiola 400 mg x 100 capsules (Swanson sale) 4.79 40,000 0.05 Swanson Rhodiola 400 mg x 100 capsules (Swanson) 7.99 40,000 0.08 Swanson Rhodiola 400 mg x 100 capsules (Amazon) 10.89 40,000 0.11 Now Foods Rhodiola 500 mg x 60 capsules 18.25 30,000 0.24","title":"Rhodiola"},{"location":"Health/#ritalin","text":"Ritalin is the brand name for the stimulant methylphenidate.","title":"Ritalin"},{"location":"Health/#rosemary","text":"Mark Moss with Northumbria University has published several studies on the cognitive benefits of various herbs, including peppermint, chamomile, rosemary, and lavender. The studies on rosemary in particular were motivated by traditional associations of rosemary with memory. Aromatherapy with rosemary oil resulted in improvement in cognitive performance ( 2017 ). Drinking rosemary water produced a small benefit to memory ( 2018 ) Cognitive benefit of rosemary aromatherapy was associated with concentration of 1,8-cineole. ( 2012 ) An interesting study contrasted the impact of aromatherapies of lavender and rosemary oils. Rosemary enhanced performance for overall quality of memory, but also impaired speed of memory compared to control. Lavender actually impaired performance of working memory. Rosemary boosted alertness in comparison to both control and lavender. Both lavender and rosemary enhanced contentment. ( 2003 ) Product $ mg $/500 mg Swanson Rosemary Extract 500 mg x 60 capules (Swanson sale) 5.13 30,000 0.09 Swanson Rosemary Extract 500 mg x 60 capules (Swanson) 7.19 30,000 0.12 Swanson Rosemary Extract 500 mg x 60 capules (Amazon) 9.99 30,000 0.17","title":"Rosemary"},{"location":"Health/#saffron","text":"Saffron ( Crocus sativus L. ) A 2020 review found that there was some evidence that associated saffron with improvement in cognitive performance, especially in subjects with neurodegenerative disease.","title":"Saffron"},{"location":"Health/#saw-palmetto","text":"Saw palmetto ( Serenoa repens ) is a dwarf palm tree native to southeast North America that has long been used as a medicinal herb by Native Americans. Studies suggest that it blocks the conversion of testosterone to DHT , and may be effective as a treatment for androgenic alopecia. ( src ) Dosage may be 160-320 mg Product $ g $/500mg BulkSupplements Saw Palmetto 1 kg 43.96 1,000 0.02 Swanson Saw Palmetto 540 mg x 250 capsules sale 15.99 5.69 135 0.06 0.02 Swanson Saw Palmetto 540 mg x 100 capsules sale 4.59 2.75 54 0.04 0.03","title":"Saw palmetto"},{"location":"Health/#taurine","text":"Dosages of taurine in studies are commonly 500 - 2,000 mg /day, however higher doses appear to be well-tolerated.","title":"Taurine"},{"location":"Health/#tryptophan","text":"Tryptophan is a precursor of the neurotransmitters serotonin and melatonin.","title":"Tryptophan"},{"location":"Health/#ubiquinol","text":"Ubiquinol (QH) is the fully reduced form of ubiquinone . Intense exercise depletes CoQ10, but supplementation with ubiquinol prevents this deprivation. Ubuiquinol is more bioavailable than CoQ10 by an order of magnitude. ( 2013 ) Like vitamin E , ubiquinol is a lipid-soluble antioxidant, which gives it the special task of protecting sensitive cell membranes. Ubiquinol is depleted before vitamin E because it reacts with radicals first, and it can also replenish depleted vitamins E and C. ( 2013 ) Healthy and well-trained fireman given 200mg/day ubiquinol for two weeks experienced increases in the biomarkers of bone formation and energy mobilization, suggesting ergogenic effects. ( 2020 ) CoQ10 was positively associated with antioxidant capacity, muscle mass, muscle strength, and muscle endurance in patients with osteoarthritis. ( 2020 ) Subjects with mild cognitive impairment who received 200 mg/day ubiquinol for a year experienced improved cerebral vasoreactivity compared to baseline and placebo, but no significant neurological improvement. ( 2021 ) In a double-blind, placebo-controlled study, ubiquinol supplementation increased physical perforrmance. ( 2013 ) Ubiquinol supplementation to mice reduced a variety of markers associated with fatigue and increased muscle and liver glycogen content, which provide energy during exercise. ( 2019 ) A systematic review and meta-analysis found that long-term supplementation with CoQ10 (most studies were 2-4 months while one was 13 months) at dosages varying from 34-225 mg/day can significantly reduce blood pressue. ( 2007 ) Product $ mg $/100 mg Jarrow QH-absorb (Publix sale) 6.71 6,000 0.11 NOW Supplements Ubiquinol 100 mg x 120 softgels 38.69 12,000 0.32 Swanson Ubiquinol 100 mg x 120 softgels (2-pack) 79.99 24,000 0.33 Swanson Ubiquinol 100 mg x 120 softgels 42.99 12,000 0.36","title":"Ubiquinol"},{"location":"Health/#vitamin-b12","text":"Vitamin B12 (cobalamin, methylcobalamin) is a cofactor and precursor of neurotransmitters acetylcholine, dopamine, GABA, norepinephrine, and serotonin. Even mild B12 deficiency was associated with cognitive decline over 8 years. The BLAtwelve Study tested the effects of Bacopa monnieri , Lycopene , Astaxanthin , and Vitamin B12, finding that a mix of the four compounds administered orally once a day to subjects aged 60 years old or more experienced improved cognitive performance after 8 weeks. Product $ mg $/5,000 mcg Amazon Elements B12 5000 mcg x 65 lozenges 12.99 325 0.20","title":"Vitamin B12"},{"location":"Health/#vitamin-c","text":"Vitamin C ( \"ascorbic acid\") is a water-soluble antioxidant that can interact directly with free radicals. Its efficacy as an antioxidant has been closely associated with that of Vitamin E , which it may regenerate by reducing the Vitamin E radical. A 2018 study suggested that Vitamin C could inhibit visceral adipocyte hypertrophy A long-term study published in 2008 found that there was no significant effect of vitamin C on cardiovascular health. The body tightly controls serum concentration of vitamin C. Dosages of 200-300 mg/day result in concentrations of 70 micromol/L, whereas dosages of 1.25 g/day produce concentrations of only 135 micromol/L. Product $ M $/1,000 mg Swanson Vitamin C 1,000 mg x 250 capsules 8.04 11.49 250 0.032 0.045 Swanson Vitamin C 500 mg x 400 capsules 7.69 10.99 200 0.039 0.055 Amazon Elements Vitamin C 1000 mg x 300 tablets 17.99 300 0.06","title":"Vitamin C"},{"location":"Health/#vitamin-d3","text":"Vitamin D3 (cholecalciferol) can regulate lipid peroxidation as a form of neuroprotection by inducing the synthesis of parvalbumin , a protein that binds to Ca 2+ ( 2020 ). Claims that Vitamin D3 can boost testosterone are unfounded ( 2017 , 2019 ) Product $ IU $/5,000 IU Swanson D3 5000 IU x 250 softgels (Swanson sale) 8.24 1,250,000 IU 0.03 Swanson D3 5000 IU x 250 softgels (Swanson) 10.99 1,250,000 IU 0.04 Amazon Elements D3 5000 IU x 180 softgels 9.34 900,000 IU 0.05","title":"Vitamin D3"},{"location":"Health/#vitamin-e","text":"Vitamin E (alpha-tocopherol) is a lipid soluble antioxidant that interferes with the propagation of lipid radicals. Its efficacy as an antioxidant has been closely associated with that of Vitamin C .","title":"Vitamin E"},{"location":"Health/#zinc","text":"Men who received 30 mg of zinc a day showed increased levels of free testosterone . The tolerable upper intake level for adult men is 40 mg/day . Product $ M $/30 mg Swanson Zinc Gluconate 30 mg a 250 tabs 3.14 4.49 7.5 0.01 0.02","title":"Zinc"},{"location":"Health/#glossary","text":"","title":"\ud83d\udcd8 Glossary"},{"location":"Health/#anxiolytic","text":"","title":"anxiolytic"},{"location":"Health/#apoptosis","text":"programmed or controlled cell death; cf. necrosis","title":"apoptosis"},{"location":"Health/#ayurveda","text":"a natural system of medicine originating in India (ref Bacopa )","title":"Ayurveda"},{"location":"Health/#cholinergic","text":"relating to or denoting nerve cells in which acetylcholine acts as a neurotransmitter","title":"cholinergic"},{"location":"Health/#crossover-trial","text":"A trial where subjects are randomly assigned to study arms where each arm consists of two consecutive bouts of treatment separated by a washout period. A concern with crossover trials is that there is a chance that subsequent rounds of treatment may be influenced by earlier ones.","title":"crossover trial"},{"location":"Health/#dht","text":"Dihydrotestosterone is produced after testosterone is aromatized by 5-alpha-reductase.","title":"DHT"},{"location":"Health/#enantiomer","text":"molecules that are mirror images of one another","title":"enantiomer"},{"location":"Health/#endothelium","text":"cells that line the interior surface of blood vessels and lymphatic vessels","title":"endothelium"},{"location":"Health/#epithelium","text":"one of the four basic types of animal tissue","title":"epithelium"},{"location":"Health/#ergogenic","text":"intended to enhance physical performance, stamina, or recovery","title":"ergogenic"},{"location":"Health/#free-radical-reaction","text":"","title":"free radical reaction"},{"location":"Health/#gonadotropin-releasing-hormone-gnrh","text":"","title":"gonadotropin-releasing hormone (GnRH)"},{"location":"Health/#hepatic","text":"related to the liver","title":"hepatic"},{"location":"Health/#hirsutism","text":"A condition causing male-pattern hair growth in women.","title":"hirsutism"},{"location":"Health/#hypothalamic-pituitary-gonadal-hpg-axis","text":"An endocrine control mechanism involved in the regulation of testosterone in males and estrogen in females. In this concept, the hypothalamus produces GnRH which binds to secretory cells of the anterior pituitary. Binding of GnRH stimulates these cells to produce LH and FSH, which then produce different effects in the sexes: production of estrogen and inhibin in the ovaries of the female and testosterone and sperm in the testes of the male.","title":"Hypothalamic-pituitary-gonadal (HPG) axis"},{"location":"Health/#follicle-stimualting-hormone-fsh","text":"","title":"follicle-stimualting hormone (FSH)"},{"location":"Health/#international-unit-iu","text":"a unit of measurement for the amount of a substance that varies based on the substance being measured.","title":"International Unit (IU)"},{"location":"Health/#isomer","text":"each of several compounds with the same formula but a different arrangement of atoms and different properties","title":"isomer"},{"location":"Health/#lipid-profile","text":"Ratio between HDL (good cholesterol) and LDL (bad cholesterol)","title":"lipid profile"},{"location":"Health/#lipolysis","text":"The breakdown of fats and other lipids by hydrolysis to release fatty acids. The process of mobilizing stored energy during fasting or exercise. The metabolic pathway through which lipid triglycerides are hydrolyzed into a glycerol and three fatty acids.","title":"lipolysis"},{"location":"Health/#luteinizing-hormone-lh","text":"","title":"luteinizing hormone (LH)"},{"location":"Health/#mini-mental-state-examination-mmse","text":"scoring 20 or above corresponds to normal cognitive functions","title":"Mini Mental State Examination (MMSE)"},{"location":"Health/#myopathy","text":"any disease that affects muscle tissue","title":"myopathy"},{"location":"Health/#necrosis","text":"uncontrolled cell death as a result of shock; cf. apoptosis nerve Growth Factor (NGF)","title":"necrosis"},{"location":"Health/#neurodegenerative-disease","text":"diseases that result in progressive degeneration or death of neuronal cells","title":"neurodegenerative disease"},{"location":"Health/#phenol","text":"An aromatic organic compound with the molecular formula C 6 H 5 OH. ( wiki )","title":"phenol"},{"location":"Health/#pleiotropic","text":"having more than one effect, especially having multiple phenotypic expressions","title":"pleiotropic"},{"location":"Health/#polyphenol","text":"A large family of naturally occurring organic compounds composed of multiple phenol units.","title":"polyphenol"},{"location":"Health/#sarcopenia","text":"age-related loss of muscle mass and function","title":"sarcopenia"},{"location":"Health/#squamous","text":"referring to the shape of cells that are wide and flat, such as those found in the lining of the mouth, esophagus, and blood vessels (cf. cuboidal and columnar)","title":"squamous"},{"location":"Health/#statin","text":"a family of drugs used for treating hyperlipidaemia with a recognized capacity to prevent cardiovascular disease event","title":"statin"},{"location":"Health/#vascular-resistance","text":"the resistance that must be overcome to push blood through the circulatory system","title":"vascular resistance"},{"location":"Health/#vasoreactivity","text":"a >= 30% decrease in pulmonary vascular resistance (PVR) with vasodilator compared to baseline","title":"vasoreactivity"},{"location":"Health/#vat","text":"Visceral adipose tissue (VAT) is fat surrounding the intra-abdominal organs which has been associated with various medical pathologies; alongside subcutaneous adipose tissue (SAT) , one of the two types of body fat tissue","title":"VAT"},{"location":"Health/#visceral-obesity","text":"abnormally high deposition of visceral adipose tissue (VAT)","title":"visceral obesity"},{"location":"Intelligence/","text":"\ud83d\udd0e Intelligence China Chinese writers often wrote on intelligence matters in contrast with Western writers[^1] Historical writings on intelligence continue to be used by military institutions of PRC Fragments of earlier works \u201cprotobooks\u201d survive in later works. Sunzi wrote that warfare must be undertaken only after a comprehensive analysis of relative strengths and weaknesses of combatants, the world\u2019s first net assessment procedure. He also advocated deception on the battlefield. Later writers responded to Sunzi, updating and interpreting his precepts: Guan Zhong Li Quan, Li Jing. Sunzi condemned generals who failed to gather necessary intelligence and disparaged divination and consulting ancestors in favor of hard intelligence. Later writers like Shi Zimei also upbraided rulers who ignored these rules. Works advocating covert action - Toubi Fuban - Warring States period works on statecraft, e.g. Han Feizi Sunzi\u2019s Five Types of Spy 1. Local Spy Knowledgeable spies living outside their native habitat, including emigrants, travelers and exiles (e.g. defectors) 2. Internal spy Officials 3. Turned spy Double agent 4. Living spy Talented people dispatched abroad to observe and then report back 5. Dead spy Spies deliberately sent on suicide missions without their knowledge. Enemy agents could also be deceived and provided with false information, causing them to be executed when they report false information to their masters. Subversion By end of Spring and Autumn Period, subversion was an accepted goal of espionage. Assassination used most often in periods of fragmentation. Six Secret Teachings compiled two chapters on systematic programs of subversion. These two chapters, with Sunzi\u2019s \u201cEmploying Spies\u201d in Art of War formed the basis of espionage as late as the Ming Dynasty. Later military writers also devoted at least a few paragraphs to subversion: Le Quan\u2019s Techniques for Secret Plots, Huqian Jing, Bingfa Baiyan. These took historical and semihistorical episodes as examples (Wu Yue Chunqua). Covert action and subversion used by Qin in the western hinterland. \u201cDebauching\u201d kings. Sunzi: \u201cYou must first know the names of the defensive commander, his assistants, staff, door guards, and attendants for any armies that you want to strike, cities you want to attack, and men you want to assassinate.\u201d Egypt Egyptian intelligence had its origins in the urban policing organizations established during the British Khedive and occupation. Mamur Zapt s were the de facto chiefs of secret police in Cairo and Alexandria, operating networks of mukbireen informants, by the time of British occupation in 1882. The Central Special Office (CSO) was established in 1911 after the Coptic Prime Minister was assassinated by a young nationalist. When discontent exploded in 1919, and CSO couldn\u2019t respond effectively, the Interior Ministry formed the Special Section (SS) to centralize intelligence collection. CSO was closed and files transferred to SS in 1925. Egyptian Military Intelligence was being Egyptianized in the late 1930s. MI officers were complicit in the army coup of 23 July 1952. Once the Free Officers were in power, they set about creating a new intelligence community. The Military Intelligence Directorate (MID) as it was then called, became active in covert action and subversion across the Middle East and Africa. Fear of communist subversion fueled the creation of the General Intelligence Directorate (GID) , subordinate to the Interior Minister, which was led at first by Zakaria Muhi al-Din and stood up within months of the coup. GID inherited and took the place of the old political police organs. GID was renamed the State Security Investigations Service (SSIS) in 1971. As part of Gamal Abd al-Nasser\u2019s emphasis on covert action, the Egyptian General Intelligence Service (EGIS) was established by March 1954, also headed by Zakaria Muhi al-Din. EGIS was the capstone of the new Egyptian government\u2019s emphasis on projecting influence through covert action, and it was modeled after the CIA. During the heyday of Egyptian subversion abroad, Salah Nasr was EGIS Director. Egyptian covert action grew out of the effort to end the British occupation, remove British influence from government organs, and eject the British from the Canal Zone. In 1952, MID created a Special Resistance Office to form a guerrilla army against British military units based along the Suez Canal. The campaign became the template for Egyptian subversion abroad for the next decade and half. MID also sent officers to Sudan to recruit sources and build influence for possible unity with Sudan. While Egypt occupied the Gaza Strip, Palestinian infiltrators were a source of irritation for MID. However, MID sponsored infiltrations using Bedouins, Palestinians, and even its own scouts for intelligence-gathering. These infiltrations eventually were made from southern Lebanon as well. The death of an Israeli settler during one such raid sparked a cycle of reprisals that led to war. The four years after the British withdrawal from the Suez in 1956 marked Egypt at the height of its regional power. Nasser was the hero of the Arab world, and Cairo was the wellspring of a vigorous new Pan-Arab nationalism. However, Egyptian subversion against conservative Arab states and British protectorates was checked in many places by the United States. After Egypt and Syria joined the United Arab Republic (UAR), Nasser began covert action campaigns in Jordan and Lebanon, combining subversion with radio propaganda. In Lebanon, UAR meddling triggered a larger crisis that drew the attention of the US and ended Egyptian arming of Lebanese insurgents. A 1958 coup in Iraq initially drew feelersfrom the Egyptians, who hoped to draw Iraq into their circle of influence. However, old animosities arose again, and by 1959 Egypt was broadcasting radio propaganda, and senior UAR spymasters backed a short-lived army revolt in Mosul. Alienated by Nasser, Saudi Arabia fended off feeble Egypt\u2019s attempts to use its military presence and attache corps there for subversion. Nasser created African Affairs Branch, led by Mohamed Fa\u2019iq, to pursue Egyptian interests clandestinely across Africa. Key targets were Ethiopia and Congo. Egypt promoted Eritrean Liberation Front (ELF) after Suez Crisis and assisted Eritrean declaration of independence by 1960. Close relations with Somalia, which also had territorial disputes with Ethiopia. Egypt provided money, arms to friendly regime in Congo, flew Ghanaian and Egyptian troops to prop up regime. Also provided arms to resistance movements in Angola, Cameroon, Nigeria, and Portuguese Guiana, and training to Congo Brazzaville and Tanzania. MID broke down under pressure of war. Israelis had strong SIGINT and intercepted an insecure phone call between Nasser and King Hussein. Israel also captured a valuable cache of Egyptian intelligence documents in Gaza on MID\u2019s Palestinian infiltrators. Voice of the Arabs (VOA) served as a propaganda arm buttressing Egypt\u2019s foreign subversion policies. Daily radio programming was handled by Egyptian State Broadcasting (ESB) and used transmitters supplied by CIA. ESB turned to former Nazi propagandists, who must have felt at home with anti-Jewish programming. ESB grew out of EGIS promotion of VOA. EGIS had a representative on ESB\u2019s board of directors. EGIS was also responsible for running clandestine radio stations, reliant usually reliant on mobile transmitters. Voice of the Arab Nation targeted Iraq during the 1956 war when the British and French knocked out overt ESB transmitters. Egypt also operated a clandestine radio station called Voice of Free Iran in the early 1960s and gave support to anti-shah clerics. Egyptian intelligence had been preparing Yemen for subversion since 1950s. VOA broadcasted propaganda, and the signal to begin the coup in September 1962 was delivered by VOA. Egyptian army was overwhelmed by Yemeni rural insurgency, because Cairo didn\u2019t plan for a long war. No good cultural intelligence on Yemeni tribes. Intervention antagonized Great Britain and Saudi Arabia. MID provided training to National Liberation Front (NLF) which was set up by Egypt as an anti-British umbrella organization. NLF systematically kidnapped, tortured, and executed Aden SB officers and informants. Egypt still relied on broken Enigma-based encryption. GCHQ and NSA had access to military COMINT. Despite successful Egyptian-backed coup and energetic covert action campaign against British Protectorate in South Yemen, Egypt could not maintain political leverage over Yemen. South Yemen turned to GDR for intelligence training. The British were Egypt\u2019s first instructors in counterintelligence, and also some of its first victims. MI and Cairo SB busted a German spy ring in cooperation with British intelligence (The Key to Rebecca). Operation Susannah was an Israeli terror plot meant to undermine international faith in Cairo\u2019s ability to secure the Suez Canal. In 1954, an Israeli case officer activated sabotage cells among the remaining Egyptian Jewry. Egyptians doubled the Israeli case officer and rolled up the ring. Huge success for nascent Egyptian CI and was an impetus in formation of EGIS. GID rolled up a spy network led by the British expat James Swinburn in 1956. Unfortunately, this had the knock-on effect of forcing the British to rely on their formidable SIGINT collection. GCHQ had broken Egypt\u2019s diplomatic cipher and routinely intercepted Egyptian military communications. Egyptian CI penetrated several Israeli spy rings after the Suez War. The Goudswaard Ring of European and Egyptian nationals was recruited by Mossad. Eventually the GID turned them and were using them to pass disinformation. The Thomas Ring gathered military documents and passed them to Israel in microfilm hidden in furniture which was then exported. In 1961, GID rolled up this network. In the run-up to the Yom Kippur War, Egyptian denial and deception extended to their erstwhile allies the Soviet Union. Soviet photoreconnaissance satellites monitored the war. After the Yom Kippur War, CIA resumed its relationship with Egypt, and along with providing training and equipment, CIA began to aggressively recruit from throughout Egyptian society. Egypt joined the Safari Club (a coalition of intelligence agencies from France, Saudi Arabia, Iran, and Morocco), which provided aid to Zaire, Somalia, Djibouti and anti-Qadhafi groups in Libya and Egypt as well as the Afghan resistance. After Camp David, Egyptian intelligence had to contend with Libyan subversion and radio propaganda, mimicking Egyptian tactics of yore. Covert action escalated to a short border war in 1977. Libyan subversion continued, and in 1978 SSIS uncovered a clandestine organization for the \u201cliberation\u201d of Egypt through sabotage and assassination. Egypt also stopped an Iranian who planned to undermine the peace process by conducting and bomb attacks in Egypt. In 1979, Egyptian CI began a covert war with Soviet and East European services. SSIS claimed to have uncovered a Bulgarian intelligence network that year. In 1981, Soviet and Hungarian diplomats were expelled after Egyptian security discovered a spy ring allegedly inciting sectarian unrest. Israel attempted to subvert Egypt early on by using the Jewish population in Cairo and Alexandria as a base. IDF created sabotage cells which were activated in 1954 when Egypt and UK were negotiating a British withdrawal from the Canal Zone. Operation Susannah was rolled up, and the operatives brutally tortured in a MID prison. The case had political ripple effects within Israel that reached into the 1960s. Several Israeli spy rings were rolled up in the late 1950s and 1960s. The Goudswaard Ring, run by Mossad in a false-flag operation that posed it as a NATO service, was doubled and began passing disinformation. The Thomas Ring, which gathered military documents and passed them to Israel in microfilm hidden in exported furniture, was rolled up by GID in 1961. EGIS claimed to have run a \u201csuper-spy\u201d named Jack Bitton before and during the Six-Day War (1967). Bitton opened a travel agency in Tel Aviv in in 1954 or 1955. Some claim he was doubled by the Israelis, but Egypt claims he provided warning of Israel\u2019s attack through his contacts with senior Israelis. For Egyptians, Bitton is Egypt\u2019s master spy and the subject of a highly popular television serial in the 1980s. Israeli spy Wolfgang Lotz mapped out Egypt\u2019s SAM facilities and other important military infrastructure. Israel also operated an agent \u201cSulayman\u201d who reported the movements of individual Egyptian units during the war. Sulayman may have provided encryption material, because Israeli MI (Aman) broke Egypt\u2019s military cipher before the war. Israeli deception measures went undetected by the Egyptians. IDF created an entire fake military unit in Eilat, complete with encrypted and unencrypted radio traffic, which succeeded in tricking the Egyptians. In the run-up to the Yom Kippur War, Egypt suffered from COMSEC shortfalls which were aggravated by excellent Israeli SIGINT. After the 1967 war, Israeli SIGINT stations on the banks of the Suez could reach deeper into Egypt. Israel even planted listening devices on communications lines used by an Egyptian headquarters complex. Egyptian CI also discovered bugs in hollowed-out telephone poles which Israel was using to tap military communications between Red Sea bases and Cairo. Israel\u2019s \u201cTop Source\u201d, Ashraf Marwan, was turned by the Egyptians before the onset of war and gave false warnings of Egyptian attack that cost Israel money and lulled it into a state of complacency. Anwar Sadat and MID formulated a plan of strategic deception to accomplish diplomatic victory in a limited war (British deception before Battle of el-Alamein). EGIS planted deceptive media stories on military unpreparedness. Military coordinated movements to avoid US imagery satellites. Brinksmanship produced calm-alarm-calm cycle that bred complacency in Israelis. Egyptians improved intelligence collection by recruiting Bedouins and obtaining valuable intelligence on Israeli plans and codenames. Also leveraged OSINT. Ashraf Marwan, Sadat\u2019s new presidential gatekeeper, was recruited by Israel but doubled by Egyptians, gave false warnings of imminent attacks. After early success in the war, Israelis exploited weaknesses and were ready to pounce when ceasefire was declared. Egypt and Israel made peace and began sharing intelligence on Palestinians. Sources: Sirrs, Owen L. The Egyptian Intelligence Service: A History of the Mukhabarat, 1910-2009 (Studies in Intelligence) . Finland Finnish economy\u2019s reliance on technology is growing and so is economic espionage. National innovation system is considered protected. Finland\u2019s intelligence community - Finnish Security Intelligence Service (Suojehpolisi, Supo) has been performing counterespionage, counterterrorism, and security since 1949. - Finnish Military Intelligence Command (FINMIC) is under Defense Command\u2019s Intelligence Division. - Finnish Intelligence Research Establishment (Viestikoelaitos) is Finnish Defense Force\u2019s (FDF) SIGINT unit, under Finnish Air Force (FAF). Predecessors to Supo - Detective Central Police (Etsiva keskuspoliisi) 1919-1937 - State Police (Vattriollinen poliisi, Valpo) 1937-1949 - Red Valpo, when dominated by Communists, 1945-1949 - Finnish Security Police est. 1949, name changed to Supo 2010. Organizational structure Supo is subordinate to the Ministry of Interior, while other Nordic services are subordinate to the Ministry of Justice. Supo handles no foreign HUMINT sources. Four organizational reforms over past 20 years - In 1992, Supo had three directorates: Counterespionage, Security, and Development and Support - In 1998, operational and development matters were divided more clearly - In 2004, Supo was reorganized to develop research and analysis functions. Line organization was introduced. - In 2009, Supo was divided into Operational and Strategic Branches. - Operational Branch : Counterespionage Unit, Counterintelligence Unit, Security and Regional Unit, Field Surveillance Unit - Strategic Branch : Situational Awareness Unit, International Relations Unit, Internal Surveillance Unit - Communications Office directly subordinate to C/Supo Personnel Supo employs 220 people and has a 17 million euro budget. - 55% are police personnel (30% command, 40% senior, 30% officers) - 1/3 of employees have a degree - Average age of 44 years Iran In 1978-1979 (1357 H.S.) some former SAVAK officers revealed what they knew in a televised news conference. SAVAK had 9 or 10 chief directorates: First Chief Directorate : administration Second Chief Directorate : foreign intelligence Third Chief Directorate : internal security Fourth Chief Directorate : counterintelligence Fifth Chief Directorate : technical espionage, including telephone intercept and clandestine audio recordings, censorship, photography, and flaps and seals Sixth Chief Directorate : budgeting Seventh Chief Directorate : analysis, including compilation of daily \"bulletins\" Eighth Chief Directorate : counterespionage Ninth Chief Directorate : individual biographies and passport affairs Iraj Faridi, former chief of operations of the first section of the Third Chief Directorate, mentioned the Trident intelligence agreement but seemed to think the relationship with Turkey, not Israel, was sensitive. He claimed to have 17 years of service in SAVAK during the press conference. Mohsen Toluei Trident A three-way intelligence sharing agreement between Iran, Israel, and Turkey. This was prominently covered by the press when former Mossad officer Yossi Alpher published his book Periphery: Israel's Search for Allies in the Middle East . Russia Kouzminov, Alexander. Biological espionage: Special Operations . Directorate S, Department 12 Responsible for obtaining intelligence to aid the Soviet BW program, as well as new strains of pathogens. Requirements included Western governments, state commissions, civil defense and laboratories. Vladimir Kuzichkin worked as Illegals support officer in S/12 for 10 years Laboratory X founded in 1920s, transferred to NKVD in 1937. Soviet biological espionage - Morris and Leontina Cohen, controlled by Gordon Lonsdale, obtained information on British BW program. - Marcus Klinberg was director of Israeli BW program, spied for USSR. Communications Intelligence and Tsarist Russia by Thomas R. Hammant Ministry of Foreign Affairs Since Peter the Great, COMINT involving foreign governments and their representatives was the responsibility of the Ministry of Foreign Affairs (MID). Methods used included opening diplomatic letters (perlustration) and decrypting the contents, if encrypted, by cryptanalysis or purchasing codebooks. MID was aided by the \u201cBlack Cabinets\u201d of the Imperial Russian Postal Service. Post offices in major cities of the Russian Empire had Black Cabinets, which photographed contents of suspect correspondence and disseminating the information to the appropriate ministry. When the contents of a letter were encrypted, it was worked on not by a Black Cabinet but by a \u201csimilar establishment attached to the Ministry of Foreign Affairs.\u201d Little is known about MID\u2019s cryptographic organization, but it may have been brought under the control of the Minister of Foreign Affairs himself in the 1900s. Codebooks could be easily acquired on the open market. Russian COMINT in World War I Army and Navy operated independent COMINT elements. At each Army HQ, radio intelligence operations were controlled by Chief of Army Communications through his assistant for technical matters. Each army\u2019s radio battalion had a radio intelligence squad or section which operated two stations: one monitored enemy communications, and the other station then recorded them once detected. Intercepts of encrypted German Army radiograms were sent to a \u201cspecial bureau\u201d of Chief Directorate of the General Staff in St. Petersburg for cryptanalysis. Generally, COMINT was poorly organized under the Russian Army. Black Sea and Baltic Sea Fleets established independent COMINT services in autumn 1914 after German naval codebooks were recovered from a sunken German cruiser. Copies of the codebook were shared with the British and French, and there was continued COMINT collaboration between the Allies throughout the war. The Baltic Sea Fleet\u2019s first radio intercept station was established close to Tallin. Intelligence was sent by underground cable to the Communications Service of the Southern Region. Each region (North, East, and South) had a Central Radio Station (CRS) that produced all-source intelligence and supported fleet communications. By 1916, Northern Region had 5 DF and 5 intercept stations, and Southern Region had 5 DF and 4 intercept stations. Southern Region also established a Radio Intelligence Center, probably to administer COMINT from these stations, which was subordinate to the CRS, and other Regions may have had similar units. The Baltic Sea Fleet\u2019s greatest debt to COMINT was accrued on 31 July 1915, when the Russians learned the German Navy planned to seize the city of Riga. Cryptanalysis of the messages, aerial reconnaissance, and shore-based observation posts allowed Russian ships to be ready for the attack when it came, and the Russians prevailed. The Black Sea Fleet\u2019s first radio intercept station was at Sevastopol, and although its Communications Service had two regions to administer, there are fewer details available about Black Sea Fleet COMINT. Black Sea Fleet was greatly helped by Turkey\u2019s use of German codes during the war. In December 1916, Black Sea Fleet decrypted information that indicated a German submarine was to return to Constantinople and included the location of the mine-swept channel by which it was to pass. Russian minelayers went to work, and within 48 hours the Russians learned the submarine had been sunk. From the Okhrana to the KGB, by Christopher Andrew Okhrana est. 1881 Soviet intelligence MO and methods rooted in Tsarist secret service Okhrana active measures campaign to persuade French investors to invest in Russia. By 1914, a quarter of France\u2019s FDI was in Russia, three times as much as in its own empire, 80% of it in government loans. Peter Rachkowsky, head of Okhrana\u2019s Paris-based Foreign Agency from 1884 to 1982 may have been responsible for producing the Protocols of the Elders of Zion. HUMINT Colonel Alfred Redl, senior Austrian MI officer. In winter 1901-2, Colonel Batyushin, head of Russian MI in Warsaw, discovered Redl was a homosexual. Redl sold Austria\u2019s mobilization plans against Russia and Serbia until his suicide in 1913. Roman Malinovsky, worker who became one of the most trusted Bolsheviks. One of 6 Bolshevik deputies to Duma (1912), then chair of Bolshevik faction when Mensheviks broke off. When Lenin smelled a rat and set up a committee to investigate the possibility of penetration, Malinovsky was a member of the committee. Okhrana sent him out of the country with a 6000 ruble payoff, and when Okhrana files were opened in 1917 Lenin couldn\u2019t believe Malinovsky had been a traitor. SIGINT Okhrana began stealing cipher material to assist SIGINT at the beginning of the 20th century, decades before anyone but the French. Okhrana bribed embassy to services to make impressions of keys and smuggle them out, as well as papers to be photographed. SIGINT was used in diplomatic negotiations with Germany over the Bosphorus Canal. Bolshevik Revolution damaged SIGINT, dispersed codebreakers and cryptologists to other countries, where they sometimes joined those SIGINT services. For a decade after the Revolution, Soviet diplomatic traffic was easily decrypted. Soviets adopted the one-time pad in 1927. GB adopted the tactic of stealing cryptographic material. KGB and GRU operated a joint unit headed by Gleb Boki. SIGINT was responsible for a panic regarding a possible Japanese surprise attack 1931-1932. Intercepted attache cables were published in the Moscow press. Soviets provided edited excerpts of diplomatic intercepts to Germany to influence Germans into signing Molotov-Ribbentrop Pact. Maskirovka on Eastern Front achieved comparable results to British and Western denial and deception (XX system) Capture of hundreds of German signals personnel among tens of thousands of German POWs resulted in a windfall for Soviet SIGINT, 1943. Multiple penetrations of NSA, 1959-1963. Sword and the Shield, the by Christopher Andrew Chapter 2: From Lenin\u2019s Cheka to Stalin\u2019s OGPU The paranoid instincts and shadowy methods of the Cheka and its successors were motivated by persecution of Bolshevik revolutionaries during the Tsarist period and provoked by agents provocateurs planted by the Tsarist Okhrana and foreign powers. - Cheka founded on 19171220 only weeks after Bolshevik Revolution: Feliks Dzerzhinsky - Foreign Intelligence Department (INO) established 19201220: Made use chiefly of Illegals because Soviet state had no legal residencies abroad - Foiled plots encouraged paranoia of young Cheka - Envoys\u2019 plot by naive young diplomats, caught in the net laid by the Cheka - Agents provocateurs Eduard Berzin and Yan Buikis: Berzin received Order of the Red Star, became Cheka officer, but then fell victim to Stalin\u2019s Terror and was shot in 1937; Buikis survived by changing his identity - Okhrana agents provocateurs - Bolshevik experience as an underground movement: Use of pseudonyms (\u2019Lenin\u2019, \u2018Stalin\u2019) Chapter 3: The Great Illegals The Great Illegals of the interwar period leveraged their personal flair and charisma to achieve remarkable successes against target countries with very weak security posture. Some of their earliest successes were in obtaining diplomatic cipher material, which was passed to a large SIGINT agency where diplomatic traffic was deciphered. Stalin didn\u2019t trust anyone to analyze the intelligence for him and acted as his own intelligence analyst. This reinforced his warped worldview as the secret services produced reports that catered to his paranoid suspicions. - Great Illegals were unique and remarkable spies: multilingual Central Europeans with great faith in Communist future; freer from bureaucracy, in comparison to post-war period - Target countries had very lax security - First successes were in obtaining diplomatic ciphers - Dmitri Aleksandrovich Bystroletov (HANS, ANDREI): very handsome, extroverted: portrait hangs in secret memory room of SVR Center in Yasenovo - Seduced female staff in foreign embassies: Prague, 1927: seduced 29 y.o. secretary in French embassy (LAROCHE) who provided British and Italian diplomatic ciphers and classified communiques for 2 years - Agents introduced ANDREI to other sources of information - Oldham , who provided British ciphers, provided introduction to Raymond Oake (SHELLEY) - De Ry provided Italian ciphers, provided introduction to Rodolphe Lemoine - Rodolphe Lemoine (JOSEPH) - Passion for espionage: began work for French Deuxi\u00e8me Bureau (DB) in 1918 - Recruited German cipher clerk in 1931 who was DB\u2019s most important source for a decade - Some intel fed into Enigma code breaking machines - Eventually passed to Ignace Reiss (RAYMOND), who defected in 1937 - Henri Christian Piecke (COOPER) - Flamboyant Dutch artist - John H. King (MAG) - Irish, hated English - Classified Foreign Office communications coroborated by DUNCAN (below) - Moisei Markovich Akselrod (OST, OSTO) - Jewish family, born 1898 - hired by INO in 1922 - Multilingual: Arabic, French, German, English, Italian - Francesco Constantini (DUNCAN) - Classified documents, ciphers from British Embassy in Rome - Also sold documents to Italian intelligence - Continued to provide intelligence after dismissal through brother Secondo Constantini (DUDLEY), also employed at embassy - Executed during Great Terror - Joint OGPU/Fourth Department SIGINT agency decrypted diplomatic traffic - largest SIGINT agency in the world at the time - No analysis of intelligence - Stalin considered analysis to be \u201cguesswork\u201d - Conspiracy theories of Stalin\u2019s continue to survive to this day Chapter 4: The Magnificent Five Arnold Deutsch established the recruiting strategy for the Magnificent Five, young talents in Oxford and Cambridge Universities with Communist sympathies who became the most successful Soviet penetrations of Western governments during WW2. Arnold Deutsch (STEFAN, OTTO): True believer in Communism, chemistry PhD from Vienna University, five years after entering as undergraduate; began work for INO in 1932. Recruited 20 agents, including C5, over 4 years as controller Kim Philby (SOHNCHEN, SYNOK): heterosexual athlete; was not productive before 1937, when he was sent to Spain as war correspondent, wounded, and ultimately awarded medal by Franco, whom he was supposed to assassinate Donald MacLean (WAISE, SIROTA): bisexual, approached by Philby in 1934; Foreign Office Guy Burgess (MADCHEN): flamboyant homosexual and social butterfly Joined SIS in 1938, in newly founded Section D (covert action and influence) Anthony Blunt: homosexual, introduction by Burgess Talent spotter John Cairncross (MOLI\u00c8RE, LISZT): polygamist, spotted by Blunt, approached by Burgess, recruited by Klugmann; Foreign Office Norman Klugmann (MER): prominent Communist activist who acted as talent spotter for NKVD, recr. 1936. Given away by Ignace Poretsky in 1937 Teodor Maly (MANN) Hungarian POW during WW1, joined Bolsheviks during Revolution Head of London residency in 1936, where he completed recruitment of C5 with Deutsch Recalled to Moscow during Great Terror, shot in 1937 (Ch. 5) Internal turmoil in Soviet Union affected espionage Hunt for Trotskyites became priority by end of 1937 Great Terror: all 3 of Deutsch\u2019s residents during residency in London were executed Chapter 5: Terror The fantasy of a Trotskyite conspiracy increasingly obsessed Stalin during the 1930s, who directed the NKVD and OGPU to penetrate Trotsky\u2019s organization. Trotskyites became targets of a cell of assassins called the Administration for Special Tasks, based out of the Paris residency. The Great Terror resulted in the liquidation of so many NKVD officers that tradecraft suffered. The Cambridge Five themselves, despite the quality of intelligence they provided, were suspected of being plants. \u2022 Mark Zborowski (MAKS, MAK, TULIP, KANT): Russian-born Polish Communist who deeply penetrated Trotsky\u2019s entourage \u25e6 Confidant of Lev Sedov, elder son of Trotsky \u25aa Entrusted with key to Sedov\u2019s letterbox and Trotsky\u2019s most confidential files and archives \u25aa Convinced Sedov to go to a Russian clinic for appendicitis while assassination was being planned \u25e6 After Sedov\u2019s death, encouraged internecine warfare between Trotskyites \u25e6 Orlov knew his first name and attempted to warn Trotsky after defection in 1938 \u2022 NKVD Administration for Special Tasks specialized in assassination and abduction, especially in France, headed by Yasha Serebryansky, resident in Paris \u25e6 Largest section of Soviet foreign intelligence by 1938, claiming to have 212 illegals in 16 countries \u25e6 Trained members of International Brigades in sabotage \u25e6 Main task was surveillance and destabilization of French Trotskyites \u25aa theft of Trotsky\u2019s papers from a Paris flat coordinated by Zborowski, who escaped suspicion \u25e6 Abduction of General Yevgeni Karlovich Miller \u25aa Entourage penetrated: Miller\u2019s deputy was NKVD agent \u25aa Another illegal was used to surveil Miller \u25aa Miller disappeared in broad daylight on a Paris street, drugged, packed in a heavy trunk, and sent to Moscow by Soviet freighter where he was interrogated and shot \u25e6 Assassination of Lev Sedov \u25aa Op was aborted after furor re. NKVD involvement in Miller\u2019s disappearance \u25aa Sedov developed appendicitis, died mysteriously a few days after a successful operation in Russian clinic (at Zborowski\u2019s insistence) \u25aa NKVD had a sophisticated medical section called the Kamera, experimented with lethal drugs \u25e6 Assassination of Rudolf Klement: secretary of Trotsky\u2019s Fourth International \u25e6 Assassination of Ignace Poretsky (Reiss, RAYMOND) using machine gun and chocolates laced with strychnine \u25e6 Assassination of Leon Trotsky: operation UTKA \u201cduck\u201d became chief Soviet foreign policy objective, to be effected by three groups \u25aa Penetration by illegal Ram\u00f3n Mercader (RAYMOND, alias Frank Jacson sic ), who seduced a Trotskyite secretary \u25aa Succeeded in killing Trotsky with icepick, caught and sentenced to 20 years imprisonment \u25aa Hero\u2019s welcome in Moscow 1960 \u25aa Assault on villa led by David Alfaro Siqueiros (KONE), Communist painter \u25aa Iosif Romualdovich Grigulevich (MAKS, FELIPE), member of Serebryanksy\u2019s cell, real leader of assault \u25aa Escaped to Argentina where he planted hundreds of mines in cargo ships bound for Germany \u2022 Spanish Civil War was training ground for saboteurs and battlefield against Franco\u2019s fascists as well as Trotskyites \u25e6 Orlov coordinated two-front war in Spain, ultimate goal was to build a secret police force under Soviet control \u25aa NKVD assassins murdered Andreu Nin, head of a Trotskyist workers\u2019 organization, as well as dozens of other Trotskyites in Spain \u25aa Orlov eventually defected to the US \u25e6 Stanislav Alekseyevich Vaupshasov: top assassin - Led raids on Polish and Lithuanian border villages dressed in Polish and Lithuanian army uniforms in the 1920s - Murdered a colleague in 1929 - Constructed and guarded secret crematorium which disposed of NKVD victims (SVR still considers this topic sensitive, gave hush money to female relative of the NKVD agent in charge of guarding this crematorium) - Great Terror sprung from Stalin\u2019s obsession with counterrevolutionaries - Leadership of NKVD liquidated and re-liquidated - Nikolai Ivanovich Yezhov, head of NKVD 1936-1938: author of Great Terror, replaced Yagoda who soon made absurd confessions - Replaced by Lavrenti Beria in December 1938 before accused of conspiracy - Abram Slutsky, chief of INO, poisoned by cyanide in 1938 - Slutsky\u2019s successors also shot before the end of the same year - NKVD officers were liquidated - Had to be careful even of body language or sighing - Officers most quick to denounce peers of imaginary crimes were most likely to survive Most of the Great Illegals were liquidated by 1938, except for: - Deutsch who was betrayed in 1937 by Ignace Poretsky (Reiss, RAYMOND) - Bystroletov brutally tortured before confession, imprisonment; wife sent to gulag where she cut her own throat with a kitchen knife; mother poisoned herself - Serebryansky himself recalled to Moscow and condemned in 1938 - Show trials depicted a vast, absurd conspiracy authored by Stalin, who proofread transcripts before publication - Great Terror and British operations - C5 transferred to legal residency, where controllers were much less experienced - MacLean seduced his new Soviet controller (NORMA, ADA) - Condemnation of C5\u2019s controllers and recruiters as enemies of the people placed their intelligence and reliability under question - Beria eventually disbanded the residency in 1940, Centre ordered all contact with Philby and Burgess to be broken off - Ideological commitment of C5 remained strong even after the Molotov-Ribbentropp pact - British agents were motivated out of revilement for fascism - Some agents ended their espionage - Histories of Stalin era still whitewash the emphasis on assassination of political opposition in Western Europe South Africa Intelligence's role in supporting counter-revolutionary operations of the apartheid state, as well as in the AND and SACP's efforts to overthrow the state. Five key uses of intelligence in counter-revolutionary struggle:[^4] Targeting enemies of the state, internal (including South West Africa) and external (including in Europe) ZA relationships with other states (Angola, Botswana, Mozambique, Tanzania, Zambia, and Zimbabwe) as anti colonial movement spread, isolating ZA Anti-Communist paradigm of Cold War, alliance with US, UK, West Germany, and Israel Overcoming anti-apartheid sanctions from the 1970s and 80s Developing a nuclear weapon Relationship with British services strained because of Afrikaner resentment, British concern of infiltration by Afrikaner nationalists. In the nineteenth century, the Boer republics (Transvaal Republic and Orange Free State) maintained limited intelligence organizations led by Cornelius Smidt and Willem Leyds (Transvaaler Secret Service).[^5] South African Police Detective Branch (SAP/DB) conducted counter-subversion and counterintelligence after 1899-1902 Anglo-Boer War and the establishment of the Union of South Africa in 1910. SAP/DB concentrated mostly on Nazi sympathizers in South West Africa until 1948, when the National Party (NP) was elected.[^6] MI5 conducted foreign intelligence but also watched radical Afrikaner groups such as Ossewa-Brandwag (OB), a paramilitary organization in competition with NP and with many sympathizers in SAP. Union Defense Forces, considered an anglophile institution, cooperated with MI5 on internal threats (Afrikaner nationalists and Republicans) and British Special Operations Executive (SOE), operating out of Durban. MI5 Director General Sir Percy Sillitoe served in South Africa and Rhodesia and was a key influence in shaping the South African intelligence structure. SAP Special Branch (SB) founded as Special Staff to hunt Nazis in 1939 before being redirected to investigating political crimes. Increasingly into the 1950s, became known as Security Branch. Union Defense Forces Department of Military Intelligence (DMI) created Feb 1940 but neglected after 1948. After independence from Britain in 1961, UDF became South African Defense Forces (SADF) and established a new Directorate of Military Intelligence (DMI) July 1962. Rivalry between SB and DMI survived administrative efforts to coordinate intelligence functions: State Security Committee (est. 1963) and State Security Advisory Board (est. 1966). SB\u2019s criminal emphasis hampered intelligence work, and a central intelligence agency was needed, sought first in Republican Intelligence (RI) spun off from SB 1963, then found in BOSS. Bureau for State Security (BOSS) founded 19690513 as a central intelligence apparatus to mitigate the rivalry between DMI and SAP. Reorganized as National Intelligence Service (NIS) in 1980. Grew from 500 personnel in 1969 to more than 1,000 by 1978. Six departments: Subversion, Counter-espionage, Political and Economic Intelligence, Military Intelligence, Administration, and National Evaluation, Research and Special Studies. Forged a relationship with Portuguese and Rhodesian intelligence services. Brought down by government\u2019s increased reliance on COIN strategies (vice counterintelligence and counter-espionage) and by the Information Scandal. African National Congress (ANC) , Umkhonto weSizwe (MK) , and South African Communist Party (SACP) established intelligence structures separate but parallel to that of the government during their armed struggle against the apartheid state. ANC established Department of Intelligence and Security (DIS). DMI gained control of \u201cTotal National Strategy\u201d and achieved dominance over BOSS. DMI strategists implemented various counter-insurgency (COIN) strategies they learned from abroad in ZA and SWA. Botha appointed Minister of Defence in 1966 and began a campaign to reinvigorate the SADF. By the beginning of Project SAVANNAH (South African intervention in Angola[^7] in 1975, Botha had begun a process of streamlining the SADF that would culminate years later in the incorporation of COIN principles in the South African Army. Area Defence Policy, later known as National Security Management System, was fully incorporated into the counter-insurgency forces of SADF. COIN forces were made up of part-time SADF personnel from the Citizen Force and Commandos and were divided into ten regional commands covering the countryside. SADF doubled in size between 1975 and 1990, reaching almost 100,000 with another 325,000 in the Citizen Force and Reserves ZA security confronted radical new challenges from 1975 to 1978. Key governments on ZA's periphery had turned hostile. Marxist MPLA took over Angola and hosted SWAPO bases on the border of South West Africa. Mozambique's new government was Marxist. Mozambique became a shelter for Rhodesian guerrillas, threatening ZA's last counter-revolutionary ally. Radicalization of blacks after 1976 uprising in Soweto overwhelmed internal security, which responded by introducing COIN concepts and reacting more harshly to protests. SADF COIN expertise was gained in collaboration with Rhodesian ISS and SOF during the 1960s and 1970s. ZA intelligence establishment absorbed members of Rhodesian Security Forces in the transition to majority rule in 1980 (Operation Winter).[^8] COIN Strategy Adopted Wholesale, New Units Established Opposition to apartheid state grew more radical while the state\u2019s response to opponents hardened. A sustained domestic protest movement called United Democratic Front ultimately gave ANC-MK their long sought-after internal subversion capability within ZA. Pretoria adopted COIN strategies in response to failing effort against MPLA in AO and SWAPO in SWA and deteriorating domestic security as MK stepped up attacks. K-Unit \u201cKoevoet\u201d founded by SAP/SB in January 1979, building on their decades of cooperation with Rhodesians in COIN operations. Formed after the Selous Scouts and Portuguese Flechas for the purpose of turning captured ANC, SWAPO, and PAC guerrillas called askaris. Turned SWAPO fighters who collaborated with Koevoet were known as makakunyanas \"blood-suckers\", and Koevoet collaborators were targeted for assassination by SWAPO. They were paid for each guerrilla killed. SAP/C1 \u201cVlakplaas\u201d named after the police farm outside Pretoria. Originally established 1979 when BG Johan Coetzee, C/SB, decided to use COIN activities for counter-revolutionary purposes within ZA. Revealed by Coetzee in the press in 1989 and disbanded in 1993. Koevoet elements including de Kock were withdrawn from SWA to form C1 in May 1983. Identification, tracking, and \"rehabilitation\" (turning) of ANC and PAC guerrillas. C1 also assassinated up to 65 people from 1980 to 1991. C1\u2019s operations in Swaziland were to disrupt the ANC/MK structures there. 22 ANC members were assassinated in Swaziland in the 1980s. C2 established concurrently to track activists leaving ZA and to interrogate arrested guerrillas SAP/G section , responsible for the penetration of ANC abroad, was resurrected after the reorganization of BOSS in 1980. G section attempted to assassinate ANC/MK strategist Joe Slovo, killing his wife instead. G section also blew up ANC\u2019s London office Directorate Covert Collection (Direktoraat von Koverte Insameling) established at an unclear date, but possible predecessor was Directorate of Covert Information, active in SWA by 1982. Established multiple front companies with the goal of duplicating successful COIN operations of AO and SWA within South Africa. Spread of liberation movements across Southern Africa removed governments traditionally friendly to Pretoria, which saw all black liberation movements as Communist-backed threats to regime. South African government resorted to policy of destabilizing neighbors and conducting covert action to remove safe havens for ANC/MK. Eventually South Africa\u2019s hand in regional politics became transparent. DMI integrated intelligence collection with covert action by taking control of South African Special Forces (SASF) in 1979. Rhodesian special forces integrated into SADF in 1980, further buttressing COIN capabilities. Directorate of Special Tasks (DST) formed in mid-1970s from a corps of DMI operators named Spesmagte, similar to Recces. DST was responsible for overseeing contra-mobilization and counter-revolutionary activities of DMI throughout southern Africa as part of a strategy to deny ANC-SACP safe havens in Frontline States. DST began operations in an office in Rundu, Namibia in 1976 in the wake of South African withdrawal from AO. First chief was COL Cornelius van Niekerk. DST terminated operations in the early 1990s. Two sections: - DST-1 (external operations) covered UNITA, RENAMO, and Zimbabwe - DST-2 (internal operations) covered Lesotho Liberation Army (LLA), and Operations MARION and KATZEN DST maintained logistical infrastructure throughout Southern Africa and conducted several operations, providing support to anti-Marxist proxies in neighboring countries: - DISA/SILWER : support to UNITA in Angola - DRAMA : support to Zimbabwean dissidents - PIKI/PUNDU MILIA/ALTAR : support to RENAMO and operations against FRELIMO in Mozambique. 5 Recce was principally responsible because many 5 Recce personnel had trained with the Selous Scouts. When RENAMO\u2019s headquarters moved from Phalaborwa, Transvaal (!) to Gorongosa, Mozambique, South African officers followed offering intelligence training. - PLATHOND : support to surrogate force in Zambia - CAPSIZE/LATSA : support to Lesotho proxy grou 5 Recce based out of Phalaborwa, Transvaal. Supported Op PIKI/PUNDU MILIA. Also used pseudo-operations against SWAPO in Namibia, sometimes cooperating with Koevoet. DCC mobilized contras in Namibia. Various operations: - ETANGO : DCC and other DMI units attempted to establish a conservative contra based in Ovambo tribalism to counter SWAPO. - EZUVA : Similar project to establish a contra among the Kavango. Many experienced contra-mobilizers from DCC moved into domestic contra-mobilization 1985-1986, setting up groups to foment black-on-black violence and undermine support of ANC and UDF: 23 such projects by 1986. Operations included: - Operation MARION provided security training and weapons to more than 200 Inkatha cadres 1986-1990. These units later conducted targeted killings. Inkatha had been supported by BOSS as an alternative to ANC from 1975, including funds and stage-managing internal political rivals to Chief Buthulezi. - Operation KATZEN was an attempt to organize a contra group among the Xhosa known as the Xhosa Resistance Movement (XWB, known as Iliso Lomzi). Cooperated with Army Intelligence Hammer units, which conducted special operations. Civil Cooperation Bureau (CCB) , also known as Burgerlike Samewerkingsburo (BSB), was formed in 1986 out of Operation BARNACLE. Formed by SASF to fulfill the requirement of domestic intelligence collection, which was used primarily for external operations. CCB was imagined to be fully functional only in the mid-nineties, possibly to conduct counter-revolutionary warfare after the transfer to black rule had been completed. CCB was organized as a corporation into Regions that coordinated covert activities in concert with other state bodies. CCB numbered up to 250-300 individuals. By the late 1980s, CCB had established numerous front-companies and businesses and was involved in lucrative criminal activities. Operations: - CCB conducted internal assassinations in line with the state\u2019s emphasis on counter-revolutionary warfare. As such, CCB was DMI\u2019s equivalent to SAP\u2019s C1. - CCB supported SASF in operations in the Frontline States by undertaking reconnaissance of ANC targets. 3 Recce (active 1980-1981) absorbed the remnants of Rhodesian special forces which fled Zimbabwe in 1980 as well as DMI\u2019s D-40 assassination unit (active 1979-1980) led by the Rhodesian Garth Barrett. 3 Recce operated against Zimbabwe, exploding very destructive bombs and assassinating ANC\u2019s representative in Harare.[^9] Operation BARNACLE , conducted by a group 30-40 mostly black ex-Rhodesians, was a project to use CBW to assassinate guerrillas, SWAPO prisoners of war, and members of South African security forces suspected of disloyalty with poison. BARNACLE was to be a completely independent resource at the disposal of the country\u2019s leaders, to serve as a hedge against the prospect of the government granting too much power to blacks. BARNACLE actors were not accountable to official security organs or to SADF commanders: they reported directly to General Office Commanding Special Forces (GOC-SF). Later reorganized into the Civil Cooperation Bureau. Notable People BG J.P. Tolletjie Botha ran Directorate Covert Collection. COL Jan Breytenbach founded the Reconnaissance Commandos, 32 Battalion, and the Directorate Special Tasks. Chief Buthulezi was leader of Inkatha and a BOSS stooge. COL Eugene de Kock was one of the most ruthless and effective of Koevoet's commanders. Eleven tours of duty between 1968 and 1973 in Rhodesia with Rhodesian SAS and Rhodesian African Rifles. Commanded Koevoet for four years before he requested a transfer to SAP/C1, where he assumed command in 1985. De Kock emphasized the assassination program and introduced paramilitary training for police members. During this time, De Kock became known as Prime Evil for his mercilessness. MAJ Craig Williamson was a counterintelligence operative in SAP/G Section from... to... Williamson infiltrated ANC by using the International University Exchange Fund (IUEF), but was exposed in 1980. Williamson served in SAP/G until December 1985. Williamson also established Longreach Pty Ltd in April 1986, which served as a front company for SAP/SB operations and also coordinated DMI and SASF operations. Sweden There was no specific institution in Sweden for intelligence before the 1930s. Navy intercepted communications and diplomats gather intelligence.[^10] Military Intelligence Before WW2 General Staff gathered intelligence against Norway during the war of secession Swedish Defense Staff established in 1937. Intelligence branch formed with 20 officers. Mostly OSINT collection and attache reporting from 15 attaches and SIGINT. Intelligence Organizations Formed in WW2 C-Bureau (central byr\u00e5a) under Defense Staff, but separate from Intelligence Division, established large intelligence network that was not fully documented. Cryptographic Department (CD) of Defense Staff took over SIGINT from Navy using civilians to succesfully break Soviet crypto and comms. Results were shared with Finnish. Decrypted machine crypto used over landlines. 150,000 cables over two years, until Germans changed codes. Swedish were unable to fully exploit this windfall of intelligence. CD reformed as F\u00e5ssvarets Radioanstalt (FRA), directly under Ministry of Defense. General Security Service (Allm\u00e4nna S\u00e4kerhetaj\u00e4nsten, GSS) formed out of a secret, extralegal government decision, established massive program of unlimited authority to monitor telephone, telegraph, and mail communications. Dissolved after scandal in 1946. Postwar Developments 2 of the 3 wartime intelligence agencies would be resurrected after dissolution 1947-1948. India Kautilya\u2019s Arthashastra[^11] - Constitutes a doctrine of statecraft using espionage as a basic means of governance. - Comprehensive textbook on statecraft, foreign diplomacy, and war emphasizing the collection of domestic and foreign intelligence. - Rediscovered and translated by Orientalist Rudrapatnam Shamasasty in 1909-1915. Written by Kautilya, trusted advisor to Chadragupta Maurya, founder of Mauryan Empire 321-185 BCE. Eight Institutes of Espionage Four were forms of religious cover (fraudulent disciple, recluse, ascetic, and mendicant woman), which were to take advantage of the intensely religious population of India. Religious class had access to other castes. Wandering female spies were to take religious cover, poor widows of Brahman caste were to target upper castes while Sudra-caste women willing to shave their head were to target lower caste communities. Classmate spies referred to recruitment pool, and the preferred choice for courier. Firebrands were to be used for covert action as assassins, agent provocateurs, and saboteurs. Result was a pervasive surveillance network covering the whole country. Treasury was also to have intelligence function Householder spies were to ascertain validity of assets Merchant spies were to monitor price changes and foreign goods Networks of spies under cover of bands of thieves would monitor the criminal underworld Provocation and entrapment are standard tactics in Arthashastra. Islamic Caliphate Pre-Islamic Arab tradition of intelligence predated a more developed intelligence culture in the Islamic age.[^12] Various words referring to spy or scout: - jasus : foreign spy - tajassasah : discouraged in the Qur\u2019an - \u2018ain : \u201ceye\u201d - suhhar : night sentinels who kept watch for strangers at approaches to market town or crossroads - rabi\u2019ah : lookout - other words Espionage in Early Arab States Lakhmid and Ghassanid buffer states between Sasanian Persia and Byzantium: - Story in 10th c. Kitab al-Aghani relates Lakhmid spies catching a would-be assassin and killing him in the 6th century - Scouts in Arabian peninsula Brigandage among Bedouin Skirmishes and raids ( suluk ) involved use of scouts - al-Basus War was remembered as the days of rabi\u2019ah - Abu Faraj al-Asbahani\u2019s anthology of Arabic verse - Muhammad b. al-Tabari on agents using disguises Muhammad and Intelligence Qur\u2019anic regulations on espionage reflect importance of clandestine activites in early Islam. The Prophet Muhammad\u2019s involvement in intelligence and espionage: - Muhammad gathered information on early converts, seeking candidates with honesty, trustworthiness, and the ability to keep a secret - Muhammad possessed detailed knowledge of clan loyalties and politics, and used this knowledge in negotiations with Bedouin - Abdullah b. Abu Bakr mingled with Quraishis of Mecca and report back to him at night in his cave. Abu Bakr\u2019s sister Asma also spied for the Prophet: first spies for Islam. - Abu al-Fadhl al-Abbas ran spy network in Mecca - Many \u2018ains from various corners of Arabia, from every town and tribe - Muhammad deliberately retreated during the Battle of Uhud to allow his lookouts to determine the size of the army, whether it had mounted camels (to retreat) or horses (to attack) - Muhammad debriefed two boys who were caught drawing water from a well before the Battle of Badr. They divulged key intelligence on the closing Quraishi army. Deception After the indecisive Battle of Uhud, Muhammad sent one of his \u2018ains to deceive the Quraishis into thinking a large host was approaching, causing them to retreat. Assassination of poets Poets had a complex role in Arab society and were highly influential - Asma of Marwan was stabbed in her sleep, but w\u2019o Prophet\u2019s prior approval - Abu Afak killed by fellow tribesmen - False prophet al-Aswad al-Ansi became influential and killed the governor of Yemen. Muhammad ordered Wabrak b. Yahmus to organize a plot. Wabrak recruited a circle of Persian Muslim converts and the governor\u2019s wife, who facilitated the operatives\u2019 infiltration into the castle where al-Ansi was murdered Intelligence by Islam\u2019s enemies Abu Sufyan determined Muslim spies were present by finding date seeds in camel dung, indicating the animals had Medinan fodder Multiple assassination attempts on Muhammad\u2019s life Umar b. al-Khattab in charge of counterintelligence and security Byzantines sent a monk who claimed to be a convert to Islam and established a mosque in Medina. Muslim agents surveilled the mosque after suspicious comings and goings and ultimately demolished the mosque. US Corporate espionage Vice reported that McDonald's had established an intelligence unit to monitor workers who supported increasing the minimum wage. The unit had targeted the Fight for $15 movement for increasing the wage to $15 an hour in particular. One intelligence report titled \"Ongoing FF$15 Activity Against McDonald's During the COVID-19 Crisis\" contained an analysis of the activities of labor activists. McDonald's had been using two different data collection software suites to collect open-source intelligence on the social networks of workers involved in the labor movement. COMINT War Department set up first organized cryptanalytic office in June 1917, numbering 3 people. By war\u2019s end, it would grow to 150. Navy In 1917 and 1918, Navy set up medium frequency DF stations along Atlantic coast to track U-boats. HFDF stations were researched and deployed by 1938. Strategic HFDF stations were established at Manila, Guam, Midway, Oahu, Dutch Harbor, Samoa, Canal Zone in Panama, San Juan Puerto Rico, and Greenland. US began tracking Japanese warships and merchant vessels in 1939, five years after the Japanese had begun tracking US vessels. Navy had established the Code and Signal Section of Naval Communications for producing codes and ciphers for use by Navy. Registered Publication Section, responsible for distribution of secret and confidential documents, was spun off in 1923. Navy funded development of Electric Cipher Machine from 1922. Communications Intelligence Organization (CIO) was established 1924. Intercept stations were established in the Pacific Area (Shanghai, Oahu, Peking, Guam, Manila, Bar Harbor, Astoria), and Washington DC. Cryptanalytic Units established in Manila and Pearl Harbor. Training was done with technical manuals, using the codes to send messages. Minor intercept activities were performed in strategic HFDF stations. In 1938, CIO became the Communications Security Group (CSG) and took over all Navy DF stations. By 1941, CSG had 730 total personnel. Arm Army Signal Corps founded 1860 by Albert James Myer, inventor of wig-wag flag signaling method. Signal Intelligence Service (SIS) founded 1930 as a secret part of Signal Corps for cryptanalysis. By 1939, SIS made use of 7 intercept sites from the Philippines and Hawaii in the West to the East Coast of the US. These were the sources of SIS intercepts until after Pearl Harbor. At SIS HQ in Arlington Hall, traffic was split between four analytic sections: - J: Japanese - G: German - I: Italian - M: Mexican and Latin American Although SIS intercepted tens of thousands of IJA messages from its station in Manila, these messages could not be fully exploited because IJA ciphers were not broken. However, SIS broke several diplomatic ciphers including Purple. After the war, SIS changed its name to the Army Security Agency (ASA) in 1945. In 1947, ASA and the Army Intelligence Agency were merged into the newly formed Intelligence and Security Command (INSCOM). Radio Free Europe Established 1949 by the National Committee for a Free Europe (NCFE), an anticommunist organization with Allen Dulles and Dwight D. Eisenhower as board members. Funded by CIA until 1972. Targeted Eastern European countries (as opposed to Radio Liberty). Radio Liberty Established by American Committee for the Liberation of the Peoples of Russia (Amcomlib) 1951. By 1954 was broadcasting in several other Central Asian languages. Foreign Broadcast Information Service Foreign Broadcast Monitoring Service, or FBMS, established 1941 under FCC to monitor Axis shortwave broadcasts to the US. Name changed to FBIS 1947 when it was made part of the new CIA. France (Andrew Orr) France took over Syria and Turkey\u2019s SE (Cilicia) after WW1 and perceived Turkish War of Independence as threat to its new imperial interests. Military intelligence services of Army and Navy monitored Kemalist movements and always saw the hand of German and Russian commies behind Turkey\u2019s developments. 3 reasons German closely involved in Ottoman military affairs from 1883 and German general Otto Liman von Sanders commanded the Ottoman Army Germans had also promoted pan-Islamic movement during the War Germans possibly still controlled Russia, according to the French, reasoning that they had sent Lenin to Russia inside of a sealed train car Colonial intelligence services lacked some of the checks on extreme predictions when reporting on events outside of Europe Military Intelligence French Army\u2019s Service de renseignments guerre ( SR Guerre ), which was part of a unified French intelligence organization during the War, but then reverted to the Army\u2019s II Bureau after its end. During the Turkish War of Independence, SRG deployed a small number of officers to the Middle East, stations opened in Constantinople in 1919, Algiers 1925, Rabat and Tangiers 1929. French Navy service ( SR Marine ) was similarly structured to Army, but with more familiarity with Mediterranean Sea and the Middle East SRM opened Constantinople station in 1919 SR sources included Europeans and Americans fleeing Turkey, human informants, and newspaper articles, as well as intercepted radio messages sometimes by way of the Brits Coincidence of treaty signings between USSR, Turkey, and Afghanistan and Persia led SR personnel to believe there was a plot brewing Poland From Anglo-Polish HUMINT, by PRJ Winter Histories of WW2 intelligence exalt COMINT successes of GCCS to the detriment of MI6 Historiography of British WW2 intelligence Agents - Paul Thummel (codename A-54), senior officer of German MI (Abwehr), recruited by Czechs 1936. SIS and Czechs ran him jointly from 1939 until he was arrested by the Gestapo and died in prison 1942. - Warlock, on staff of German High Command (Oberkommando der Wehrmacht, OKW), turned by 1941. - Knopf, reported on OKW intentions to take Malta, but records show that Germans were noncommital to the plan (Op Herkules) which would only support Italia High Command. Knopf may have been turned by this time. MI14\u2019s evaluation gave Knopf a mixed, but generally positive score. Polish government-in-exile settled in London and cooperated with British intelligence (II Bureau of Polish General Staff) SIS cooperation with Poles was driven out of desperation because aside from Warlock they had no useful penetrations of Nazi Germany CX reports were SIS, JX reports were from Poles. Some JX reports found in British National archives, including one report on Malta which was passed to Churchill himself. Poles ran sources reporting from the heart of OKW and OKH (Oberkommando des Heeres, Supreme High Command of the German Army) British interception of Polish communications confirmed Knopf\u2019s bona fides (as agent number 594) and the Poles\u2019 as well [^1]: Sawyer, Ralph D. \u201cSubversive Information: The Historical Thrust of Chinese Intelligence.\u201d Intelligence Elsewhere: Spies and Espionage Outside the Anglosphere . Ed. Philip H. J. Davies, Ed. Kristian C. Gustafson [^2]: [^3]: Homstr\u00f6m, Lauri. \u201cFinnish Security and Intelligence Service.\u201d Intelligence Elsewhere: Spies and Espionage Outside the Anglosphere .Ed. Philip H. J. Davies, Ed. Kristian C. Gustafson [^4]: O\u2019Brien, Kevin A. The South African Intelligence Services: From apartheid to democracy, 1948-2005 . Routledge: New York, NY 2011. [^5]: Blackburn, Douglas and Caddel, W. Waithman. Secret Service in South Africa . Honolulu: University Press of the Pacific, 2001. Swanepoel, P.C. Really Inside BOSS: A Tale of South Africa\u2019s Late Intelligence Service (And Something about the CIA) . Pretoria, 2008. [^6]: Ref. Kent Fedorowich, \u201cGerman espionage and British counter-intelligence in ZA and Mozambique, 1939-1944\u201d , The Historical Journal 48:1 [^7]: Robin Hallett, \u201cThe ZA Intervention in Angola,\u201d African Affairs 77:312 (July 1978) [^8]: Ngwabi Bhebe, Terence Ranger, Soldiers in Zimbabwe's Liberation War , 1995. [^9]: D-40 in turn was the reconstitution of the supposedly disbanded Z-squads used by BOSS for assassinations until its reorganization in 1979. [^10]: Agrell, Wilhelm. \u201cSweden: Intelligence the Middle Way.\u201d Intelligence Elsewhere: Spies and Espionage Outside the Anglosphere . Ed. Philip H. J. Davies, Ed. Kristian C. Gustafson [^11]: Davies, Philip H. J. \u201cThe Original Surveillance State: Kautilya\u2019s Arthashastra and Government by Espionage in Classical India. Intelligence Elsewhere: Spies and Espionage Outside the Anglosphere . Ed. Philip H. J. Davies, Ed. Kristian C. Gustafson [^12]: Al-Asmari, Abdulaziz A. \u201cOrigins of an Arab and Islamic Intelligence Culture.\u201d Intelligence Elsewhere: Spies and Espionage Outside the Anglosphere . Ed. Philip H. J. Davies, Ed. Kristian C. Gustafson United States Linda Zall established a program to analyze classified historical statellite imagery to analyze not foreign militaries but changes in the environment, in particular the extent of ice retreat in the polar regions of the Earth. The effort was sparked by then-Senator Al Gore whose letter to the CIA led to the establishment of the MEDEA program which declassified satellite imagery and oceanographic data. John Walker was a notorious spy who volunteered to the Soviet embassy in Washington in 1967. Especially after the North Koreans captured the USS Pueblo, Walker's information on the key list of the KL-47 cryptographic machine meant the Soviets were able to read US Navy communications until the entire system was replaced. John Walker was managed by former KGB general Oleg Kalugin. The acoustic characteristics of the Victor III submarine were made substantially less detectable as a result of Walker's revelation that the Soviet submarine fleet was easily tracked. The stealthy Akula-class submarines, launched in 1985, also benefited from the import of a Toshiba CNC milling machine , which in combination with Norwegian CNC machines , allowed propellors to be designed that were much quieter than before.","title":"\ud83d\udd0e Intelligence"},{"location":"Intelligence/#intelligence","text":"","title":"\ud83d\udd0e Intelligence"},{"location":"Intelligence/#china","text":"Chinese writers often wrote on intelligence matters in contrast with Western writers[^1] Historical writings on intelligence continue to be used by military institutions of PRC Fragments of earlier works \u201cprotobooks\u201d survive in later works. Sunzi wrote that warfare must be undertaken only after a comprehensive analysis of relative strengths and weaknesses of combatants, the world\u2019s first net assessment procedure. He also advocated deception on the battlefield. Later writers responded to Sunzi, updating and interpreting his precepts: Guan Zhong Li Quan, Li Jing. Sunzi condemned generals who failed to gather necessary intelligence and disparaged divination and consulting ancestors in favor of hard intelligence. Later writers like Shi Zimei also upbraided rulers who ignored these rules. Works advocating covert action - Toubi Fuban - Warring States period works on statecraft, e.g. Han Feizi Sunzi\u2019s Five Types of Spy 1. Local Spy Knowledgeable spies living outside their native habitat, including emigrants, travelers and exiles (e.g. defectors) 2. Internal spy Officials 3. Turned spy Double agent 4. Living spy Talented people dispatched abroad to observe and then report back 5. Dead spy Spies deliberately sent on suicide missions without their knowledge. Enemy agents could also be deceived and provided with false information, causing them to be executed when they report false information to their masters.","title":"China"},{"location":"Intelligence/#subversion","text":"By end of Spring and Autumn Period, subversion was an accepted goal of espionage. Assassination used most often in periods of fragmentation. Six Secret Teachings compiled two chapters on systematic programs of subversion. These two chapters, with Sunzi\u2019s \u201cEmploying Spies\u201d in Art of War formed the basis of espionage as late as the Ming Dynasty. Later military writers also devoted at least a few paragraphs to subversion: Le Quan\u2019s Techniques for Secret Plots, Huqian Jing, Bingfa Baiyan. These took historical and semihistorical episodes as examples (Wu Yue Chunqua). Covert action and subversion used by Qin in the western hinterland.","title":"Subversion"},{"location":"Intelligence/#debauching-kings","text":"Sunzi: \u201cYou must first know the names of the defensive commander, his assistants, staff, door guards, and attendants for any armies that you want to strike, cities you want to attack, and men you want to assassinate.\u201d","title":"\u201cDebauching\u201d kings."},{"location":"Intelligence/#egypt","text":"Egyptian intelligence had its origins in the urban policing organizations established during the British Khedive and occupation. Mamur Zapt s were the de facto chiefs of secret police in Cairo and Alexandria, operating networks of mukbireen informants, by the time of British occupation in 1882. The Central Special Office (CSO) was established in 1911 after the Coptic Prime Minister was assassinated by a young nationalist. When discontent exploded in 1919, and CSO couldn\u2019t respond effectively, the Interior Ministry formed the Special Section (SS) to centralize intelligence collection. CSO was closed and files transferred to SS in 1925. Egyptian Military Intelligence was being Egyptianized in the late 1930s. MI officers were complicit in the army coup of 23 July 1952. Once the Free Officers were in power, they set about creating a new intelligence community. The Military Intelligence Directorate (MID) as it was then called, became active in covert action and subversion across the Middle East and Africa. Fear of communist subversion fueled the creation of the General Intelligence Directorate (GID) , subordinate to the Interior Minister, which was led at first by Zakaria Muhi al-Din and stood up within months of the coup. GID inherited and took the place of the old political police organs. GID was renamed the State Security Investigations Service (SSIS) in 1971. As part of Gamal Abd al-Nasser\u2019s emphasis on covert action, the Egyptian General Intelligence Service (EGIS) was established by March 1954, also headed by Zakaria Muhi al-Din. EGIS was the capstone of the new Egyptian government\u2019s emphasis on projecting influence through covert action, and it was modeled after the CIA. During the heyday of Egyptian subversion abroad, Salah Nasr was EGIS Director. Egyptian covert action grew out of the effort to end the British occupation, remove British influence from government organs, and eject the British from the Canal Zone. In 1952, MID created a Special Resistance Office to form a guerrilla army against British military units based along the Suez Canal. The campaign became the template for Egyptian subversion abroad for the next decade and half. MID also sent officers to Sudan to recruit sources and build influence for possible unity with Sudan. While Egypt occupied the Gaza Strip, Palestinian infiltrators were a source of irritation for MID. However, MID sponsored infiltrations using Bedouins, Palestinians, and even its own scouts for intelligence-gathering. These infiltrations eventually were made from southern Lebanon as well. The death of an Israeli settler during one such raid sparked a cycle of reprisals that led to war. The four years after the British withdrawal from the Suez in 1956 marked Egypt at the height of its regional power. Nasser was the hero of the Arab world, and Cairo was the wellspring of a vigorous new Pan-Arab nationalism. However, Egyptian subversion against conservative Arab states and British protectorates was checked in many places by the United States. After Egypt and Syria joined the United Arab Republic (UAR), Nasser began covert action campaigns in Jordan and Lebanon, combining subversion with radio propaganda. In Lebanon, UAR meddling triggered a larger crisis that drew the attention of the US and ended Egyptian arming of Lebanese insurgents. A 1958 coup in Iraq initially drew feelersfrom the Egyptians, who hoped to draw Iraq into their circle of influence. However, old animosities arose again, and by 1959 Egypt was broadcasting radio propaganda, and senior UAR spymasters backed a short-lived army revolt in Mosul. Alienated by Nasser, Saudi Arabia fended off feeble Egypt\u2019s attempts to use its military presence and attache corps there for subversion. Nasser created African Affairs Branch, led by Mohamed Fa\u2019iq, to pursue Egyptian interests clandestinely across Africa. Key targets were Ethiopia and Congo. Egypt promoted Eritrean Liberation Front (ELF) after Suez Crisis and assisted Eritrean declaration of independence by 1960. Close relations with Somalia, which also had territorial disputes with Ethiopia. Egypt provided money, arms to friendly regime in Congo, flew Ghanaian and Egyptian troops to prop up regime. Also provided arms to resistance movements in Angola, Cameroon, Nigeria, and Portuguese Guiana, and training to Congo Brazzaville and Tanzania. MID broke down under pressure of war. Israelis had strong SIGINT and intercepted an insecure phone call between Nasser and King Hussein. Israel also captured a valuable cache of Egyptian intelligence documents in Gaza on MID\u2019s Palestinian infiltrators. Voice of the Arabs (VOA) served as a propaganda arm buttressing Egypt\u2019s foreign subversion policies. Daily radio programming was handled by Egyptian State Broadcasting (ESB) and used transmitters supplied by CIA. ESB turned to former Nazi propagandists, who must have felt at home with anti-Jewish programming. ESB grew out of EGIS promotion of VOA. EGIS had a representative on ESB\u2019s board of directors. EGIS was also responsible for running clandestine radio stations, reliant usually reliant on mobile transmitters. Voice of the Arab Nation targeted Iraq during the 1956 war when the British and French knocked out overt ESB transmitters. Egypt also operated a clandestine radio station called Voice of Free Iran in the early 1960s and gave support to anti-shah clerics. Egyptian intelligence had been preparing Yemen for subversion since 1950s. VOA broadcasted propaganda, and the signal to begin the coup in September 1962 was delivered by VOA. Egyptian army was overwhelmed by Yemeni rural insurgency, because Cairo didn\u2019t plan for a long war. No good cultural intelligence on Yemeni tribes. Intervention antagonized Great Britain and Saudi Arabia. MID provided training to National Liberation Front (NLF) which was set up by Egypt as an anti-British umbrella organization. NLF systematically kidnapped, tortured, and executed Aden SB officers and informants. Egypt still relied on broken Enigma-based encryption. GCHQ and NSA had access to military COMINT. Despite successful Egyptian-backed coup and energetic covert action campaign against British Protectorate in South Yemen, Egypt could not maintain political leverage over Yemen. South Yemen turned to GDR for intelligence training. The British were Egypt\u2019s first instructors in counterintelligence, and also some of its first victims. MI and Cairo SB busted a German spy ring in cooperation with British intelligence (The Key to Rebecca). Operation Susannah was an Israeli terror plot meant to undermine international faith in Cairo\u2019s ability to secure the Suez Canal. In 1954, an Israeli case officer activated sabotage cells among the remaining Egyptian Jewry. Egyptians doubled the Israeli case officer and rolled up the ring. Huge success for nascent Egyptian CI and was an impetus in formation of EGIS. GID rolled up a spy network led by the British expat James Swinburn in 1956. Unfortunately, this had the knock-on effect of forcing the British to rely on their formidable SIGINT collection. GCHQ had broken Egypt\u2019s diplomatic cipher and routinely intercepted Egyptian military communications. Egyptian CI penetrated several Israeli spy rings after the Suez War. The Goudswaard Ring of European and Egyptian nationals was recruited by Mossad. Eventually the GID turned them and were using them to pass disinformation. The Thomas Ring gathered military documents and passed them to Israel in microfilm hidden in furniture which was then exported. In 1961, GID rolled up this network. In the run-up to the Yom Kippur War, Egyptian denial and deception extended to their erstwhile allies the Soviet Union. Soviet photoreconnaissance satellites monitored the war. After the Yom Kippur War, CIA resumed its relationship with Egypt, and along with providing training and equipment, CIA began to aggressively recruit from throughout Egyptian society. Egypt joined the Safari Club (a coalition of intelligence agencies from France, Saudi Arabia, Iran, and Morocco), which provided aid to Zaire, Somalia, Djibouti and anti-Qadhafi groups in Libya and Egypt as well as the Afghan resistance. After Camp David, Egyptian intelligence had to contend with Libyan subversion and radio propaganda, mimicking Egyptian tactics of yore. Covert action escalated to a short border war in 1977. Libyan subversion continued, and in 1978 SSIS uncovered a clandestine organization for the \u201cliberation\u201d of Egypt through sabotage and assassination. Egypt also stopped an Iranian who planned to undermine the peace process by conducting and bomb attacks in Egypt. In 1979, Egyptian CI began a covert war with Soviet and East European services. SSIS claimed to have uncovered a Bulgarian intelligence network that year. In 1981, Soviet and Hungarian diplomats were expelled after Egyptian security discovered a spy ring allegedly inciting sectarian unrest. Israel attempted to subvert Egypt early on by using the Jewish population in Cairo and Alexandria as a base. IDF created sabotage cells which were activated in 1954 when Egypt and UK were negotiating a British withdrawal from the Canal Zone. Operation Susannah was rolled up, and the operatives brutally tortured in a MID prison. The case had political ripple effects within Israel that reached into the 1960s. Several Israeli spy rings were rolled up in the late 1950s and 1960s. The Goudswaard Ring, run by Mossad in a false-flag operation that posed it as a NATO service, was doubled and began passing disinformation. The Thomas Ring, which gathered military documents and passed them to Israel in microfilm hidden in exported furniture, was rolled up by GID in 1961. EGIS claimed to have run a \u201csuper-spy\u201d named Jack Bitton before and during the Six-Day War (1967). Bitton opened a travel agency in Tel Aviv in in 1954 or 1955. Some claim he was doubled by the Israelis, but Egypt claims he provided warning of Israel\u2019s attack through his contacts with senior Israelis. For Egyptians, Bitton is Egypt\u2019s master spy and the subject of a highly popular television serial in the 1980s. Israeli spy Wolfgang Lotz mapped out Egypt\u2019s SAM facilities and other important military infrastructure. Israel also operated an agent \u201cSulayman\u201d who reported the movements of individual Egyptian units during the war. Sulayman may have provided encryption material, because Israeli MI (Aman) broke Egypt\u2019s military cipher before the war. Israeli deception measures went undetected by the Egyptians. IDF created an entire fake military unit in Eilat, complete with encrypted and unencrypted radio traffic, which succeeded in tricking the Egyptians. In the run-up to the Yom Kippur War, Egypt suffered from COMSEC shortfalls which were aggravated by excellent Israeli SIGINT. After the 1967 war, Israeli SIGINT stations on the banks of the Suez could reach deeper into Egypt. Israel even planted listening devices on communications lines used by an Egyptian headquarters complex. Egyptian CI also discovered bugs in hollowed-out telephone poles which Israel was using to tap military communications between Red Sea bases and Cairo. Israel\u2019s \u201cTop Source\u201d, Ashraf Marwan, was turned by the Egyptians before the onset of war and gave false warnings of Egyptian attack that cost Israel money and lulled it into a state of complacency. Anwar Sadat and MID formulated a plan of strategic deception to accomplish diplomatic victory in a limited war (British deception before Battle of el-Alamein). EGIS planted deceptive media stories on military unpreparedness. Military coordinated movements to avoid US imagery satellites. Brinksmanship produced calm-alarm-calm cycle that bred complacency in Israelis. Egyptians improved intelligence collection by recruiting Bedouins and obtaining valuable intelligence on Israeli plans and codenames. Also leveraged OSINT. Ashraf Marwan, Sadat\u2019s new presidential gatekeeper, was recruited by Israel but doubled by Egyptians, gave false warnings of imminent attacks. After early success in the war, Israelis exploited weaknesses and were ready to pounce when ceasefire was declared. Egypt and Israel made peace and began sharing intelligence on Palestinians. Sources: Sirrs, Owen L. The Egyptian Intelligence Service: A History of the Mukhabarat, 1910-2009 (Studies in Intelligence) .","title":"Egypt"},{"location":"Intelligence/#finland","text":"Finnish economy\u2019s reliance on technology is growing and so is economic espionage. National innovation system is considered protected. Finland\u2019s intelligence community - Finnish Security Intelligence Service (Suojehpolisi, Supo) has been performing counterespionage, counterterrorism, and security since 1949. - Finnish Military Intelligence Command (FINMIC) is under Defense Command\u2019s Intelligence Division. - Finnish Intelligence Research Establishment (Viestikoelaitos) is Finnish Defense Force\u2019s (FDF) SIGINT unit, under Finnish Air Force (FAF). Predecessors to Supo - Detective Central Police (Etsiva keskuspoliisi) 1919-1937 - State Police (Vattriollinen poliisi, Valpo) 1937-1949 - Red Valpo, when dominated by Communists, 1945-1949 - Finnish Security Police est. 1949, name changed to Supo 2010.","title":"Finland"},{"location":"Intelligence/#organizational-structure","text":"Supo is subordinate to the Ministry of Interior, while other Nordic services are subordinate to the Ministry of Justice. Supo handles no foreign HUMINT sources. Four organizational reforms over past 20 years - In 1992, Supo had three directorates: Counterespionage, Security, and Development and Support - In 1998, operational and development matters were divided more clearly - In 2004, Supo was reorganized to develop research and analysis functions. Line organization was introduced. - In 2009, Supo was divided into Operational and Strategic Branches. - Operational Branch : Counterespionage Unit, Counterintelligence Unit, Security and Regional Unit, Field Surveillance Unit - Strategic Branch : Situational Awareness Unit, International Relations Unit, Internal Surveillance Unit - Communications Office directly subordinate to C/Supo","title":"Organizational structure"},{"location":"Intelligence/#personnel","text":"Supo employs 220 people and has a 17 million euro budget. - 55% are police personnel (30% command, 40% senior, 30% officers) - 1/3 of employees have a degree - Average age of 44 years","title":"Personnel"},{"location":"Intelligence/#iran","text":"In 1978-1979 (1357 H.S.) some former SAVAK officers revealed what they knew in a televised news conference. SAVAK had 9 or 10 chief directorates: First Chief Directorate : administration Second Chief Directorate : foreign intelligence Third Chief Directorate : internal security Fourth Chief Directorate : counterintelligence Fifth Chief Directorate : technical espionage, including telephone intercept and clandestine audio recordings, censorship, photography, and flaps and seals Sixth Chief Directorate : budgeting Seventh Chief Directorate : analysis, including compilation of daily \"bulletins\" Eighth Chief Directorate : counterespionage Ninth Chief Directorate : individual biographies and passport affairs Iraj Faridi, former chief of operations of the first section of the Third Chief Directorate, mentioned the Trident intelligence agreement but seemed to think the relationship with Turkey, not Israel, was sensitive. He claimed to have 17 years of service in SAVAK during the press conference. Mohsen Toluei","title":"Iran"},{"location":"Intelligence/#trident","text":"A three-way intelligence sharing agreement between Iran, Israel, and Turkey. This was prominently covered by the press when former Mossad officer Yossi Alpher published his book Periphery: Israel's Search for Allies in the Middle East .","title":"Trident"},{"location":"Intelligence/#russia","text":"Kouzminov, Alexander. Biological espionage: Special Operations . Directorate S, Department 12 Responsible for obtaining intelligence to aid the Soviet BW program, as well as new strains of pathogens. Requirements included Western governments, state commissions, civil defense and laboratories. Vladimir Kuzichkin worked as Illegals support officer in S/12 for 10 years Laboratory X founded in 1920s, transferred to NKVD in 1937. Soviet biological espionage - Morris and Leontina Cohen, controlled by Gordon Lonsdale, obtained information on British BW program. - Marcus Klinberg was director of Israeli BW program, spied for USSR.","title":"Russia"},{"location":"Intelligence/#communications-intelligence-and-tsarist-russia-by-thomas-r-hammant","text":"","title":"Communications Intelligence and Tsarist Russia by Thomas R. Hammant"},{"location":"Intelligence/#ministry-of-foreign-affairs","text":"Since Peter the Great, COMINT involving foreign governments and their representatives was the responsibility of the Ministry of Foreign Affairs (MID). Methods used included opening diplomatic letters (perlustration) and decrypting the contents, if encrypted, by cryptanalysis or purchasing codebooks. MID was aided by the \u201cBlack Cabinets\u201d of the Imperial Russian Postal Service. Post offices in major cities of the Russian Empire had Black Cabinets, which photographed contents of suspect correspondence and disseminating the information to the appropriate ministry. When the contents of a letter were encrypted, it was worked on not by a Black Cabinet but by a \u201csimilar establishment attached to the Ministry of Foreign Affairs.\u201d Little is known about MID\u2019s cryptographic organization, but it may have been brought under the control of the Minister of Foreign Affairs himself in the 1900s. Codebooks could be easily acquired on the open market.","title":"Ministry of Foreign Affairs"},{"location":"Intelligence/#russian-comint-in-world-war-i","text":"Army and Navy operated independent COMINT elements. At each Army HQ, radio intelligence operations were controlled by Chief of Army Communications through his assistant for technical matters. Each army\u2019s radio battalion had a radio intelligence squad or section which operated two stations: one monitored enemy communications, and the other station then recorded them once detected. Intercepts of encrypted German Army radiograms were sent to a \u201cspecial bureau\u201d of Chief Directorate of the General Staff in St. Petersburg for cryptanalysis. Generally, COMINT was poorly organized under the Russian Army. Black Sea and Baltic Sea Fleets established independent COMINT services in autumn 1914 after German naval codebooks were recovered from a sunken German cruiser. Copies of the codebook were shared with the British and French, and there was continued COMINT collaboration between the Allies throughout the war. The Baltic Sea Fleet\u2019s first radio intercept station was established close to Tallin. Intelligence was sent by underground cable to the Communications Service of the Southern Region. Each region (North, East, and South) had a Central Radio Station (CRS) that produced all-source intelligence and supported fleet communications. By 1916, Northern Region had 5 DF and 5 intercept stations, and Southern Region had 5 DF and 4 intercept stations. Southern Region also established a Radio Intelligence Center, probably to administer COMINT from these stations, which was subordinate to the CRS, and other Regions may have had similar units. The Baltic Sea Fleet\u2019s greatest debt to COMINT was accrued on 31 July 1915, when the Russians learned the German Navy planned to seize the city of Riga. Cryptanalysis of the messages, aerial reconnaissance, and shore-based observation posts allowed Russian ships to be ready for the attack when it came, and the Russians prevailed. The Black Sea Fleet\u2019s first radio intercept station was at Sevastopol, and although its Communications Service had two regions to administer, there are fewer details available about Black Sea Fleet COMINT. Black Sea Fleet was greatly helped by Turkey\u2019s use of German codes during the war. In December 1916, Black Sea Fleet decrypted information that indicated a German submarine was to return to Constantinople and included the location of the mine-swept channel by which it was to pass. Russian minelayers went to work, and within 48 hours the Russians learned the submarine had been sunk.","title":"Russian COMINT in World War I"},{"location":"Intelligence/#from-the-okhrana-to-the-kgb-by-christopher-andrew","text":"Okhrana est. 1881 Soviet intelligence MO and methods rooted in Tsarist secret service Okhrana active measures campaign to persuade French investors to invest in Russia. By 1914, a quarter of France\u2019s FDI was in Russia, three times as much as in its own empire, 80% of it in government loans. Peter Rachkowsky, head of Okhrana\u2019s Paris-based Foreign Agency from 1884 to 1982 may have been responsible for producing the Protocols of the Elders of Zion.","title":"From the Okhrana to the KGB, by Christopher Andrew"},{"location":"Intelligence/#humint","text":"Colonel Alfred Redl, senior Austrian MI officer. In winter 1901-2, Colonel Batyushin, head of Russian MI in Warsaw, discovered Redl was a homosexual. Redl sold Austria\u2019s mobilization plans against Russia and Serbia until his suicide in 1913. Roman Malinovsky, worker who became one of the most trusted Bolsheviks. One of 6 Bolshevik deputies to Duma (1912), then chair of Bolshevik faction when Mensheviks broke off. When Lenin smelled a rat and set up a committee to investigate the possibility of penetration, Malinovsky was a member of the committee. Okhrana sent him out of the country with a 6000 ruble payoff, and when Okhrana files were opened in 1917 Lenin couldn\u2019t believe Malinovsky had been a traitor.","title":"HUMINT"},{"location":"Intelligence/#sigint","text":"Okhrana began stealing cipher material to assist SIGINT at the beginning of the 20th century, decades before anyone but the French. Okhrana bribed embassy to services to make impressions of keys and smuggle them out, as well as papers to be photographed. SIGINT was used in diplomatic negotiations with Germany over the Bosphorus Canal. Bolshevik Revolution damaged SIGINT, dispersed codebreakers and cryptologists to other countries, where they sometimes joined those SIGINT services. For a decade after the Revolution, Soviet diplomatic traffic was easily decrypted. Soviets adopted the one-time pad in 1927. GB adopted the tactic of stealing cryptographic material. KGB and GRU operated a joint unit headed by Gleb Boki. SIGINT was responsible for a panic regarding a possible Japanese surprise attack 1931-1932. Intercepted attache cables were published in the Moscow press. Soviets provided edited excerpts of diplomatic intercepts to Germany to influence Germans into signing Molotov-Ribbentrop Pact. Maskirovka on Eastern Front achieved comparable results to British and Western denial and deception (XX system) Capture of hundreds of German signals personnel among tens of thousands of German POWs resulted in a windfall for Soviet SIGINT, 1943. Multiple penetrations of NSA, 1959-1963.","title":"SIGINT"},{"location":"Intelligence/#sword-and-the-shield-the-by-christopher-andrew","text":"","title":"Sword and the Shield, the by Christopher Andrew"},{"location":"Intelligence/#chapter-2-from-lenins-cheka-to-stalins-ogpu","text":"The paranoid instincts and shadowy methods of the Cheka and its successors were motivated by persecution of Bolshevik revolutionaries during the Tsarist period and provoked by agents provocateurs planted by the Tsarist Okhrana and foreign powers. - Cheka founded on 19171220 only weeks after Bolshevik Revolution: Feliks Dzerzhinsky - Foreign Intelligence Department (INO) established 19201220: Made use chiefly of Illegals because Soviet state had no legal residencies abroad - Foiled plots encouraged paranoia of young Cheka - Envoys\u2019 plot by naive young diplomats, caught in the net laid by the Cheka - Agents provocateurs Eduard Berzin and Yan Buikis: Berzin received Order of the Red Star, became Cheka officer, but then fell victim to Stalin\u2019s Terror and was shot in 1937; Buikis survived by changing his identity - Okhrana agents provocateurs - Bolshevik experience as an underground movement: Use of pseudonyms (\u2019Lenin\u2019, \u2018Stalin\u2019)","title":"Chapter 2: From Lenin\u2019s Cheka to Stalin\u2019s OGPU"},{"location":"Intelligence/#chapter-3-the-great-illegals","text":"The Great Illegals of the interwar period leveraged their personal flair and charisma to achieve remarkable successes against target countries with very weak security posture. Some of their earliest successes were in obtaining diplomatic cipher material, which was passed to a large SIGINT agency where diplomatic traffic was deciphered. Stalin didn\u2019t trust anyone to analyze the intelligence for him and acted as his own intelligence analyst. This reinforced his warped worldview as the secret services produced reports that catered to his paranoid suspicions. - Great Illegals were unique and remarkable spies: multilingual Central Europeans with great faith in Communist future; freer from bureaucracy, in comparison to post-war period - Target countries had very lax security - First successes were in obtaining diplomatic ciphers - Dmitri Aleksandrovich Bystroletov (HANS, ANDREI): very handsome, extroverted: portrait hangs in secret memory room of SVR Center in Yasenovo - Seduced female staff in foreign embassies: Prague, 1927: seduced 29 y.o. secretary in French embassy (LAROCHE) who provided British and Italian diplomatic ciphers and classified communiques for 2 years - Agents introduced ANDREI to other sources of information - Oldham , who provided British ciphers, provided introduction to Raymond Oake (SHELLEY) - De Ry provided Italian ciphers, provided introduction to Rodolphe Lemoine - Rodolphe Lemoine (JOSEPH) - Passion for espionage: began work for French Deuxi\u00e8me Bureau (DB) in 1918 - Recruited German cipher clerk in 1931 who was DB\u2019s most important source for a decade - Some intel fed into Enigma code breaking machines - Eventually passed to Ignace Reiss (RAYMOND), who defected in 1937 - Henri Christian Piecke (COOPER) - Flamboyant Dutch artist - John H. King (MAG) - Irish, hated English - Classified Foreign Office communications coroborated by DUNCAN (below) - Moisei Markovich Akselrod (OST, OSTO) - Jewish family, born 1898 - hired by INO in 1922 - Multilingual: Arabic, French, German, English, Italian - Francesco Constantini (DUNCAN) - Classified documents, ciphers from British Embassy in Rome - Also sold documents to Italian intelligence - Continued to provide intelligence after dismissal through brother Secondo Constantini (DUDLEY), also employed at embassy - Executed during Great Terror - Joint OGPU/Fourth Department SIGINT agency decrypted diplomatic traffic - largest SIGINT agency in the world at the time - No analysis of intelligence - Stalin considered analysis to be \u201cguesswork\u201d - Conspiracy theories of Stalin\u2019s continue to survive to this day","title":"Chapter 3: The Great Illegals"},{"location":"Intelligence/#chapter-4-the-magnificent-five","text":"Arnold Deutsch established the recruiting strategy for the Magnificent Five, young talents in Oxford and Cambridge Universities with Communist sympathies who became the most successful Soviet penetrations of Western governments during WW2. Arnold Deutsch (STEFAN, OTTO): True believer in Communism, chemistry PhD from Vienna University, five years after entering as undergraduate; began work for INO in 1932. Recruited 20 agents, including C5, over 4 years as controller Kim Philby (SOHNCHEN, SYNOK): heterosexual athlete; was not productive before 1937, when he was sent to Spain as war correspondent, wounded, and ultimately awarded medal by Franco, whom he was supposed to assassinate Donald MacLean (WAISE, SIROTA): bisexual, approached by Philby in 1934; Foreign Office Guy Burgess (MADCHEN): flamboyant homosexual and social butterfly Joined SIS in 1938, in newly founded Section D (covert action and influence) Anthony Blunt: homosexual, introduction by Burgess Talent spotter John Cairncross (MOLI\u00c8RE, LISZT): polygamist, spotted by Blunt, approached by Burgess, recruited by Klugmann; Foreign Office Norman Klugmann (MER): prominent Communist activist who acted as talent spotter for NKVD, recr. 1936. Given away by Ignace Poretsky in 1937 Teodor Maly (MANN) Hungarian POW during WW1, joined Bolsheviks during Revolution Head of London residency in 1936, where he completed recruitment of C5 with Deutsch Recalled to Moscow during Great Terror, shot in 1937 (Ch. 5) Internal turmoil in Soviet Union affected espionage Hunt for Trotskyites became priority by end of 1937 Great Terror: all 3 of Deutsch\u2019s residents during residency in London were executed","title":"Chapter 4: The Magnificent Five"},{"location":"Intelligence/#chapter-5-terror","text":"The fantasy of a Trotskyite conspiracy increasingly obsessed Stalin during the 1930s, who directed the NKVD and OGPU to penetrate Trotsky\u2019s organization. Trotskyites became targets of a cell of assassins called the Administration for Special Tasks, based out of the Paris residency. The Great Terror resulted in the liquidation of so many NKVD officers that tradecraft suffered. The Cambridge Five themselves, despite the quality of intelligence they provided, were suspected of being plants. \u2022 Mark Zborowski (MAKS, MAK, TULIP, KANT): Russian-born Polish Communist who deeply penetrated Trotsky\u2019s entourage \u25e6 Confidant of Lev Sedov, elder son of Trotsky \u25aa Entrusted with key to Sedov\u2019s letterbox and Trotsky\u2019s most confidential files and archives \u25aa Convinced Sedov to go to a Russian clinic for appendicitis while assassination was being planned \u25e6 After Sedov\u2019s death, encouraged internecine warfare between Trotskyites \u25e6 Orlov knew his first name and attempted to warn Trotsky after defection in 1938 \u2022 NKVD Administration for Special Tasks specialized in assassination and abduction, especially in France, headed by Yasha Serebryansky, resident in Paris \u25e6 Largest section of Soviet foreign intelligence by 1938, claiming to have 212 illegals in 16 countries \u25e6 Trained members of International Brigades in sabotage \u25e6 Main task was surveillance and destabilization of French Trotskyites \u25aa theft of Trotsky\u2019s papers from a Paris flat coordinated by Zborowski, who escaped suspicion \u25e6 Abduction of General Yevgeni Karlovich Miller \u25aa Entourage penetrated: Miller\u2019s deputy was NKVD agent \u25aa Another illegal was used to surveil Miller \u25aa Miller disappeared in broad daylight on a Paris street, drugged, packed in a heavy trunk, and sent to Moscow by Soviet freighter where he was interrogated and shot \u25e6 Assassination of Lev Sedov \u25aa Op was aborted after furor re. NKVD involvement in Miller\u2019s disappearance \u25aa Sedov developed appendicitis, died mysteriously a few days after a successful operation in Russian clinic (at Zborowski\u2019s insistence) \u25aa NKVD had a sophisticated medical section called the Kamera, experimented with lethal drugs \u25e6 Assassination of Rudolf Klement: secretary of Trotsky\u2019s Fourth International \u25e6 Assassination of Ignace Poretsky (Reiss, RAYMOND) using machine gun and chocolates laced with strychnine \u25e6 Assassination of Leon Trotsky: operation UTKA \u201cduck\u201d became chief Soviet foreign policy objective, to be effected by three groups \u25aa Penetration by illegal Ram\u00f3n Mercader (RAYMOND, alias Frank Jacson sic ), who seduced a Trotskyite secretary \u25aa Succeeded in killing Trotsky with icepick, caught and sentenced to 20 years imprisonment \u25aa Hero\u2019s welcome in Moscow 1960 \u25aa Assault on villa led by David Alfaro Siqueiros (KONE), Communist painter \u25aa Iosif Romualdovich Grigulevich (MAKS, FELIPE), member of Serebryanksy\u2019s cell, real leader of assault \u25aa Escaped to Argentina where he planted hundreds of mines in cargo ships bound for Germany \u2022 Spanish Civil War was training ground for saboteurs and battlefield against Franco\u2019s fascists as well as Trotskyites \u25e6 Orlov coordinated two-front war in Spain, ultimate goal was to build a secret police force under Soviet control \u25aa NKVD assassins murdered Andreu Nin, head of a Trotskyist workers\u2019 organization, as well as dozens of other Trotskyites in Spain \u25aa Orlov eventually defected to the US \u25e6 Stanislav Alekseyevich Vaupshasov: top assassin - Led raids on Polish and Lithuanian border villages dressed in Polish and Lithuanian army uniforms in the 1920s - Murdered a colleague in 1929 - Constructed and guarded secret crematorium which disposed of NKVD victims (SVR still considers this topic sensitive, gave hush money to female relative of the NKVD agent in charge of guarding this crematorium) - Great Terror sprung from Stalin\u2019s obsession with counterrevolutionaries - Leadership of NKVD liquidated and re-liquidated - Nikolai Ivanovich Yezhov, head of NKVD 1936-1938: author of Great Terror, replaced Yagoda who soon made absurd confessions - Replaced by Lavrenti Beria in December 1938 before accused of conspiracy - Abram Slutsky, chief of INO, poisoned by cyanide in 1938 - Slutsky\u2019s successors also shot before the end of the same year - NKVD officers were liquidated - Had to be careful even of body language or sighing - Officers most quick to denounce peers of imaginary crimes were most likely to survive Most of the Great Illegals were liquidated by 1938, except for: - Deutsch who was betrayed in 1937 by Ignace Poretsky (Reiss, RAYMOND) - Bystroletov brutally tortured before confession, imprisonment; wife sent to gulag where she cut her own throat with a kitchen knife; mother poisoned herself - Serebryansky himself recalled to Moscow and condemned in 1938 - Show trials depicted a vast, absurd conspiracy authored by Stalin, who proofread transcripts before publication - Great Terror and British operations - C5 transferred to legal residency, where controllers were much less experienced - MacLean seduced his new Soviet controller (NORMA, ADA) - Condemnation of C5\u2019s controllers and recruiters as enemies of the people placed their intelligence and reliability under question - Beria eventually disbanded the residency in 1940, Centre ordered all contact with Philby and Burgess to be broken off - Ideological commitment of C5 remained strong even after the Molotov-Ribbentropp pact - British agents were motivated out of revilement for fascism - Some agents ended their espionage - Histories of Stalin era still whitewash the emphasis on assassination of political opposition in Western Europe","title":"Chapter 5: Terror"},{"location":"Intelligence/#south-africa","text":"Intelligence's role in supporting counter-revolutionary operations of the apartheid state, as well as in the AND and SACP's efforts to overthrow the state. Five key uses of intelligence in counter-revolutionary struggle:[^4] Targeting enemies of the state, internal (including South West Africa) and external (including in Europe) ZA relationships with other states (Angola, Botswana, Mozambique, Tanzania, Zambia, and Zimbabwe) as anti colonial movement spread, isolating ZA Anti-Communist paradigm of Cold War, alliance with US, UK, West Germany, and Israel Overcoming anti-apartheid sanctions from the 1970s and 80s Developing a nuclear weapon Relationship with British services strained because of Afrikaner resentment, British concern of infiltration by Afrikaner nationalists. In the nineteenth century, the Boer republics (Transvaal Republic and Orange Free State) maintained limited intelligence organizations led by Cornelius Smidt and Willem Leyds (Transvaaler Secret Service).[^5] South African Police Detective Branch (SAP/DB) conducted counter-subversion and counterintelligence after 1899-1902 Anglo-Boer War and the establishment of the Union of South Africa in 1910. SAP/DB concentrated mostly on Nazi sympathizers in South West Africa until 1948, when the National Party (NP) was elected.[^6] MI5 conducted foreign intelligence but also watched radical Afrikaner groups such as Ossewa-Brandwag (OB), a paramilitary organization in competition with NP and with many sympathizers in SAP. Union Defense Forces, considered an anglophile institution, cooperated with MI5 on internal threats (Afrikaner nationalists and Republicans) and British Special Operations Executive (SOE), operating out of Durban. MI5 Director General Sir Percy Sillitoe served in South Africa and Rhodesia and was a key influence in shaping the South African intelligence structure. SAP Special Branch (SB) founded as Special Staff to hunt Nazis in 1939 before being redirected to investigating political crimes. Increasingly into the 1950s, became known as Security Branch. Union Defense Forces Department of Military Intelligence (DMI) created Feb 1940 but neglected after 1948. After independence from Britain in 1961, UDF became South African Defense Forces (SADF) and established a new Directorate of Military Intelligence (DMI) July 1962. Rivalry between SB and DMI survived administrative efforts to coordinate intelligence functions: State Security Committee (est. 1963) and State Security Advisory Board (est. 1966). SB\u2019s criminal emphasis hampered intelligence work, and a central intelligence agency was needed, sought first in Republican Intelligence (RI) spun off from SB 1963, then found in BOSS. Bureau for State Security (BOSS) founded 19690513 as a central intelligence apparatus to mitigate the rivalry between DMI and SAP. Reorganized as National Intelligence Service (NIS) in 1980. Grew from 500 personnel in 1969 to more than 1,000 by 1978. Six departments: Subversion, Counter-espionage, Political and Economic Intelligence, Military Intelligence, Administration, and National Evaluation, Research and Special Studies. Forged a relationship with Portuguese and Rhodesian intelligence services. Brought down by government\u2019s increased reliance on COIN strategies (vice counterintelligence and counter-espionage) and by the Information Scandal. African National Congress (ANC) , Umkhonto weSizwe (MK) , and South African Communist Party (SACP) established intelligence structures separate but parallel to that of the government during their armed struggle against the apartheid state. ANC established Department of Intelligence and Security (DIS). DMI gained control of \u201cTotal National Strategy\u201d and achieved dominance over BOSS. DMI strategists implemented various counter-insurgency (COIN) strategies they learned from abroad in ZA and SWA. Botha appointed Minister of Defence in 1966 and began a campaign to reinvigorate the SADF. By the beginning of Project SAVANNAH (South African intervention in Angola[^7] in 1975, Botha had begun a process of streamlining the SADF that would culminate years later in the incorporation of COIN principles in the South African Army. Area Defence Policy, later known as National Security Management System, was fully incorporated into the counter-insurgency forces of SADF. COIN forces were made up of part-time SADF personnel from the Citizen Force and Commandos and were divided into ten regional commands covering the countryside. SADF doubled in size between 1975 and 1990, reaching almost 100,000 with another 325,000 in the Citizen Force and Reserves ZA security confronted radical new challenges from 1975 to 1978. Key governments on ZA's periphery had turned hostile. Marxist MPLA took over Angola and hosted SWAPO bases on the border of South West Africa. Mozambique's new government was Marxist. Mozambique became a shelter for Rhodesian guerrillas, threatening ZA's last counter-revolutionary ally. Radicalization of blacks after 1976 uprising in Soweto overwhelmed internal security, which responded by introducing COIN concepts and reacting more harshly to protests. SADF COIN expertise was gained in collaboration with Rhodesian ISS and SOF during the 1960s and 1970s. ZA intelligence establishment absorbed members of Rhodesian Security Forces in the transition to majority rule in 1980 (Operation Winter).[^8] COIN Strategy Adopted Wholesale, New Units Established Opposition to apartheid state grew more radical while the state\u2019s response to opponents hardened. A sustained domestic protest movement called United Democratic Front ultimately gave ANC-MK their long sought-after internal subversion capability within ZA. Pretoria adopted COIN strategies in response to failing effort against MPLA in AO and SWAPO in SWA and deteriorating domestic security as MK stepped up attacks. K-Unit \u201cKoevoet\u201d founded by SAP/SB in January 1979, building on their decades of cooperation with Rhodesians in COIN operations. Formed after the Selous Scouts and Portuguese Flechas for the purpose of turning captured ANC, SWAPO, and PAC guerrillas called askaris. Turned SWAPO fighters who collaborated with Koevoet were known as makakunyanas \"blood-suckers\", and Koevoet collaborators were targeted for assassination by SWAPO. They were paid for each guerrilla killed. SAP/C1 \u201cVlakplaas\u201d named after the police farm outside Pretoria. Originally established 1979 when BG Johan Coetzee, C/SB, decided to use COIN activities for counter-revolutionary purposes within ZA. Revealed by Coetzee in the press in 1989 and disbanded in 1993. Koevoet elements including de Kock were withdrawn from SWA to form C1 in May 1983. Identification, tracking, and \"rehabilitation\" (turning) of ANC and PAC guerrillas. C1 also assassinated up to 65 people from 1980 to 1991. C1\u2019s operations in Swaziland were to disrupt the ANC/MK structures there. 22 ANC members were assassinated in Swaziland in the 1980s. C2 established concurrently to track activists leaving ZA and to interrogate arrested guerrillas SAP/G section , responsible for the penetration of ANC abroad, was resurrected after the reorganization of BOSS in 1980. G section attempted to assassinate ANC/MK strategist Joe Slovo, killing his wife instead. G section also blew up ANC\u2019s London office Directorate Covert Collection (Direktoraat von Koverte Insameling) established at an unclear date, but possible predecessor was Directorate of Covert Information, active in SWA by 1982. Established multiple front companies with the goal of duplicating successful COIN operations of AO and SWA within South Africa. Spread of liberation movements across Southern Africa removed governments traditionally friendly to Pretoria, which saw all black liberation movements as Communist-backed threats to regime. South African government resorted to policy of destabilizing neighbors and conducting covert action to remove safe havens for ANC/MK. Eventually South Africa\u2019s hand in regional politics became transparent. DMI integrated intelligence collection with covert action by taking control of South African Special Forces (SASF) in 1979. Rhodesian special forces integrated into SADF in 1980, further buttressing COIN capabilities. Directorate of Special Tasks (DST) formed in mid-1970s from a corps of DMI operators named Spesmagte, similar to Recces. DST was responsible for overseeing contra-mobilization and counter-revolutionary activities of DMI throughout southern Africa as part of a strategy to deny ANC-SACP safe havens in Frontline States. DST began operations in an office in Rundu, Namibia in 1976 in the wake of South African withdrawal from AO. First chief was COL Cornelius van Niekerk. DST terminated operations in the early 1990s. Two sections: - DST-1 (external operations) covered UNITA, RENAMO, and Zimbabwe - DST-2 (internal operations) covered Lesotho Liberation Army (LLA), and Operations MARION and KATZEN DST maintained logistical infrastructure throughout Southern Africa and conducted several operations, providing support to anti-Marxist proxies in neighboring countries: - DISA/SILWER : support to UNITA in Angola - DRAMA : support to Zimbabwean dissidents - PIKI/PUNDU MILIA/ALTAR : support to RENAMO and operations against FRELIMO in Mozambique. 5 Recce was principally responsible because many 5 Recce personnel had trained with the Selous Scouts. When RENAMO\u2019s headquarters moved from Phalaborwa, Transvaal (!) to Gorongosa, Mozambique, South African officers followed offering intelligence training. - PLATHOND : support to surrogate force in Zambia - CAPSIZE/LATSA : support to Lesotho proxy grou 5 Recce based out of Phalaborwa, Transvaal. Supported Op PIKI/PUNDU MILIA. Also used pseudo-operations against SWAPO in Namibia, sometimes cooperating with Koevoet. DCC mobilized contras in Namibia. Various operations: - ETANGO : DCC and other DMI units attempted to establish a conservative contra based in Ovambo tribalism to counter SWAPO. - EZUVA : Similar project to establish a contra among the Kavango. Many experienced contra-mobilizers from DCC moved into domestic contra-mobilization 1985-1986, setting up groups to foment black-on-black violence and undermine support of ANC and UDF: 23 such projects by 1986. Operations included: - Operation MARION provided security training and weapons to more than 200 Inkatha cadres 1986-1990. These units later conducted targeted killings. Inkatha had been supported by BOSS as an alternative to ANC from 1975, including funds and stage-managing internal political rivals to Chief Buthulezi. - Operation KATZEN was an attempt to organize a contra group among the Xhosa known as the Xhosa Resistance Movement (XWB, known as Iliso Lomzi). Cooperated with Army Intelligence Hammer units, which conducted special operations. Civil Cooperation Bureau (CCB) , also known as Burgerlike Samewerkingsburo (BSB), was formed in 1986 out of Operation BARNACLE. Formed by SASF to fulfill the requirement of domestic intelligence collection, which was used primarily for external operations. CCB was imagined to be fully functional only in the mid-nineties, possibly to conduct counter-revolutionary warfare after the transfer to black rule had been completed. CCB was organized as a corporation into Regions that coordinated covert activities in concert with other state bodies. CCB numbered up to 250-300 individuals. By the late 1980s, CCB had established numerous front-companies and businesses and was involved in lucrative criminal activities. Operations: - CCB conducted internal assassinations in line with the state\u2019s emphasis on counter-revolutionary warfare. As such, CCB was DMI\u2019s equivalent to SAP\u2019s C1. - CCB supported SASF in operations in the Frontline States by undertaking reconnaissance of ANC targets. 3 Recce (active 1980-1981) absorbed the remnants of Rhodesian special forces which fled Zimbabwe in 1980 as well as DMI\u2019s D-40 assassination unit (active 1979-1980) led by the Rhodesian Garth Barrett. 3 Recce operated against Zimbabwe, exploding very destructive bombs and assassinating ANC\u2019s representative in Harare.[^9] Operation BARNACLE , conducted by a group 30-40 mostly black ex-Rhodesians, was a project to use CBW to assassinate guerrillas, SWAPO prisoners of war, and members of South African security forces suspected of disloyalty with poison. BARNACLE was to be a completely independent resource at the disposal of the country\u2019s leaders, to serve as a hedge against the prospect of the government granting too much power to blacks. BARNACLE actors were not accountable to official security organs or to SADF commanders: they reported directly to General Office Commanding Special Forces (GOC-SF). Later reorganized into the Civil Cooperation Bureau.","title":"South Africa"},{"location":"Intelligence/#notable-people","text":"BG J.P. Tolletjie Botha ran Directorate Covert Collection. COL Jan Breytenbach founded the Reconnaissance Commandos, 32 Battalion, and the Directorate Special Tasks. Chief Buthulezi was leader of Inkatha and a BOSS stooge. COL Eugene de Kock was one of the most ruthless and effective of Koevoet's commanders. Eleven tours of duty between 1968 and 1973 in Rhodesia with Rhodesian SAS and Rhodesian African Rifles. Commanded Koevoet for four years before he requested a transfer to SAP/C1, where he assumed command in 1985. De Kock emphasized the assassination program and introduced paramilitary training for police members. During this time, De Kock became known as Prime Evil for his mercilessness. MAJ Craig Williamson was a counterintelligence operative in SAP/G Section from... to... Williamson infiltrated ANC by using the International University Exchange Fund (IUEF), but was exposed in 1980. Williamson served in SAP/G until December 1985. Williamson also established Longreach Pty Ltd in April 1986, which served as a front company for SAP/SB operations and also coordinated DMI and SASF operations.","title":"Notable People"},{"location":"Intelligence/#sweden","text":"There was no specific institution in Sweden for intelligence before the 1930s. Navy intercepted communications and diplomats gather intelligence.[^10]","title":"Sweden"},{"location":"Intelligence/#military-intelligence-before-ww2","text":"General Staff gathered intelligence against Norway during the war of secession Swedish Defense Staff established in 1937. Intelligence branch formed with 20 officers. Mostly OSINT collection and attache reporting from 15 attaches and SIGINT.","title":"Military Intelligence Before WW2"},{"location":"Intelligence/#intelligence-organizations-formed-in-ww2","text":"C-Bureau (central byr\u00e5a) under Defense Staff, but separate from Intelligence Division, established large intelligence network that was not fully documented. Cryptographic Department (CD) of Defense Staff took over SIGINT from Navy using civilians to succesfully break Soviet crypto and comms. Results were shared with Finnish. Decrypted machine crypto used over landlines. 150,000 cables over two years, until Germans changed codes. Swedish were unable to fully exploit this windfall of intelligence. CD reformed as F\u00e5ssvarets Radioanstalt (FRA), directly under Ministry of Defense. General Security Service (Allm\u00e4nna S\u00e4kerhetaj\u00e4nsten, GSS) formed out of a secret, extralegal government decision, established massive program of unlimited authority to monitor telephone, telegraph, and mail communications. Dissolved after scandal in 1946.","title":"Intelligence Organizations Formed in WW2"},{"location":"Intelligence/#postwar-developments","text":"2 of the 3 wartime intelligence agencies would be resurrected after dissolution 1947-1948.","title":"Postwar Developments"},{"location":"Intelligence/#india","text":"Kautilya\u2019s Arthashastra[^11] - Constitutes a doctrine of statecraft using espionage as a basic means of governance. - Comprehensive textbook on statecraft, foreign diplomacy, and war emphasizing the collection of domestic and foreign intelligence. - Rediscovered and translated by Orientalist Rudrapatnam Shamasasty in 1909-1915. Written by Kautilya, trusted advisor to Chadragupta Maurya, founder of Mauryan Empire 321-185 BCE.","title":"India"},{"location":"Intelligence/#eight-institutes-of-espionage","text":"Four were forms of religious cover (fraudulent disciple, recluse, ascetic, and mendicant woman), which were to take advantage of the intensely religious population of India. Religious class had access to other castes. Wandering female spies were to take religious cover, poor widows of Brahman caste were to target upper castes while Sudra-caste women willing to shave their head were to target lower caste communities. Classmate spies referred to recruitment pool, and the preferred choice for courier. Firebrands were to be used for covert action as assassins, agent provocateurs, and saboteurs. Result was a pervasive surveillance network covering the whole country. Treasury was also to have intelligence function Householder spies were to ascertain validity of assets Merchant spies were to monitor price changes and foreign goods Networks of spies under cover of bands of thieves would monitor the criminal underworld Provocation and entrapment are standard tactics in Arthashastra.","title":"Eight Institutes of Espionage"},{"location":"Intelligence/#islamic-caliphate","text":"Pre-Islamic Arab tradition of intelligence predated a more developed intelligence culture in the Islamic age.[^12] Various words referring to spy or scout: - jasus : foreign spy - tajassasah : discouraged in the Qur\u2019an - \u2018ain : \u201ceye\u201d - suhhar : night sentinels who kept watch for strangers at approaches to market town or crossroads - rabi\u2019ah : lookout - other words","title":"Islamic Caliphate"},{"location":"Intelligence/#espionage-in-early-arab-states","text":"Lakhmid and Ghassanid buffer states between Sasanian Persia and Byzantium: - Story in 10th c. Kitab al-Aghani relates Lakhmid spies catching a would-be assassin and killing him in the 6th century - Scouts in Arabian peninsula","title":"Espionage in Early Arab States"},{"location":"Intelligence/#brigandage-among-bedouin","text":"Skirmishes and raids ( suluk ) involved use of scouts - al-Basus War was remembered as the days of rabi\u2019ah - Abu Faraj al-Asbahani\u2019s anthology of Arabic verse - Muhammad b. al-Tabari on agents using disguises","title":"Brigandage among Bedouin"},{"location":"Intelligence/#muhammad-and-intelligence","text":"Qur\u2019anic regulations on espionage reflect importance of clandestine activites in early Islam. The Prophet Muhammad\u2019s involvement in intelligence and espionage: - Muhammad gathered information on early converts, seeking candidates with honesty, trustworthiness, and the ability to keep a secret - Muhammad possessed detailed knowledge of clan loyalties and politics, and used this knowledge in negotiations with Bedouin - Abdullah b. Abu Bakr mingled with Quraishis of Mecca and report back to him at night in his cave. Abu Bakr\u2019s sister Asma also spied for the Prophet: first spies for Islam. - Abu al-Fadhl al-Abbas ran spy network in Mecca - Many \u2018ains from various corners of Arabia, from every town and tribe - Muhammad deliberately retreated during the Battle of Uhud to allow his lookouts to determine the size of the army, whether it had mounted camels (to retreat) or horses (to attack) - Muhammad debriefed two boys who were caught drawing water from a well before the Battle of Badr. They divulged key intelligence on the closing Quraishi army.","title":"Muhammad and Intelligence"},{"location":"Intelligence/#deception","text":"After the indecisive Battle of Uhud, Muhammad sent one of his \u2018ains to deceive the Quraishis into thinking a large host was approaching, causing them to retreat.","title":"Deception"},{"location":"Intelligence/#assassination-of-poets","text":"Poets had a complex role in Arab society and were highly influential - Asma of Marwan was stabbed in her sleep, but w\u2019o Prophet\u2019s prior approval - Abu Afak killed by fellow tribesmen - False prophet al-Aswad al-Ansi became influential and killed the governor of Yemen. Muhammad ordered Wabrak b. Yahmus to organize a plot. Wabrak recruited a circle of Persian Muslim converts and the governor\u2019s wife, who facilitated the operatives\u2019 infiltration into the castle where al-Ansi was murdered","title":"Assassination of poets"},{"location":"Intelligence/#intelligence-by-islams-enemies","text":"Abu Sufyan determined Muslim spies were present by finding date seeds in camel dung, indicating the animals had Medinan fodder Multiple assassination attempts on Muhammad\u2019s life Umar b. al-Khattab in charge of counterintelligence and security Byzantines sent a monk who claimed to be a convert to Islam and established a mosque in Medina. Muslim agents surveilled the mosque after suspicious comings and goings and ultimately demolished the mosque.","title":"Intelligence by Islam\u2019s enemies"},{"location":"Intelligence/#us","text":"","title":"US"},{"location":"Intelligence/#corporate-espionage","text":"Vice reported that McDonald's had established an intelligence unit to monitor workers who supported increasing the minimum wage. The unit had targeted the Fight for $15 movement for increasing the wage to $15 an hour in particular. One intelligence report titled \"Ongoing FF$15 Activity Against McDonald's During the COVID-19 Crisis\" contained an analysis of the activities of labor activists. McDonald's had been using two different data collection software suites to collect open-source intelligence on the social networks of workers involved in the labor movement.","title":"Corporate espionage"},{"location":"Intelligence/#comint","text":"War Department set up first organized cryptanalytic office in June 1917, numbering 3 people. By war\u2019s end, it would grow to 150.","title":"COMINT"},{"location":"Intelligence/#navy","text":"In 1917 and 1918, Navy set up medium frequency DF stations along Atlantic coast to track U-boats. HFDF stations were researched and deployed by 1938. Strategic HFDF stations were established at Manila, Guam, Midway, Oahu, Dutch Harbor, Samoa, Canal Zone in Panama, San Juan Puerto Rico, and Greenland. US began tracking Japanese warships and merchant vessels in 1939, five years after the Japanese had begun tracking US vessels. Navy had established the Code and Signal Section of Naval Communications for producing codes and ciphers for use by Navy. Registered Publication Section, responsible for distribution of secret and confidential documents, was spun off in 1923. Navy funded development of Electric Cipher Machine from 1922. Communications Intelligence Organization (CIO) was established 1924. Intercept stations were established in the Pacific Area (Shanghai, Oahu, Peking, Guam, Manila, Bar Harbor, Astoria), and Washington DC. Cryptanalytic Units established in Manila and Pearl Harbor. Training was done with technical manuals, using the codes to send messages. Minor intercept activities were performed in strategic HFDF stations. In 1938, CIO became the Communications Security Group (CSG) and took over all Navy DF stations. By 1941, CSG had 730 total personnel.","title":"Navy"},{"location":"Intelligence/#arm","text":"Army Signal Corps founded 1860 by Albert James Myer, inventor of wig-wag flag signaling method. Signal Intelligence Service (SIS) founded 1930 as a secret part of Signal Corps for cryptanalysis. By 1939, SIS made use of 7 intercept sites from the Philippines and Hawaii in the West to the East Coast of the US. These were the sources of SIS intercepts until after Pearl Harbor. At SIS HQ in Arlington Hall, traffic was split between four analytic sections: - J: Japanese - G: German - I: Italian - M: Mexican and Latin American Although SIS intercepted tens of thousands of IJA messages from its station in Manila, these messages could not be fully exploited because IJA ciphers were not broken. However, SIS broke several diplomatic ciphers including Purple. After the war, SIS changed its name to the Army Security Agency (ASA) in 1945. In 1947, ASA and the Army Intelligence Agency were merged into the newly formed Intelligence and Security Command (INSCOM).","title":"Arm"},{"location":"Intelligence/#radio-free-europe","text":"Established 1949 by the National Committee for a Free Europe (NCFE), an anticommunist organization with Allen Dulles and Dwight D. Eisenhower as board members. Funded by CIA until 1972. Targeted Eastern European countries (as opposed to Radio Liberty).","title":"Radio Free Europe"},{"location":"Intelligence/#radio-liberty","text":"Established by American Committee for the Liberation of the Peoples of Russia (Amcomlib) 1951. By 1954 was broadcasting in several other Central Asian languages. Foreign Broadcast Information Service Foreign Broadcast Monitoring Service, or FBMS, established 1941 under FCC to monitor Axis shortwave broadcasts to the US. Name changed to FBIS 1947 when it was made part of the new CIA.","title":"Radio Liberty"},{"location":"Intelligence/#france-andrew-orr","text":"France took over Syria and Turkey\u2019s SE (Cilicia) after WW1 and perceived Turkish War of Independence as threat to its new imperial interests. Military intelligence services of Army and Navy monitored Kemalist movements and always saw the hand of German and Russian commies behind Turkey\u2019s developments. 3 reasons German closely involved in Ottoman military affairs from 1883 and German general Otto Liman von Sanders commanded the Ottoman Army Germans had also promoted pan-Islamic movement during the War Germans possibly still controlled Russia, according to the French, reasoning that they had sent Lenin to Russia inside of a sealed train car Colonial intelligence services lacked some of the checks on extreme predictions when reporting on events outside of Europe","title":"France (Andrew Orr)"},{"location":"Intelligence/#military-intelligence","text":"French Army\u2019s Service de renseignments guerre ( SR Guerre ), which was part of a unified French intelligence organization during the War, but then reverted to the Army\u2019s II Bureau after its end. During the Turkish War of Independence, SRG deployed a small number of officers to the Middle East, stations opened in Constantinople in 1919, Algiers 1925, Rabat and Tangiers 1929. French Navy service ( SR Marine ) was similarly structured to Army, but with more familiarity with Mediterranean Sea and the Middle East SRM opened Constantinople station in 1919 SR sources included Europeans and Americans fleeing Turkey, human informants, and newspaper articles, as well as intercepted radio messages sometimes by way of the Brits Coincidence of treaty signings between USSR, Turkey, and Afghanistan and Persia led SR personnel to believe there was a plot brewing","title":"Military Intelligence"},{"location":"Intelligence/#poland","text":"From Anglo-Polish HUMINT, by PRJ Winter Histories of WW2 intelligence exalt COMINT successes of GCCS to the detriment of MI6 Historiography of British WW2 intelligence Agents - Paul Thummel (codename A-54), senior officer of German MI (Abwehr), recruited by Czechs 1936. SIS and Czechs ran him jointly from 1939 until he was arrested by the Gestapo and died in prison 1942. - Warlock, on staff of German High Command (Oberkommando der Wehrmacht, OKW), turned by 1941. - Knopf, reported on OKW intentions to take Malta, but records show that Germans were noncommital to the plan (Op Herkules) which would only support Italia High Command. Knopf may have been turned by this time. MI14\u2019s evaluation gave Knopf a mixed, but generally positive score. Polish government-in-exile settled in London and cooperated with British intelligence (II Bureau of Polish General Staff) SIS cooperation with Poles was driven out of desperation because aside from Warlock they had no useful penetrations of Nazi Germany CX reports were SIS, JX reports were from Poles. Some JX reports found in British National archives, including one report on Malta which was passed to Churchill himself. Poles ran sources reporting from the heart of OKW and OKH (Oberkommando des Heeres, Supreme High Command of the German Army) British interception of Polish communications confirmed Knopf\u2019s bona fides (as agent number 594) and the Poles\u2019 as well [^1]: Sawyer, Ralph D. \u201cSubversive Information: The Historical Thrust of Chinese Intelligence.\u201d Intelligence Elsewhere: Spies and Espionage Outside the Anglosphere . Ed. Philip H. J. Davies, Ed. Kristian C. Gustafson [^2]: [^3]: Homstr\u00f6m, Lauri. \u201cFinnish Security and Intelligence Service.\u201d Intelligence Elsewhere: Spies and Espionage Outside the Anglosphere .Ed. Philip H. J. Davies, Ed. Kristian C. Gustafson [^4]: O\u2019Brien, Kevin A. The South African Intelligence Services: From apartheid to democracy, 1948-2005 . Routledge: New York, NY 2011. [^5]: Blackburn, Douglas and Caddel, W. Waithman. Secret Service in South Africa . Honolulu: University Press of the Pacific, 2001. Swanepoel, P.C. Really Inside BOSS: A Tale of South Africa\u2019s Late Intelligence Service (And Something about the CIA) . Pretoria, 2008. [^6]: Ref. Kent Fedorowich, \u201cGerman espionage and British counter-intelligence in ZA and Mozambique, 1939-1944\u201d , The Historical Journal 48:1 [^7]: Robin Hallett, \u201cThe ZA Intervention in Angola,\u201d African Affairs 77:312 (July 1978) [^8]: Ngwabi Bhebe, Terence Ranger, Soldiers in Zimbabwe's Liberation War , 1995. [^9]: D-40 in turn was the reconstitution of the supposedly disbanded Z-squads used by BOSS for assassinations until its reorganization in 1979. [^10]: Agrell, Wilhelm. \u201cSweden: Intelligence the Middle Way.\u201d Intelligence Elsewhere: Spies and Espionage Outside the Anglosphere . Ed. Philip H. J. Davies, Ed. Kristian C. Gustafson [^11]: Davies, Philip H. J. \u201cThe Original Surveillance State: Kautilya\u2019s Arthashastra and Government by Espionage in Classical India. Intelligence Elsewhere: Spies and Espionage Outside the Anglosphere . Ed. Philip H. J. Davies, Ed. Kristian C. Gustafson [^12]: Al-Asmari, Abdulaziz A. \u201cOrigins of an Arab and Islamic Intelligence Culture.\u201d Intelligence Elsewhere: Spies and Espionage Outside the Anglosphere . Ed. Philip H. J. Davies, Ed. Kristian C. Gustafson","title":"Poland"},{"location":"Intelligence/#united-states","text":"Linda Zall established a program to analyze classified historical statellite imagery to analyze not foreign militaries but changes in the environment, in particular the extent of ice retreat in the polar regions of the Earth. The effort was sparked by then-Senator Al Gore whose letter to the CIA led to the establishment of the MEDEA program which declassified satellite imagery and oceanographic data. John Walker was a notorious spy who volunteered to the Soviet embassy in Washington in 1967. Especially after the North Koreans captured the USS Pueblo, Walker's information on the key list of the KL-47 cryptographic machine meant the Soviets were able to read US Navy communications until the entire system was replaced. John Walker was managed by former KGB general Oleg Kalugin. The acoustic characteristics of the Victor III submarine were made substantially less detectable as a result of Walker's revelation that the Soviet submarine fleet was easily tracked. The stealthy Akula-class submarines, launched in 1985, also benefited from the import of a Toshiba CNC milling machine , which in combination with Norwegian CNC machines , allowed propellors to be designed that were much quieter than before.","title":"United States"},{"location":"Kubernetes/","text":"\u2388\ufe0f Kubernetes Kubernetes (Greek for \"helmsman\", \"pilot\", or \"captain\" and \"k8s\" for short) has emerged as the leading container orchestrator in the industry since 2018. It provides a layer that abstracts infrastructure, including computers, networks, and other computers, for applications deployed on top. History Kubernetes was first announced by Google in mid-2014. It had been developed by Google after deciding to open-source the Borg system, a cluster and container management system that formed the automation infrastructure that powered the entire Google enterprise. Kubernetes coalesced from a fusion between developers working on Borg and Compute Engine . Borg eventually evolved into Omega. By that time, Amazon had established a market advantage and the developers decided to change their approach by introducing a disruptive technology to drive the relevance of the Compute platform they had built. They created a ubiquitous abstraction that could run better than anyone else. At the time, Google had been trying to engage the Linux kernel team and trying to overcome their skepticism. Internally, the project was framed as offering \"Borg as a Service\", although there were concerns that Google was in danger of revealing trade secrets. Google ultimately donated iKubernetes to the Cloud Native Computing Foundation . Architecture Kubernetes can be visualized as a system built from layers, with each higher layer abstracting the complexity of the lower levels. One server serves as the master , exposing an API for users and clients, assigning or scheduling work, and orchestrating communication between other components. Other machines in the cluster are called nodes or workers and accept and run workloads using available resources. Azure A volume represents a way to store, retrieve, and persist data across pods and through the application lifecycle. In the context of Azure, Kubernetes can use two types of data volume: Azure Disks using Azure Premium (SSDs) or Azure Standard (HDDs). Azure Files using a SMB 3.0 share backed by an Azure Storage account. \ud83d\udcd8 Glossary Deployment Desired State Management The Desired State Management system is used by Kubernetes to describe a cluster's desired state declaratively. kube-apiserver One of the three processes run by a master node kube-controller-manager One of the three processes run by a master node kube-scheduler One of the three processes run by a master node Kubelet Master node A master node runs 3 processes, called master (control plane) components: kube-apiserver kube-controller-manager kube-scheduler . Node A node or worker is any container host that accepts workloads from the master node. Each node is equipped with a container runtime like Docker, which it uses to create and destroy containers according to instructions from the master server. Each node runs 2 processes kubelet , which communicates with Kubernetes cluster services kube-proxy PersistentVolume PersistentVolumeClaim A PersistentVolumeClaim requests either Disk or File storage of a particular StorageClass, access mode, and size. It is bound to a PersistentVolume once an available storage resource has been assigned to the pod requesting it. Pod A pod is the most basic unit that K8s deals with, representing one or more tightly-coupled containers that should be controlled as a single application (typically one main container with subsidiary helper containers). Within a pod you can specify more than one container image. A pod's containers should: operate closely together share a lifecycle always be scheduled on the same node ReplicaSet Worker","title":"\u2388&#xfe0f; Kubernetes"},{"location":"Kubernetes/#kubernetes","text":"Kubernetes (Greek for \"helmsman\", \"pilot\", or \"captain\" and \"k8s\" for short) has emerged as the leading container orchestrator in the industry since 2018. It provides a layer that abstracts infrastructure, including computers, networks, and other computers, for applications deployed on top.","title":"\u2388&#xfe0f; Kubernetes"},{"location":"Kubernetes/#history","text":"Kubernetes was first announced by Google in mid-2014. It had been developed by Google after deciding to open-source the Borg system, a cluster and container management system that formed the automation infrastructure that powered the entire Google enterprise. Kubernetes coalesced from a fusion between developers working on Borg and Compute Engine . Borg eventually evolved into Omega. By that time, Amazon had established a market advantage and the developers decided to change their approach by introducing a disruptive technology to drive the relevance of the Compute platform they had built. They created a ubiquitous abstraction that could run better than anyone else. At the time, Google had been trying to engage the Linux kernel team and trying to overcome their skepticism. Internally, the project was framed as offering \"Borg as a Service\", although there were concerns that Google was in danger of revealing trade secrets. Google ultimately donated iKubernetes to the Cloud Native Computing Foundation .","title":"History"},{"location":"Kubernetes/#architecture","text":"Kubernetes can be visualized as a system built from layers, with each higher layer abstracting the complexity of the lower levels. One server serves as the master , exposing an API for users and clients, assigning or scheduling work, and orchestrating communication between other components. Other machines in the cluster are called nodes or workers and accept and run workloads using available resources.","title":"Architecture"},{"location":"Kubernetes/#azure","text":"A volume represents a way to store, retrieve, and persist data across pods and through the application lifecycle. In the context of Azure, Kubernetes can use two types of data volume: Azure Disks using Azure Premium (SSDs) or Azure Standard (HDDs). Azure Files using a SMB 3.0 share backed by an Azure Storage account.","title":"Azure"},{"location":"Kubernetes/#glossary","text":"","title":"\ud83d\udcd8 Glossary"},{"location":"Kubernetes/#deployment","text":"","title":"Deployment"},{"location":"Kubernetes/#desired-state-management","text":"The Desired State Management system is used by Kubernetes to describe a cluster's desired state declaratively.","title":"Desired State Management"},{"location":"Kubernetes/#kube-apiserver","text":"One of the three processes run by a master node","title":"kube-apiserver"},{"location":"Kubernetes/#kube-controller-manager","text":"One of the three processes run by a master node","title":"kube-controller-manager"},{"location":"Kubernetes/#kube-scheduler","text":"One of the three processes run by a master node","title":"kube-scheduler"},{"location":"Kubernetes/#kubelet","text":"","title":"Kubelet"},{"location":"Kubernetes/#master-node","text":"A master node runs 3 processes, called master (control plane) components: kube-apiserver kube-controller-manager kube-scheduler .","title":"Master node"},{"location":"Kubernetes/#node","text":"A node or worker is any container host that accepts workloads from the master node. Each node is equipped with a container runtime like Docker, which it uses to create and destroy containers according to instructions from the master server. Each node runs 2 processes kubelet , which communicates with Kubernetes cluster services kube-proxy","title":"Node"},{"location":"Kubernetes/#persistentvolume","text":"","title":"PersistentVolume"},{"location":"Kubernetes/#persistentvolumeclaim","text":"A PersistentVolumeClaim requests either Disk or File storage of a particular StorageClass, access mode, and size. It is bound to a PersistentVolume once an available storage resource has been assigned to the pod requesting it.","title":"PersistentVolumeClaim"},{"location":"Kubernetes/#pod","text":"A pod is the most basic unit that K8s deals with, representing one or more tightly-coupled containers that should be controlled as a single application (typically one main container with subsidiary helper containers). Within a pod you can specify more than one container image. A pod's containers should: operate closely together share a lifecycle always be scheduled on the same node","title":"Pod"},{"location":"Kubernetes/#replicaset","text":"","title":"ReplicaSet"},{"location":"Kubernetes/#worker","text":"","title":"Worker"},{"location":"Linux/","text":"\ud83d\udc27 Linux Audio ALSA Advanced Linux Sound Architecture (ALSA) replaced the earlier \"Open Sound System\". ( src ) ALSA kernel modules are designed to offer an interface that \"corresponds to that of the hardware\" to keep the modules simple, and similar cards will offer a similar interface. They offer two interfaces: operational and configuration Operational interface through the /dev/ tree, with 3 main types of devices - PCM for recording or playing digitized sound samples - CTL for manipulating the internal mixer and routing of the card - MIDI to control the MIDI port, if it exists - Optionally, sequencer devices may also exist if the card has a builtin sound synthesizer with an associated timer device Status and configuration interface via the /proc/asound/ tree (ref amixer ) PCM devices come in two varieties: output and input and are numbered from 0, which is generally for analog multichannel sound. Cards have input or output sockets , and the mixer is controlled by the CTL device and routes sound samples among devices and sockets. Controls come in 3 types; - Playback controls are associated with an output device or copy (input-to-output) routes - Capture controls are associated with an input device or copy (output-to-input) routes - Feature controls drive features of the card or mixer, usually just a switch to enable or disable the feature, though some also have levels. The Master Volume control is the most typical example, which allows control of the internal amplifier feature of the card. A more interesting example is that of a 3d spatializer that can be represented by a switch to enable or disable it as well as two levels. Typical channel assignments - 0 : front left - 1 : front right - 2 : rear left - 3 : rear right PulseAudio PulseAudio is a sound server for POSIX OSes and a fixture on many Linux distributions. PulseAudio is built around sources and sinks (i.e. devices) connected to source outputs and sink inputs (streams) - Source is an input device that produces samples, usually running a thread with its own event loop, generating sample chunks which are posted to all connected source outputs - Source output is a recording stream which consumes samples from a source - Sink is an output device that consumes samples, usually running a thread with its own event loop mixing sample chunks from connect sink inputs - Sink input is a playback stream, connected to a sink and producing samples for it \ud83e\udd7e Boot Bootloaders like GRUB (GRand Unified Bootloader) or u-boot turns on power supplies and scans buses and interfaces to locate the kernel image and the root filesystem. LILO (LInux LOader) is also another bootloader that can be found on older Linux systems. Microcontrollers may be listening when the system is nominally off; they typically have their own BIOS and kernels and are inaccessible from the main system: Baseboard Management Controller (BMC) responds to wake-on-LAN (WOL) Intel Management Engine (IME) x86_64 software suite for remote management of systems; firmware is based on Minix and runs on the Platform Controller Hub processor, not the main CPU System Management Mode (SMM) launches UEFI software initrd (Initial RAM disk) is a temporary file system that's loaded into memory when the system boots Linux kernel is typically named vmlinux (or vmlinuz when compressed). Kernel ring buffer contains messages related to the Linux kernel. A ring buffer is a data structure that is always the same size; old messages are discarded as new ones come in, once the buffer is full. dmesg is used to see its contents, and the messages are also stored in /var/log/dmesg Init SystemD was designed by a pair of Red Hat developers in 2010 to be a general purpose system manager. It offers parallel execution, explicit dependencies between services, an escape from slow shell scripts, and per-daemon resource control and watchdogs. It was intended to address multiple shortcomings with SysVinit , a daemon process which was used by most distros until recently. In SysVinit, processes started serially and synchronously, wasting time and system resources. For years, a common hack was to run services in the background, simulating concurrency. Upstart was another init system developed by Canonical for Ubuntu meant to replace SysVinit, but it was abandoned in 2014. SystemD introduces the concepts of [ Units ] which are subdivided into various unit types, each of which is associated with a filename extension. YouTube Linode [Targets][target] (SystemD) Runlevels (System V Init) poweroff.target 0 rescue.target 1 multi-user.target 3 graphical.target 5 reboot.target 6 emergency.target emergency SystemD searches for units from most specific to most general. /usr/lib/systemd/system : default location where unit files are installed by packages /run/systemd/system : runtime configuration /etc/systemd/system : takes precedence over unit files located anywhere else Associated programs: SystemD [hostnamectl][hostnamectl] journalctl [localectl][localectl] [loginctl][loginctl] [systemctl][systemctl] [systemd-delta][systemd-delta] timedatectl SysVinit chkconfig init runlevel service telinit Upstart initctl \ud83d\udcbe Disk Index node (inode) is a data structure that stores all the information about a file except its name and data Most modern Linux distributions use the ext4 filesystem, which descends from ext3 and ext2 , and ultimately ext . Other filesystems in use include btrfs , xfs , and zfs Source: ref Extended File System was first implemented in 1992 by Remy Card to address limitations in the MINIX filesystem, which was used to develop the first Linux kernel. It could address up to 2GB of storage and handle 255-character filenames and had only one timestap per file. ext2 was developed by Remy Card only a year after ext 's release as a commercial-grade filesystem, influenced by BSD's Berkeley Fast File System. It was prone to corruption if the system crashed or lost power while data was being written and performance losses due to fragmentation. Nevertheless, it was quickly and widely adopted, and still used as a format for USB drives. ext3 was adopted by mainline Linux in 2001 and uses journaling , whereby disk writes are stored as transactions in a special allocation, which allows a rebooted system to roll back incomplete transactions. 3 journaling modes: journal , ordered , and writeback ext4 was added to mainline Linux in 2008, developed by Theodore Ts'o, and improves upon ext3 but is still reliant on old technology. ZFS is a true next-generation filesystem with a problematic license. ZFS on Linux (ZOL) is considered the ugly stepchild of the ZFS community despite the fact that the Linux implementation has the most features and the most community support. ZFS is too tightly bound to the operation of the kernel to operate in true userspace, and that is why each implementation is different for operating systems. B-Tree Filesystem \"butter fs\" was adopted by SUSE Enterprise Linux, but support was dropped by Red Hat in 2017. Processes A process runs in its own user address space , a protected space which can't be disturbed by other users - all processes on a Linux system are child processes of a common parent: the init process which is executed by the kernel at boot time (PID 1) - every Linux process inherits the environment (PATH variable, etc) and other attributes of its parent process Every process has a parent; a process can spawn children in a process that is actually made of two separate system calls. FORK copy process invoking it as fork() EXEC parent then overwrites this copy with the program that has to be executed which replaces or overlays the text and data areas as exec() system call WAIT parent waits for SIGTERM signal which the child will send upon completion wait() system call TEXT SEGMENT : executable code DATA SEGMENT : variables and arrays the program uses during execution USER SEGMENT : process attributes process ID (PID) real user ID of user who created it (stored in /etc/passwd) real group ID priority Internal commands ( cd , echo , etc. and variable assignments) do not spawn child processes Shell scripts are executed by spawning a sub-shell, which becomes the script's parent External commands are spawned as children of the parent as described above Cgroups Control group (cgroups) is a Linux kernel feature that isolates a collection of processes and is the concept behind containers . allow you to allocate resources (CPU time, system memory, network bandwidth, or combinations thereof) among user-defined groups of processes like processes, cgroups are hierarchical and inherit attributes from parents, but they from separate trees branching off from subsystems (also: resource controller , or just controller ), each of which represent a single system resouce blkio sets limits on input/output access from block devices cpu which provides access to the CPU many more... different from namespaces good for limiting the resources available to a container systemd uses cgroups first heard about in Linux Unplugged 289, in the context of Fedora supporting v2 whereas most userspace applications support v1 Process IDs in the same namespace can have access to one another, whereas those in different namespaces cannot. Spawning a process in a new namespace prevents it from seeing the host's context, so an interactive shell like zsh spawned in its own namespace will report its PID as 1 , even though the host will assign its own PID. Security Library injections Similar to DLL files on Windows systems, .so (\"shared object\") library files on Linux allow code to be shared by various processes. They are vulnerable to injection attacks. One file in particular, linux-vdso.so.1 , finds and locates other shared libraries and is mapped by the kernel into the address space of every process. This library-loading mechanism can be exploited through the use of the environment variable LD_PRELOAD , which is considered the most convenient way to load a shared library in a process at startup. If defined, this variable is read by the system and the library is loaded immediately after linux-vdso.so.1 into every process that is run. ^ This attack can be detected using the osquery tool. This tool represents the system as a relational database which can then be queried, in particular against the process_envs table. SELinux Display SELinux contexts for processes ps auxZ Display SELinux context for files ls -Z Associated programs: chcon getenforce getsebool restorecon semanage sestatus setenforce setsebool FACL Filesystem access control lists (FACL) allow you to grant permissions to more than one group, i.e. in cases where more than one department of a corporation needs access to the same files. They are made up of access control entries (ACE). FACL permissions will be indicated in a ls -l command by the presence of a \"+\" after the symbolic notation for the traditional UGO permissions. Acl is a dependency of systemd . To enable it, add \",acl\" to options in fstab file, then mount/unmount disk. If enabling FACL on root partition, system has to be rebooted. Applications IRC Connecting to Twitch requires the use of an OAuth token, which can only be granted by another web application. One such web application is made available here . The token obviates the need to specify a username, since the Twitch account's username is used. irc.chat.twitch.tv password must be oauth token (beginning with \"oauth:\") There are many IRC clients available: irssi weechat Resources: Twitch IRC documentation Tiling window managers Overview of tiling window managers: ( src ) i3 is perhaps the most popular tiling window manager, trending toward manual rather than dynamic . It is typically used with polybar . dwm one of the oldest and lightest tiling window managers. Because suckless wants the source code not to exceed 2,000 lines of code, a lot of functionality is incorporated by means of \"patches\", which modify the source code using diff files. Workspaces are called tags . A window can be associated with more than one tag, placing it on more than one workspace. Each monitor has a separate pool of workspaces. xmonad is a tiling window manager made especially difficult to configure because the program written in Haskell, as the config must be. All monitors share the same pool of workspaces. Unusually for tiling window managers, when using multiple monitors, switching to another workspace actually switches the position of that workspace with the previous one. That is, the workspace that had previously been on the active monitor is sent to the workspace being called. awesome originated as a fork of dwm , it offers creature comforts that make it the easiest to adjust to as a new user of tiling window managers. It is written in Lua, as its config must be. Like dwm, each monitor has an independent pool of workspaces. herbstluft has a single pool of workspaces that is shared across all monitors. bspwm (\"Binary Space Partitioning Window Manager\") uses tree partitioning as the logic for organizing tiles, with the default being the \"dwindle\" pattern. Like awesome, bspwm uses a shared pool of workspaces, but they are individually assigned to monitors in the configuration file. Notably, it uses two config files: .bspwmrc which determines what programs to autoload but doesn't contain any key bindings .sxhkdrc which uses a syntax similar to i3 or herbstluft. Simulating a tiling window manager in KDE Run Command > Run Command (Meta+R) KDE Daemon > Launch Konsole (Meta+Return) Desktop navigation keyboard shortcuts: KWin > Switch to Desktop (Meta+Fkey) Switch to Previous/Next Desktop (Meta + PgUp/PgDown) Switch to Window Above/Below/to the Left/Right (Meta+ K/J/H/L) Window to Desktop (Meta+Shift+Fkey) Window to Previous/Next Desktop (Meta+Shift_PgUp/PgDown) Panel screen edge -> top Pager settings > General: Display only Desktop name Configure Desktops Change names of desktops to Font Awesome icons Check \"Show on-screen display when switching\" Key definitions are those provided in the output of xmodmap floating_modifier : holding this key will allow windows to be dragged around with the mouse Window splitting uses the convention opposite to that of vim: - split h : split horizontally (to the right) - split v : split vertically (down) Tasks Custom resolution Specify a custom resolution in a VM github.io cvt 2560 1440 xrandr --newmode \"2560x1440_60.00\" 312 .25 2560 2752 3024 3488 1440 1443 1448 1493 -hsync +vsync xrandr --addmode Virtual-1 2560x1440_60.00 xrandr --output Virtual-1 --mode 2560x1440_60.0 X forwarding ssh -Y user@host Have remote system use local computer {me.luna.edu}'s X display export DISPLAY = me.luna.edu:0 Install Samba Install and configure Samba server [src][https://vitux.com/how-to-install-and-configure-samba-on-ubuntu/] Install samba sudo apt install samba Verify the samba service smbd is running sudo systemctl status smbd Configure Samba sudo mkdir /samba # Create a directory for the share sudo chmod -R 0777 /samba sudo chown -R nobody:nobody /samba # Remove ownership Open firewall rule sudo firewall-cmd --permanent --add-service = samba sudo firewall-cmd --reload Configure Samba config file at [/etc/samba/smb.conf][/etc/samba/smb.conf] [samba-share] comment = Samba on Ubuntu path = /samba read only = no browsable = yes Set up a Samba account for $USER sudo smbpasswd -a $USER Restart Samba service [vitux.com][https://vitux.com/how-to-install-and-configure-samba-on-ubuntu/] tecmint.com sudo systemctl restart smbd.service Install and configure Samba as a client sudo apt install smbclient Access samba share at $SHARE at server $HOST using user credential $USER sudo smbclient // $HOST / $USER -U $USER This will display the Samba CLI smb: \\> Diagnosing network problems Test from the inside out, starting with the loopback ping looback address, testing the TCP/IP stack ping the hardware interface ping another host on the network ping the gateway ping an IP address on the Internet ping a hostname on the Internet Display contents of a random file ls | sort -R | sed 1q | xargs cat Find out which commands you use most often history | awk '{print $2' | sort | uniq -c | sort -rn | head Count the number of occurrences of a string | uniq -c | sort - Change hostname sudo hostnamectl set-hostname newhostname Check kernel version linuxize.com uname -srm hostnamectl | grep \"Kernel\" cat /proc/version \ud83d\udcbf Distributions Fully-featured desktop environments are distinct from window managers , which are more focused in scope Alpine Linux Security-oriented, lightweight Linux distribution used in containers and hardware. BSD Berkeley Software Distribution (BSD) began in the 70s and was based on AT&T original code. First source distributions required user to purchase a source license from AT&T, since much of the BSD source was derivative of UNIX. Berkeley finally released a \"wholly-BSD\" product as Network Release 1 in 1989, which satisfied vendor demand for the TCP/IP networking code for PC. Work immediately began to reconstruct the remaining functionality of UNIX, which was completed in Network Release 2, released in 1991, which was based entirely on Berkeley code. Eventually this resulted in the 386BSD distribution, which then spawned five interrelated BSD distros: BSDI (now Wind River), NetBSD, FreeBSD, OpenBSD, and Darwin/Mac OS X Unix System Laboratories (USL) sued BSDI after BSDI attempted to market its product as a real UNIX, and other BSD distributions were affected by disputed code. Ultimately 3 out of the 18,000 files that made up the Network Release 2 distribution were removed, which became known as 4.4BSD-lite, released in 1994. This legal dispute was partly to blame for Linux's rapid ascent in popularity. ( src ) Clear Linux Rolling release distro from Intel with a custom package management system based on bundles , collections of packages that contain everything an application requires, including dependencies. Clear's update process also has the ability to do delta downloads , preserving bandwidth. It does not provide access with unusual licenses, like ZFS, Chrome, or FFmpeg. ( src ) Red Hat After a shift in late 2020, CentOS Stream is no considered upstream to RHEL (ahead by a point-release), but downstream from Fedora. CentOS is a community distribution of Linux that was created by Gregory Kurtzer in 2004 and acquired by Red Hat in 2014. It has traditionally been considered downstream to RHEL , incorporating changes to RHEL after a delay of several months. In fact, it is a rebuild of the publicly available source RPMs (SRPMs) of RHEL packages, which historically allowed CentOS maintainers to simply package and ship them rebranded . For years, CentOS was the distribution of choice for experience Linux administrators who did not feel the need to pay for Red Hat's support. In December 2020, Red Hat announced that CentOS 8 support will end at the end of 2021 (rather than 2029), while CentOS 7 will continue to be supported until 2024. This represented a shift in focus from CentOS Linux to CentOS Stream and a change from a fixed-release (or \"stable point release\") to a rolling-release distribution model. CentOS Stream was announced in September 2019 as a distribution of CentOS maintained on a model previously misidentified as rolling-release but now described as \"continuously delivered\", organized into Streams. CentOS Stream originated in an effort to get more community participation in development of RHEL, rather than merely passive consumption. Fedora is a community distribution supported by Red Hat launched as \"Fedora Core\" in 2003. It has traditionally been considered as upstream to RHEL, serving as a testing ground for new features and improvements. Fedora CoreOS is a Fedora edition built specifically for running containerized workloads securely and at scale. Because containers can be deployed across many nodes for redundancy, the system can be updated and rebooted automatically without affecting uptime. CoreOS systems are meant to be immutable infrastructure , meaning they are only configured through the provisioning process and not modified in-place. All systems start with a generic OS image, but on first boot it uses a system called Ignition to read an Ignition config (which is converted from a Fedora CoreOS Config file) from the cloud or a remote URL, by which it provisions itself, creating disk partitions, file systems, users, etc. CoreOS automatically installs upgrades automatically without user intervention, although they can be stopped if a problem is found. Red Hat Enterprise Linux (RHEL) is Red Hat's commercial Linux distribution. SUSE OpenSUSE Leap is a rebuild of SUSE Linux Enterprise Server , similar to how CentOS was historically a rebuild of RHEL. SUSE Linux Enterprise Server (SLES) (\"slee\") is SUSE's fixed-release distribution of Linux intended for enterprises, and as such is comparable to Red Hat's RHEL. WSL Windows Subsystem for Linux (WSL) is shipped with Windows and tied to the Windows release cycle. Windows ships from a single massive codebase, of which WSL is part. WSL was written mostly in C and and has 3 million monthly active users. ( src ) WSL implements user services to connect to WSL distros and to run Windows-native applications like CMD.exe. WSL implements a 9P Protocol file server to provide seamless integration of the virtualized Linux filesystem and that of the Windows host. WSL 1 worked under a translation architecture where system calls were translated to NT kernel calls. This meant that applications that used system calls that were newer or more difficult to implement, like GUI applications or Docker, did not run on v1. WSL2 shifted to a lighweight virtualization model using the Linux kernel. Now Docker runs on WSL2 and GUI applications can run by using an X server. WSL v1 is available on Azure VMs if nested virtualization is enabled. WSL2 support is forthcoming. VHDs for WSL distributions are available at %LOCALAPPDATA%\\Packages\\<PackageFamilyName>\\LocalState where <PackageFamilyName> reflects the name of the Microsoft Store package of the distro, i.e.: - CanonicalGroupLimited.Ubuntu20.04onWindows_79rhkp1fndgsc - TheDebianProject.DebianGNULinux_76v4gfsz19hv4 Remove a WSL distribution wsl.exe --unregister Ubuntu-20.04 By default, WSL appears to copy the Windows native hosts file at %SystemRoot%\\System32\\drivers\\etc\\hosts to the distro's /etc/hosts file. Because WSL2 uses virtualized ethernet adapters with a unique IP address, using SSH is complicated. You must use netsh to forward requests to a local port to the internal VM's address and port. Windows Firewall must also allow connections to this port. The WSL2 distro must also have the SSH daemon running, as normal. \ud83d\udcd8 Glossary address space An address space consists of a set of pages in memory allocated to the process. (ULSAH: 90) display manager Basically display managers are the login screens, while the GUI manipulated during normal use represents the desktop environment (i.e. GNOME, KDE, XFCE, etc). MDA Mail Delivery Agent (MDA) - service that downloads email from an MTA, such as procmail and fetchmail MTA Mail Transfer Agent (MTA) - email server, such as sendmail, postfix , smail, and qmail MUA Mail User Agent (MUA) - program that allows a user to view mail, such as mutt, pine, printmail, elm, mail, Thunderbird, Evolution, and Eudora process A process consists of an address space and a set of data structures within the kernel. qmail MTA designed as a drop-in replacement for Sendmail, notable for being the first to be \"security-aware\". Its various modular subcomponents run independently and are mutually untrustful. It uses SMTP to exchange messages with other MTAs. It was written by Dan Bernstein, a professor of mathematics famous for litigating against the US government with regard to export controls on encryption algorithms. qmail was deprecated and removed from Arch repos in 2005. thread A thread is an execution context within a process. (ULSAH: 91) /etc/bluetooth/input.conf Fix bluetooth mouse constantly disconnecting ( src ) UserspaceHID = true /etc/bluetooth/main.conf Power on Bluetooth adapter at startup ( src ) [Policy] AutoEnable = true /etc/group Colon-delimited file describing group membership $GROUP:$PASSWORD:$GID:$USER1:$USER2:$USER3... /etc/resolv.conf Use DNS queries prior to consulting /etc/hosts nameserver dns nameserver files /etc/shadow Colon-delimited file containing password hashes for every user listed in [/etc/passwd][/etc/passwd] $USERNAME:$PASSWORD:$LASTCHANGED:$MIN:$MAX:$WARN:$INACTIVE:$EXPIRE $USERNAME Login name $PASSWORD Encrypted password; dollar signs delimit encryption hash function ( $1 , $2a , $2y , $5 , or $6 ), then salt, then hash value. After locking the account with usermod -L , an exclamation point ! is placed in front of this field, making the password inoperable and locking the account. When an account has not yet had a password set, this value is !! $LASTCHANGED Days since 01/01/1970 that password was last changed $MIN minimum number of days required between password changes $MAX maximum number of days the password is valid before user is forced to change password $WARN number of days the password is to expire that user is warned that password must be changed $INACTIVE number of days after password expires that account is disabled $EXPIRE days since 01/01/1970 that account is disabled","title":"\ud83d\udc27 Linux"},{"location":"Linux/#linux","text":"","title":"\ud83d\udc27 Linux"},{"location":"Linux/#audio","text":"","title":"Audio"},{"location":"Linux/#alsa","text":"Advanced Linux Sound Architecture (ALSA) replaced the earlier \"Open Sound System\". ( src ) ALSA kernel modules are designed to offer an interface that \"corresponds to that of the hardware\" to keep the modules simple, and similar cards will offer a similar interface. They offer two interfaces: operational and configuration Operational interface through the /dev/ tree, with 3 main types of devices - PCM for recording or playing digitized sound samples - CTL for manipulating the internal mixer and routing of the card - MIDI to control the MIDI port, if it exists - Optionally, sequencer devices may also exist if the card has a builtin sound synthesizer with an associated timer device Status and configuration interface via the /proc/asound/ tree (ref amixer ) PCM devices come in two varieties: output and input and are numbered from 0, which is generally for analog multichannel sound. Cards have input or output sockets , and the mixer is controlled by the CTL device and routes sound samples among devices and sockets. Controls come in 3 types; - Playback controls are associated with an output device or copy (input-to-output) routes - Capture controls are associated with an input device or copy (output-to-input) routes - Feature controls drive features of the card or mixer, usually just a switch to enable or disable the feature, though some also have levels. The Master Volume control is the most typical example, which allows control of the internal amplifier feature of the card. A more interesting example is that of a 3d spatializer that can be represented by a switch to enable or disable it as well as two levels. Typical channel assignments - 0 : front left - 1 : front right - 2 : rear left - 3 : rear right","title":"ALSA"},{"location":"Linux/#pulseaudio","text":"PulseAudio is a sound server for POSIX OSes and a fixture on many Linux distributions. PulseAudio is built around sources and sinks (i.e. devices) connected to source outputs and sink inputs (streams) - Source is an input device that produces samples, usually running a thread with its own event loop, generating sample chunks which are posted to all connected source outputs - Source output is a recording stream which consumes samples from a source - Sink is an output device that consumes samples, usually running a thread with its own event loop mixing sample chunks from connect sink inputs - Sink input is a playback stream, connected to a sink and producing samples for it","title":"PulseAudio"},{"location":"Linux/#boot","text":"Bootloaders like GRUB (GRand Unified Bootloader) or u-boot turns on power supplies and scans buses and interfaces to locate the kernel image and the root filesystem. LILO (LInux LOader) is also another bootloader that can be found on older Linux systems. Microcontrollers may be listening when the system is nominally off; they typically have their own BIOS and kernels and are inaccessible from the main system: Baseboard Management Controller (BMC) responds to wake-on-LAN (WOL) Intel Management Engine (IME) x86_64 software suite for remote management of systems; firmware is based on Minix and runs on the Platform Controller Hub processor, not the main CPU System Management Mode (SMM) launches UEFI software initrd (Initial RAM disk) is a temporary file system that's loaded into memory when the system boots Linux kernel is typically named vmlinux (or vmlinuz when compressed). Kernel ring buffer contains messages related to the Linux kernel. A ring buffer is a data structure that is always the same size; old messages are discarded as new ones come in, once the buffer is full. dmesg is used to see its contents, and the messages are also stored in /var/log/dmesg","title":"\ud83e\udd7e Boot"},{"location":"Linux/#init","text":"SystemD was designed by a pair of Red Hat developers in 2010 to be a general purpose system manager. It offers parallel execution, explicit dependencies between services, an escape from slow shell scripts, and per-daemon resource control and watchdogs. It was intended to address multiple shortcomings with SysVinit , a daemon process which was used by most distros until recently. In SysVinit, processes started serially and synchronously, wasting time and system resources. For years, a common hack was to run services in the background, simulating concurrency. Upstart was another init system developed by Canonical for Ubuntu meant to replace SysVinit, but it was abandoned in 2014. SystemD introduces the concepts of [ Units ] which are subdivided into various unit types, each of which is associated with a filename extension. YouTube Linode [Targets][target] (SystemD) Runlevels (System V Init) poweroff.target 0 rescue.target 1 multi-user.target 3 graphical.target 5 reboot.target 6 emergency.target emergency SystemD searches for units from most specific to most general. /usr/lib/systemd/system : default location where unit files are installed by packages /run/systemd/system : runtime configuration /etc/systemd/system : takes precedence over unit files located anywhere else Associated programs: SystemD [hostnamectl][hostnamectl] journalctl [localectl][localectl] [loginctl][loginctl] [systemctl][systemctl] [systemd-delta][systemd-delta] timedatectl SysVinit chkconfig init runlevel service telinit Upstart initctl","title":"Init"},{"location":"Linux/#disk","text":"Index node (inode) is a data structure that stores all the information about a file except its name and data Most modern Linux distributions use the ext4 filesystem, which descends from ext3 and ext2 , and ultimately ext . Other filesystems in use include btrfs , xfs , and zfs Source: ref Extended File System was first implemented in 1992 by Remy Card to address limitations in the MINIX filesystem, which was used to develop the first Linux kernel. It could address up to 2GB of storage and handle 255-character filenames and had only one timestap per file. ext2 was developed by Remy Card only a year after ext 's release as a commercial-grade filesystem, influenced by BSD's Berkeley Fast File System. It was prone to corruption if the system crashed or lost power while data was being written and performance losses due to fragmentation. Nevertheless, it was quickly and widely adopted, and still used as a format for USB drives. ext3 was adopted by mainline Linux in 2001 and uses journaling , whereby disk writes are stored as transactions in a special allocation, which allows a rebooted system to roll back incomplete transactions. 3 journaling modes: journal , ordered , and writeback ext4 was added to mainline Linux in 2008, developed by Theodore Ts'o, and improves upon ext3 but is still reliant on old technology. ZFS is a true next-generation filesystem with a problematic license. ZFS on Linux (ZOL) is considered the ugly stepchild of the ZFS community despite the fact that the Linux implementation has the most features and the most community support. ZFS is too tightly bound to the operation of the kernel to operate in true userspace, and that is why each implementation is different for operating systems. B-Tree Filesystem \"butter fs\" was adopted by SUSE Enterprise Linux, but support was dropped by Red Hat in 2017.","title":"\ud83d\udcbe Disk"},{"location":"Linux/#processes","text":"A process runs in its own user address space , a protected space which can't be disturbed by other users - all processes on a Linux system are child processes of a common parent: the init process which is executed by the kernel at boot time (PID 1) - every Linux process inherits the environment (PATH variable, etc) and other attributes of its parent process Every process has a parent; a process can spawn children in a process that is actually made of two separate system calls. FORK copy process invoking it as fork() EXEC parent then overwrites this copy with the program that has to be executed which replaces or overlays the text and data areas as exec() system call WAIT parent waits for SIGTERM signal which the child will send upon completion wait() system call TEXT SEGMENT : executable code DATA SEGMENT : variables and arrays the program uses during execution USER SEGMENT : process attributes process ID (PID) real user ID of user who created it (stored in /etc/passwd) real group ID priority Internal commands ( cd , echo , etc. and variable assignments) do not spawn child processes Shell scripts are executed by spawning a sub-shell, which becomes the script's parent External commands are spawned as children of the parent as described above","title":"Processes"},{"location":"Linux/#cgroups","text":"Control group (cgroups) is a Linux kernel feature that isolates a collection of processes and is the concept behind containers . allow you to allocate resources (CPU time, system memory, network bandwidth, or combinations thereof) among user-defined groups of processes like processes, cgroups are hierarchical and inherit attributes from parents, but they from separate trees branching off from subsystems (also: resource controller , or just controller ), each of which represent a single system resouce blkio sets limits on input/output access from block devices cpu which provides access to the CPU many more... different from namespaces good for limiting the resources available to a container systemd uses cgroups first heard about in Linux Unplugged 289, in the context of Fedora supporting v2 whereas most userspace applications support v1 Process IDs in the same namespace can have access to one another, whereas those in different namespaces cannot. Spawning a process in a new namespace prevents it from seeing the host's context, so an interactive shell like zsh spawned in its own namespace will report its PID as 1 , even though the host will assign its own PID.","title":"Cgroups"},{"location":"Linux/#security","text":"","title":"Security"},{"location":"Linux/#library-injections","text":"Similar to DLL files on Windows systems, .so (\"shared object\") library files on Linux allow code to be shared by various processes. They are vulnerable to injection attacks. One file in particular, linux-vdso.so.1 , finds and locates other shared libraries and is mapped by the kernel into the address space of every process. This library-loading mechanism can be exploited through the use of the environment variable LD_PRELOAD , which is considered the most convenient way to load a shared library in a process at startup. If defined, this variable is read by the system and the library is loaded immediately after linux-vdso.so.1 into every process that is run. ^ This attack can be detected using the osquery tool. This tool represents the system as a relational database which can then be queried, in particular against the process_envs table.","title":"Library injections"},{"location":"Linux/#selinux","text":"Display SELinux contexts for processes ps auxZ Display SELinux context for files ls -Z Associated programs: chcon getenforce getsebool restorecon semanage sestatus setenforce setsebool","title":"SELinux"},{"location":"Linux/#facl","text":"Filesystem access control lists (FACL) allow you to grant permissions to more than one group, i.e. in cases where more than one department of a corporation needs access to the same files. They are made up of access control entries (ACE). FACL permissions will be indicated in a ls -l command by the presence of a \"+\" after the symbolic notation for the traditional UGO permissions. Acl is a dependency of systemd . To enable it, add \",acl\" to options in fstab file, then mount/unmount disk. If enabling FACL on root partition, system has to be rebooted.","title":"FACL"},{"location":"Linux/#applications","text":"","title":"Applications"},{"location":"Linux/#irc","text":"Connecting to Twitch requires the use of an OAuth token, which can only be granted by another web application. One such web application is made available here . The token obviates the need to specify a username, since the Twitch account's username is used. irc.chat.twitch.tv password must be oauth token (beginning with \"oauth:\") There are many IRC clients available: irssi weechat Resources: Twitch IRC documentation","title":"IRC"},{"location":"Linux/#tiling-window-managers","text":"Overview of tiling window managers: ( src ) i3 is perhaps the most popular tiling window manager, trending toward manual rather than dynamic . It is typically used with polybar . dwm one of the oldest and lightest tiling window managers. Because suckless wants the source code not to exceed 2,000 lines of code, a lot of functionality is incorporated by means of \"patches\", which modify the source code using diff files. Workspaces are called tags . A window can be associated with more than one tag, placing it on more than one workspace. Each monitor has a separate pool of workspaces. xmonad is a tiling window manager made especially difficult to configure because the program written in Haskell, as the config must be. All monitors share the same pool of workspaces. Unusually for tiling window managers, when using multiple monitors, switching to another workspace actually switches the position of that workspace with the previous one. That is, the workspace that had previously been on the active monitor is sent to the workspace being called. awesome originated as a fork of dwm , it offers creature comforts that make it the easiest to adjust to as a new user of tiling window managers. It is written in Lua, as its config must be. Like dwm, each monitor has an independent pool of workspaces. herbstluft has a single pool of workspaces that is shared across all monitors. bspwm (\"Binary Space Partitioning Window Manager\") uses tree partitioning as the logic for organizing tiles, with the default being the \"dwindle\" pattern. Like awesome, bspwm uses a shared pool of workspaces, but they are individually assigned to monitors in the configuration file. Notably, it uses two config files: .bspwmrc which determines what programs to autoload but doesn't contain any key bindings .sxhkdrc which uses a syntax similar to i3 or herbstluft. Simulating a tiling window manager in KDE Run Command > Run Command (Meta+R) KDE Daemon > Launch Konsole (Meta+Return) Desktop navigation keyboard shortcuts: KWin > Switch to Desktop (Meta+Fkey) Switch to Previous/Next Desktop (Meta + PgUp/PgDown) Switch to Window Above/Below/to the Left/Right (Meta+ K/J/H/L) Window to Desktop (Meta+Shift+Fkey) Window to Previous/Next Desktop (Meta+Shift_PgUp/PgDown) Panel screen edge -> top Pager settings > General: Display only Desktop name Configure Desktops Change names of desktops to Font Awesome icons Check \"Show on-screen display when switching\" Key definitions are those provided in the output of xmodmap floating_modifier : holding this key will allow windows to be dragged around with the mouse Window splitting uses the convention opposite to that of vim: - split h : split horizontally (to the right) - split v : split vertically (down)","title":"Tiling window managers"},{"location":"Linux/#tasks","text":"","title":"Tasks"},{"location":"Linux/#custom-resolution","text":"Specify a custom resolution in a VM github.io cvt 2560 1440 xrandr --newmode \"2560x1440_60.00\" 312 .25 2560 2752 3024 3488 1440 1443 1448 1493 -hsync +vsync xrandr --addmode Virtual-1 2560x1440_60.00 xrandr --output Virtual-1 --mode 2560x1440_60.0","title":"Custom resolution"},{"location":"Linux/#x-forwarding","text":"ssh -Y user@host Have remote system use local computer {me.luna.edu}'s X display export DISPLAY = me.luna.edu:0","title":"X forwarding"},{"location":"Linux/#install-samba","text":"Install and configure Samba server [src][https://vitux.com/how-to-install-and-configure-samba-on-ubuntu/] Install samba sudo apt install samba Verify the samba service smbd is running sudo systemctl status smbd Configure Samba sudo mkdir /samba # Create a directory for the share sudo chmod -R 0777 /samba sudo chown -R nobody:nobody /samba # Remove ownership Open firewall rule sudo firewall-cmd --permanent --add-service = samba sudo firewall-cmd --reload Configure Samba config file at [/etc/samba/smb.conf][/etc/samba/smb.conf] [samba-share] comment = Samba on Ubuntu path = /samba read only = no browsable = yes Set up a Samba account for $USER sudo smbpasswd -a $USER Restart Samba service [vitux.com][https://vitux.com/how-to-install-and-configure-samba-on-ubuntu/] tecmint.com sudo systemctl restart smbd.service Install and configure Samba as a client sudo apt install smbclient Access samba share at $SHARE at server $HOST using user credential $USER sudo smbclient // $HOST / $USER -U $USER This will display the Samba CLI smb: \\>","title":"Install Samba"},{"location":"Linux/#diagnosing-network-problems","text":"Test from the inside out, starting with the loopback ping looback address, testing the TCP/IP stack ping the hardware interface ping another host on the network ping the gateway ping an IP address on the Internet ping a hostname on the Internet Display contents of a random file ls | sort -R | sed 1q | xargs cat Find out which commands you use most often history | awk '{print $2' | sort | uniq -c | sort -rn | head Count the number of occurrences of a string | uniq -c | sort - Change hostname sudo hostnamectl set-hostname newhostname Check kernel version linuxize.com uname -srm hostnamectl | grep \"Kernel\" cat /proc/version","title":"Diagnosing network problems"},{"location":"Linux/#distributions","text":"Fully-featured desktop environments are distinct from window managers , which are more focused in scope","title":"\ud83d\udcbf Distributions"},{"location":"Linux/#alpine-linux","text":"Security-oriented, lightweight Linux distribution used in containers and hardware.","title":"Alpine Linux"},{"location":"Linux/#bsd","text":"Berkeley Software Distribution (BSD) began in the 70s and was based on AT&T original code. First source distributions required user to purchase a source license from AT&T, since much of the BSD source was derivative of UNIX. Berkeley finally released a \"wholly-BSD\" product as Network Release 1 in 1989, which satisfied vendor demand for the TCP/IP networking code for PC. Work immediately began to reconstruct the remaining functionality of UNIX, which was completed in Network Release 2, released in 1991, which was based entirely on Berkeley code. Eventually this resulted in the 386BSD distribution, which then spawned five interrelated BSD distros: BSDI (now Wind River), NetBSD, FreeBSD, OpenBSD, and Darwin/Mac OS X Unix System Laboratories (USL) sued BSDI after BSDI attempted to market its product as a real UNIX, and other BSD distributions were affected by disputed code. Ultimately 3 out of the 18,000 files that made up the Network Release 2 distribution were removed, which became known as 4.4BSD-lite, released in 1994. This legal dispute was partly to blame for Linux's rapid ascent in popularity. ( src )","title":"BSD"},{"location":"Linux/#clear-linux","text":"Rolling release distro from Intel with a custom package management system based on bundles , collections of packages that contain everything an application requires, including dependencies. Clear's update process also has the ability to do delta downloads , preserving bandwidth. It does not provide access with unusual licenses, like ZFS, Chrome, or FFmpeg. ( src )","title":"Clear Linux"},{"location":"Linux/#red-hat","text":"After a shift in late 2020, CentOS Stream is no considered upstream to RHEL (ahead by a point-release), but downstream from Fedora. CentOS is a community distribution of Linux that was created by Gregory Kurtzer in 2004 and acquired by Red Hat in 2014. It has traditionally been considered downstream to RHEL , incorporating changes to RHEL after a delay of several months. In fact, it is a rebuild of the publicly available source RPMs (SRPMs) of RHEL packages, which historically allowed CentOS maintainers to simply package and ship them rebranded . For years, CentOS was the distribution of choice for experience Linux administrators who did not feel the need to pay for Red Hat's support. In December 2020, Red Hat announced that CentOS 8 support will end at the end of 2021 (rather than 2029), while CentOS 7 will continue to be supported until 2024. This represented a shift in focus from CentOS Linux to CentOS Stream and a change from a fixed-release (or \"stable point release\") to a rolling-release distribution model. CentOS Stream was announced in September 2019 as a distribution of CentOS maintained on a model previously misidentified as rolling-release but now described as \"continuously delivered\", organized into Streams. CentOS Stream originated in an effort to get more community participation in development of RHEL, rather than merely passive consumption. Fedora is a community distribution supported by Red Hat launched as \"Fedora Core\" in 2003. It has traditionally been considered as upstream to RHEL, serving as a testing ground for new features and improvements. Fedora CoreOS is a Fedora edition built specifically for running containerized workloads securely and at scale. Because containers can be deployed across many nodes for redundancy, the system can be updated and rebooted automatically without affecting uptime. CoreOS systems are meant to be immutable infrastructure , meaning they are only configured through the provisioning process and not modified in-place. All systems start with a generic OS image, but on first boot it uses a system called Ignition to read an Ignition config (which is converted from a Fedora CoreOS Config file) from the cloud or a remote URL, by which it provisions itself, creating disk partitions, file systems, users, etc. CoreOS automatically installs upgrades automatically without user intervention, although they can be stopped if a problem is found. Red Hat Enterprise Linux (RHEL) is Red Hat's commercial Linux distribution.","title":"Red Hat"},{"location":"Linux/#suse","text":"OpenSUSE Leap is a rebuild of SUSE Linux Enterprise Server , similar to how CentOS was historically a rebuild of RHEL. SUSE Linux Enterprise Server (SLES) (\"slee\") is SUSE's fixed-release distribution of Linux intended for enterprises, and as such is comparable to Red Hat's RHEL.","title":"SUSE"},{"location":"Linux/#wsl","text":"Windows Subsystem for Linux (WSL) is shipped with Windows and tied to the Windows release cycle. Windows ships from a single massive codebase, of which WSL is part. WSL was written mostly in C and and has 3 million monthly active users. ( src ) WSL implements user services to connect to WSL distros and to run Windows-native applications like CMD.exe. WSL implements a 9P Protocol file server to provide seamless integration of the virtualized Linux filesystem and that of the Windows host. WSL 1 worked under a translation architecture where system calls were translated to NT kernel calls. This meant that applications that used system calls that were newer or more difficult to implement, like GUI applications or Docker, did not run on v1. WSL2 shifted to a lighweight virtualization model using the Linux kernel. Now Docker runs on WSL2 and GUI applications can run by using an X server. WSL v1 is available on Azure VMs if nested virtualization is enabled. WSL2 support is forthcoming. VHDs for WSL distributions are available at %LOCALAPPDATA%\\Packages\\<PackageFamilyName>\\LocalState where <PackageFamilyName> reflects the name of the Microsoft Store package of the distro, i.e.: - CanonicalGroupLimited.Ubuntu20.04onWindows_79rhkp1fndgsc - TheDebianProject.DebianGNULinux_76v4gfsz19hv4 Remove a WSL distribution wsl.exe --unregister Ubuntu-20.04 By default, WSL appears to copy the Windows native hosts file at %SystemRoot%\\System32\\drivers\\etc\\hosts to the distro's /etc/hosts file. Because WSL2 uses virtualized ethernet adapters with a unique IP address, using SSH is complicated. You must use netsh to forward requests to a local port to the internal VM's address and port. Windows Firewall must also allow connections to this port. The WSL2 distro must also have the SSH daemon running, as normal.","title":"WSL"},{"location":"Linux/#glossary","text":"","title":"\ud83d\udcd8 Glossary"},{"location":"Linux/#address-space","text":"An address space consists of a set of pages in memory allocated to the process. (ULSAH: 90)","title":"address space"},{"location":"Linux/#display-manager","text":"Basically display managers are the login screens, while the GUI manipulated during normal use represents the desktop environment (i.e. GNOME, KDE, XFCE, etc).","title":"display manager"},{"location":"Linux/#mda","text":"Mail Delivery Agent (MDA) - service that downloads email from an MTA, such as procmail and fetchmail","title":"MDA"},{"location":"Linux/#mta","text":"Mail Transfer Agent (MTA) - email server, such as sendmail, postfix , smail, and qmail","title":"MTA"},{"location":"Linux/#mua","text":"Mail User Agent (MUA) - program that allows a user to view mail, such as mutt, pine, printmail, elm, mail, Thunderbird, Evolution, and Eudora","title":"MUA"},{"location":"Linux/#process","text":"A process consists of an address space and a set of data structures within the kernel.","title":"process"},{"location":"Linux/#qmail","text":"MTA designed as a drop-in replacement for Sendmail, notable for being the first to be \"security-aware\". Its various modular subcomponents run independently and are mutually untrustful. It uses SMTP to exchange messages with other MTAs. It was written by Dan Bernstein, a professor of mathematics famous for litigating against the US government with regard to export controls on encryption algorithms. qmail was deprecated and removed from Arch repos in 2005.","title":"qmail"},{"location":"Linux/#thread","text":"A thread is an execution context within a process. (ULSAH: 91)","title":"thread"},{"location":"Linux/#etcbluetoothinputconf","text":"Fix bluetooth mouse constantly disconnecting ( src ) UserspaceHID = true","title":"/etc/bluetooth/input.conf"},{"location":"Linux/#etcbluetoothmainconf","text":"Power on Bluetooth adapter at startup ( src ) [Policy] AutoEnable = true","title":"/etc/bluetooth/main.conf"},{"location":"Linux/#etcgroup","text":"Colon-delimited file describing group membership $GROUP:$PASSWORD:$GID:$USER1:$USER2:$USER3... /etc/resolv.conf Use DNS queries prior to consulting /etc/hosts nameserver dns nameserver files","title":"/etc/group"},{"location":"Linux/#etcshadow","text":"Colon-delimited file containing password hashes for every user listed in [/etc/passwd][/etc/passwd] $USERNAME:$PASSWORD:$LASTCHANGED:$MIN:$MAX:$WARN:$INACTIVE:$EXPIRE $USERNAME Login name $PASSWORD Encrypted password; dollar signs delimit encryption hash function ( $1 , $2a , $2y , $5 , or $6 ), then salt, then hash value. After locking the account with usermod -L , an exclamation point ! is placed in front of this field, making the password inoperable and locking the account. When an account has not yet had a password set, this value is !! $LASTCHANGED Days since 01/01/1970 that password was last changed $MIN minimum number of days required between password changes $MAX maximum number of days the password is valid before user is forced to change password $WARN number of days the password is to expire that user is warned that password must be changed $INACTIVE number of days after password expires that account is disabled $EXPIRE days since 01/01/1970 that account is disabled","title":"/etc/shadow"},{"location":"Cloud/ARM/","text":"Azure Resource Manager (ARM) is the interface for managing and organizing cloud resources. ? An ARM template is a JSON file that precisely defines all ARM resources in a deployment. An ARM template can be deployed into a resource group as a single operation. ARM templates are typically adapted from existing Azure Quickstart templates, which are contributed by the community and hosted on a gallery . The Azure Resource Manager Visualizer assists users in seeing what the template will do before actually deploying. The Custom Script Extension is a way to run scripts on Azure VMs and represents one of the ways to automate configuration of new deployments. ? If you explort a deployment to a template, only the resources deployed in that deployment will be templatized. In the case of a complex deployment that had several phases, the ultimate result of the deployment can be obtained by exporting the template from the resource group. Structure A template must have at least the following sections. $schema contentVersion resources A template may have the following optional sections parameters variables functions outputs { \"$schema\" : \"http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\" , \"contentVersion\" : \"\" , \"parameters\" : { }, \"variables\" : { }, \"functions\" : [ ], \"resources\" : [ ], \"outputs\" : { } } Resources Create a public IP address. { \"type\" : \"Microsoft.Network/publicIPAddresses\" , \"apiVersion\" : \"2018-08-01\" , \"name\" : \"[variables('publicIPAddressName')]\" , \"location\" : \"[parameters('location')]\" , \"properties\" : { \"publicIPAllocationMethod\" : \"Dynamic\" , \"dnsSettings\" : { \"domainNameLabel\" : \"[parameters('dnsLabelPrefix')]\" } } } Create a storage account { \"type\" : \"Microsoft.Storage/storageAccounts\" , \"apiVersion\" : \"2019-06-01\" , \"name\" : \"[variables('storageAccountName')]\" , \"location\" : \"[parameters('location')]\" , \"sku\" : { \"name\" : \"[parameters('storageAccountType')]\" }, \"kind\" : \"StorageV2\" , \"properties\" : {} } Create a Azure Data Explorer cluster Parameters \"adminUsername\" : { \"type\" : \"string\" , \"metadata\" : { \"description\" : \"Username for the Virtual Machine.\" }}, \"adminPassword\" : { \"type\" : \"securestring\" , \"metadata\" : { \"description\" : \"Password for the Virtual Machine.\" }} Simple storage account { \"storageAccountType\" : { \"type\" : \"string\" , \"defaultValue\" : \"Standard_LRS\" , \"allowedValues\" : [ \"Standard_LRS\" , \"Standard_GRS\" , \"Standard_ZRS\" , \"Premium_LRS\" ], \"metadata\" : { \"description\" : \"Storage Account type\" }}, \"location\" : { \"type\" : \"string\" , \"defaultValue\" : \"[resourceGroup().location]\" , \"metadata\" : { \"description\" : \"Location for all resources.\" }}} Variables { \"nicName\" : \"myVMNic\" , \"addressPrefix\" : \"10.0.0.0/16\" , \"subnetName\" : \"Subnet\" , \"subnetPrefix\" : \"10.0.0.0/24\" , \"publicIPAddressName\" : \"myPublicIP\" , \"virtualNetworkName\" : \"MyVNET\" } Simple storage account { \"storageAccountName\" : \"[concat('store', uniquestring(resourceGroup().id))]\" }, Functions Create a globally unique name, useful for some resources that require it. concat is a built-in function. [ { \"namespace\" : \"contoso\" , \"members\" : { \"uniqueName\" : { \"parameters\" : [ { \"name\" : \"namePrefix\" , \"type\" : \"string\" } ], \"output\" : { \"type\" : \"string\" , \"value\" : \"[concat(toLower(parameters('namePrefix')), uniqueString(resourceGroup().id))]\" }}}} ] Outputs \"outputs\" : { \"hostname\" : { \"type\" : \"string\" , \"value\" : \"[reference(variables('publicIPAddressName')).dnsSettings.fqdn]\" } } Standard storage account { \"$schema\" : \"http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\" , \"contentVersion\" : \"\" , \"parameters\" : { }, \"variables\" : { }, \"functions\" : [ ], \"resources\" : [ ], \"outputs\" : { \"storageAccountName\" : { \"type\" : \"string\" , \"value\" : \"[variables('storageAccountName')]\" } } } Tasks Deploy a VM quickstart template Create a resource group Create a resource group RESOURCEGROUP = learn-quickstart-vm-rg LOCATION = eastus az group create --name $RESOURCEGROUP --location $LOCATION Create template parameters Validate template USERNAME = azureuser PASSWORD = $( openssl rand -base64 32 ) DNS_LABEL_PREFIX = mydeployment- $RANDOM az deployment group validate --resource-group $RESOURCEGROUP --template-uri \"https://raw.githubusercontent.com/Azure/azure-quickstart-templates/master/101-vm-simple-windows/azuredeploy.json\" --parameters adminUsername = $USERNAME --parameters adminPassword = $PASSWORD --parameters dnsLabelPrefix = $DNS_LABEL_PREFIX Deploy template USERNAME = azureuser PASSWORD = $( openssl rand -base64 32 ) DNS_LABEL_PREFIX = mydeployment- $RANDOM az deployment group create --name MyDeployment --resource-group $RESOURCEGROUP --template-uri \"https://raw.githubusercontent.com/Azure/azure-quickstart-templates/master/101-vm-simple-windows/azuredeploy.json\" --parameters adminUsername = $USERNAME --parameters adminPassword = $PASSWORD --parameters dnsLabelPrefix = $DNS_LABEL_PREFIX Verify deployment az deployment group show --name MyDeployment --resource-group $RESOURCEGROUP ARM ? { \"name\" : \"[concat(variables('vmName'), '/', 'ConfigureIIS')]\" , \"type\" : \"Microsoft.Compute/virtualMachines/extensions\" , \"apiVersion\" : \"2018-06-01\" , \"location\" : \"[parameters('location')]\" , \"properties\" : { \"publisher\" : \"Microsoft.Compute\" , \"type\" : \"CustomScriptExtension\" , \"typeHandlerVersion\" : \"1.9\" , \"autoUpgradeMinorVersion\" : true , \"settings\" : { \"fileUris\" : [ \"https://raw.githubusercontent.com/MicrosoftDocs/mslearn-welcome-to-azure/master/configure-iis.ps1\" ] }, \"protectedSettings\" : { \"commandToExecute\" : \"powershell -ExecutionPolicy Unrestricted -File configure-iis.ps1\" } }, \"dependsOn\" : [ \"[resourceId('Microsoft.Compute/virtualMachines/', variables('vmName'))]\" ] }","title":"ARM"},{"location":"Cloud/ARM/#structure","text":"A template must have at least the following sections. $schema contentVersion resources A template may have the following optional sections parameters variables functions outputs { \"$schema\" : \"http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\" , \"contentVersion\" : \"\" , \"parameters\" : { }, \"variables\" : { }, \"functions\" : [ ], \"resources\" : [ ], \"outputs\" : { } }","title":"Structure"},{"location":"Cloud/ARM/#resources","text":"Create a public IP address. { \"type\" : \"Microsoft.Network/publicIPAddresses\" , \"apiVersion\" : \"2018-08-01\" , \"name\" : \"[variables('publicIPAddressName')]\" , \"location\" : \"[parameters('location')]\" , \"properties\" : { \"publicIPAllocationMethod\" : \"Dynamic\" , \"dnsSettings\" : { \"domainNameLabel\" : \"[parameters('dnsLabelPrefix')]\" } } } Create a storage account { \"type\" : \"Microsoft.Storage/storageAccounts\" , \"apiVersion\" : \"2019-06-01\" , \"name\" : \"[variables('storageAccountName')]\" , \"location\" : \"[parameters('location')]\" , \"sku\" : { \"name\" : \"[parameters('storageAccountType')]\" }, \"kind\" : \"StorageV2\" , \"properties\" : {} } Create a Azure Data Explorer cluster","title":"Resources"},{"location":"Cloud/ARM/#parameters","text":"\"adminUsername\" : { \"type\" : \"string\" , \"metadata\" : { \"description\" : \"Username for the Virtual Machine.\" }}, \"adminPassword\" : { \"type\" : \"securestring\" , \"metadata\" : { \"description\" : \"Password for the Virtual Machine.\" }} Simple storage account { \"storageAccountType\" : { \"type\" : \"string\" , \"defaultValue\" : \"Standard_LRS\" , \"allowedValues\" : [ \"Standard_LRS\" , \"Standard_GRS\" , \"Standard_ZRS\" , \"Premium_LRS\" ], \"metadata\" : { \"description\" : \"Storage Account type\" }}, \"location\" : { \"type\" : \"string\" , \"defaultValue\" : \"[resourceGroup().location]\" , \"metadata\" : { \"description\" : \"Location for all resources.\" }}}","title":"Parameters"},{"location":"Cloud/ARM/#variables","text":"{ \"nicName\" : \"myVMNic\" , \"addressPrefix\" : \"10.0.0.0/16\" , \"subnetName\" : \"Subnet\" , \"subnetPrefix\" : \"10.0.0.0/24\" , \"publicIPAddressName\" : \"myPublicIP\" , \"virtualNetworkName\" : \"MyVNET\" } Simple storage account { \"storageAccountName\" : \"[concat('store', uniquestring(resourceGroup().id))]\" },","title":"Variables"},{"location":"Cloud/ARM/#functions","text":"Create a globally unique name, useful for some resources that require it. concat is a built-in function. [ { \"namespace\" : \"contoso\" , \"members\" : { \"uniqueName\" : { \"parameters\" : [ { \"name\" : \"namePrefix\" , \"type\" : \"string\" } ], \"output\" : { \"type\" : \"string\" , \"value\" : \"[concat(toLower(parameters('namePrefix')), uniqueString(resourceGroup().id))]\" }}}} ]","title":"Functions"},{"location":"Cloud/ARM/#outputs","text":"\"outputs\" : { \"hostname\" : { \"type\" : \"string\" , \"value\" : \"[reference(variables('publicIPAddressName')).dnsSettings.fqdn]\" } } Standard storage account { \"$schema\" : \"http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\" , \"contentVersion\" : \"\" , \"parameters\" : { }, \"variables\" : { }, \"functions\" : [ ], \"resources\" : [ ], \"outputs\" : { \"storageAccountName\" : { \"type\" : \"string\" , \"value\" : \"[variables('storageAccountName')]\" } } }","title":"Outputs"},{"location":"Cloud/ARM/#tasks","text":"Deploy a VM quickstart template","title":"Tasks"},{"location":"Cloud/ARM/#create-a-resource-group","text":"Create a resource group RESOURCEGROUP = learn-quickstart-vm-rg LOCATION = eastus az group create --name $RESOURCEGROUP --location $LOCATION Create template parameters","title":"Create a resource group"},{"location":"Cloud/ARM/#validate-template","text":"USERNAME = azureuser PASSWORD = $( openssl rand -base64 32 ) DNS_LABEL_PREFIX = mydeployment- $RANDOM az deployment group validate --resource-group $RESOURCEGROUP --template-uri \"https://raw.githubusercontent.com/Azure/azure-quickstart-templates/master/101-vm-simple-windows/azuredeploy.json\" --parameters adminUsername = $USERNAME --parameters adminPassword = $PASSWORD --parameters dnsLabelPrefix = $DNS_LABEL_PREFIX","title":"Validate template"},{"location":"Cloud/ARM/#deploy-template","text":"USERNAME = azureuser PASSWORD = $( openssl rand -base64 32 ) DNS_LABEL_PREFIX = mydeployment- $RANDOM az deployment group create --name MyDeployment --resource-group $RESOURCEGROUP --template-uri \"https://raw.githubusercontent.com/Azure/azure-quickstart-templates/master/101-vm-simple-windows/azuredeploy.json\" --parameters adminUsername = $USERNAME --parameters adminPassword = $PASSWORD --parameters dnsLabelPrefix = $DNS_LABEL_PREFIX Verify deployment az deployment group show --name MyDeployment --resource-group $RESOURCEGROUP","title":"Deploy template"},{"location":"Cloud/ARM/#arm","text":"? { \"name\" : \"[concat(variables('vmName'), '/', 'ConfigureIIS')]\" , \"type\" : \"Microsoft.Compute/virtualMachines/extensions\" , \"apiVersion\" : \"2018-06-01\" , \"location\" : \"[parameters('location')]\" , \"properties\" : { \"publisher\" : \"Microsoft.Compute\" , \"type\" : \"CustomScriptExtension\" , \"typeHandlerVersion\" : \"1.9\" , \"autoUpgradeMinorVersion\" : true , \"settings\" : { \"fileUris\" : [ \"https://raw.githubusercontent.com/MicrosoftDocs/mslearn-welcome-to-azure/master/configure-iis.ps1\" ] }, \"protectedSettings\" : { \"commandToExecute\" : \"powershell -ExecutionPolicy Unrestricted -File configure-iis.ps1\" } }, \"dependsOn\" : [ \"[resourceId('Microsoft.Compute/virtualMachines/', variables('vmName'))]\" ] }","title":"ARM"},{"location":"Cloud/Azure-AD/","text":"Azure AD Azure AD has its own set of roles which apply to Azure AD resources and which are distinct from those of Azure RBAC . The terms tenant and directory are deeply connected and often confused with one another. A tenant refers to an instance of Azure AD that is tied to a subscription, and refers to the organization. Each tenant is associated with a dedicated and trusted directory that includes the tenant's users, groups, and apps. Roles: Global Administrator can manage access to administrative features in AAD and can grant administrator roles to other users. An AAD Global Administrator can also temporarily elevate their own access to the Azure RBAC role of User Access Administrator in order to manage all Azure subscriptions and management groups . Whoever signs up for the directory is automatically assigned this role. Device administrator In order to make sure AD users can change their password either locally or in the cloud, Azure AD has to be upgraded to Premium . Enterprise State Roaming allows users to securely synchronize user settings and application settings to Azure. Self-Service Password Reset (SSPR) is supported for all users. SSPR registration can be configured by group or for all domain users, but not individual users. B2B Business-to-business (B2B) collaboration allows you to invite guest users into your own ( What is guest user access in Azure Active Directory B2B? ) Joining a device When you join a device to an Azure AD tenant's domain, Azure AD creates local administrator accounts on the device for: - The user joining the device - The Azure AD global administrator - The Azure AD device administrator SSPR Tasks Sources: - Portal - PowerShell Add users in bulk Import members by first navigating to the group to which they will be added, then importing from a CSV. A template is available. Azure Portal Sources: Bulk create users in Azure Active Directory Bulk add group members in Azure Active Directory Licenses Note: The user to be licensed must first have a Usage location set. Azure Portal Use the ISO 3166-1 A2 two-letter country or region code to set this value in PowerShell Set-AzureADUser -UsageLocation 'US' Azure Portal Sources Assign or remove licenses in the Azure Active Directory Portal Configure Microsoft 365 user account properties with PowerShell Enable MFA Create a Conditional Access policy to enforce MFA with specified users. Azure Portal Enable [SSPR][SSPR] Azure Portal Add custom domain name AZ-103: 410 Sources Tutorial: Secure user sign-in events with Azure Multi-Factor Authentication How to manage the local administrators group on Azure AD joined devices Password policies and account restrictions in Azure AD","title":"Azure AD"},{"location":"Cloud/Azure-AD/#azure-ad","text":"Azure AD has its own set of roles which apply to Azure AD resources and which are distinct from those of Azure RBAC . The terms tenant and directory are deeply connected and often confused with one another. A tenant refers to an instance of Azure AD that is tied to a subscription, and refers to the organization. Each tenant is associated with a dedicated and trusted directory that includes the tenant's users, groups, and apps. Roles: Global Administrator can manage access to administrative features in AAD and can grant administrator roles to other users. An AAD Global Administrator can also temporarily elevate their own access to the Azure RBAC role of User Access Administrator in order to manage all Azure subscriptions and management groups . Whoever signs up for the directory is automatically assigned this role. Device administrator In order to make sure AD users can change their password either locally or in the cloud, Azure AD has to be upgraded to Premium . Enterprise State Roaming allows users to securely synchronize user settings and application settings to Azure. Self-Service Password Reset (SSPR) is supported for all users. SSPR registration can be configured by group or for all domain users, but not individual users.","title":"Azure AD"},{"location":"Cloud/Azure-AD/#b2b","text":"Business-to-business (B2B) collaboration allows you to invite guest users into your own ( What is guest user access in Azure Active Directory B2B? )","title":"B2B"},{"location":"Cloud/Azure-AD/#joining-a-device","text":"When you join a device to an Azure AD tenant's domain, Azure AD creates local administrator accounts on the device for: - The user joining the device - The Azure AD global administrator - The Azure AD device administrator","title":"Joining a device"},{"location":"Cloud/Azure-AD/#sspr","text":"","title":"SSPR"},{"location":"Cloud/Azure-AD/#tasks","text":"Sources: - Portal - PowerShell","title":"Tasks"},{"location":"Cloud/Azure-AD/#add-users-in-bulk","text":"Import members by first navigating to the group to which they will be added, then importing from a CSV. A template is available. Azure Portal Sources: Bulk create users in Azure Active Directory Bulk add group members in Azure Active Directory","title":"Add users in bulk"},{"location":"Cloud/Azure-AD/#licenses","text":"Note: The user to be licensed must first have a Usage location set. Azure Portal Use the ISO 3166-1 A2 two-letter country or region code to set this value in PowerShell Set-AzureADUser -UsageLocation 'US' Azure Portal Sources Assign or remove licenses in the Azure Active Directory Portal Configure Microsoft 365 user account properties with PowerShell","title":"Licenses"},{"location":"Cloud/Azure-AD/#enable-mfa","text":"Create a Conditional Access policy to enforce MFA with specified users. Azure Portal Enable [SSPR][SSPR] Azure Portal","title":"Enable MFA"},{"location":"Cloud/Azure-AD/#add-custom-domain-name","text":"AZ-103: 410","title":"Add custom domain name"},{"location":"Cloud/Azure-AD/#sources","text":"Tutorial: Secure user sign-in events with Azure Multi-Factor Authentication How to manage the local administrators group on Azure AD joined devices Password policies and account restrictions in Azure AD","title":"Sources"},{"location":"Cloud/Azure-App-Service/","text":"Pricing tiers: - Free/Shared : uses a shared infrastructure with minimal storage. No options for deploying different staged versions, routing of traffic, or backups - Basic : Dedicated compute for app, including avaiilability of SSL and manual scaling of app instance number. - Standard : Daily backups, automatic scaling of app instances, deployment slots, and user routing with Traffic Manager - Premium : more frequent backups, increased storage, and greater number of deployment slots and instance scaling options. Tasks List available runtimes az webapp list-runtimes --linux Deploy image from ACR Sources az webapp create --name $n --resource-group $g --plan $p --deployment-container-image-name $registry .azurecr.io/ $image :latest az webapp config appsettings set -n $n -g $g --settings \"WEBSITES_PORT=8000\" # Service principal ID $p = az webapp identity assign -n $n -g $g --query principalId -o tsv $s = az account show --query id --output tsv # Grant web app permission to access the container registry az role assignment create --assignee $p --scope /subscriptions/ $s /resourceGroups/ $g /provides/Microsoft.ContainerRegistry/registries/ $registry # Deploy image (note that `$registry` is specified twice in this command) az webapp config container set -n $n -g $g --docker-custom-image-name $registry .azurecr.io/ $image :latest --docker-registry-server-url https:// $registry .azurecr.io # Restart web app after making changes az webapp restart -n $n -g $g Deploy from GitHub #!/bin/bash # Replace the following URL with a public GitHub repo URL gitrepo = https://github.com/Azure-Samples/php-docs-hello-world webappname = mywebapp $RANDOM # Create a resource group. az group create --location westeurope --name myResourceGroup # Create an App Service plan in `FREE` tier. az appservice plan create --name $webappname --resource-group myResourceGroup --sku FREE # Create a web app. az webapp create --name $webappname --resource-group myResourceGroup --plan $webappname # Deploy code from a public GitHub repository. az webapp deployment source config --name $webappname --resource-group myResourceGroup \\ --repo-url $gitrepo --branch master --manual-integration # Copy the result of the following command into a browser to see the web app. echo http:// $webappname .azurewebsites.net $gitrepo = \"<replace-with-URL-of-your-own-GitHub-repo>\" $gittoken = \"<replace-with-a-GitHub-access-token>\" $webappname = \"mywebapp $( Get-Random ) \" $location = \"West Europe\" # Create a resource group. New-AzResourceGroup -Name myResourceGroup -Location $location # Create an App Service plan in Free tier. New-AzAppServicePlan -Name $webappname -Location $location ` -ResourceGroupName myResourceGroup -Tier Free # Create a web app. New-AzWebApp -Name $webappname -Location $location -AppServicePlan $webappname ` -ResourceGroupName myResourceGroup # SET GitHub $PropertiesObject = @{ token = $gittoken ; } Set-AzResource -PropertyObject $PropertiesObject ` -ResourceId / providers / Microsoft . Web / sourcecontrols / GitHub -ApiVersion 2015 - 08 - 01 -Force # Configure GitHub deployment from your GitHub repo and deploy once. $PropertiesObject = @{ repoUrl = \"$gitrepo\" ; branch = \"master\" ; } Set-AzResource -PropertyObject $PropertiesObject -ResourceGroupName myResourceGroup ` -ResourceType Microsoft . Web / sites / sourcecontrols -ResourceName $webappname / web ` -ApiVersion 2015 - 08 - 01 -Force Create Front Door Add the front-door extension if using Azure CLI az extension add --name front-door Create web apps in two different regions. $webapp1 = New-AzWebApp -Name \"WebAppContoso- $( Get-Random ) \" -Location centralus -ResourceGroupName myResourceGroupFD -AppServicePlan myAppServicePlanCentralUS $webapp2 = New-AzWebApp -Name \"WebAppContoso- $( Get-Random ) \" -Location southcentralus -ResourceGroupName myResourceGroupFD -AppServicePlan myAppServicePlanSouthCentralUS az appservice plan create --name myAppServicePlanCentralUS --resource-group myRGFDCentral az webapp create --name WebAppContoso1 --resource-group myRGFDCentral --plan myAppServicePlanCentralUS az appservice plan create --name myAppServicePlanSouthCentralUS -resource-groupg myRGFDSouthCentral az webapp create --name WebAppContoso2 --resource-group myRGFDSouthCentral --plan myAppServicePlanSouthCentralUS Create a frontend object # Create a unique name $fdname = \"contoso-frontend- $( Get-Random ) \" #Create the frontend object $FrontendEndObject = New-AzFrontDoorFrontendEndpointObject ` -Name \"frontendEndpoint1\" ` -HostName $fdname \".azurefd.net\" Create the backend pool # Create backend objects that points to the hostname of the web apps $backendObject1 = New-AzFrontDoorBackendObject ` -Address $webapp1 . DefaultHostName $backendObject2 = New-AzFrontDoorBackendObject ` -Address $webapp2 . DefaultHostName # Create a health probe object $HealthProbeObject = New-AzFrontDoorHealthProbeSettingObject ` -Name \"HealthProbeSetting\" # Create the load balancing setting object $LoadBalancingSettingObject = New-AzFrontDoorLoadBalancingSettingObject ` -Name \"Loadbalancingsetting\" ` -SampleSize \"4\" ` -SuccessfulSamplesRequired \"2\" ` -AdditionalLatencyInMilliseconds \"0\" # Create a backend pool using the backend objects, health probe, and load balancing settings $BackendPoolObject = New-AzFrontDoorBackendPoolObject ` -Name \"myBackendPool\" ` -FrontDoorName $fdname ` -ResourceGroupName myResourceGroupFD ` -Backend $backendObject1 , $backendObject2 ` -HealthProbeSettingsName \"HealthProbeSetting\" ` -LoadBalancingSettingsName \"Loadbalancingsetting\" New-AzFrontDoor ` -Name $fdname ` -ResourceGroupName myResourceGroupFD ` -RoutingRule $RoutingRuleObject ` -BackendPool $BackendPoolObject ` -FrontendEndpoint $FrontendEndObject ` -LoadBalancingSetting $LoadBalancingSettingObject ` -HealthProbeSetting $HealthProbeObject az network front-door create \\ --resource-group myRGFDCentral \\ --name contoso-frontend \\ --accepted-protocols http https \\ --backend-address webappcontoso1.azurewebsites.net webappcontoso2.azurewebsites.net","title":"Azure App Service"},{"location":"Cloud/Azure-App-Service/#tasks","text":"","title":"Tasks"},{"location":"Cloud/Azure-App-Service/#list-available-runtimes","text":"az webapp list-runtimes --linux","title":"List available runtimes"},{"location":"Cloud/Azure-App-Service/#deploy-image-from-acr","text":"Sources az webapp create --name $n --resource-group $g --plan $p --deployment-container-image-name $registry .azurecr.io/ $image :latest az webapp config appsettings set -n $n -g $g --settings \"WEBSITES_PORT=8000\" # Service principal ID $p = az webapp identity assign -n $n -g $g --query principalId -o tsv $s = az account show --query id --output tsv # Grant web app permission to access the container registry az role assignment create --assignee $p --scope /subscriptions/ $s /resourceGroups/ $g /provides/Microsoft.ContainerRegistry/registries/ $registry # Deploy image (note that `$registry` is specified twice in this command) az webapp config container set -n $n -g $g --docker-custom-image-name $registry .azurecr.io/ $image :latest --docker-registry-server-url https:// $registry .azurecr.io # Restart web app after making changes az webapp restart -n $n -g $g","title":"Deploy image from ACR"},{"location":"Cloud/Azure-App-Service/#deploy-from-github","text":"#!/bin/bash # Replace the following URL with a public GitHub repo URL gitrepo = https://github.com/Azure-Samples/php-docs-hello-world webappname = mywebapp $RANDOM # Create a resource group. az group create --location westeurope --name myResourceGroup # Create an App Service plan in `FREE` tier. az appservice plan create --name $webappname --resource-group myResourceGroup --sku FREE # Create a web app. az webapp create --name $webappname --resource-group myResourceGroup --plan $webappname # Deploy code from a public GitHub repository. az webapp deployment source config --name $webappname --resource-group myResourceGroup \\ --repo-url $gitrepo --branch master --manual-integration # Copy the result of the following command into a browser to see the web app. echo http:// $webappname .azurewebsites.net $gitrepo = \"<replace-with-URL-of-your-own-GitHub-repo>\" $gittoken = \"<replace-with-a-GitHub-access-token>\" $webappname = \"mywebapp $( Get-Random ) \" $location = \"West Europe\" # Create a resource group. New-AzResourceGroup -Name myResourceGroup -Location $location # Create an App Service plan in Free tier. New-AzAppServicePlan -Name $webappname -Location $location ` -ResourceGroupName myResourceGroup -Tier Free # Create a web app. New-AzWebApp -Name $webappname -Location $location -AppServicePlan $webappname ` -ResourceGroupName myResourceGroup # SET GitHub $PropertiesObject = @{ token = $gittoken ; } Set-AzResource -PropertyObject $PropertiesObject ` -ResourceId / providers / Microsoft . Web / sourcecontrols / GitHub -ApiVersion 2015 - 08 - 01 -Force # Configure GitHub deployment from your GitHub repo and deploy once. $PropertiesObject = @{ repoUrl = \"$gitrepo\" ; branch = \"master\" ; } Set-AzResource -PropertyObject $PropertiesObject -ResourceGroupName myResourceGroup ` -ResourceType Microsoft . Web / sites / sourcecontrols -ResourceName $webappname / web ` -ApiVersion 2015 - 08 - 01 -Force","title":"Deploy from GitHub"},{"location":"Cloud/Azure-App-Service/#create-front-door","text":"Add the front-door extension if using Azure CLI az extension add --name front-door Create web apps in two different regions. $webapp1 = New-AzWebApp -Name \"WebAppContoso- $( Get-Random ) \" -Location centralus -ResourceGroupName myResourceGroupFD -AppServicePlan myAppServicePlanCentralUS $webapp2 = New-AzWebApp -Name \"WebAppContoso- $( Get-Random ) \" -Location southcentralus -ResourceGroupName myResourceGroupFD -AppServicePlan myAppServicePlanSouthCentralUS az appservice plan create --name myAppServicePlanCentralUS --resource-group myRGFDCentral az webapp create --name WebAppContoso1 --resource-group myRGFDCentral --plan myAppServicePlanCentralUS az appservice plan create --name myAppServicePlanSouthCentralUS -resource-groupg myRGFDSouthCentral az webapp create --name WebAppContoso2 --resource-group myRGFDSouthCentral --plan myAppServicePlanSouthCentralUS Create a frontend object # Create a unique name $fdname = \"contoso-frontend- $( Get-Random ) \" #Create the frontend object $FrontendEndObject = New-AzFrontDoorFrontendEndpointObject ` -Name \"frontendEndpoint1\" ` -HostName $fdname \".azurefd.net\" Create the backend pool # Create backend objects that points to the hostname of the web apps $backendObject1 = New-AzFrontDoorBackendObject ` -Address $webapp1 . DefaultHostName $backendObject2 = New-AzFrontDoorBackendObject ` -Address $webapp2 . DefaultHostName # Create a health probe object $HealthProbeObject = New-AzFrontDoorHealthProbeSettingObject ` -Name \"HealthProbeSetting\" # Create the load balancing setting object $LoadBalancingSettingObject = New-AzFrontDoorLoadBalancingSettingObject ` -Name \"Loadbalancingsetting\" ` -SampleSize \"4\" ` -SuccessfulSamplesRequired \"2\" ` -AdditionalLatencyInMilliseconds \"0\" # Create a backend pool using the backend objects, health probe, and load balancing settings $BackendPoolObject = New-AzFrontDoorBackendPoolObject ` -Name \"myBackendPool\" ` -FrontDoorName $fdname ` -ResourceGroupName myResourceGroupFD ` -Backend $backendObject1 , $backendObject2 ` -HealthProbeSettingsName \"HealthProbeSetting\" ` -LoadBalancingSettingsName \"Loadbalancingsetting\" New-AzFrontDoor ` -Name $fdname ` -ResourceGroupName myResourceGroupFD ` -RoutingRule $RoutingRuleObject ` -BackendPool $BackendPoolObject ` -FrontendEndpoint $FrontendEndObject ` -LoadBalancingSetting $LoadBalancingSettingObject ` -HealthProbeSetting $HealthProbeObject az network front-door create \\ --resource-group myRGFDCentral \\ --name contoso-frontend \\ --accepted-protocols http https \\ --backend-address webappcontoso1.azurewebsites.net webappcontoso2.azurewebsites.net","title":"Create Front Door"},{"location":"Cloud/Azure-Backup/","text":"Azure Backup can backup on-prem servers, cloud-based VMs, and virtualized workloads like SQL Server and Sharepoint. However Azure SQL databases are already backed up by an automatic service by default. AZ-103 p. 159 On-prem machines can be backed up using several agents AZ-103 p. 162 MARS Agent System Center Data Protection Manager (DPM) or Microsoft Azure Backup Server (MABS) can be used as backup servers. The backup server can then be backed up to a Recovery Services vault Azure VMs can be backed up Directly using an extension on the Azure VM Agent , which comes preinstalled on Marketplace images Specific files and folders on a VM can be backed up by running the MARS agent To the MABS running in Azure, which can then be backed up to a Recovery Services vault Storage accounts can be backed up, but not blob storage. Blob storage is already replicated locally, which provides fault-tolerance. Instead, you can use snapshots. When installed, the Get-AzVM command exposes a ProvisionVMAgent property with a boolean value under OSProfile.WindowsConfiguration . Containers There appear to be resources that house items to be protected that can be enumerated . Reports Log Analytics workspaces must be located in the same region as the Recovery Services vault in order to store Backup reports. Pre-Checks Azure Backup pre-checks complete with various statuses that indicate potential problems Passed : VM configuration is conducive for successful backups Warning : Issues that might lead to backup failures Critical : Issues that will lead to backup failures Tasks Create Recovery Services Vault Azure Portal Azure PowerShell New-AzRecoveryServicesVault -Name $n -ResourceGroupName $rgName -Location $l Azure CLI az backup vault create --name $n --resource-group $rgName --Location $l Enable MFA This requires MFA to be enabled. Azure Portal Enable multi-factor authentication for the Recovery services vault by going to the vault in the Portal, then Properties > Security settings: Update > Choose Yes in the dropdown. An option to generate a security PIN will appear in this same blade. Recover files Azure Portal Download the executable (for Windows VMs) or PowerShell script (for Linux VMs). A Python script is generated when downloading to a Linux machine. Configure Backup reports Sources - Configure Azure Backup reports A Log Analytics workspace must exist. Turn on diagnostics in the Recovery Services vault Select Archive to a storage account ( NOT Send to Log Analytics), providing a storage account to store information needed for report. Select AzureBackupReport under log section, which will collect all needed data models and information for the backup report. Connect to Azure Backup in PowerBI using a service content pack. Define new backup protection policy Azure PowerShell $SchPol = Get-AzRecoveryServicesBackupSchedulePolicyObject -WorkloadType \"AzureVM\" $SchPol . ScheduleRunTimes . Clear () $Dt = Get-Date $SchPol . ScheduleRunTimes . Add ( $Dt . ToUniversalTime ()) $RetPol = Get-AzRecoveryServicesBackupRetentionPolicyObject -WorkloadType \"AzureVM\" $RetPol . DailySchedule . DurationCountInDays = 365 New-AzRecoveryServicesBackupProtectionPolicy -Name \"NewPolicy\" -WorkloadType AzureVM -RetentionPolicy $RetPol -SchedulePolicy $SchPol Configure VM backup Azure PowerShell $policy = Get-AzRecoveryServicesBackupProtectionPolicy -Name \"DefaultPolicy\" Enable-AzRecoveryServicesBackupProtection -ResourceGroupName $g -Name $n -Policy $policy Azure CLI # GRS by default az backup protection enable-for-vm -g $g -v $v --vm vm --policy-name DefaultPolicy # LRS az backup vault backup-properties set -n $v -g $g --backup-storage-redundancy \"LocallyRedundant\" Initiate VM backup Azure PowerShell $backupcontainer = Get-AzRecoveryServicesBackupContainer -ContainerType \"AzureVM\" -FriendlyName \"myVM\" $item = Get-AzRecoveryServicesBackupItem -Container $backupcontainer -WorkloadType \"AzureVM\" Backup-AzRecoveryServicesBackupItem -Item $item --container-name / -c appears to accept the name of the VM itself. Azure CLI az backup protection backup-now -g myResourceGroup -n myRecoveryServicesVault --container-name myVM --item-name myVM --retain-until 18 -10-2017 --backup-management-type AzureIaasVM List containers -BackupManagementType accepts the following values - AzureVM - MARS - AzureWorkload - AzureStorage -ContainerType accepts: - AzureVM - Windows - AzureSQL - AzureStorage - AzureVMAppContainer $v = Get-AzRecoveryServicesVault -ResourceGroupName $rg -Name vault Get-AzRecoveryServicesBackupContainer -ContainerType Windows -BackupManagementType MARS -VaultId $v . ID This returns a list of JSON objects. --backup-management-type accepts the following values: - AzureIaasVM - AzureStorage - AzureWorkload az backup container list -g $g -v $v --backup-management-type AzureIaasVM Preserve only the \"name\" attribute of the first item, which itself is a semicolon-delimited string of values. ( Start backup now ) az backup container list -g $g -v $v --backup-management-type AzureIaasVM --query [ 0 ] .name Sources AZ-103: 2.4 AZ-104: 5.2 Azure Backup architecture and components Azure Virtual Machine Agent overview Understanding and using the Azure Linux Agent Restore files from VM Back up a VM - Azure CLI , PowerShell Get-AzRecoveryServicesBackupContainer New-AzRecoveryServicesBackupProtectionPolicy az backup container list","title":"Azure Backup"},{"location":"Cloud/Azure-Backup/#containers","text":"There appear to be resources that house items to be protected that can be enumerated .","title":"Containers"},{"location":"Cloud/Azure-Backup/#reports","text":"Log Analytics workspaces must be located in the same region as the Recovery Services vault in order to store Backup reports.","title":"Reports"},{"location":"Cloud/Azure-Backup/#pre-checks","text":"Azure Backup pre-checks complete with various statuses that indicate potential problems Passed : VM configuration is conducive for successful backups Warning : Issues that might lead to backup failures Critical : Issues that will lead to backup failures","title":"Pre-Checks"},{"location":"Cloud/Azure-Backup/#tasks","text":"Create Recovery Services Vault Azure Portal Azure PowerShell New-AzRecoveryServicesVault -Name $n -ResourceGroupName $rgName -Location $l Azure CLI az backup vault create --name $n --resource-group $rgName --Location $l Enable MFA This requires MFA to be enabled. Azure Portal Enable multi-factor authentication for the Recovery services vault by going to the vault in the Portal, then Properties > Security settings: Update > Choose Yes in the dropdown. An option to generate a security PIN will appear in this same blade. Recover files Azure Portal Download the executable (for Windows VMs) or PowerShell script (for Linux VMs). A Python script is generated when downloading to a Linux machine.","title":"Tasks"},{"location":"Cloud/Azure-Backup/#configure-backup-reports","text":"Sources - Configure Azure Backup reports A Log Analytics workspace must exist. Turn on diagnostics in the Recovery Services vault Select Archive to a storage account ( NOT Send to Log Analytics), providing a storage account to store information needed for report. Select AzureBackupReport under log section, which will collect all needed data models and information for the backup report. Connect to Azure Backup in PowerBI using a service content pack. Define new backup protection policy Azure PowerShell $SchPol = Get-AzRecoveryServicesBackupSchedulePolicyObject -WorkloadType \"AzureVM\" $SchPol . ScheduleRunTimes . Clear () $Dt = Get-Date $SchPol . ScheduleRunTimes . Add ( $Dt . ToUniversalTime ()) $RetPol = Get-AzRecoveryServicesBackupRetentionPolicyObject -WorkloadType \"AzureVM\" $RetPol . DailySchedule . DurationCountInDays = 365 New-AzRecoveryServicesBackupProtectionPolicy -Name \"NewPolicy\" -WorkloadType AzureVM -RetentionPolicy $RetPol -SchedulePolicy $SchPol Configure VM backup Azure PowerShell $policy = Get-AzRecoveryServicesBackupProtectionPolicy -Name \"DefaultPolicy\" Enable-AzRecoveryServicesBackupProtection -ResourceGroupName $g -Name $n -Policy $policy Azure CLI # GRS by default az backup protection enable-for-vm -g $g -v $v --vm vm --policy-name DefaultPolicy # LRS az backup vault backup-properties set -n $v -g $g --backup-storage-redundancy \"LocallyRedundant\" Initiate VM backup Azure PowerShell $backupcontainer = Get-AzRecoveryServicesBackupContainer -ContainerType \"AzureVM\" -FriendlyName \"myVM\" $item = Get-AzRecoveryServicesBackupItem -Container $backupcontainer -WorkloadType \"AzureVM\" Backup-AzRecoveryServicesBackupItem -Item $item --container-name / -c appears to accept the name of the VM itself. Azure CLI az backup protection backup-now -g myResourceGroup -n myRecoveryServicesVault --container-name myVM --item-name myVM --retain-until 18 -10-2017 --backup-management-type AzureIaasVM","title":"Configure Backup reports"},{"location":"Cloud/Azure-Backup/#list-containers","text":"-BackupManagementType accepts the following values - AzureVM - MARS - AzureWorkload - AzureStorage -ContainerType accepts: - AzureVM - Windows - AzureSQL - AzureStorage - AzureVMAppContainer $v = Get-AzRecoveryServicesVault -ResourceGroupName $rg -Name vault Get-AzRecoveryServicesBackupContainer -ContainerType Windows -BackupManagementType MARS -VaultId $v . ID This returns a list of JSON objects. --backup-management-type accepts the following values: - AzureIaasVM - AzureStorage - AzureWorkload az backup container list -g $g -v $v --backup-management-type AzureIaasVM Preserve only the \"name\" attribute of the first item, which itself is a semicolon-delimited string of values. ( Start backup now ) az backup container list -g $g -v $v --backup-management-type AzureIaasVM --query [ 0 ] .name","title":"List containers"},{"location":"Cloud/Azure-Backup/#sources","text":"AZ-103: 2.4 AZ-104: 5.2 Azure Backup architecture and components Azure Virtual Machine Agent overview Understanding and using the Azure Linux Agent Restore files from VM Back up a VM - Azure CLI , PowerShell Get-AzRecoveryServicesBackupContainer New-AzRecoveryServicesBackupProtectionPolicy az backup container list","title":"Sources"},{"location":"Cloud/Azure-File-Service/","text":"Tasks Mount Azure File Share Windows Connect to and mount an Azure File Share (Windows File Explorer) Right-click on This PC Click Map Network Drive option Specify drive letter to be used Specify folder: \\\\<storageAccount>.files.core.windows.net\\<shareName> Click Finish In the dialog box that opens login with the username: AZURE\\<storageName> Password should be access key for the storage account net use x \\\\erstandard01.file.core.windows.net\\logs /u:AZURE\\erstandard01 <accessKey> Automatically reconnect after reboot in Windows cmdkey /add:storageAccountName.file.core.windows.net /user:AZURE\\storageAccountName /pass:storageAccountKey $storageKey = ( Get-AzStorageAccountKey -ResourceGroupName $g -Name $storageNAme ). Value [ 0 ] $acctKey = ConvertTo-SecureString -String $storageKey -AsPlainText -Force $credential = New-Object System . Management . Automation . PSCredential -ArgumentList \"Azure\\$storageName\" , $acctKey New-PSDrive -Name \"Z\" -PSProvider FileSystem -Root \"\\\\$storageName.file.core.windows.net\\$shareName\" -Credential $credential Linux Mounting to /logs sudo mount -t cifs // $storageAccount .file.core.windows.net/logs /logs -o \"vers=3.0,username= $storageAccount ,password= $storageAccountKey ,dir_mode=0777,file_mode=0777,sec=ntlmssp\" Sources: Deploy Azure File Sync AZ-103: p. 148","title":"Tasks"},{"location":"Cloud/Azure-File-Service/#tasks","text":"","title":"Tasks"},{"location":"Cloud/Azure-File-Service/#mount-azure-file-share","text":"","title":"Mount Azure File Share"},{"location":"Cloud/Azure-File-Service/#windows","text":"Connect to and mount an Azure File Share (Windows File Explorer) Right-click on This PC Click Map Network Drive option Specify drive letter to be used Specify folder: \\\\<storageAccount>.files.core.windows.net\\<shareName> Click Finish In the dialog box that opens login with the username: AZURE\\<storageName> Password should be access key for the storage account net use x \\\\erstandard01.file.core.windows.net\\logs /u:AZURE\\erstandard01 <accessKey> Automatically reconnect after reboot in Windows cmdkey /add:storageAccountName.file.core.windows.net /user:AZURE\\storageAccountName /pass:storageAccountKey $storageKey = ( Get-AzStorageAccountKey -ResourceGroupName $g -Name $storageNAme ). Value [ 0 ] $acctKey = ConvertTo-SecureString -String $storageKey -AsPlainText -Force $credential = New-Object System . Management . Automation . PSCredential -ArgumentList \"Azure\\$storageName\" , $acctKey New-PSDrive -Name \"Z\" -PSProvider FileSystem -Root \"\\\\$storageName.file.core.windows.net\\$shareName\" -Credential $credential","title":"Windows"},{"location":"Cloud/Azure-File-Service/#linux","text":"Mounting to /logs sudo mount -t cifs // $storageAccount .file.core.windows.net/logs /logs -o \"vers=3.0,username= $storageAccount ,password= $storageAccountKey ,dir_mode=0777,file_mode=0777,sec=ntlmssp\"","title":"Linux"},{"location":"Cloud/Azure-File-Service/#sources","text":"Deploy Azure File Sync AZ-103: p. 148","title":"Sources:"},{"location":"Cloud/Azure-IAM/","text":"Azure IAM Azure methods of administering access to resources can be divided into two groups Role-Based Access Controls (RBAC) are supported only by Azure Portal and the ARM APIs. RBAC is configured by selecting a role and associating it with a security principal , such as a user, group, or service identity. Child reosurces inherit the roles of their parents (\"role inheritance\"). Classic subscription administrators Classic subscription administrators Classic subscription administrators have full access to a subcription. They can access resources through Azure Portal, ARM APIs (PowerShell and CLI), and classic deployment model APIs. By default, the account that is used to sign up for a subscription is automatically set as both Account Administrator and Service Administrator . There can only be one Account Administrator per account and only 1 Service Administrator per subscription. Co-Administrators have the same access as Service Administrators, and there can be 200 of them per subscription, but cannot change the association of subscriptions to directories. Roles Components of a role assignment include: Security principal : objects associated with a role definition and a scope to apply RBAC to azure resources (i.e. a user, group, service principal, or managed identity which is an application registration that is managed automatically by Azure and an Azure service) User principal : identity associated with a user or group of users. Service principal : identity associated with an application. Role definition : list of permissions which define what actions can or cannot be performed against a resource. In addition to the 4 foundational built-in roles , there are many other built-in roles and custom roles can be defined using a JSON file. Scope Scopes There are four scopes at which RBAC can be applied: Management group Subscriptions Resource groups Resources Azure RBAC roles can be used to grant rights to 2 types of principals: User principal : identity associated with a user or group of users. Service principal : identity associated with an application. RBAC roles can also be applied to a subscription through Management Groups , which represent the recommended practice for ensuring consistent application of tenant-wide security. Management groups form a hierarchy where each child inherits policy from its single parent while having additional controls. There is a single Management Group at the root of the hierarchy, associated with the Azure AD tenant (which is associated, in turn, with a subscription) that cannot be moved or deleted. Role assignments Current assignments for classic admins can be seen in the Properties blade of a subscription in Azure Portal. Co-Administrator assignments can be added by opening the Access Control (IAM) blade of a subscription, then clicking the Add co-administrator button. RBAC roles are supported only by Azure Portal and the ARM APIs. Access policy is applied to a scope , which includes subscriptions, resource groups, or resources: a policy applied to a subscription is said to be at the \"subscription scope\". Policy can also be applied to Management Groups, which is an additional scope above subscription. In this way, several subscriptions can inherit a single policy through a Management Group. RBAC roles can also be applied to a subscription through Management Groups , which represent the recommended practice for ensuring consistent application of tenant-wide security. Management groups form a hierarchy where each child inherits policy from its single parent while having additional controls. There is a single Management Group at the root of the hierarchy, associated with the Azure AD tenant (which is associated, in turn, with a subscription) that cannot be moved or deleted. Role definitions Custom roles configure two types of privileges and are specified by two different properties of the definition JSON file: Management and Data. This provides safety from allowing unrestricted access to data. The values of these properties is an array of strings, each of which follows the format Company.ProviderName/ResourceType/Action where action can be of values read , write , action , delete , or * . Privilege Property that defines allowed permissions Property that defines denied permissions Management Actions NotActions Data DataActions NotDataActions Unrestricted \"Actions\" : [ \"*\" ] Network resources (read only) \"Actions\" : [ \"Microsoft.Network/*/read\" ] Example role definitions: Contributor { \"Name\" : \"Contributor\" , \"Id\" : \"b24988ac-6180-42a0-ab88-20f7382dd24c\" , \"IsCustom\" : false , \"Description\" : \"Lets you manage everything except access to resources.\" , \"Actions\" : [ \"*\" ], \"NotActions\" : [ \"Microsoft.Authorization/*/Delete\" , \"Microsoft.Authorization/*/Write\" , \"Microsoft.Authorization/elevateAccess/Action\" , \"Microsoft.Blueprint/blueprintAssignments/write\" , \"Microsoft.Blueprint/blueprintAssignments/delete\" ], \"DataActions\" : [], \"NotDataActions\" : [], \"AssignableScopes\" : [ \"/\" ] } Tasks Create assignment Assign the Owner role to a user at the subscription scope Navigate to resource group > Access Control (IAM) > Role Assignments tab > Add > Add Role Assignment Open Subscription > Access Control (IAM) > Add Role Assignment> select a Role > Select target principal Access control (AIM) pane > Add > Add role assignment Select a role in the Role dropdown and a user in the Select field. Then Save Azure Portal PowerShell # Resource group scope New-AzRoleAssignment -SignInName \"rbacuser@example.com\" -RoleDefinitionName \"Virtual Machine Contributor\" -ResourceGroupName ExamRefRG # Subscription scope New-AzRoleAssignment -SignInName \"rbacuser@example.com\" -RoleDefinitionName \"Owner\" -Scope \"/subscriptions/$subId\" Azure CLI # Resource group scope az role assignment create --assignee \"rbacuser@example.com\" --role \"Virtual Machine Contributor\" --resource-group ExamRefRG # Subscription scope az role assignment create --assignee \"rbacuser@example.com\" --role \"Owner\" --subscription $subId Delete assignment Navigate to resource group > Access Control (IAM) > Role Assignments tab > Select one or more security principals > Remove Remove RBAC assignments from a user Azure PowerShell Remove-AzRoleAssignment -SignInName \"cloudadmin@opsgility.onmicrosoft.com\" -RoleDefinitionName \"Virtual Machine Contributor\" -ResourceGroupName ExamRefRG Remove-AzRoleAssignment -SignInName $u -ResourceGroupName $rgName -RoleDefinitionName \"Virtual Machine Contributor\" Azure AD group $g = Get-AzADGroup -SearchString \"Cloud Admins\" Remove-AzRoleAssignment -ObjectId $g . Id -ResourceGroupName $rg -RoleDefinitionName \"Virtual Machine Contributor\" az role assignment delete --assignee $u --resource-group $rg --role \"Virtual Machine Contributor\" Azure AD group g = $( az ad group list --query \"[?displayName=='Cloud Admins'].objectId\" -o tsv ) az role assignment delete --role \"Virtual Machine Contributor\" -\u2013assignee-object-id $g --resource-group $rg Read assignment Azure PowerShell Get-AzRoleDefinition -Name \"Virtual Machine Contributor\" | ConvertTo-Json Azure CLI az role definition list -n \"Virtual Machine Contributor\" List custom roles available for assignment Azure PowerShell Get-AzRoleDefinition | Where-Object { $_ . IsCustom -eq $true } Azure CLI az role definition list --custom-role-only -o table View all role assignments in a subscription az role assignment list --all Create role definition Azure PowerShell New-AzRoleDefinition -InputFile \"C:\\ARM_templates\\customrole1.json\" Configure cost center quotas and tagging Grant an AD group RBAC rights Azure PowerShell $group = Get-AzADGroup -SearchString \"Cloud Admins\" New-AzRoleAssignment -ObjectId $group . Id -RoleDefinitionName \"Virtual Machine Contributor\" -ResourceGroupName ExamRefRG Remove RBAC assignments from a group Azure PowerShell $adGroup = Get-AzADGRoup -SearchString \"Cloud Admins\" Remove-AzRoleAssignment -ResourceGroupName $rgName -ObjectId $adGroup . Id -RoleDefinitionName \"Virtual Machine Contributor\" Elevate permissions For Azure AD Global Administrators who want to temporarily elevate permissions Sign into Azure portal as an Azure AD Global Administrator. ? Navigate to Azure Active Directory > Properties . At the bottom of the page, under \"Access management for Azure resources\" click Yes then Save . Sign out and sign in again. Assign roles Revoke elevated access by returning to Azure Active Directory > Properties and selecting No under \"Access management for Azure resources\". Sources Elevating global administrator access Understand Azure role definitions SSPR Administrator accounts are treated differently from other user accounts for SSPR and have a \"strong default two-gate password reset policy\", which requires two pieces of authentication data and foregoes the use of security questions.","title":"Azure IAM"},{"location":"Cloud/Azure-IAM/#azure-iam","text":"Azure methods of administering access to resources can be divided into two groups Role-Based Access Controls (RBAC) are supported only by Azure Portal and the ARM APIs. RBAC is configured by selecting a role and associating it with a security principal , such as a user, group, or service identity. Child reosurces inherit the roles of their parents (\"role inheritance\"). Classic subscription administrators","title":"Azure IAM"},{"location":"Cloud/Azure-IAM/#classic-subscription-administrators","text":"Classic subscription administrators have full access to a subcription. They can access resources through Azure Portal, ARM APIs (PowerShell and CLI), and classic deployment model APIs. By default, the account that is used to sign up for a subscription is automatically set as both Account Administrator and Service Administrator . There can only be one Account Administrator per account and only 1 Service Administrator per subscription. Co-Administrators have the same access as Service Administrators, and there can be 200 of them per subscription, but cannot change the association of subscriptions to directories.","title":"Classic subscription administrators"},{"location":"Cloud/Azure-IAM/#roles","text":"Components of a role assignment include: Security principal : objects associated with a role definition and a scope to apply RBAC to azure resources (i.e. a user, group, service principal, or managed identity which is an application registration that is managed automatically by Azure and an Azure service) User principal : identity associated with a user or group of users. Service principal : identity associated with an application. Role definition : list of permissions which define what actions can or cannot be performed against a resource. In addition to the 4 foundational built-in roles , there are many other built-in roles and custom roles can be defined using a JSON file. Scope","title":"Roles"},{"location":"Cloud/Azure-IAM/#scopes","text":"There are four scopes at which RBAC can be applied: Management group Subscriptions Resource groups Resources Azure RBAC roles can be used to grant rights to 2 types of principals: User principal : identity associated with a user or group of users. Service principal : identity associated with an application. RBAC roles can also be applied to a subscription through Management Groups , which represent the recommended practice for ensuring consistent application of tenant-wide security. Management groups form a hierarchy where each child inherits policy from its single parent while having additional controls. There is a single Management Group at the root of the hierarchy, associated with the Azure AD tenant (which is associated, in turn, with a subscription) that cannot be moved or deleted.","title":"Scopes"},{"location":"Cloud/Azure-IAM/#role-assignments","text":"Current assignments for classic admins can be seen in the Properties blade of a subscription in Azure Portal. Co-Administrator assignments can be added by opening the Access Control (IAM) blade of a subscription, then clicking the Add co-administrator button. RBAC roles are supported only by Azure Portal and the ARM APIs. Access policy is applied to a scope , which includes subscriptions, resource groups, or resources: a policy applied to a subscription is said to be at the \"subscription scope\". Policy can also be applied to Management Groups, which is an additional scope above subscription. In this way, several subscriptions can inherit a single policy through a Management Group. RBAC roles can also be applied to a subscription through Management Groups , which represent the recommended practice for ensuring consistent application of tenant-wide security. Management groups form a hierarchy where each child inherits policy from its single parent while having additional controls. There is a single Management Group at the root of the hierarchy, associated with the Azure AD tenant (which is associated, in turn, with a subscription) that cannot be moved or deleted.","title":"Role assignments"},{"location":"Cloud/Azure-IAM/#role-definitions","text":"Custom roles configure two types of privileges and are specified by two different properties of the definition JSON file: Management and Data. This provides safety from allowing unrestricted access to data. The values of these properties is an array of strings, each of which follows the format Company.ProviderName/ResourceType/Action where action can be of values read , write , action , delete , or * . Privilege Property that defines allowed permissions Property that defines denied permissions Management Actions NotActions Data DataActions NotDataActions Unrestricted \"Actions\" : [ \"*\" ] Network resources (read only) \"Actions\" : [ \"Microsoft.Network/*/read\" ] Example role definitions: Contributor { \"Name\" : \"Contributor\" , \"Id\" : \"b24988ac-6180-42a0-ab88-20f7382dd24c\" , \"IsCustom\" : false , \"Description\" : \"Lets you manage everything except access to resources.\" , \"Actions\" : [ \"*\" ], \"NotActions\" : [ \"Microsoft.Authorization/*/Delete\" , \"Microsoft.Authorization/*/Write\" , \"Microsoft.Authorization/elevateAccess/Action\" , \"Microsoft.Blueprint/blueprintAssignments/write\" , \"Microsoft.Blueprint/blueprintAssignments/delete\" ], \"DataActions\" : [], \"NotDataActions\" : [], \"AssignableScopes\" : [ \"/\" ] }","title":"Role definitions"},{"location":"Cloud/Azure-IAM/#tasks","text":"","title":"Tasks"},{"location":"Cloud/Azure-IAM/#create-assignment","text":"Assign the Owner role to a user at the subscription scope Navigate to resource group > Access Control (IAM) > Role Assignments tab > Add > Add Role Assignment Open Subscription > Access Control (IAM) > Add Role Assignment> select a Role > Select target principal Access control (AIM) pane > Add > Add role assignment Select a role in the Role dropdown and a user in the Select field. Then Save Azure Portal PowerShell # Resource group scope New-AzRoleAssignment -SignInName \"rbacuser@example.com\" -RoleDefinitionName \"Virtual Machine Contributor\" -ResourceGroupName ExamRefRG # Subscription scope New-AzRoleAssignment -SignInName \"rbacuser@example.com\" -RoleDefinitionName \"Owner\" -Scope \"/subscriptions/$subId\" Azure CLI # Resource group scope az role assignment create --assignee \"rbacuser@example.com\" --role \"Virtual Machine Contributor\" --resource-group ExamRefRG # Subscription scope az role assignment create --assignee \"rbacuser@example.com\" --role \"Owner\" --subscription $subId","title":"Create assignment"},{"location":"Cloud/Azure-IAM/#delete-assignment","text":"Navigate to resource group > Access Control (IAM) > Role Assignments tab > Select one or more security principals > Remove Remove RBAC assignments from a user Azure PowerShell Remove-AzRoleAssignment -SignInName \"cloudadmin@opsgility.onmicrosoft.com\" -RoleDefinitionName \"Virtual Machine Contributor\" -ResourceGroupName ExamRefRG Remove-AzRoleAssignment -SignInName $u -ResourceGroupName $rgName -RoleDefinitionName \"Virtual Machine Contributor\" Azure AD group $g = Get-AzADGroup -SearchString \"Cloud Admins\" Remove-AzRoleAssignment -ObjectId $g . Id -ResourceGroupName $rg -RoleDefinitionName \"Virtual Machine Contributor\" az role assignment delete --assignee $u --resource-group $rg --role \"Virtual Machine Contributor\" Azure AD group g = $( az ad group list --query \"[?displayName=='Cloud Admins'].objectId\" -o tsv ) az role assignment delete --role \"Virtual Machine Contributor\" -\u2013assignee-object-id $g --resource-group $rg Read assignment Azure PowerShell Get-AzRoleDefinition -Name \"Virtual Machine Contributor\" | ConvertTo-Json Azure CLI az role definition list -n \"Virtual Machine Contributor\" List custom roles available for assignment Azure PowerShell Get-AzRoleDefinition | Where-Object { $_ . IsCustom -eq $true } Azure CLI az role definition list --custom-role-only -o table View all role assignments in a subscription az role assignment list --all Create role definition Azure PowerShell New-AzRoleDefinition -InputFile \"C:\\ARM_templates\\customrole1.json\"","title":"Delete assignment"},{"location":"Cloud/Azure-IAM/#configure-cost-center-quotas-and-tagging","text":"Grant an AD group RBAC rights Azure PowerShell $group = Get-AzADGroup -SearchString \"Cloud Admins\" New-AzRoleAssignment -ObjectId $group . Id -RoleDefinitionName \"Virtual Machine Contributor\" -ResourceGroupName ExamRefRG Remove RBAC assignments from a group Azure PowerShell $adGroup = Get-AzADGRoup -SearchString \"Cloud Admins\" Remove-AzRoleAssignment -ResourceGroupName $rgName -ObjectId $adGroup . Id -RoleDefinitionName \"Virtual Machine Contributor\"","title":"Configure cost center quotas and tagging"},{"location":"Cloud/Azure-IAM/#elevate-permissions","text":"For Azure AD Global Administrators who want to temporarily elevate permissions Sign into Azure portal as an Azure AD Global Administrator. ? Navigate to Azure Active Directory > Properties . At the bottom of the page, under \"Access management for Azure resources\" click Yes then Save . Sign out and sign in again. Assign roles Revoke elevated access by returning to Azure Active Directory > Properties and selecting No under \"Access management for Azure resources\". Sources Elevating global administrator access Understand Azure role definitions","title":"Elevate permissions"},{"location":"Cloud/Azure-IAM/#sspr","text":"Administrator accounts are treated differently from other user accounts for SSPR and have a \"strong default two-gate password reset policy\", which requires two pieces of authentication data and foregoes the use of security questions.","title":"SSPR"},{"location":"Cloud/Azure-IaaS/","text":"High availability These high-availability options are mutually exclusive: Availability zones protect from datacenter-level failures by providing physical and logical separation between VM instances. Zones have independent power sources, network, and cooling, and there are at least 3 zones in every region. Availability sets offer redundancy for VMs in the same application tier, ensuring at least one VM is available in the case of a host update or problem. Each VM is assigned a fault domain (representing separate physical racks in the datacenter) and an update domain (representing groups of VMs and underlying physical hardware that can be rebooted at the same time for host updates). Availability sets have a maximum of 20 update domains and 3 fault domains. VM scale sets (VMSS) support the ability to dynamically add and remove instances. By default, a VMSS supports up to 100 instances or up to 1000 instances if deployed with the property singlePlacementGroup set to false (called a large-scale set ). A placement group is a concept similar to an availability set in that it assigns fault and upgrade domains. By default, a scale set consists of only a single placement group, but disabling this setting allows the scale set to be composed of multiple placement groups. If a custom image is used instead of one in the gallery, the limit is actually 300 instances. Load balancers Load balancers redistribute traffic from a frontend pool to a backend pool using rules. Health probes determine the health of the VMs in the backend pool: if they don't respond then new connections won't be sent. By default, Azure Load Balancer stores rules in a 5-tuple: Source IP address Source port Destination IP address Destination ports IP Protocol number Azure Load Balancers come in 2 pricing tiers: Basic which is free Standard which is charged based on the number of rules and the data that is processed. Both boot and guest OS diagnostics can be enabled on or after VM creation. Azure VMs have built-in extensions that enable configuration management. 2 most common extensions for configuration management: Windows PowerShell Desired State Configuration (DSC) allows you to define the state of a VM using PowerShell Desired State Configuration language A VM may have more than one Network Interface Card (NIC) , but they must belong to the same region as the VM itself. All NICs on a VM must also be attached to the same VNet. Tasks Find Marketplace image Produce the publisher (e.g. \"MicrosoftWindowsServer\") from Location Get-AzVMImagePublisher Produce the offer (e.g. \"WindowsServer\") from Location and PublisherName Get-AzVMImageOffer Produce the Sku from Location and PublisherName and Offer Get-AzVMImageSku Requires PublisherName, Offer, Location, Skus, and Version ( -Version * will produce a list of available version numbers) Get-AzVMImage Deploy from image Azure PowerShell Specify a managed image for use in a new virtual machine $image = Get-AzImage -ImageName $imageName -ResourceGroupName $g $vmConfig = Set-AzVMSourceImage -VM $vmConfig -Id $image . Id Specify a legacy unmanaged image for use in a new virtual machine $osDiskUri = \"https://examrefstorage.blob.core.windows.net/vhd/os-disk\" $imageUri = \"https://examrefstorage.blob.core.windows.net/images/legacy-image.vhd\" $vm = Set-AzVMOSDisk -VM $vm -Name $osDiskName -VhdUri $osDiskUri -CreateOption fromImage -SourceImageUri $imageUri -Windows Azure CLI Specify a managed image for use in a new virtual machine az vm create -g $g -n $vmName --image $imageName Specify a legacy unmanaged image for use in a new virtual machine az vm create -g $g -n $vmName --image $osDiskUri --generate-ssh-keys Access Enable-PSRemoting # If a network connection is Public, this command will not work Enable-PSRemoting -SkipNetworkProfileCheck -Force The remote computer must also have WinRM up and running. Azure can enable PowerShell on the target machine Invoke-AzVMRunCommand -AsJob -ResourceGroupName \"RG\" -VMName \"Socrates\" -CommandId EnableRemotePS Using Azure Cloud PowerShell Enable-AzVMPSRemoting -Name Socrates -ResourceGroupName \"RG\" Add the VM's public IP address $IP to the trusted hosts of the local workstation (must be run as administrator): Set-Item WSMan : \\ localhost \\ Client \\ TrustedHosts -Value $IP Traffic to ports 5985 and 5986 must be allowed through Windows firewall and, if the computer is an Azure VM, the Network Security Group. Enter-PSSession -ComputerName 123 . 47 . 78 . 90 -Credential $cred etsn 123 . 45 . 67 . 89 -Credential ( Get-Credential ) Invoke-AzVMRunCommand -ResourceGroupName RG -VMName VM -CommandId 'RunPowerShellScript' -ScriptPath C : \\ injectedscript . ps1 Open firewall WinRM ports 5985 and 5986 PowerShell New-NetFirewallRule -DisplayName \"WinRMHTTP\" -Direction Inbound -LocalPort 5985 -Protocol TCP -Action Allow New-NetFirewallRule -DisplayName \"WinRMHTTPS\" -Direction Inbound -LocalPort 5986 -Protocol TCP -Action Allow netsh netsh advfirewall firewall add rule name=WinRMHTTP dir=in action=allow protocol=TCP localport=5985 netsh advfirewall firewall add rule name=WinRMHTTPS dir=in action=allow protocol=TCP localport=5986 RDP Saving the .rdp file for later use $g = \"ExamRefRG\" $vmName = \"ExamRefVM\" $Path = \"C:\\path\\to\\ExamRefVM.rdp\" Get-AzRemoteDesktopFile -ResourceGroupName $g -Name $vmName -LocalPath $path Backup $t = Get-AzRecoveryServicesVault -Name t1 Set-AzRecoveryServicesBackupProperties -Vault $t -BackupStorageRedundancy GeoRedundant SSH $VirtualMachine = Get-AzVM -ResourceGroupName \"ResourceGroup11\" -Name \"VirtualMachine07\" Add-AzVMSshPublicKey -VM $VirtualMachine -KeyData \"MIIDszCCApugAwIBAgIJALBV9YJCF/tAMA0GCSq12Ib3DQEB21QUAMEUxCzAJBgNV\" -Path \"/home/admin/.ssh/authorized_keys\" Resize VM PowerShell $vm = Get-AzVM -ResourceGroupName $g -VMName $n $vm . HardwareProfile . VMSize = \"Standard_DS2_V2\" Update-AzVM -VM $vm -ResourceGroupName $g Azure CLI az vm list-vm-resize-options --resource-group $g --name $vmName --output table az vm resize --resource-group $g --name $vmName --size Standard_DS3_v2 Windows Server Core PowerShell Simply: New-AzVM -ResourceGroupName \"RG\" -Name \"VM\" -Location \"EastUS\" -Size \"Standard-B2s\" -Credential ( Get-Credential ) New-AzVM Greeks Socrates $vm The VM's NIC has to be linked to an NSG , a Subnet , and a Public IP Address . # Create a VNet with a subnet $subnet = New-AzVirtualNetworkSubnetConfig -Name \"subnet1\" -AddressPrefix \"10.0.0.0/24\" $vnet = New-AzVirtualNetwork -Name \"vnet\" -ResourceGroupName \"RG\" -Location \"East US\" -AddressPrefix \"10.0.0.0/16\" -Subnet $subnet # Create a Network Interface from the IP and VNet $ip = New-AzPublicIpAddress -Name \"wscore-ip\" -ResourceGroupName \"RG\" -Location \"East US\" -AllocationMethod Dynamic $nic = New-AzNetworkInterface -Name \"wscore-nic\" -ResourceGroupName \"RG\" -Location \"East US\" -SubnetId $vnet . Subnets [ 0 ]. Id -PublicIpAddressId $pip . Id -NetworkSecurityGroupId $nsg . Id $vm = New-AzVMConfig -VMName \"Socrates\" -VMSize \"Standard_B1s\" Set-AzVMOperatingSystem -VM $vm -Windows -ComputerName Socrates -Credential $aztestadmin Set-AzVMSourceImage -VM $vm -PublisherName \"MicrosoftWindowsServer\" -Offer \"WindowsServer\" -Skus \"2016-Datacenter-Server-Core\" -Version 2016 . 127 . 20190603 Set-AzVMOSDisk -VM $vm -CreateOption fromImage Add-AzVMNetworkInterface -VM $vm -NetworkInterface $nic # No `-Name`, since we set `-VMName` when initializing the PSVirtualMachine object with `New-AzVMConfig` New-AzVM -AsJob -VM $vm -Location \"East US\" -ResourceGroupName \"RG\" Azure CLI az vm create -n Socrates -g RG -l \"East US\" --image \"MicrosoftWindowsServer:WindowsServer:2016-Datacenter-Server-Core:2016.127.20190603\" --admin-username aztestadmin --admin-password $password --nics socrates-nic Display status of VMs Azure CLI az vm list --resource-group $RESOURCEGROUP --output table Enable IIS using the Custom Script Extension to run a PowerShell script on the VM. Azure CLI az vm extension set --resource-group $RESOURCEGROUP --vm-name SimpleWinVM --name CustomScriptExtension --publisher Microsoft.Compute --version 1 .9 --settings '{\"fileUris\":[\"https://raw.githubusercontent.com/MicrosoftDocs/mslearn-welcome-to-azure/master/configure-iis.ps1\"]}' --protected-settings '{\"commandToExecute\": \"powershell -ExecutionPolicy Unrestricted -File configure-iis.ps1\"}' Source Add NIC PowerShell Stop-AzVM -Name $vmName -ResourceGroupName $g Add-AzVMNetworkInterface -VM $vm -Id $nicId -Primary Update-AzVm -ResourceGroupName $g -VM $vm Azure CLI az network nic create --resource-group $g --name $nicName --vnet-name $ExamRefVNET --subnet $subnetName az vm nic add -g $g --vm-name $vmName --nics $nicName --primary-nic Redeploy PowerShell Set-AzVM -Redeploy -ResourceGroupName ExamRefRG -Name ExamRefVM Azure CLI az vm redeploy --resource-group ExamRefRG --name ExamRefVM Create managed VM PowerShell Set-AzVM -ResourceGroupName $g -Name $vmName -Generalized $vm = Get-AzVM -ResourceGroupName $g -Name $vmName $image = New-AzImageConfig -Location $location -SourceVirtualMachineId $vm . ID New-AzImage -Image $image -ImageName $imageName -ResourceGroupName $g Azure CLI az vm deallocate --resource-group $g --name $vmName az vm generalize --resource-group $g --name $vmName az image create --resource-group $g --name $imageName --source $vmName Create $subnets = @() $subnet1Name = \"Subnet1\" $subnet2Name = \"Subnet2\" $subnet1AddressPrefix = \"10.0.0.0/24\" $subnet2AddressPrefix = \"10.0.1.0/24\" $vnetAddressSpace = \"10.0.0.0/16\" $vnetName = \"ExamRefVNET\" New-AzResourceGroup -Name $resourceGroupName -Location $location Create a virtual network $subnets = @() $subnets += New-AzVirtualNetworkSubnetConfig -Name $subnet1Name -AddressPrefix $subnet1AddressPrefix $subnets += New-AzVirtualNetworkSubnetConfig -Name $subnet2Name -AddressPrefix $subnet2AddressPrefix $vnet = New-AzVirtualNetwork -Name $vnetName -Location $location -AddressPrefix $vnetAddressSpace -Subnet $subnets $pip = New-AzPublicIpAddress -Name $ipName -ResourceGroupName $g -Location $location -AllocationMethod Dynamic -DomainNameLabel $dnsName $nsgRules = @() $nsgRules += New-AzNetworkSecurityRuleConfig -Name \"RDP\" -Description \"RemoteDesktop\" -Protocol Tcp -SourcePortRange \"*\" -DestinationPortRange \"3389\" -SourceAddressPrefix \"*\" -DestinationAddressPrefix \"*\" -Access Allow -Priority 110 -Direction Inbound $nsg = New-AzNetworkSecurityGroup -ResourceGroupName $resourceGroupName -Name \"ExamREfNSG\" -SecurityRules $nsgRules -Location $location $nic = New-AzNetworkInterface -Name $nicNAme -ResourceGroupName $resourceGroupName -Location $location -SubnetId $vnet . Subnets [ 0 ]. Id -PublicIpAddressId $pip . Id -NetworkSecurityGroupId $nsg . Id Add-AzVMNetworkInterface -VM $vm -NetworkInterface $nic $vm = New-AzVMConfig -VMName $vmName -VMSize $vmSize Set-AzVMOperatingSystem -Windows -ComputerName $vmName -Credential $cred -ProvisionVMAgent -VM $vm Set-AzVMSourceImage -PublisherName $pubName -Offer $offerName -Skus $skuName -Version \"latest\" -VM $vm Set-AzVMOSDisk -CreateOption fromImage -VM $vm New-AzVM -ResourceGroupName $resourceGroupName -Location $location -VM $vm az group create --name $g --location $location vmName = \"myUbuntuVM\" imageName = \"UbuntuLTS\" az vm create --resource-group $g --name $vmName --image $imageName --generate-ssh-keys Create a virtual network vnetName = \"ExamRefVNET\" vnetAddressPrefix = \"10.0.0.0/16\" az network vnet create --resource-group $g -n ExamRefVNET --address-prefixes $vnetAddressPrefix -l $location dnsRecord = \"examrefdns123123\" ipName = \"ExamRefIP\" az network public-ip create -n $ipName -g $g --allocation-method Dynamic --dns-name $dnsRecord -l $location nsgName = \"webnsg\" az network nsg create -n $nsgName -g $g -l $location Create a NSG rules to allow inbound SSH and HTTP az network nsg rule create -n SSH --nsg-name ... --priority 100 -g ... --access Allow --description \"SSH Access\" --direction Inbound --protocol Tcp --destination-address-prefix \"*\" --destination-port-range 22 --source-address-prefix \"*\" --source-port-range \"*\" az network nsg rule create -n HTTP --nsg-name ... --priority 101 -g ... --access Allow --description \"Web Access\" --direction Inbound --protocol Tcp --destination-address-prefix \"*\" --destination-port-range 80 --source-address-prefix \"*\" --source-port-range \"* # Create a network interface nicname = \"WebVMNic1\" az network nic create -n $nicname -g $g --subnet $Subnet1Name --network-security-group $nsgName --vnet-name $vnetName --public-ip-address $ipName -l $location # Retrieve a list of marketplace images az vm image list --all # Retrieve form factors available in each region az vm list-sizes --location ... # Create the VM imageName = \"Canonical:UbuntuServer:16.04-LTS:latest\" vmSize = \"Standard_DS1_V2\" user = demouser vmName = \"WebVM\" az vm create -n $vmName -g $g -l $location --size $vmSize --nics $nicname --image $imageName --generate-ssh-keys VHD Add a new disk to a VM PowerShell $dataDiskName = \"MyDataDisk\" $location = \"WestUS\" $diskConfig = New-AzDiskConfig -SkuName Premium_LRS -Location $location -CreateOption Empty -DiskSizeGB 128 $dataDisk1 = New-AzDisk -DiskName $dataDiskName -Disk $diskConfig -ResourceGroupNAme ExamRefRG $vm = Get-AzVM -Name ExamRefVM -ResourceGroupName ExamRefRG $vm = Add-AzVMDataDisk -VM $vm -Name $dataDiskName -CreateOption Attach -ManagedDiskId $dataDisk1 . Id -Lun 1 Update-AzVM -VM $vm -ResourceGroupName ExamRefRG Azure CLI az vm disk attach -g ExamRefRG --vm-name ExamRefVM --name myDataDisk --new --size-gb 128 --sku Premium_LRS Modify host cache setting PowerShell $vm = Get-AzVM -ResourceGroupName $g -Name $vmName Set-AzVMDataDisk -VM $vm -Lun 0 -Caching ReadOnly Update-AzVM -ResourceGroupName $g -VM $vm Azure CLI az vm disk attach --vm-name $vmName --resource-group $g --size-gb 128 --disk $diskName --caching ReadWrite -new az vm unmanaged-disk attach Diagnostics Enable on deployment Enable after deployment Set-AzVmDiagnosticsExtension Enable diagnostics using a storage account specified in a XML configuration file Set-AzVMDiagnosticsExtension -ResourceGroupName \"ResourceGroup01\" -VMName \"VirtualMachine02\" -DiagnosticsConfigurationPath \"diagnostics_publicconfig.xml\" Providing storage account name absent in config, or overriding it if present Set-AzVMDiagnosticsExtension -ResourceGroupName \"ResourceGroup1\" -VMName \"VirtualMachine2\" -DiagnosticsConfigurationPath diagnostics_publicconfig . xml -StorageAccountName \"MyStorageAccount\" Explicitly providing storage account name and key Set-AzVMDiagnosticsExtension -ResourceGroupName \"ResourceGroup01\" -VMName \"VirtualMachine02\" -DiagnosticsConfigurationPath \"diagnostics_publicconfig.xml\" -StorageAccountName \"MyStorageAccount\" -StorageAccountKey $storage_key ARM templates Deploy a named ARM template PowerShell New-AzResourceGroupDeployment -Mode Complete -Name simpleVMDeployment -ResourceGroupName ExamRefRG Azure CLI az group deployment create --name simpleVMDeployment --mode Complete --resource-group ExamRefRG Export a resource group to an ARM template PowerShell Save-AzResourceGroupDeploymentTemplate -ResourceGroupName ExamRefRG -DeploymentName simpleVMDeployment Azure CLI az group deployment export --name simpleVMDeployment --resource-group ExamRefRG Create from existing resource group Export-AzResourceGroup -ResourceGroupName ExamRefRG Pass a template file during deployment New-AzResourceGroupDeployment -Name MyDeployment -ResourceGroupName ExamRefRG -TemplateFile C : \\ MyTemplates \\ AppTemplate . json az group export --name ExamRefRG az group deployment create --name MyDeployment --resource-group ExamRefRG --template-file AppTemplate.json --parameters @dev-env.json View all available sizes in a location Move a resource to another resource group or subscription (PowerShell) $resourceID = Get-AzResource -ResourceGroupName ExamRefRG | Format-Table -Property ResourceId Move-AzResource -DestinationResourceGroupName ExamRefDestRG -ResourceId $resourceID Move-AzResource -DestinationSubscriptionId $subscriptionID -DestinationResourceGroupName ExamRefDestRG -ResourceId $resourceID Move a resource to another resource group or subscription (Azure CLI) az resource list -g ExamRefRG az resource move --destination-group ExamRefDestRG --ids $resourceID az resource move --destination-group ExamrefDestRG --destination-subscription-id $subscriptionID --ids $resourceID DSC Package Package a DSC script into a zip file Publish-AzVMDscConfiguration -ConfigurationPath .\\ ContosoWeb . ps1 -OutputArchivePath .\\ ContosoWeb . zip Apply Publish a packaged DSC script to a storage account $g = \"ExamRefRG\" $location =- \"WestUS\" $vmName = \"ExamRefVM\" $storageName = \"dscstorageer1\" $configurationName = \"Main\" $archiveBlob = \"ContosoWeb.ps1.zip\" $configurationPath = \".\\ContosoWeb.ps1\" Publish-AzVMDscConfiguration -ConfigurationPath $configurationPath -ResourceGroupName $g -StorageAccountName $storageName Set-AzVmDscExtension -Version 2 . 76 -ResourceGroupName $g -VMName $vmName -ArchiveStorageAccountNAme $storageName -ArchiveBlobName $archiveBNlob -AutoUpdate : $false -ConfigurationName $configurationName VMSS Create a VMSS with IIS installed from a custom script extension $g = \"ExamRefRG\" $location = \"WestUS\" $vmSize = \"Standard_DS2_V2\" $capacity = 2 New-AzResourceGroup -Name $g -Location $location $vmssConfig = New-AzVmssConfig -Location $location -SkuCapacity $capacity -SkuName $vmSize -UpgradePolicyMode Automatic $publicIP = New-AzPublicIpAddress -ResourceGroupName $g -Location $locaiton -AllocationMethod Static -Name $publicIPName $frontendIP = New-AzLoadBalancerFrontendIpConfig -Name \"lbFrontEndPool\" -PublicIpAddress $publicIP $backendPool = New-AzLoadBalancerBackendAddressPoolConfig -Name \"lbBackEndPool\" $lb = New-AzLoadBalancer -ResourceGroupName $g -Name \"lbrule\" -Location $location -FrontendIPConfiguration $frontendIP -BackendAddressPool $backendPool Add-AzLoadBalancerProbeConfig -Name \"lbrule\" -LoadBalancer $lb -Protocol http -Port 80 -IntervalInSeconds 15 -ProbeCount 2 -RequestPath \"/\" Set-AzLoadBalancer -LoadBalancer $lb Reference a VM image from the gallery Set-AzVmssStorageProfile $vmssConfig -ImageReferencePublisher MicrosoftWindowsServer -ImageReferenceOffer WindowsServer -ImageReferenceSku 2016-Datacenter -ImageReferenceVersion latest -OsDiskCreateOption FromImage Set up information for authenticating with the VM Set-AzVmssOsProfile $vmssConfig -AdminUsername \"azureuser\" -AdminPassword \"P@ssword!\" -ComputerNamePrefix \"ssVM\" Create VNet resources $subnet = New-AzVirtualNetworkSubnetConfig -Name \"web\" -AddressPrefix 10 . 0 . 0 . 0 / 24 $vnet = New-AzVirtualNetwork -ResourceGroupName $g -Name $ssName -Location $location -AddressPrefix 10 . 0 . 0 . 0 / 16 -Subnet $subnet $ipConfig = New-AzVmssIpConfig -Name \"vmssIPConfig\" -LoadBalancerBackendAddressPoolsId $lb . BackendAddressPools [ 0 ]. Id -SubnetId $vnet . Subnets [ 0 ]. Id Attach the VNet to the config object Add-AzVmssNetworkInterfaceConfiguration -VirtualMachineScaleSet $vmssConfig -Name \"network-config\" -Primary $true -IPConfiguration $ipConfig Create the scale set with the config object New-AzVmss -ResourceGroupName $g -Name $scaleSetName -VirtualMachineScaleSet $vmssConfig g = \"ExamRefRG\" ssName = \"erscaleset\" userName = \"azureuser\" password = \"P@ssword!\" vmPrefix = \"ssVM\" location = \"WestUS\" az group create --name $g --location $location az vmss create --resource-group $g --name $ssName --image UbuntuLTS --authentication-type password --admin-username $userName --admin-password $password Custom script extension Use the custom script extension to install packages and Windows features and roles to VMs # Deploy the Active Directory Domain Services role Install-WindowsFeature -Name \"AD-Domain-Services\" -IncludeManagementTools -IncludeAllSubFeature Install-ADDSForest -DomainName $domain -DomainMode Win2012 -ForestMode Win2012 -Force -SafeModeAdministratorPassword $smPassword # Use `Set-AzVMCustomScriptExtension` to run script on a VM $vmName = \"ExamRefVM\" $scriptName = \"deploy-ad.ps1\" $domain = \"contoso.com\" $extensionName = \"installAD\" $location = \"WestUS\" $scriptUri = \"https://raw.githubusercontent.com/opsgility/lab-support-public/master/script-extensions/deploy-ad.ps1\" $scriptArgument = \"contoso.com $password\" Set-AzVMCustomScriptExtension -ResourceGroupName $g -VMName $vmName -FileUri $scriptUri -Argument \"$domain $password\" -Run $scriptName -Name $extensionName -Location $location vmName = \"LinuxVM\" extensionName = \"InstallApache\" az vm extension set --resource-group $g --vm-name $vmName --name customScript --publisher Microsoft.Azure.Extensions --protected-settings ./cseconfig.json Enable IIS using the Custom Script Extension to run a PowerShell script on the VM. ? az vm extension set --resource-group $RESOURCEGROUP --vm-name SimpleWinVM --name CustomScriptExtension --publisher Microsoft.Compute --version 1 .9 --settings '{\"fileUris\":[\"https://raw.githubusercontent.com/MicrosoftDocs/mslearn-welcome-to-azure/master/configure-iis.ps1\"]}' --protected-settings '{\"commandToExecute\": \"powershell -ExecutionPolicy Unrestricted -File configure-iis.ps1\"}' Create availability set Azure PowerShell New-AzavailabilitySet -ResourceGroupName $g -Name $n -Location $l -PlatformUpdateDomainCount 10 -PlatformFaultDomainCount 3 -Sku \"Aligned\" Azure CLI az vm availability-set create -n $n -g $g --platform-fault-domain-count 3 --platform-update-domain-count 10 Invoke a command on a VM Run a shell script, $script can be supplied inline az vm run-command invoke --command-id RunShellScript --scripts $script Parameters can be passed to the script argument az vm run-command invoke --command-id RunShellScript --scripts 'echo $1 $2' --parameters hello world Run a PowerShell script az vm run-command invoke --command-id RunPowerShellScript -n Socrates -g RG Run a script file az vm run-command invoke --command-id RunPowerShellScript -n Socrates -g RG --scripts @script.ps1 --parameters 'arg1=somefoo' 'arg2=somebar' Available values for command-id can be enumerated: az vm run-command list Dedicated host","title":"Azure IaaS"},{"location":"Cloud/Azure-IaaS/#high-availability","text":"These high-availability options are mutually exclusive: Availability zones protect from datacenter-level failures by providing physical and logical separation between VM instances. Zones have independent power sources, network, and cooling, and there are at least 3 zones in every region. Availability sets offer redundancy for VMs in the same application tier, ensuring at least one VM is available in the case of a host update or problem. Each VM is assigned a fault domain (representing separate physical racks in the datacenter) and an update domain (representing groups of VMs and underlying physical hardware that can be rebooted at the same time for host updates). Availability sets have a maximum of 20 update domains and 3 fault domains. VM scale sets (VMSS) support the ability to dynamically add and remove instances. By default, a VMSS supports up to 100 instances or up to 1000 instances if deployed with the property singlePlacementGroup set to false (called a large-scale set ). A placement group is a concept similar to an availability set in that it assigns fault and upgrade domains. By default, a scale set consists of only a single placement group, but disabling this setting allows the scale set to be composed of multiple placement groups. If a custom image is used instead of one in the gallery, the limit is actually 300 instances.","title":"High availability"},{"location":"Cloud/Azure-IaaS/#load-balancers","text":"Load balancers redistribute traffic from a frontend pool to a backend pool using rules. Health probes determine the health of the VMs in the backend pool: if they don't respond then new connections won't be sent. By default, Azure Load Balancer stores rules in a 5-tuple: Source IP address Source port Destination IP address Destination ports IP Protocol number Azure Load Balancers come in 2 pricing tiers: Basic which is free Standard which is charged based on the number of rules and the data that is processed. Both boot and guest OS diagnostics can be enabled on or after VM creation. Azure VMs have built-in extensions that enable configuration management. 2 most common extensions for configuration management: Windows PowerShell Desired State Configuration (DSC) allows you to define the state of a VM using PowerShell Desired State Configuration language A VM may have more than one Network Interface Card (NIC) , but they must belong to the same region as the VM itself. All NICs on a VM must also be attached to the same VNet.","title":"Load balancers"},{"location":"Cloud/Azure-IaaS/#tasks","text":"","title":"Tasks"},{"location":"Cloud/Azure-IaaS/#find-marketplace-image","text":"Produce the publisher (e.g. \"MicrosoftWindowsServer\") from Location Get-AzVMImagePublisher Produce the offer (e.g. \"WindowsServer\") from Location and PublisherName Get-AzVMImageOffer Produce the Sku from Location and PublisherName and Offer Get-AzVMImageSku Requires PublisherName, Offer, Location, Skus, and Version ( -Version * will produce a list of available version numbers) Get-AzVMImage Deploy from image Azure PowerShell Specify a managed image for use in a new virtual machine $image = Get-AzImage -ImageName $imageName -ResourceGroupName $g $vmConfig = Set-AzVMSourceImage -VM $vmConfig -Id $image . Id Specify a legacy unmanaged image for use in a new virtual machine $osDiskUri = \"https://examrefstorage.blob.core.windows.net/vhd/os-disk\" $imageUri = \"https://examrefstorage.blob.core.windows.net/images/legacy-image.vhd\" $vm = Set-AzVMOSDisk -VM $vm -Name $osDiskName -VhdUri $osDiskUri -CreateOption fromImage -SourceImageUri $imageUri -Windows Azure CLI Specify a managed image for use in a new virtual machine az vm create -g $g -n $vmName --image $imageName Specify a legacy unmanaged image for use in a new virtual machine az vm create -g $g -n $vmName --image $osDiskUri --generate-ssh-keys","title":"Find Marketplace image"},{"location":"Cloud/Azure-IaaS/#access","text":"Enable-PSRemoting # If a network connection is Public, this command will not work Enable-PSRemoting -SkipNetworkProfileCheck -Force The remote computer must also have WinRM up and running. Azure can enable PowerShell on the target machine Invoke-AzVMRunCommand -AsJob -ResourceGroupName \"RG\" -VMName \"Socrates\" -CommandId EnableRemotePS Using Azure Cloud PowerShell Enable-AzVMPSRemoting -Name Socrates -ResourceGroupName \"RG\" Add the VM's public IP address $IP to the trusted hosts of the local workstation (must be run as administrator): Set-Item WSMan : \\ localhost \\ Client \\ TrustedHosts -Value $IP Traffic to ports 5985 and 5986 must be allowed through Windows firewall and, if the computer is an Azure VM, the Network Security Group. Enter-PSSession -ComputerName 123 . 47 . 78 . 90 -Credential $cred etsn 123 . 45 . 67 . 89 -Credential ( Get-Credential ) Invoke-AzVMRunCommand -ResourceGroupName RG -VMName VM -CommandId 'RunPowerShellScript' -ScriptPath C : \\ injectedscript . ps1 Open firewall WinRM ports 5985 and 5986 PowerShell New-NetFirewallRule -DisplayName \"WinRMHTTP\" -Direction Inbound -LocalPort 5985 -Protocol TCP -Action Allow New-NetFirewallRule -DisplayName \"WinRMHTTPS\" -Direction Inbound -LocalPort 5986 -Protocol TCP -Action Allow netsh netsh advfirewall firewall add rule name=WinRMHTTP dir=in action=allow protocol=TCP localport=5985 netsh advfirewall firewall add rule name=WinRMHTTPS dir=in action=allow protocol=TCP localport=5986","title":"Access"},{"location":"Cloud/Azure-IaaS/#rdp","text":"Saving the .rdp file for later use $g = \"ExamRefRG\" $vmName = \"ExamRefVM\" $Path = \"C:\\path\\to\\ExamRefVM.rdp\" Get-AzRemoteDesktopFile -ResourceGroupName $g -Name $vmName -LocalPath $path","title":"RDP"},{"location":"Cloud/Azure-IaaS/#backup","text":"$t = Get-AzRecoveryServicesVault -Name t1 Set-AzRecoveryServicesBackupProperties -Vault $t -BackupStorageRedundancy GeoRedundant","title":"Backup"},{"location":"Cloud/Azure-IaaS/#ssh","text":"$VirtualMachine = Get-AzVM -ResourceGroupName \"ResourceGroup11\" -Name \"VirtualMachine07\" Add-AzVMSshPublicKey -VM $VirtualMachine -KeyData \"MIIDszCCApugAwIBAgIJALBV9YJCF/tAMA0GCSq12Ib3DQEB21QUAMEUxCzAJBgNV\" -Path \"/home/admin/.ssh/authorized_keys\" Resize VM PowerShell $vm = Get-AzVM -ResourceGroupName $g -VMName $n $vm . HardwareProfile . VMSize = \"Standard_DS2_V2\" Update-AzVM -VM $vm -ResourceGroupName $g Azure CLI az vm list-vm-resize-options --resource-group $g --name $vmName --output table az vm resize --resource-group $g --name $vmName --size Standard_DS3_v2","title":"SSH"},{"location":"Cloud/Azure-IaaS/#windows-server-core","text":"PowerShell Simply: New-AzVM -ResourceGroupName \"RG\" -Name \"VM\" -Location \"EastUS\" -Size \"Standard-B2s\" -Credential ( Get-Credential ) New-AzVM Greeks Socrates $vm The VM's NIC has to be linked to an NSG , a Subnet , and a Public IP Address . # Create a VNet with a subnet $subnet = New-AzVirtualNetworkSubnetConfig -Name \"subnet1\" -AddressPrefix \"10.0.0.0/24\" $vnet = New-AzVirtualNetwork -Name \"vnet\" -ResourceGroupName \"RG\" -Location \"East US\" -AddressPrefix \"10.0.0.0/16\" -Subnet $subnet # Create a Network Interface from the IP and VNet $ip = New-AzPublicIpAddress -Name \"wscore-ip\" -ResourceGroupName \"RG\" -Location \"East US\" -AllocationMethod Dynamic $nic = New-AzNetworkInterface -Name \"wscore-nic\" -ResourceGroupName \"RG\" -Location \"East US\" -SubnetId $vnet . Subnets [ 0 ]. Id -PublicIpAddressId $pip . Id -NetworkSecurityGroupId $nsg . Id $vm = New-AzVMConfig -VMName \"Socrates\" -VMSize \"Standard_B1s\" Set-AzVMOperatingSystem -VM $vm -Windows -ComputerName Socrates -Credential $aztestadmin Set-AzVMSourceImage -VM $vm -PublisherName \"MicrosoftWindowsServer\" -Offer \"WindowsServer\" -Skus \"2016-Datacenter-Server-Core\" -Version 2016 . 127 . 20190603 Set-AzVMOSDisk -VM $vm -CreateOption fromImage Add-AzVMNetworkInterface -VM $vm -NetworkInterface $nic # No `-Name`, since we set `-VMName` when initializing the PSVirtualMachine object with `New-AzVMConfig` New-AzVM -AsJob -VM $vm -Location \"East US\" -ResourceGroupName \"RG\" Azure CLI az vm create -n Socrates -g RG -l \"East US\" --image \"MicrosoftWindowsServer:WindowsServer:2016-Datacenter-Server-Core:2016.127.20190603\" --admin-username aztestadmin --admin-password $password --nics socrates-nic Display status of VMs Azure CLI az vm list --resource-group $RESOURCEGROUP --output table Enable IIS using the Custom Script Extension to run a PowerShell script on the VM. Azure CLI az vm extension set --resource-group $RESOURCEGROUP --vm-name SimpleWinVM --name CustomScriptExtension --publisher Microsoft.Compute --version 1 .9 --settings '{\"fileUris\":[\"https://raw.githubusercontent.com/MicrosoftDocs/mslearn-welcome-to-azure/master/configure-iis.ps1\"]}' --protected-settings '{\"commandToExecute\": \"powershell -ExecutionPolicy Unrestricted -File configure-iis.ps1\"}' Source","title":"Windows Server Core"},{"location":"Cloud/Azure-IaaS/#add-nic","text":"PowerShell Stop-AzVM -Name $vmName -ResourceGroupName $g Add-AzVMNetworkInterface -VM $vm -Id $nicId -Primary Update-AzVm -ResourceGroupName $g -VM $vm Azure CLI az network nic create --resource-group $g --name $nicName --vnet-name $ExamRefVNET --subnet $subnetName az vm nic add -g $g --vm-name $vmName --nics $nicName --primary-nic Redeploy PowerShell Set-AzVM -Redeploy -ResourceGroupName ExamRefRG -Name ExamRefVM Azure CLI az vm redeploy --resource-group ExamRefRG --name ExamRefVM Create managed VM PowerShell Set-AzVM -ResourceGroupName $g -Name $vmName -Generalized $vm = Get-AzVM -ResourceGroupName $g -Name $vmName $image = New-AzImageConfig -Location $location -SourceVirtualMachineId $vm . ID New-AzImage -Image $image -ImageName $imageName -ResourceGroupName $g Azure CLI az vm deallocate --resource-group $g --name $vmName az vm generalize --resource-group $g --name $vmName az image create --resource-group $g --name $imageName --source $vmName","title":"Add NIC"},{"location":"Cloud/Azure-IaaS/#create","text":"$subnets = @() $subnet1Name = \"Subnet1\" $subnet2Name = \"Subnet2\" $subnet1AddressPrefix = \"10.0.0.0/24\" $subnet2AddressPrefix = \"10.0.1.0/24\" $vnetAddressSpace = \"10.0.0.0/16\" $vnetName = \"ExamRefVNET\" New-AzResourceGroup -Name $resourceGroupName -Location $location Create a virtual network $subnets = @() $subnets += New-AzVirtualNetworkSubnetConfig -Name $subnet1Name -AddressPrefix $subnet1AddressPrefix $subnets += New-AzVirtualNetworkSubnetConfig -Name $subnet2Name -AddressPrefix $subnet2AddressPrefix $vnet = New-AzVirtualNetwork -Name $vnetName -Location $location -AddressPrefix $vnetAddressSpace -Subnet $subnets $pip = New-AzPublicIpAddress -Name $ipName -ResourceGroupName $g -Location $location -AllocationMethod Dynamic -DomainNameLabel $dnsName $nsgRules = @() $nsgRules += New-AzNetworkSecurityRuleConfig -Name \"RDP\" -Description \"RemoteDesktop\" -Protocol Tcp -SourcePortRange \"*\" -DestinationPortRange \"3389\" -SourceAddressPrefix \"*\" -DestinationAddressPrefix \"*\" -Access Allow -Priority 110 -Direction Inbound $nsg = New-AzNetworkSecurityGroup -ResourceGroupName $resourceGroupName -Name \"ExamREfNSG\" -SecurityRules $nsgRules -Location $location $nic = New-AzNetworkInterface -Name $nicNAme -ResourceGroupName $resourceGroupName -Location $location -SubnetId $vnet . Subnets [ 0 ]. Id -PublicIpAddressId $pip . Id -NetworkSecurityGroupId $nsg . Id Add-AzVMNetworkInterface -VM $vm -NetworkInterface $nic $vm = New-AzVMConfig -VMName $vmName -VMSize $vmSize Set-AzVMOperatingSystem -Windows -ComputerName $vmName -Credential $cred -ProvisionVMAgent -VM $vm Set-AzVMSourceImage -PublisherName $pubName -Offer $offerName -Skus $skuName -Version \"latest\" -VM $vm Set-AzVMOSDisk -CreateOption fromImage -VM $vm New-AzVM -ResourceGroupName $resourceGroupName -Location $location -VM $vm az group create --name $g --location $location vmName = \"myUbuntuVM\" imageName = \"UbuntuLTS\" az vm create --resource-group $g --name $vmName --image $imageName --generate-ssh-keys Create a virtual network vnetName = \"ExamRefVNET\" vnetAddressPrefix = \"10.0.0.0/16\" az network vnet create --resource-group $g -n ExamRefVNET --address-prefixes $vnetAddressPrefix -l $location dnsRecord = \"examrefdns123123\" ipName = \"ExamRefIP\" az network public-ip create -n $ipName -g $g --allocation-method Dynamic --dns-name $dnsRecord -l $location nsgName = \"webnsg\" az network nsg create -n $nsgName -g $g -l $location Create a NSG rules to allow inbound SSH and HTTP az network nsg rule create -n SSH --nsg-name ... --priority 100 -g ... --access Allow --description \"SSH Access\" --direction Inbound --protocol Tcp --destination-address-prefix \"*\" --destination-port-range 22 --source-address-prefix \"*\" --source-port-range \"*\" az network nsg rule create -n HTTP --nsg-name ... --priority 101 -g ... --access Allow --description \"Web Access\" --direction Inbound --protocol Tcp --destination-address-prefix \"*\" --destination-port-range 80 --source-address-prefix \"*\" --source-port-range \"* # Create a network interface nicname = \"WebVMNic1\" az network nic create -n $nicname -g $g --subnet $Subnet1Name --network-security-group $nsgName --vnet-name $vnetName --public-ip-address $ipName -l $location # Retrieve a list of marketplace images az vm image list --all # Retrieve form factors available in each region az vm list-sizes --location ... # Create the VM imageName = \"Canonical:UbuntuServer:16.04-LTS:latest\" vmSize = \"Standard_DS1_V2\" user = demouser vmName = \"WebVM\" az vm create -n $vmName -g $g -l $location --size $vmSize --nics $nicname --image $imageName --generate-ssh-keys","title":"Create"},{"location":"Cloud/Azure-IaaS/#vhd","text":"Add a new disk to a VM PowerShell $dataDiskName = \"MyDataDisk\" $location = \"WestUS\" $diskConfig = New-AzDiskConfig -SkuName Premium_LRS -Location $location -CreateOption Empty -DiskSizeGB 128 $dataDisk1 = New-AzDisk -DiskName $dataDiskName -Disk $diskConfig -ResourceGroupNAme ExamRefRG $vm = Get-AzVM -Name ExamRefVM -ResourceGroupName ExamRefRG $vm = Add-AzVMDataDisk -VM $vm -Name $dataDiskName -CreateOption Attach -ManagedDiskId $dataDisk1 . Id -Lun 1 Update-AzVM -VM $vm -ResourceGroupName ExamRefRG Azure CLI az vm disk attach -g ExamRefRG --vm-name ExamRefVM --name myDataDisk --new --size-gb 128 --sku Premium_LRS Modify host cache setting PowerShell $vm = Get-AzVM -ResourceGroupName $g -Name $vmName Set-AzVMDataDisk -VM $vm -Lun 0 -Caching ReadOnly Update-AzVM -ResourceGroupName $g -VM $vm Azure CLI az vm disk attach --vm-name $vmName --resource-group $g --size-gb 128 --disk $diskName --caching ReadWrite -new az vm unmanaged-disk attach","title":"VHD"},{"location":"Cloud/Azure-IaaS/#diagnostics","text":"","title":"Diagnostics"},{"location":"Cloud/Azure-IaaS/#enable-on-deployment","text":"","title":"Enable on deployment"},{"location":"Cloud/Azure-IaaS/#enable-after-deployment","text":"Set-AzVmDiagnosticsExtension Enable diagnostics using a storage account specified in a XML configuration file Set-AzVMDiagnosticsExtension -ResourceGroupName \"ResourceGroup01\" -VMName \"VirtualMachine02\" -DiagnosticsConfigurationPath \"diagnostics_publicconfig.xml\" Providing storage account name absent in config, or overriding it if present Set-AzVMDiagnosticsExtension -ResourceGroupName \"ResourceGroup1\" -VMName \"VirtualMachine2\" -DiagnosticsConfigurationPath diagnostics_publicconfig . xml -StorageAccountName \"MyStorageAccount\" Explicitly providing storage account name and key Set-AzVMDiagnosticsExtension -ResourceGroupName \"ResourceGroup01\" -VMName \"VirtualMachine02\" -DiagnosticsConfigurationPath \"diagnostics_publicconfig.xml\" -StorageAccountName \"MyStorageAccount\" -StorageAccountKey $storage_key","title":"Enable after deployment"},{"location":"Cloud/Azure-IaaS/#arm-templates","text":"Deploy a named ARM template PowerShell New-AzResourceGroupDeployment -Mode Complete -Name simpleVMDeployment -ResourceGroupName ExamRefRG Azure CLI az group deployment create --name simpleVMDeployment --mode Complete --resource-group ExamRefRG Export a resource group to an ARM template PowerShell Save-AzResourceGroupDeploymentTemplate -ResourceGroupName ExamRefRG -DeploymentName simpleVMDeployment Azure CLI az group deployment export --name simpleVMDeployment --resource-group ExamRefRG Create from existing resource group Export-AzResourceGroup -ResourceGroupName ExamRefRG Pass a template file during deployment New-AzResourceGroupDeployment -Name MyDeployment -ResourceGroupName ExamRefRG -TemplateFile C : \\ MyTemplates \\ AppTemplate . json az group export --name ExamRefRG az group deployment create --name MyDeployment --resource-group ExamRefRG --template-file AppTemplate.json --parameters @dev-env.json","title":"ARM templates"},{"location":"Cloud/Azure-IaaS/#view-all-available-sizes-in-a-location","text":"Move a resource to another resource group or subscription (PowerShell) $resourceID = Get-AzResource -ResourceGroupName ExamRefRG | Format-Table -Property ResourceId Move-AzResource -DestinationResourceGroupName ExamRefDestRG -ResourceId $resourceID Move-AzResource -DestinationSubscriptionId $subscriptionID -DestinationResourceGroupName ExamRefDestRG -ResourceId $resourceID Move a resource to another resource group or subscription (Azure CLI) az resource list -g ExamRefRG az resource move --destination-group ExamRefDestRG --ids $resourceID az resource move --destination-group ExamrefDestRG --destination-subscription-id $subscriptionID --ids $resourceID","title":"View all available sizes in a location"},{"location":"Cloud/Azure-IaaS/#dsc","text":"","title":"DSC"},{"location":"Cloud/Azure-IaaS/#package","text":"Package a DSC script into a zip file Publish-AzVMDscConfiguration -ConfigurationPath .\\ ContosoWeb . ps1 -OutputArchivePath .\\ ContosoWeb . zip","title":"Package"},{"location":"Cloud/Azure-IaaS/#apply","text":"Publish a packaged DSC script to a storage account $g = \"ExamRefRG\" $location =- \"WestUS\" $vmName = \"ExamRefVM\" $storageName = \"dscstorageer1\" $configurationName = \"Main\" $archiveBlob = \"ContosoWeb.ps1.zip\" $configurationPath = \".\\ContosoWeb.ps1\" Publish-AzVMDscConfiguration -ConfigurationPath $configurationPath -ResourceGroupName $g -StorageAccountName $storageName Set-AzVmDscExtension -Version 2 . 76 -ResourceGroupName $g -VMName $vmName -ArchiveStorageAccountNAme $storageName -ArchiveBlobName $archiveBNlob -AutoUpdate : $false -ConfigurationName $configurationName","title":"Apply"},{"location":"Cloud/Azure-IaaS/#vmss","text":"Create a VMSS with IIS installed from a custom script extension $g = \"ExamRefRG\" $location = \"WestUS\" $vmSize = \"Standard_DS2_V2\" $capacity = 2 New-AzResourceGroup -Name $g -Location $location $vmssConfig = New-AzVmssConfig -Location $location -SkuCapacity $capacity -SkuName $vmSize -UpgradePolicyMode Automatic $publicIP = New-AzPublicIpAddress -ResourceGroupName $g -Location $locaiton -AllocationMethod Static -Name $publicIPName $frontendIP = New-AzLoadBalancerFrontendIpConfig -Name \"lbFrontEndPool\" -PublicIpAddress $publicIP $backendPool = New-AzLoadBalancerBackendAddressPoolConfig -Name \"lbBackEndPool\" $lb = New-AzLoadBalancer -ResourceGroupName $g -Name \"lbrule\" -Location $location -FrontendIPConfiguration $frontendIP -BackendAddressPool $backendPool Add-AzLoadBalancerProbeConfig -Name \"lbrule\" -LoadBalancer $lb -Protocol http -Port 80 -IntervalInSeconds 15 -ProbeCount 2 -RequestPath \"/\" Set-AzLoadBalancer -LoadBalancer $lb Reference a VM image from the gallery Set-AzVmssStorageProfile $vmssConfig -ImageReferencePublisher MicrosoftWindowsServer -ImageReferenceOffer WindowsServer -ImageReferenceSku 2016-Datacenter -ImageReferenceVersion latest -OsDiskCreateOption FromImage Set up information for authenticating with the VM Set-AzVmssOsProfile $vmssConfig -AdminUsername \"azureuser\" -AdminPassword \"P@ssword!\" -ComputerNamePrefix \"ssVM\" Create VNet resources $subnet = New-AzVirtualNetworkSubnetConfig -Name \"web\" -AddressPrefix 10 . 0 . 0 . 0 / 24 $vnet = New-AzVirtualNetwork -ResourceGroupName $g -Name $ssName -Location $location -AddressPrefix 10 . 0 . 0 . 0 / 16 -Subnet $subnet $ipConfig = New-AzVmssIpConfig -Name \"vmssIPConfig\" -LoadBalancerBackendAddressPoolsId $lb . BackendAddressPools [ 0 ]. Id -SubnetId $vnet . Subnets [ 0 ]. Id Attach the VNet to the config object Add-AzVmssNetworkInterfaceConfiguration -VirtualMachineScaleSet $vmssConfig -Name \"network-config\" -Primary $true -IPConfiguration $ipConfig Create the scale set with the config object New-AzVmss -ResourceGroupName $g -Name $scaleSetName -VirtualMachineScaleSet $vmssConfig g = \"ExamRefRG\" ssName = \"erscaleset\" userName = \"azureuser\" password = \"P@ssword!\" vmPrefix = \"ssVM\" location = \"WestUS\" az group create --name $g --location $location az vmss create --resource-group $g --name $ssName --image UbuntuLTS --authentication-type password --admin-username $userName --admin-password $password","title":"VMSS"},{"location":"Cloud/Azure-IaaS/#custom-script-extension","text":"Use the custom script extension to install packages and Windows features and roles to VMs # Deploy the Active Directory Domain Services role Install-WindowsFeature -Name \"AD-Domain-Services\" -IncludeManagementTools -IncludeAllSubFeature Install-ADDSForest -DomainName $domain -DomainMode Win2012 -ForestMode Win2012 -Force -SafeModeAdministratorPassword $smPassword # Use `Set-AzVMCustomScriptExtension` to run script on a VM $vmName = \"ExamRefVM\" $scriptName = \"deploy-ad.ps1\" $domain = \"contoso.com\" $extensionName = \"installAD\" $location = \"WestUS\" $scriptUri = \"https://raw.githubusercontent.com/opsgility/lab-support-public/master/script-extensions/deploy-ad.ps1\" $scriptArgument = \"contoso.com $password\" Set-AzVMCustomScriptExtension -ResourceGroupName $g -VMName $vmName -FileUri $scriptUri -Argument \"$domain $password\" -Run $scriptName -Name $extensionName -Location $location vmName = \"LinuxVM\" extensionName = \"InstallApache\" az vm extension set --resource-group $g --vm-name $vmName --name customScript --publisher Microsoft.Azure.Extensions --protected-settings ./cseconfig.json Enable IIS using the Custom Script Extension to run a PowerShell script on the VM. ? az vm extension set --resource-group $RESOURCEGROUP --vm-name SimpleWinVM --name CustomScriptExtension --publisher Microsoft.Compute --version 1 .9 --settings '{\"fileUris\":[\"https://raw.githubusercontent.com/MicrosoftDocs/mslearn-welcome-to-azure/master/configure-iis.ps1\"]}' --protected-settings '{\"commandToExecute\": \"powershell -ExecutionPolicy Unrestricted -File configure-iis.ps1\"}' Create availability set Azure PowerShell New-AzavailabilitySet -ResourceGroupName $g -Name $n -Location $l -PlatformUpdateDomainCount 10 -PlatformFaultDomainCount 3 -Sku \"Aligned\" Azure CLI az vm availability-set create -n $n -g $g --platform-fault-domain-count 3 --platform-update-domain-count 10","title":"Custom script extension"},{"location":"Cloud/Azure-IaaS/#invoke-a-command-on-a-vm","text":"Run a shell script, $script can be supplied inline az vm run-command invoke --command-id RunShellScript --scripts $script Parameters can be passed to the script argument az vm run-command invoke --command-id RunShellScript --scripts 'echo $1 $2' --parameters hello world Run a PowerShell script az vm run-command invoke --command-id RunPowerShellScript -n Socrates -g RG Run a script file az vm run-command invoke --command-id RunPowerShellScript -n Socrates -g RG --scripts @script.ps1 --parameters 'arg1=somefoo' 'arg2=somebar' Available values for command-id can be enumerated: az vm run-command list","title":"Invoke a command on a VM"},{"location":"Cloud/Azure-IaaS/#dedicated-host","text":"","title":"Dedicated host"},{"location":"Cloud/Azure-Load-Balancer/","text":"Azure Load Balancers are used to distribute inbound traffic across a pool of backend servers running in a VNet. They are defined by connecting a frontend and backend pool configurations with rules . - Backend Address Pool - Health Probe , specifying port and interval, can be of types TCP, HTTP, and (Standard SKU only) HTTPS - HTTP and HTTPS probes will return an unhealthy status if a HTTP status code other than 200 is received. - Load Balancer Rule defines how a frontend address and port is mapped to the destination port and address on the backend Load Balancers are available in Basic and Standard SKUs. Standard load balancers require a Standard Public IP unless they are intended for internal use only. Standard Public IPs do not allow any inbound communication by default and must have network security rules configured. Only Standard Health Probes support HTTPS. Basic backend SKUs must be comprised of either a single VM, VMs in the same availability set, or a VM scale set. Basic By default, Azure Load Balancer is set to timeout idle TCP connections after 4 minutes, but this can be configured . Azure load balancing rules route based on a 5-tuple hash , calculated from source and destination IP addresses and ports, as well as protocol. This means that traffic from any IP address will typically go to the same backend node, resulting in a modicum of affinity that can be configured . Frontend A frontend is defined by a 3-tuple composed of an IP address, a transport protocol, and a port number. Multiple frontends can be assigned to a load balancer to serve multiple websites or services. There are two modes: Internal, where the frontend references a subnet and an IP address from that subnet is allocated statically or dynamically Public, where a Public IP Address resource is used to receive inbound traffic. If the LB is at Standard SKU, then the IP must also be at Standard. If the backend resources of a load balancer don't have instance-level public IP (ILPIP) addresses, they establish outbound connectivity via the frontend IP of the public load balancer. Backend Any VM can only be a member of the backend pool of a single internal load balancer and simultaneously a single external load balancer. But a VM may not be a member of more than one external load balancer, nor more than one internal load balancer. Basic SKU Backend pools must comprise either a single VM or VMs in the same availability set or scale set . Only Standard SKU backends can accept VMs in a single VNet that are not explicitly assigned to an availability set. Outbound connections Floating IP Floating IP is Azure's term for Direct Server Return (DSR) , which refers to the ability of nodes normally behind a load balancer to respond directly to client requests without overburdening the load balancer with return traffic. This prevents the load balancer from becoming a bottleneck. Technically, Azure Load Balancer always operates in a DSR flow topology even if Floating IP is not enabled, using the VMs' own IP addresses. When enabled, Floating IP changes the IP address mapping to the Frontend IP address of the load balancer. Tasks Create internal load balancer $ip = \"10.0.0.20\" $frontendIP = New-AzLoadBalancerFrontendIpConfig -Name frontend -PrivateIpAddress $ip Add to backend address pool The process in PowerShell, counterintuitively, is actually to modify the VM's NIC to add a reference to the backend pool. AZ-103: p. 365 $vm = Get-AzVM -Name VM1 -ResourceGroupName $g $vmnic = Get-AzNetworkInterface -ResourceGroupName $g | where { $_ . VirtualMachine . Id -eq $vm . Id } $lb = Get-AzLoadBalancer -Name ExamRefLB -ResourceGroupName $g $backend = Get-AzLoadBalancerBackendAddressPoolConfig -Name ExamRefBackEndPool -LoadBalancer $lb # All IP configuration settings of the NIC have to be reapplied, there is no support for incremental changes $ipconfig = Get-AzNetworkInterfaceIpConfig -Name ipconfig1 -NetworkInterface vm1nic Set-AzNetworkInterfaceIpConfig -Name ipconfig1 -NetworkInterface $vm1nic -SubnetId $ipconfig . Subnet . Id -LoadBalancerBackendAddressPoolId $backend . Id Set-AzNetworkInterface -NetworkInterface $vm1nic Azure CLI supports incremental update of the NIC, which makes this command simpler than its PowerShell equivalent. az network nic ip-config address-pool add --resource-group ExamRefRG --address-pool ExamRefBackEndPool --lb-name ExamRefLB --nic-name vm1-nic --ip-config-name ipconfig1 Configure TCP reset TCP timeout values should be greater than that used for TCP keepalives. A new load balancing rule config object can have idle timeout set on declaration. New-AzLoadBalancerRuleConfig -Name \"MyLBRule\" -FrontendIpConfiguration $fe -BackendAddressPool $be -Probe $hp -Protocol TCP -FrontendPort 80 -BackendPort 80 ` -IdleTimeoutInMinutes 15 -EnableTcpReset These can be manually changed on the load balancing object as well $lb = Get-AzLoadBalancer -Name \"myLoadBalancer\" -ResourceGroup \"myResourceGroup\" $lb . LoadBalancingRules [ 0 ]. IdleTimeoutInMinutes = '15' $lb . LoadBalancingRules [ 0 ]. EnableTcpReset = 'true' Set-AzLoadBalancer -LoadBalancer $lb az network lb rule update -g $g -n MyLBRule --lb-name myLoadBalancer --idle-timeout 15 --enable-tcp-reset true Specify affinity In the Azure Portal, affinity is specified in the Session persistence dropdown. In Azure PowerShell and CLI, the option is the load distribution named parameter - New-AzLoadBalancerRuleConfig -LoadDistribution - az network lb rule create --load-distribution Sources: Multiple frontends for Azure Load Balancer SNAT for outbound connections Azure Load Balancer Floating IP configuration AZ-103: p. 358","title":"Azure Load Balancer"},{"location":"Cloud/Azure-Load-Balancer/#frontend","text":"A frontend is defined by a 3-tuple composed of an IP address, a transport protocol, and a port number. Multiple frontends can be assigned to a load balancer to serve multiple websites or services. There are two modes: Internal, where the frontend references a subnet and an IP address from that subnet is allocated statically or dynamically Public, where a Public IP Address resource is used to receive inbound traffic. If the LB is at Standard SKU, then the IP must also be at Standard. If the backend resources of a load balancer don't have instance-level public IP (ILPIP) addresses, they establish outbound connectivity via the frontend IP of the public load balancer.","title":"Frontend"},{"location":"Cloud/Azure-Load-Balancer/#backend","text":"Any VM can only be a member of the backend pool of a single internal load balancer and simultaneously a single external load balancer. But a VM may not be a member of more than one external load balancer, nor more than one internal load balancer. Basic SKU Backend pools must comprise either a single VM or VMs in the same availability set or scale set . Only Standard SKU backends can accept VMs in a single VNet that are not explicitly assigned to an availability set.","title":"Backend"},{"location":"Cloud/Azure-Load-Balancer/#outbound-connections","text":"","title":"Outbound connections"},{"location":"Cloud/Azure-Load-Balancer/#floating-ip","text":"Floating IP is Azure's term for Direct Server Return (DSR) , which refers to the ability of nodes normally behind a load balancer to respond directly to client requests without overburdening the load balancer with return traffic. This prevents the load balancer from becoming a bottleneck. Technically, Azure Load Balancer always operates in a DSR flow topology even if Floating IP is not enabled, using the VMs' own IP addresses. When enabled, Floating IP changes the IP address mapping to the Frontend IP address of the load balancer.","title":"Floating IP"},{"location":"Cloud/Azure-Load-Balancer/#tasks","text":"","title":"Tasks"},{"location":"Cloud/Azure-Load-Balancer/#create-internal-load-balancer","text":"$ip = \"10.0.0.20\" $frontendIP = New-AzLoadBalancerFrontendIpConfig -Name frontend -PrivateIpAddress $ip","title":"Create internal load balancer"},{"location":"Cloud/Azure-Load-Balancer/#add-to-backend-address-pool","text":"The process in PowerShell, counterintuitively, is actually to modify the VM's NIC to add a reference to the backend pool. AZ-103: p. 365 $vm = Get-AzVM -Name VM1 -ResourceGroupName $g $vmnic = Get-AzNetworkInterface -ResourceGroupName $g | where { $_ . VirtualMachine . Id -eq $vm . Id } $lb = Get-AzLoadBalancer -Name ExamRefLB -ResourceGroupName $g $backend = Get-AzLoadBalancerBackendAddressPoolConfig -Name ExamRefBackEndPool -LoadBalancer $lb # All IP configuration settings of the NIC have to be reapplied, there is no support for incremental changes $ipconfig = Get-AzNetworkInterfaceIpConfig -Name ipconfig1 -NetworkInterface vm1nic Set-AzNetworkInterfaceIpConfig -Name ipconfig1 -NetworkInterface $vm1nic -SubnetId $ipconfig . Subnet . Id -LoadBalancerBackendAddressPoolId $backend . Id Set-AzNetworkInterface -NetworkInterface $vm1nic Azure CLI supports incremental update of the NIC, which makes this command simpler than its PowerShell equivalent. az network nic ip-config address-pool add --resource-group ExamRefRG --address-pool ExamRefBackEndPool --lb-name ExamRefLB --nic-name vm1-nic --ip-config-name ipconfig1","title":"Add to backend address pool"},{"location":"Cloud/Azure-Load-Balancer/#configure-tcp-reset","text":"TCP timeout values should be greater than that used for TCP keepalives. A new load balancing rule config object can have idle timeout set on declaration. New-AzLoadBalancerRuleConfig -Name \"MyLBRule\" -FrontendIpConfiguration $fe -BackendAddressPool $be -Probe $hp -Protocol TCP -FrontendPort 80 -BackendPort 80 ` -IdleTimeoutInMinutes 15 -EnableTcpReset These can be manually changed on the load balancing object as well $lb = Get-AzLoadBalancer -Name \"myLoadBalancer\" -ResourceGroup \"myResourceGroup\" $lb . LoadBalancingRules [ 0 ]. IdleTimeoutInMinutes = '15' $lb . LoadBalancingRules [ 0 ]. EnableTcpReset = 'true' Set-AzLoadBalancer -LoadBalancer $lb az network lb rule update -g $g -n MyLBRule --lb-name myLoadBalancer --idle-timeout 15 --enable-tcp-reset true","title":"Configure TCP reset"},{"location":"Cloud/Azure-Load-Balancer/#specify-affinity","text":"In the Azure Portal, affinity is specified in the Session persistence dropdown. In Azure PowerShell and CLI, the option is the load distribution named parameter - New-AzLoadBalancerRuleConfig -LoadDistribution - az network lb rule create --load-distribution Sources: Multiple frontends for Azure Load Balancer SNAT for outbound connections Azure Load Balancer Floating IP configuration AZ-103: p. 358","title":"Specify affinity"},{"location":"Cloud/Azure-Monitoring/","text":"A robust monitoring strategy implementing proactive notifications helps to increase uptime and optimize performance. Azure offers Azure Monitor and Azure Advisor . Azure Monitor brings a unified alerting experience to Azure, with a single pane of glass for interacting with metrics, the Activity Log , Log Analytics , service and resource health and service-specific insights. Alerts can be filtered by subscription (maximum of 5), resource group (maximum of 1), resource type (available selections depend on resources deployed to selected group) time range (past hour, past day, past week, and past 30 days), and other criteria. Azure Monitor can create alert rules that are built on target resources or resource type and that proactively notify you of the health of resources and can also leverage action groups that automate actions to take in certain conditions. Azure Advisor is a free, personalized guide to Azure best practices that provides recommendations to help you optimize resources. Azure Advisor offers personalized recommendations across 4 domains: High availability, Security, Performance, and Cost Feature Logs Metrics Retention Stored in Log Analytics (2 years) Stored in Monitor for 93 days, but metrics can be sent to Log Analytics and Storage accounts as well Properties Varying properties for each log, with support for rich data types such as date and time Fixed set of properties (or attributes): time, type, resource, value, and (optionally) dimensions. Data availability Triggered by an event, requiring time to process before they are available for querying Gathered at intervals and available for immediate querying. Virtual machines can be one of the most expensive resources in a cloud implementation, and there are several ways to reduce their cost Deallocate compute when not needed Delete unused virtual machines and allocate them only on demand Right-size VMs so that you don't overuse resources Advisor can also identify ExpressRoute circuits that have been \"Not Provisioned\" for more than 30 days Gateways that have been idle for more than 90 days There are two monitoring data streams: Metrics are the numerical time series data produced by resources and services within Azure. They are collected at 1-minute intervals, identified by a metric name and namespace (category). Metrics can be one-dimensional or have up to 10 dimensions. Metrics have the following properties: Time the value was collected Type of measurement made Resource associated with value Value Metrics can be stored in: Azure Monitor for 93 days Log Analytics for 2 years Storage account, where they are treated according to the retention policy and storage limits of the account. Logs come in various types Diagnostics logs (including resource logs and tenant logs ) are a type of log data that can be configured to send data to other locations, such as a Storage account or Log Analytics workspace . Diagnostics logs have to be enabled for each resource to be monitored through the Portal by enabling Diagnostics Settings, and not all resource types support diagnostic logs. Of those that do, not all resources support a retention policy or sending metric data. Azure Activity logs is a subscription level log that captures events that range from operational data (i.e. resource creation, deletion) to service health events for a subscription, but lacks resource-level detail.. Guest telemetry can relay logs from VMs with the use of diagnostics agents Log Analytics A Log Analytics workspace is a form of abstracting the process of log collection and is used to collect and aggregate logs. Like any other resource, it must be associated with a location and a resource group. Any Azure resource can only report logs to a single workspace, but Azure Monitor allows multiple workspaces to be queried simultaneously. The logs can be queried through Log Analytics or Monitor. Because a workspace is a resource, RBAC can be applied to control access to it. Log Analytics is based on Azure Data Explorer and uses Kusto . Log Analytics pricing is divided into data ingestion and data retention : - Under Pay-As-You-Go data ingestion the first 5 GB per month are free and further data is charged at a rate of $2.76/GB/month - Capacity Reservations offer a discount on Pay-as-you-go by charging a fixed amount per day ($219.52/day for 100 GB), with further discounts at higher tiers - Data retention is free for any amount of data up to 31 days, and 90 days for Azure Sentinel -enabled workspaces Operational Insights Log Analytics was previously named Operational Insights , which was named System Center Advisor prior to 2014. Application Insights Application Insights is a platfrom separate from Log Analytics which is intended to monitor web applications. Alerts Alerts can be created from the Alerts pane in the Monitor blade: Most resource blades also have Alerts in the resource menu under Monitoring. Alert rules , which are used to generate alerts, contain - Target resource , any Azure resource that generates signals, defines the scope and signals available for the alert. - Signal (i.e. metric or Activity Log) emitted by target resource. Signals are of 3 types: 1. Metrics 2. Log search queries 3. Activity logs - Conditional logic for alert combines the signal and a logical test to trigger alert. - Action Group determines what will happen when the alert is trigged. Action groups are themselves resources, and thus located in a subscription and resource group, and have: - Name - Short name is used to identify the Action Group in emails and notifications and is limited to 12 characters - Actions define the configuration for a specific action type. - Severity (0-4) Alerts can have 3 states: New and not reviewed Acknowledged issue is being actioned by an admin Closed issue that generated the alerts has been resolved and the alert has been marked as closedAlerts have many notification options, including email, SMS, mobile app, voice, and integration with automation. Actions A single action group can trigger multiple actions. Available types include: Email/SMS/Push/Voice, Azure Function, Logic App, Webhook, ITSM, and Automation Runbook IT Service Management Connector up to 10 ITSM actions can be configured with an ITSM connection Supported providers include ServiceNow, System Center Service Manager, Provance, and Cherwell Connect Azure to ITSM tools using ITSMC Webhooks Runbook runs in Azure Automation Service Runbooks The maximum number of alert notifications per hour: - Email: 60 - Voice: 12 - SMS: 12 VMs \"Virtual Machine Insights \" (or \"Azure Monitor for VMs\") is the successor to older monitoring workflow that used \"guest OS diagnostics\" in conjunction with Metrics Explorer. It requires a log analytics workspace . Diagnostic settings , conventionally, was the feature that would be enabled to allow Azure to collect metrics and logs from VMs, including event logs and performance counters. Two primary views: - Performance is a successor to the old Metrics Explorer - Map (originally \"Service Map\") Tasks Enable diagnostics on a VM Sources: - az vm diagnostics set Diagnostics log collection with a storage account Browse to the resource itself. Alternatively, open Azure Monitor and then the Diagnostics Settings blade. From there you can view all eligible resouce types and view status of log collection. $resource = Get-AzResource -Name $resourceName -ResourceGroupName $resourceGroupName $storage = Get-AzResource -Name $resourceName -ResourceGroupName $resourceGroupName Set-AzDiagnosticSetting -ResourceId $resource . ResourceId -StorageAccountId $storage . ResourceId -Enabled $true resourceId = $( az resource show -resource-group $resourceGroupName -name $resourceName --resource-type $resourceType --query id --output tsv ) az monitor diagnostic-settings create --name $diagnosticName --storage-account $storageId --resource $resouceId --resource-group $resourceGroup \\ --logs '[ { \"category\": <categoryName>, \"enabled\": true, \"retentionPolicy\": { \"days\": <numberOfDays>, \"enabled\": true } } ] ' Diagnostics log streaming to an Event Hub $rule = Get-AzServiceBusRule -ResourceGroup $resourceGroupName -Namespace $namespace -Topic $topic -Subscription $subscription -Name $ruleName Set-AzDiagnosticSetting -ResourceId $resource . ResourceId -ServiceBusRuleId $rule . Id -Enabled $true resourceId = $( az resource show -resource-group $resourceGroupName -name $resourceName --resource-type $resourceType --query id --output tsv ) az monitor diagnostic-settings create --name $diagnosticName --event-hub $eventHubName --event-hub-rule $eventHubRuleId --resource $resourceId \\ --logs '[{ \"category\": <categoryName>, \"enabled\": true }]' Diagnostics log collection in a Log Analytics workspace The PowerShell module that allows interaction with Log Analytics still refers to the service's old name . $workspace = Get-AzOperationalInsightsWorkspace -Name $logAnalyticsName -ResourceGroupName $g Set-AzDiagnosticSetting -ResourceId $r . ResourceId -WorkspaceId $workspace . ResourceId -Enabled $true az monitor diagnostic-settings create --name $diagnosticName --workspace $logAnalyticsName --resource $rid --resouce-group $g \\ --logs '[{ \"category\": <categoryName>, \"enabled\": true }]' Create an alert rule Sources: - Create, view, and manage activity log alerts by using Azure Monitor Create Log Analytics workspace Sources AZ-103: 1.2 AZ-104: 5.1 Azure Monitor for VMs","title":"Azure Monitoring"},{"location":"Cloud/Azure-Monitoring/#log-analytics","text":"A Log Analytics workspace is a form of abstracting the process of log collection and is used to collect and aggregate logs. Like any other resource, it must be associated with a location and a resource group. Any Azure resource can only report logs to a single workspace, but Azure Monitor allows multiple workspaces to be queried simultaneously. The logs can be queried through Log Analytics or Monitor. Because a workspace is a resource, RBAC can be applied to control access to it. Log Analytics is based on Azure Data Explorer and uses Kusto . Log Analytics pricing is divided into data ingestion and data retention : - Under Pay-As-You-Go data ingestion the first 5 GB per month are free and further data is charged at a rate of $2.76/GB/month - Capacity Reservations offer a discount on Pay-as-you-go by charging a fixed amount per day ($219.52/day for 100 GB), with further discounts at higher tiers - Data retention is free for any amount of data up to 31 days, and 90 days for Azure Sentinel -enabled workspaces","title":"Log Analytics"},{"location":"Cloud/Azure-Monitoring/#operational-insights","text":"Log Analytics was previously named Operational Insights , which was named System Center Advisor prior to 2014.","title":"Operational Insights"},{"location":"Cloud/Azure-Monitoring/#application-insights","text":"Application Insights is a platfrom separate from Log Analytics which is intended to monitor web applications.","title":"Application Insights"},{"location":"Cloud/Azure-Monitoring/#alerts","text":"Alerts can be created from the Alerts pane in the Monitor blade: Most resource blades also have Alerts in the resource menu under Monitoring. Alert rules , which are used to generate alerts, contain - Target resource , any Azure resource that generates signals, defines the scope and signals available for the alert. - Signal (i.e. metric or Activity Log) emitted by target resource. Signals are of 3 types: 1. Metrics 2. Log search queries 3. Activity logs - Conditional logic for alert combines the signal and a logical test to trigger alert. - Action Group determines what will happen when the alert is trigged. Action groups are themselves resources, and thus located in a subscription and resource group, and have: - Name - Short name is used to identify the Action Group in emails and notifications and is limited to 12 characters - Actions define the configuration for a specific action type. - Severity (0-4) Alerts can have 3 states: New and not reviewed Acknowledged issue is being actioned by an admin Closed issue that generated the alerts has been resolved and the alert has been marked as closedAlerts have many notification options, including email, SMS, mobile app, voice, and integration with automation.","title":"Alerts"},{"location":"Cloud/Azure-Monitoring/#actions","text":"A single action group can trigger multiple actions. Available types include: Email/SMS/Push/Voice, Azure Function, Logic App, Webhook, ITSM, and Automation Runbook IT Service Management Connector up to 10 ITSM actions can be configured with an ITSM connection Supported providers include ServiceNow, System Center Service Manager, Provance, and Cherwell Connect Azure to ITSM tools using ITSMC Webhooks Runbook runs in Azure Automation Service Runbooks The maximum number of alert notifications per hour: - Email: 60 - Voice: 12 - SMS: 12","title":"Actions"},{"location":"Cloud/Azure-Monitoring/#vms","text":"\"Virtual Machine Insights \" (or \"Azure Monitor for VMs\") is the successor to older monitoring workflow that used \"guest OS diagnostics\" in conjunction with Metrics Explorer. It requires a log analytics workspace . Diagnostic settings , conventionally, was the feature that would be enabled to allow Azure to collect metrics and logs from VMs, including event logs and performance counters. Two primary views: - Performance is a successor to the old Metrics Explorer - Map (originally \"Service Map\")","title":"VMs"},{"location":"Cloud/Azure-Monitoring/#tasks","text":"","title":"Tasks"},{"location":"Cloud/Azure-Monitoring/#enable-diagnostics-on-a-vm","text":"Sources: - az vm diagnostics set","title":"Enable diagnostics on a VM"},{"location":"Cloud/Azure-Monitoring/#diagnostics-log-collection-with-a-storage-account","text":"Browse to the resource itself. Alternatively, open Azure Monitor and then the Diagnostics Settings blade. From there you can view all eligible resouce types and view status of log collection. $resource = Get-AzResource -Name $resourceName -ResourceGroupName $resourceGroupName $storage = Get-AzResource -Name $resourceName -ResourceGroupName $resourceGroupName Set-AzDiagnosticSetting -ResourceId $resource . ResourceId -StorageAccountId $storage . ResourceId -Enabled $true resourceId = $( az resource show -resource-group $resourceGroupName -name $resourceName --resource-type $resourceType --query id --output tsv ) az monitor diagnostic-settings create --name $diagnosticName --storage-account $storageId --resource $resouceId --resource-group $resourceGroup \\ --logs '[ { \"category\": <categoryName>, \"enabled\": true, \"retentionPolicy\": { \"days\": <numberOfDays>, \"enabled\": true } } ] '","title":"Diagnostics log collection with a storage account"},{"location":"Cloud/Azure-Monitoring/#diagnostics-log-streaming-to-an-event-hub","text":"$rule = Get-AzServiceBusRule -ResourceGroup $resourceGroupName -Namespace $namespace -Topic $topic -Subscription $subscription -Name $ruleName Set-AzDiagnosticSetting -ResourceId $resource . ResourceId -ServiceBusRuleId $rule . Id -Enabled $true resourceId = $( az resource show -resource-group $resourceGroupName -name $resourceName --resource-type $resourceType --query id --output tsv ) az monitor diagnostic-settings create --name $diagnosticName --event-hub $eventHubName --event-hub-rule $eventHubRuleId --resource $resourceId \\ --logs '[{ \"category\": <categoryName>, \"enabled\": true }]'","title":"Diagnostics log streaming to an Event Hub"},{"location":"Cloud/Azure-Monitoring/#diagnostics-log-collection-in-a-log-analytics-workspace","text":"The PowerShell module that allows interaction with Log Analytics still refers to the service's old name . $workspace = Get-AzOperationalInsightsWorkspace -Name $logAnalyticsName -ResourceGroupName $g Set-AzDiagnosticSetting -ResourceId $r . ResourceId -WorkspaceId $workspace . ResourceId -Enabled $true az monitor diagnostic-settings create --name $diagnosticName --workspace $logAnalyticsName --resource $rid --resouce-group $g \\ --logs '[{ \"category\": <categoryName>, \"enabled\": true }]'","title":"Diagnostics log collection in a Log Analytics workspace"},{"location":"Cloud/Azure-Monitoring/#create-an-alert-rule","text":"Sources: - Create, view, and manage activity log alerts by using Azure Monitor","title":"Create an alert rule"},{"location":"Cloud/Azure-Monitoring/#create-log-analytics-workspace","text":"","title":"Create Log Analytics workspace"},{"location":"Cloud/Azure-Monitoring/#sources","text":"AZ-103: 1.2 AZ-104: 5.1 Azure Monitor for VMs","title":"Sources"},{"location":"Cloud/Azure-Policy/","text":"Sources - What is Azure Policy? - Azure Policy Samples Azure Policy is a service that can create, assign, and manage policies to enforce governance. Policy definitions, authored in JSON, implement policy by describing desired behavior for Azure resources when they are created or updated. AZ-103: p. 72 To implement policy, a policy definition is created first, then a policy assignment assigns it to a scope. Policy definitions can be packaged together using initiative definitions and applied to a scope using initiative assignments RBAC roles deny by default and allow explicitly . But Azure Policy allows by default and denies explicitly Policies can be applied at the management group , subscription , or resource group scope, with all child resources and resource groups being affected. Effects Sources: - Understand Azure Policy effects Every policy definition has a single effect , which includes: Audit : create a warning event in the log Modify : used to add, update, or remove properties or tags on a resource during creation or update. Append AuditIfNotExists Deny DeployIfNotExists Disabled The order of evaluation of effects is: Disabled, Append, Deny, Audit (\"DADA\") Schema Sources: - AZ-103: p.17 { \"mode\" : \"all\" , \"policyRule\" : { \"if\" : { \"allOf\" : [ { \"field\" : \"type\" , \"equals\" : \"Microsoft.Compute/virtualMachines\" }, { \"not\" : { \"field\" : \"Microsoft.Compute/virtualMachines/sku.name\" , \"in\" : \"[parameters('listOfAllowedSKUs')]\" } } ] }, \"then\" : { \"effect\" : \"deny\" } }, \"parameters\" : { \"listOfAllowedSKUs\" : { \"type\" : \"array\" , \"metadata\" : { \"displayName\" : \"Allowed VM SKUs\" , \"description\" : \"The list of allowed SKUs for virtual machines.\" , \"strongType\" : \"vmSKUs\" } } } } Schema { \"id\" : \"https://schema.management.azure.com/schemas/2018-05-01/policyDefinition.json#\" , \"$schema\" : \"http://json-schema.org/draft-04/schema#\" , \"title\" : \"Policy Definition\" , \"description\" : \"This schema defines Azure resource policy definition, please see https://azure.microsoft.com/en-us/documentation/articles/resource-manager-policy/ for more details.\" , \"type\" : \"object\" , \"properties\" : { \"if\" : { \"oneOf\" : [ { \"$ref\" : \"#/definitions/condition\" }, { \"$ref\" : \"#/definitions/operatorNot\" }, { \"$ref\" : \"#/definitions/operatorAnyOf\" }, { \"$ref\" : \"#/definitions/operatorAllOf\" } ] }, \"then\" : { \"type\" : \"object\" , \"properties\" : { \"effect\" : { \"type\" : \"string\" , \"enum\" : [ \"append\" , \"audit\" , \"auditIfNotExists\" , \"deny\" , \"deployIfNotExists\" ] }, \"details\" : { \"oneOf\" : [ { \"$ref\" : \"#/definitions/ifNotExistsDetails\" }, { \"$ref\" : \"#/definitions/appendDetails\" } ] } }, \"required\" : [ \"effect\" ], \"additionalProperties\" : false } }, \"required\" : [ \"if\" , \"then\" ], \"additionalProperties\" : false , \"definitions\" : { \"appendDetails\" : { \"type\" : \"array\" , \"items\" : { \"properties\" : { \"field\" : { \"type\" : \"string\" }, \"value\" : {} }, \"required\" : [ \"field\" , \"value\" ], \"additionalProperties\" : false }, \"minItems\" : 1 , \"additionalItems\" : false }, \"ifNotExistsDetails\" : { \"type\" : \"object\" , \"properties\" : { \"type\" : { \"type\" : \"string\" }, \"name\" : { \"type\" : \"string\" }, \"resourceGroupName\" : { \"type\" : \"string\" }, \"existenceScope\" : { \"type\" : \"string\" , \"enum\" : [ \"resourceGroup\" , \"subscription\" ] }, \"roleDefinitionIds\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" } }, \"existenceCondition\" : { \"oneOf\" : [ { \"$ref\" : \"#/definitions/condition\" }, { \"$ref\" : \"#/definitions/operatorNot\" }, { \"$ref\" : \"#/definitions/operatorAnyOf\" }, { \"$ref\" : \"#/definitions/operatorAllOf\" } ] }, \"deployment\" : { \"type\" : \"object\" , \"properties\" : { \"properties\" : { \"$ref\" : \"https://schema.management.azure.com/schemas/2018-05-01/Microsoft.Resources.json#/definitions/DeploymentProperties\" } } } }, \"required\" : [ \"type\" ], \"additionalProperties\" : false }, \"condition\" : { \"allOf\" : [ { \"oneOf\" : [ { \"properties\" : { \"source\" : { \"type\" : \"string\" } }, \"required\" : [ \"source\" ] }, { \"properties\" : { \"field\" : { \"type\" : \"string\" } }, \"required\" : [ \"field\" ] } ] }, { \"oneOf\" : [ { \"properties\" : { \"equals\" : { \"type\" : \"string\" } }, \"required\" : [ \"equals\" ] }, { \"properties\" : { \"notEquals\" : { \"type\" : \"string\" } }, \"required\" : [ \"notEquals\" ] }, { \"properties\" : { \"like\" : { \"type\" : \"string\" } }, \"required\" : [ \"like\" ] }, { \"properties\" : { \"notLike\" : { \"type\" : \"string\" } }, \"required\" : [ \"notLike\" ] }, { \"properties\" : { \"contains\" : { \"type\" : \"string\" } }, \"required\" : [ \"contains\" ] }, { \"properties\" : { \"notContains\" : { \"type\" : \"string\" } }, \"required\" : [ \"notContains\" ] }, { \"properties\" : { \"in\" : { \"oneOf\" : [ { \"type\" : \"array\" }, { \"type\" : \"string\" } ] } }, \"required\" : [ \"in\" ] }, { \"properties\" : { \"notIn\" : { \"oneOf\" : [ { \"type\" : \"array\" }, { \"type\" : \"string\" } ] } }, \"required\" : [ \"notIn\" ] }, { \"properties\" : { \"containsKey\" : { \"type\" : \"string\" } }, \"required\" : [ \"containsKey\" ] }, { \"properties\" : { \"notContainsKey\" : { \"type\" : \"string\" } }, \"required\" : [ \"notContainsKey\" ] }, { \"properties\" : { \"match\" : { \"type\" : \"string\" } }, \"required\" : [ \"match\" ] }, { \"properties\" : { \"notMatch\" : { \"type\" : \"string\" } }, \"required\" : [ \"notMatch\" ] }, { \"properties\" : { \"exists\" : { \"type\" : \"string\" } }, \"required\" : [ \"exists\" ] } ] } ] }, \"operatorNot\" : { \"properties\" : { \"not\" : { \"oneOf\" : [ { \"$ref\" : \"#/definitions/condition\" }, { \"$ref\" : \"#/definitions/operatorNot\" }, { \"$ref\" : \"#/definitions/operatorAnyOf\" }, { \"$ref\" : \"#/definitions/operatorAllOf\" } ] } }, \"required\" : [ \"not\" ], \"additionalProperties\" : false }, \"operatorAnyOf\" : { \"properties\" : { \"anyOf\" : { \"type\" : \"array\" , \"minItems\" : 1 , \"items\" : { \"oneOf\" : [ { \"$ref\" : \"#/definitions/condition\" }, { \"$ref\" : \"#/definitions/operatorNot\" }, { \"$ref\" : \"#/definitions/operatorAnyOf\" }, { \"$ref\" : \"#/definitions/operatorAllOf\" } ] } } }, \"required\" : [ \"anyOf\" ], \"additionalProperties\" : false }, \"operatorAllOf\" : { \"properties\" : { \"allOf\" : { \"type\" : \"array\" , \"minItems\" : 1 , \"items\" : { \"oneOf\" : [ { \"$ref\" : \"#/definitions/condition\" }, { \"$ref\" : \"#/definitions/operatorNot\" }, { \"$ref\" : \"#/definitions/operatorAnyOf\" }, { \"$ref\" : \"#/definitions/operatorAllOf\" } ] } } }, \"required\" : [ \"allOf\" ], \"additionalProperties\" : false } } }","title":"Azure Policy"},{"location":"Cloud/Azure-Policy/#effects","text":"Sources: - Understand Azure Policy effects Every policy definition has a single effect , which includes: Audit : create a warning event in the log Modify : used to add, update, or remove properties or tags on a resource during creation or update. Append AuditIfNotExists Deny DeployIfNotExists Disabled The order of evaluation of effects is: Disabled, Append, Deny, Audit (\"DADA\")","title":"Effects"},{"location":"Cloud/Azure-Policy/#schema","text":"Sources: - AZ-103: p.17 { \"mode\" : \"all\" , \"policyRule\" : { \"if\" : { \"allOf\" : [ { \"field\" : \"type\" , \"equals\" : \"Microsoft.Compute/virtualMachines\" }, { \"not\" : { \"field\" : \"Microsoft.Compute/virtualMachines/sku.name\" , \"in\" : \"[parameters('listOfAllowedSKUs')]\" } } ] }, \"then\" : { \"effect\" : \"deny\" } }, \"parameters\" : { \"listOfAllowedSKUs\" : { \"type\" : \"array\" , \"metadata\" : { \"displayName\" : \"Allowed VM SKUs\" , \"description\" : \"The list of allowed SKUs for virtual machines.\" , \"strongType\" : \"vmSKUs\" } } } } Schema { \"id\" : \"https://schema.management.azure.com/schemas/2018-05-01/policyDefinition.json#\" , \"$schema\" : \"http://json-schema.org/draft-04/schema#\" , \"title\" : \"Policy Definition\" , \"description\" : \"This schema defines Azure resource policy definition, please see https://azure.microsoft.com/en-us/documentation/articles/resource-manager-policy/ for more details.\" , \"type\" : \"object\" , \"properties\" : { \"if\" : { \"oneOf\" : [ { \"$ref\" : \"#/definitions/condition\" }, { \"$ref\" : \"#/definitions/operatorNot\" }, { \"$ref\" : \"#/definitions/operatorAnyOf\" }, { \"$ref\" : \"#/definitions/operatorAllOf\" } ] }, \"then\" : { \"type\" : \"object\" , \"properties\" : { \"effect\" : { \"type\" : \"string\" , \"enum\" : [ \"append\" , \"audit\" , \"auditIfNotExists\" , \"deny\" , \"deployIfNotExists\" ] }, \"details\" : { \"oneOf\" : [ { \"$ref\" : \"#/definitions/ifNotExistsDetails\" }, { \"$ref\" : \"#/definitions/appendDetails\" } ] } }, \"required\" : [ \"effect\" ], \"additionalProperties\" : false } }, \"required\" : [ \"if\" , \"then\" ], \"additionalProperties\" : false , \"definitions\" : { \"appendDetails\" : { \"type\" : \"array\" , \"items\" : { \"properties\" : { \"field\" : { \"type\" : \"string\" }, \"value\" : {} }, \"required\" : [ \"field\" , \"value\" ], \"additionalProperties\" : false }, \"minItems\" : 1 , \"additionalItems\" : false }, \"ifNotExistsDetails\" : { \"type\" : \"object\" , \"properties\" : { \"type\" : { \"type\" : \"string\" }, \"name\" : { \"type\" : \"string\" }, \"resourceGroupName\" : { \"type\" : \"string\" }, \"existenceScope\" : { \"type\" : \"string\" , \"enum\" : [ \"resourceGroup\" , \"subscription\" ] }, \"roleDefinitionIds\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" } }, \"existenceCondition\" : { \"oneOf\" : [ { \"$ref\" : \"#/definitions/condition\" }, { \"$ref\" : \"#/definitions/operatorNot\" }, { \"$ref\" : \"#/definitions/operatorAnyOf\" }, { \"$ref\" : \"#/definitions/operatorAllOf\" } ] }, \"deployment\" : { \"type\" : \"object\" , \"properties\" : { \"properties\" : { \"$ref\" : \"https://schema.management.azure.com/schemas/2018-05-01/Microsoft.Resources.json#/definitions/DeploymentProperties\" } } } }, \"required\" : [ \"type\" ], \"additionalProperties\" : false }, \"condition\" : { \"allOf\" : [ { \"oneOf\" : [ { \"properties\" : { \"source\" : { \"type\" : \"string\" } }, \"required\" : [ \"source\" ] }, { \"properties\" : { \"field\" : { \"type\" : \"string\" } }, \"required\" : [ \"field\" ] } ] }, { \"oneOf\" : [ { \"properties\" : { \"equals\" : { \"type\" : \"string\" } }, \"required\" : [ \"equals\" ] }, { \"properties\" : { \"notEquals\" : { \"type\" : \"string\" } }, \"required\" : [ \"notEquals\" ] }, { \"properties\" : { \"like\" : { \"type\" : \"string\" } }, \"required\" : [ \"like\" ] }, { \"properties\" : { \"notLike\" : { \"type\" : \"string\" } }, \"required\" : [ \"notLike\" ] }, { \"properties\" : { \"contains\" : { \"type\" : \"string\" } }, \"required\" : [ \"contains\" ] }, { \"properties\" : { \"notContains\" : { \"type\" : \"string\" } }, \"required\" : [ \"notContains\" ] }, { \"properties\" : { \"in\" : { \"oneOf\" : [ { \"type\" : \"array\" }, { \"type\" : \"string\" } ] } }, \"required\" : [ \"in\" ] }, { \"properties\" : { \"notIn\" : { \"oneOf\" : [ { \"type\" : \"array\" }, { \"type\" : \"string\" } ] } }, \"required\" : [ \"notIn\" ] }, { \"properties\" : { \"containsKey\" : { \"type\" : \"string\" } }, \"required\" : [ \"containsKey\" ] }, { \"properties\" : { \"notContainsKey\" : { \"type\" : \"string\" } }, \"required\" : [ \"notContainsKey\" ] }, { \"properties\" : { \"match\" : { \"type\" : \"string\" } }, \"required\" : [ \"match\" ] }, { \"properties\" : { \"notMatch\" : { \"type\" : \"string\" } }, \"required\" : [ \"notMatch\" ] }, { \"properties\" : { \"exists\" : { \"type\" : \"string\" } }, \"required\" : [ \"exists\" ] } ] } ] }, \"operatorNot\" : { \"properties\" : { \"not\" : { \"oneOf\" : [ { \"$ref\" : \"#/definitions/condition\" }, { \"$ref\" : \"#/definitions/operatorNot\" }, { \"$ref\" : \"#/definitions/operatorAnyOf\" }, { \"$ref\" : \"#/definitions/operatorAllOf\" } ] } }, \"required\" : [ \"not\" ], \"additionalProperties\" : false }, \"operatorAnyOf\" : { \"properties\" : { \"anyOf\" : { \"type\" : \"array\" , \"minItems\" : 1 , \"items\" : { \"oneOf\" : [ { \"$ref\" : \"#/definitions/condition\" }, { \"$ref\" : \"#/definitions/operatorNot\" }, { \"$ref\" : \"#/definitions/operatorAnyOf\" }, { \"$ref\" : \"#/definitions/operatorAllOf\" } ] } } }, \"required\" : [ \"anyOf\" ], \"additionalProperties\" : false }, \"operatorAllOf\" : { \"properties\" : { \"allOf\" : { \"type\" : \"array\" , \"minItems\" : 1 , \"items\" : { \"oneOf\" : [ { \"$ref\" : \"#/definitions/condition\" }, { \"$ref\" : \"#/definitions/operatorNot\" }, { \"$ref\" : \"#/definitions/operatorAnyOf\" }, { \"$ref\" : \"#/definitions/operatorAllOf\" } ] } } }, \"required\" : [ \"allOf\" ], \"additionalProperties\" : false } } }","title":"Schema"},{"location":"Cloud/Azure-Storage/","text":"DNS Storage account access SAS token SAS tokens are generated from a storage account key; if the key is invalidated then so are all SAS tokens generated from it. The user delegation SAS token itself is meant to be appended to the end of the blob's URI. CloudSkills : 40:00 Tasks Add endpoints to Azure File Sync Group Register a server to the sync group by installing Azure File Sync agent on each server. When installing, you sign in with your subscription's credentials, then register the server by providing the Subscription, Resource Group, and Storage Sync Service names. Click Add Server Endpoint . This will display a dropdown of all servers with the agent installed and associated with the sync service. Upload blob Azure CLI az storage blob upload --container-name $containerName --account-name $accountName --account-key $accountKey --file $file --name $blobName Azure AzCopy AzCopy copy localFilePath https://storageAccount.blob.core.windows.net/destinationContainer/path/to/blob?SASToken Download a blob from a container Azure AzCopy AzCopy copy https://storageAccount.blob.core.windows.net/sourceContainer/path/to/blob?SASToken localFilePath Copy a blob from one container to another Azure AzCopy AzCopy /Source:https://sourceblob.blob.core.windows.net/sourcecontainer/ /Dest:https://deststorage.blob.core.windows.net/destcontainer/ /SourceKey:sourcekey /DestKey:destkey /Pattern:disk1.vhd $blobCopyState = Start-AzStorageBlobCopy -SrcBlob $blobName -SrcContainer $srcContainer -Context $srcContext -DestContainer $destContainer -DestBlob $vhdName -DestContext $destContext $srcStorageKey = Get-AzStorageAccountKey -ResourceGroupName $sourceg -Name $srcStorageAccount $destStorageKey = Get-AzStorageAccountKey -ResourceGroupName $destg -Name $destStorageAccount $srcContext = New-AzStorageContext -StorageAccountName $srcStorageAccount -StorageAccountKey $srcStorageKey . Value [ 0 ] $destContext = New-AzStorageContext -StorageAccountNAme $destStorageAccount -StorageAccountKey $destStorageKey . Value [ 0 ] # Create new container in destination account New-AzStorageContainer -Name $destContainer -Context $destContext # Make the copy $copiedBlob = Start-AzStorageBlobCopy -SrcBlob $blobName -SrcContainer $srcContainer -Context $srcContext -DestContainer $destContainer -DestBlob $blobName -DestContext $destContext az storage blob copy start --account-name $destStorageAccount --account-key $destStorageKey --destination-blob $blobName --source-account-name $srcStorageAccount --source-container $srcContainer --source-blob $blobName --source-account-key $srcStorageKey Monitor progress of the async blob copy $copiedBlob | Get-AzStorageBlobCopyState az storage blob show --account-name $destStorageAccount --account-key $destStorageKey --container-name $destContainer --name $blobName Create SAS token $storageKey = Get-AzStorageAccountKey -ResourceGroupName $g -Name $accountName $context = New-AzStorageContext -StorageAccountName $accountName -StorageAccountKey $storageKey [ 0 ]. Value $startTime = Get-Date $endTime = $startTime . AddHours ( 4 ) New-AzStorageBlobSASToken -Container $container -Blob $blob -Permission \"rwd\" -StartTime $startTime -ExpiryTime $startTime . AddHours ( 4 ) -Context $context az storage blob generate-sas --account-name \"storageAccount\" --account-key $storageAccountKey --container-name $container --name $blobName --permissions r --expiry \"2019-05-31\" Create container $storageKey = Get-AzStorageAccountKey -Name $storageAccount -ResourceGroupName $resourceGroup $context = New-AzStorageContext -StorageAccountName $storageAccount -StorageAccountKey $storageKey . Value [ 0 ] Set-AzCurrentStorageAccount -Context $context New-AzStorageContainer -Name $container -Permission Off Upload file as blob to new container Azure PowerShell Set-AzStorageBlobContent -File $localFile -Container $container -Blob $blobName Azure CLI az storage container create --account-name $storageaccount --name $containername --public-access off Ensure App Services, backup vault, and event hub have access to a storage account Get-AzVirtualNetwork -ResourceGroupName RG01 -Name VNET01 | Set-AzVirtualNetworkSubnetConfig -Name VSUBNET01 -AddressPrefix 10 . 0 . 0 . 0 / 24 -ServiceEndpoint Microsoft . Storage | Set-AzVirtualNetwork $subnet = Get-AzVirtualNetwork -ResourceGroupName RG01 -Name VNET01 | Get-AzVirtualNetworkSubnetConfig -Name VSUBNET01 Add-AzStorageAccountNetworkRule -ResourceGroupName VNET01 -Name Storage01 -VirtualNetworkResourceId $subnet . Id Update-AzStorageAccountNetworkRuleSet -ResourceGroupName RG01 -Name STORAGE01 -Bypass Azure . Services Troubleshoot Azure File Sync Several procedures to be used when Azure File Sync is having issues Collect logs to troubleshoot issues with Azure File Sync agent installation StorageSyncAgent.msi /l*v AFSInstaller.log Remove the server from registered sync group Error message \"This server is already registered during registration\" Import-Module \"C:\\Program Files\\Azure\\StorageSyncAgent\\StorageSync.Management.ServerCmdlets.dll\" Reset-StorageSyncServer Monitoring using Log Analytics Access Activity Log data (Portal) 1. Find Management + Governance in All Services 2. Open Activity Log 3. Click Logs icon at top of Activity Log view to select an existing Log Analytics (OMS) workspace or create a new one Storage account endpoints Virtual network service endpoint Sources - AZ-103 p. 112 - Configure Azure Storage firewalls and virtual networks Specify Microsoft.Storage in the service endpoint settings of the VNet subnet Configure which VNets can access a particular storage account Display virtual network rules Azure PowerShell Get-AzStorageAccountNetworkRuleSet -ResourceGroupName $rgName -AccountName $n | Select-Object VirtualNetworkRules Azure CLI az storage account network-rule list -g $rgName -n $n --query virtualNetworkRules Enable service endpoint for Azure Storage on an existing virtual network and subnet. Azure PowerShell Get-AzVirtualNetwork -ResourceGroupName $rgName -Name $n | Set-AzVirtualNetworkSubnetConfig -Name \"mysubnet\" -AddressPrefix \"10.0.0.0/24\" -ServiceEndpoint \"Microsoft.Storage\" | Set-AzVirtualNetwork Azure CLI az network vnet subnet update -g $rgName --vnet-name $n --name \"mysubnet\" --service-endpoints \"Microsoft.Storage\" Add network rule for VNet and subnet Azure PowerShell $subnet = Get-AzVirtualNetwork -ResourceGroupName $ng -Name $nn | Get-AzVirtualNetworkSubnetConfig -Name \"mysubnet\" Add-AzStorageAccountNetworkRule -ResourceGroupName $sg -Name $sn -VirtualNetworkResourceId $subnet . Id Azure CLI subnetid = $( az network vnet subnet show -g $ng --vnet-name $nn -n \"mysubnet\" --query id --output tsv ) az storage account network-rule add -g $sg -n $sn --subnet $subnetid Remove network rule ```powershell $subnet = Get-AzVirtualNetwork -ResourceGroupName $ng -Name $nn | Get-AzVirtualNetworkSubnetConfig -Name \"mysubnet\" Remove-AzStorageAccountNetworkRule -ResourceGroupName $sg -Name $sn -VirtualNetworkResourceId $subnet.Id ``` Bypass network rules to allow access for Azure services like Event Hub and Recovery Services Vault Azure PowerShell # Display exceptions for the storage account network rules Get-AzStorageAccountNetworkRuleSet -ResourceGroupName $g -Name $n | Select-Object Bypass # Configure exceptions to storage account network rules Update-AzStorageAccountNetworkRuleSet -ResourceGroupName $g -Name $n -Bypass AzureServices , Metrics , Logging Azure CLI # Display exceptions for the storage account network rules az storage account show -g $g -n $n --query networkRuleSet.bypass # Configure exceptions to storage account network rules az storage account update -g $g -n $n --bypass Logging Metrics AzureServices Configure Azure Storage firewalls and virtual networks AZ-103: p. 107, 114, 127","title":"Azure Storage"},{"location":"Cloud/Azure-Storage/#dns","text":"","title":"DNS"},{"location":"Cloud/Azure-Storage/#storage-account-access","text":"","title":"Storage account access"},{"location":"Cloud/Azure-Storage/#sas-token","text":"SAS tokens are generated from a storage account key; if the key is invalidated then so are all SAS tokens generated from it. The user delegation SAS token itself is meant to be appended to the end of the blob's URI. CloudSkills : 40:00","title":"SAS token"},{"location":"Cloud/Azure-Storage/#tasks","text":"","title":"Tasks"},{"location":"Cloud/Azure-Storage/#add-endpoints-to-azure-file-sync-group","text":"Register a server to the sync group by installing Azure File Sync agent on each server. When installing, you sign in with your subscription's credentials, then register the server by providing the Subscription, Resource Group, and Storage Sync Service names. Click Add Server Endpoint . This will display a dropdown of all servers with the agent installed and associated with the sync service. Upload blob Azure CLI az storage blob upload --container-name $containerName --account-name $accountName --account-key $accountKey --file $file --name $blobName Azure AzCopy AzCopy copy localFilePath https://storageAccount.blob.core.windows.net/destinationContainer/path/to/blob?SASToken Download a blob from a container Azure AzCopy AzCopy copy https://storageAccount.blob.core.windows.net/sourceContainer/path/to/blob?SASToken localFilePath Copy a blob from one container to another Azure AzCopy AzCopy /Source:https://sourceblob.blob.core.windows.net/sourcecontainer/ /Dest:https://deststorage.blob.core.windows.net/destcontainer/ /SourceKey:sourcekey /DestKey:destkey /Pattern:disk1.vhd $blobCopyState = Start-AzStorageBlobCopy -SrcBlob $blobName -SrcContainer $srcContainer -Context $srcContext -DestContainer $destContainer -DestBlob $vhdName -DestContext $destContext $srcStorageKey = Get-AzStorageAccountKey -ResourceGroupName $sourceg -Name $srcStorageAccount $destStorageKey = Get-AzStorageAccountKey -ResourceGroupName $destg -Name $destStorageAccount $srcContext = New-AzStorageContext -StorageAccountName $srcStorageAccount -StorageAccountKey $srcStorageKey . Value [ 0 ] $destContext = New-AzStorageContext -StorageAccountNAme $destStorageAccount -StorageAccountKey $destStorageKey . Value [ 0 ] # Create new container in destination account New-AzStorageContainer -Name $destContainer -Context $destContext # Make the copy $copiedBlob = Start-AzStorageBlobCopy -SrcBlob $blobName -SrcContainer $srcContainer -Context $srcContext -DestContainer $destContainer -DestBlob $blobName -DestContext $destContext az storage blob copy start --account-name $destStorageAccount --account-key $destStorageKey --destination-blob $blobName --source-account-name $srcStorageAccount --source-container $srcContainer --source-blob $blobName --source-account-key $srcStorageKey","title":"Add endpoints to Azure File Sync Group"},{"location":"Cloud/Azure-Storage/#monitor-progress-of-the-async-blob-copy","text":"$copiedBlob | Get-AzStorageBlobCopyState az storage blob show --account-name $destStorageAccount --account-key $destStorageKey --container-name $destContainer --name $blobName","title":"Monitor progress of the async blob copy"},{"location":"Cloud/Azure-Storage/#create-sas-token","text":"$storageKey = Get-AzStorageAccountKey -ResourceGroupName $g -Name $accountName $context = New-AzStorageContext -StorageAccountName $accountName -StorageAccountKey $storageKey [ 0 ]. Value $startTime = Get-Date $endTime = $startTime . AddHours ( 4 ) New-AzStorageBlobSASToken -Container $container -Blob $blob -Permission \"rwd\" -StartTime $startTime -ExpiryTime $startTime . AddHours ( 4 ) -Context $context az storage blob generate-sas --account-name \"storageAccount\" --account-key $storageAccountKey --container-name $container --name $blobName --permissions r --expiry \"2019-05-31\"","title":"Create SAS token"},{"location":"Cloud/Azure-Storage/#create-container","text":"$storageKey = Get-AzStorageAccountKey -Name $storageAccount -ResourceGroupName $resourceGroup $context = New-AzStorageContext -StorageAccountName $storageAccount -StorageAccountKey $storageKey . Value [ 0 ] Set-AzCurrentStorageAccount -Context $context New-AzStorageContainer -Name $container -Permission Off Upload file as blob to new container Azure PowerShell Set-AzStorageBlobContent -File $localFile -Container $container -Blob $blobName Azure CLI az storage container create --account-name $storageaccount --name $containername --public-access off","title":"Create container"},{"location":"Cloud/Azure-Storage/#ensure-app-services-backup-vault-and-event-hub-have-access-to-a-storage-account","text":"Get-AzVirtualNetwork -ResourceGroupName RG01 -Name VNET01 | Set-AzVirtualNetworkSubnetConfig -Name VSUBNET01 -AddressPrefix 10 . 0 . 0 . 0 / 24 -ServiceEndpoint Microsoft . Storage | Set-AzVirtualNetwork $subnet = Get-AzVirtualNetwork -ResourceGroupName RG01 -Name VNET01 | Get-AzVirtualNetworkSubnetConfig -Name VSUBNET01 Add-AzStorageAccountNetworkRule -ResourceGroupName VNET01 -Name Storage01 -VirtualNetworkResourceId $subnet . Id Update-AzStorageAccountNetworkRuleSet -ResourceGroupName RG01 -Name STORAGE01 -Bypass Azure . Services","title":"Ensure App Services, backup vault, and event hub have access to a storage account"},{"location":"Cloud/Azure-Storage/#troubleshoot-azure-file-sync","text":"Several procedures to be used when Azure File Sync is having issues Collect logs to troubleshoot issues with Azure File Sync agent installation StorageSyncAgent.msi /l*v AFSInstaller.log Remove the server from registered sync group Error message \"This server is already registered during registration\" Import-Module \"C:\\Program Files\\Azure\\StorageSyncAgent\\StorageSync.Management.ServerCmdlets.dll\" Reset-StorageSyncServer","title":"Troubleshoot Azure File Sync"},{"location":"Cloud/Azure-Storage/#monitoring-using-log-analytics","text":"Access Activity Log data (Portal) 1. Find Management + Governance in All Services 2. Open Activity Log 3. Click Logs icon at top of Activity Log view to select an existing Log Analytics (OMS) workspace or create a new one","title":"Monitoring using Log Analytics"},{"location":"Cloud/Azure-Storage/#storage-account-endpoints","text":"","title":"Storage account endpoints"},{"location":"Cloud/Azure-Storage/#virtual-network-service-endpoint","text":"Sources - AZ-103 p. 112 - Configure Azure Storage firewalls and virtual networks Specify Microsoft.Storage in the service endpoint settings of the VNet subnet Configure which VNets can access a particular storage account Display virtual network rules Azure PowerShell Get-AzStorageAccountNetworkRuleSet -ResourceGroupName $rgName -AccountName $n | Select-Object VirtualNetworkRules Azure CLI az storage account network-rule list -g $rgName -n $n --query virtualNetworkRules Enable service endpoint for Azure Storage on an existing virtual network and subnet. Azure PowerShell Get-AzVirtualNetwork -ResourceGroupName $rgName -Name $n | Set-AzVirtualNetworkSubnetConfig -Name \"mysubnet\" -AddressPrefix \"10.0.0.0/24\" -ServiceEndpoint \"Microsoft.Storage\" | Set-AzVirtualNetwork Azure CLI az network vnet subnet update -g $rgName --vnet-name $n --name \"mysubnet\" --service-endpoints \"Microsoft.Storage\" Add network rule for VNet and subnet Azure PowerShell $subnet = Get-AzVirtualNetwork -ResourceGroupName $ng -Name $nn | Get-AzVirtualNetworkSubnetConfig -Name \"mysubnet\" Add-AzStorageAccountNetworkRule -ResourceGroupName $sg -Name $sn -VirtualNetworkResourceId $subnet . Id Azure CLI subnetid = $( az network vnet subnet show -g $ng --vnet-name $nn -n \"mysubnet\" --query id --output tsv ) az storage account network-rule add -g $sg -n $sn --subnet $subnetid Remove network rule ```powershell $subnet = Get-AzVirtualNetwork -ResourceGroupName $ng -Name $nn | Get-AzVirtualNetworkSubnetConfig -Name \"mysubnet\" Remove-AzStorageAccountNetworkRule -ResourceGroupName $sg -Name $sn -VirtualNetworkResourceId $subnet.Id ``` Bypass network rules to allow access for Azure services like Event Hub and Recovery Services Vault Azure PowerShell # Display exceptions for the storage account network rules Get-AzStorageAccountNetworkRuleSet -ResourceGroupName $g -Name $n | Select-Object Bypass # Configure exceptions to storage account network rules Update-AzStorageAccountNetworkRuleSet -ResourceGroupName $g -Name $n -Bypass AzureServices , Metrics , Logging Azure CLI # Display exceptions for the storage account network rules az storage account show -g $g -n $n --query networkRuleSet.bypass # Configure exceptions to storage account network rules az storage account update -g $g -n $n --bypass Logging Metrics AzureServices Configure Azure Storage firewalls and virtual networks AZ-103: p. 107, 114, 127","title":"Virtual network service endpoint"},{"location":"Cloud/Azure-VPN/","text":"Authentication Azure P2S VPN connections support several authentication methods: Azure AD authentication (Windows 10 only) RADIUS server VPN Gateway native certificate authentication The VPN gateway acts as a pass-through forwarding authentication messages between the connecting device and the RADIUS server. The RADIUS server can be deployed on-premises or in the Azure VNet, and two such servers can be deployed for high availability. If deployed on-premises, a S2S VPN to the site is required, and ExpressRoute is not usable. AD domain authentication requires a RADIUS server that integrates with the AD server. Tasks Create local network gateway $localnw = New-AzLocalNetworkGateway -Name LocalNetGW -ResourceGroupName ExamRefRG -Location \"West Europe\" -GatewayIpAddress \"53.50.123.195\" -AddressPrefix \"10.5.0.0/16\" Create VPN connection $gateway = Get-AzVirtualNetworkGateway -Name VPNGW1 -ResourceGroupName ExamRefRG $conn = New-AzVirtualNetworkGatewayConnection -Name OnPremConnection -ResourceGroupName ExamRefRG -Location 'West Europe' -VirtualNetworkGateway1 $gateway -LocalNetworkGateway2 $localnw -ConnectionType IPsec -SharedKey \"abc123\" Create a VPN Gateway $rg = ExamRefRG Create gateway subnet in VNet1 Gateway subnets are normal subnets with the name \"GatewaySubnet\" $vnet1 = Get-AzVirtualNetwork -Name VNet1 -ResourceGroupName $rg $vnet1 . Subnets += New-AzVirtualNetworkSubnetConfig -Name GatewaySubnet -AddressPrefix 10 . 1 . 1 . 0 / 27 $vnet1 = Set-AzVirtualNetwork -VirtualNetwork $vnet1 Create VPN gateway in VNet1 $gwpip = New-AzPublicIpAddress -Name VNet1-GW-IP -ResourceGroupName $rg -Location 'North Europe' -AllocationMethod Dynamic $gwsubnet = Get-AzVirtualNetworkSubnetConfig -Name 'GatewaySubnet' -VirtualNetwork $vnet1 $gwipconf = New-AzVirtualNetworkGatewayIpConfig -Name GwIPConf -Subnet $gwsubnet -PublicIpAddress $gwpip $vnet1gw = New-AzVirtualNetworkGateway -Name VNet1-GW -ResourceGroupName $rg -Location 'North Europe' -IpConfigurations $gwipconf -GatewayType Vpn -VpnType RouteBased -GatewaySku VpnGw1 Create gateway subnets in VNet2 and VNet3 az network vnet subnet create --name GatewaySubnet --vnet-name VNet1 --resource-group ExamRefRG --address-prefixes 10 .1.1.0/27 az network public-ip create --name VNet1-GW-IP --resource-group ExamRefRG --location NorthEurope az network vnet-gateway create --name VNet1-GW --resource-group ExamRefRG --gateway-type vpn --sku VpnGw1 --vpn-type RouteBased --vnet VNet1 --public-ip-addresses VNet1-GW-IP --location NorthEurope Create a VPN gateway and VNet peering Create gateway subnets in VNet2 and VNet3 $vnet2 = Get-AzVirtualNetwork -Name VNet2 -ResourceGroupName ExamRefRG $vnet2 . Subnets += New-AzVirtualNetworkSubnetConfig -Name GatewaySubnet -AddressPrefix 10 . 2 . 1 . 0 / 27 $vnet2 = Set-AzVirtualNetwork -VirtualNetwork $vnet2 $vnet3 = Get-AzVirtualNetwork -Name VNet3 -ResourceGroupName ExamRefRG $vnet3 . Subnets += New-AzVirtualNetworkSubnetConfig -Name GatewaySubnet -AddressPrefix 10 . 3 . 1 . 0 / 27 $vnet3 = Set-AzVirtualNetwork -VirtualNetwork $vnet3 Create VPN gateway in VNet2 $gwpip2 = New-AzPublicIpAddress -Name VNet2-GW-IP -ResourceGroupName ExamRefRG -Location $vnet2 . Location -AllocationMethod Dynamic $gwsubnet2 = Get-AzVirtualNetworkSubnetConfig -Name 'GatewaySubnet' -VirtualNetwork $vnet2 $gwipconf2 = New-AzVirtualNetworkGatewayIpConfig -Name GwIPConf2 -Subnet $gwsubnet2 -PublicIpAddress $gwpip2 $vnet2gw = New-AzVirtualNetworkGateway -Name VNet2-GW -ResourceGroupNAme ExamRefR -Location $vnet2 . Location -IpConfigurations $gwipconf2 -GatewayType Vpn -VpnType RouteBased -GatewaySku VpnGw1 Create VPN gateway in VNet3 $gwpip3 = New-AzPublicIpAddress -Name VNet3-GW-IP -ResourceGroupName ExamRefR -Location $vnet3 . Location -AllocationMethod Dynamic $gwsubnet3 = Get-AzVirtualNetworkSubnetConfig -Name 'GatewaySubnet' -VirtualNetwork $vnet3 $gwipconf3 = New-AzVirtualNetworkGatewayIpConfig -Name GwIPConf3 -Subnet $gwsubnet3 -PublicIpAddress $gwpip3 $vnet3gw = New-AzVirtualNetworkGateway -Name VNet3-GW -ResourceGroupNAme ExamRefRG -Location $vnet3 . Location -IpConfigurations $gwipconf3 -GatewayType Vpn -VpnType RouteBased -GatewaySku VpnGw1 Create connections New-AzVirtualNetworkGatewayConnection -Name VNet2-to-VNet3 -ResourceGroupName ExamRefRG -Location $vnet2 . Location -VirtualNetworkGateway1 $vnet2gw -VirtualNetworkGateway2 $vnet3gw -ConnectionType VNet2VNet -SharedKey \"secretkey123\" New-AzVirtualNetworkGatewayConnection -Name VNet3-to-VNet2 -ResourceGroupName ExamRefRG -Location $vnet3 . Location -VirtualNetworkGateway1 $vnet3gw -VirtualNetworkGateway2 $vnet2gw -ConnectionType VNet2VNet -SharedKey \"secretkey123\" Create gateway subnets in VNet2 and VNet3 az network vnet subnet create --name GatewaySubnet --vnet-name VNet2 --resource-group ExamRefRG --address-prefixes 10 .2.1.0/27 az network vnet subnet create --name GatewaySubnet --vnet-name VNet3 --resource-group ExamRefRG --address-prefixes 10 .3.1.0/27 Create public IP addresses for use by VPN gateways az network public-ip create --name VNet2-GW-IP --resource-group ExamRefRG --location NorthEurope az network public-ip create --name VNet3-GW-IP --resource-group ExamRefRG --location WestEurope Create VPN gateways in VNet2 and VNet 3 az network vnet-gateway create --name VNet2-GW --resource-group ExamRefRG --gateway-type vpn --sku VpnGw1 --vpn-type RouteBased --vnet VNet2 --public-ip-addresses VNet2-GW-IP --location NorthEurope az network vnet-gateway create --name VNet3-GW --resource-group ExamRefRG --gateway-type vpn --sku VpnGw1 --vpn-type RouteBased --vnet VNet3 --public-ip-addresses VNet3-GW-IP --location WestEurope Create connections between VPN gateways az network vpn-connection create --name VNet2-to-VNet3 --resource-group ExamRefRG --vnet-gateway1 VNet2-GW --vnet-gateway2 VNet3-GW --shared-key secretkey123 --location NorthEurope az network vpn-connection create --name VNet3-to-VNet2 --resource-group ExamRefRG --vnet-gateway1 VNet3-GW --vnet-gateway2 VNet2-GW --shared-key secretkey123 --location WestEurope Use VPN Troubleshoot Get the Network Watcher resource $nw = Get-AzResource | Where ResourceType -eq Microsoft . Network / networkWatchers -and Location -eq WestEurope $networkWatcher = Get-AzNetworkWatcher -Name $nw . Name -ResourceGroupName $nw . ResourceGroupName Get the connection to troubleshoot $connection = Get-AzVirtualNetworkGatewayConnection -Name Vnet1-to-Vnet2 -ResourceGroupName ExamRefRG Start VPN Troubleshoot Start-AzNetworkWatcherResourceTroubleshooting -NetworkWatcher $networkWatcher -TargetResourceId $connection . Id -StorageId $sa . Id -StoragePath \" $( $sa . PrimaryEndpoints . Blob )$( $sc . name ) \" Create a storage account and container for logs az storage account create --name examrefstorage --location westeurope --resource-group ExamRefRG --sku Standard_LRS az storage account keys list --resource-group ExamRefRG --account-name examrefstorage az storage container create --account-name examrefstorage --account-key { storageAccountKey } --name logs Start VPN Troubleshoot az network watcher troubleshooting start --resource-group ExamRefRG --resource Vnet1-to-Vnet2 --resource-type vpnConnection --storage-account examrefstorage --storage-path https://examrefstorage.blob.core.windows.net/logs --output json Create S2S VPN AZ-103: 395 $lgwip = 53.50.123.195 $key = \"abc123\" $lgw = New-AzLocalNetworkGateway -ResourceGroupName $g -Name $n -Location $l -GatewayIpAddress $lgwip -AddressPrefix \"10.5.0.0/16\" $vgw = Get-AzVirtualNetworkGateway -ResourceGroupNAme -Name New-AzVirtualNetworkGatewayConnection -ResourceGroupName $g -Name $n -Location $l -VirtualNetworkGateway1 $vgw -LocalNetworkGateway2 $lgw -ConnectionType IPsec -SharedKey $key az network local-gateway create --gateway-ip-address $lgwip --name LocalNetGW --resource-group ExamRefRG --local-address-prefixes 10 .5.0.0/16 az network vpn-connection create --name OnPremConnection --resource-group ExamRefRG --vnet-gateway1 VPNGW1 --location WestEurope --shared-key $key --local-gateway2 LocalNetGW Sources VPN Gateway design Connect Azure VPN gateways to multiple on-premises policy-based VPN devices About VPN Gateway configuration settings Highly available cross-premises and VNet-to-VNet connectivity ExpressRoute connectivity models Connect a computer to a virtual network using P2S and RADIUS authentication: PowerShell","title":"Azure VPN"},{"location":"Cloud/Azure-VPN/#authentication","text":"Azure P2S VPN connections support several authentication methods: Azure AD authentication (Windows 10 only) RADIUS server VPN Gateway native certificate authentication The VPN gateway acts as a pass-through forwarding authentication messages between the connecting device and the RADIUS server. The RADIUS server can be deployed on-premises or in the Azure VNet, and two such servers can be deployed for high availability. If deployed on-premises, a S2S VPN to the site is required, and ExpressRoute is not usable. AD domain authentication requires a RADIUS server that integrates with the AD server.","title":"Authentication"},{"location":"Cloud/Azure-VPN/#tasks","text":"","title":"Tasks"},{"location":"Cloud/Azure-VPN/#create-local-network-gateway","text":"$localnw = New-AzLocalNetworkGateway -Name LocalNetGW -ResourceGroupName ExamRefRG -Location \"West Europe\" -GatewayIpAddress \"53.50.123.195\" -AddressPrefix \"10.5.0.0/16\" Create VPN connection $gateway = Get-AzVirtualNetworkGateway -Name VPNGW1 -ResourceGroupName ExamRefRG $conn = New-AzVirtualNetworkGatewayConnection -Name OnPremConnection -ResourceGroupName ExamRefRG -Location 'West Europe' -VirtualNetworkGateway1 $gateway -LocalNetworkGateway2 $localnw -ConnectionType IPsec -SharedKey \"abc123\"","title":"Create local network gateway"},{"location":"Cloud/Azure-VPN/#create-a-vpn-gateway","text":"$rg = ExamRefRG Create gateway subnet in VNet1 Gateway subnets are normal subnets with the name \"GatewaySubnet\" $vnet1 = Get-AzVirtualNetwork -Name VNet1 -ResourceGroupName $rg $vnet1 . Subnets += New-AzVirtualNetworkSubnetConfig -Name GatewaySubnet -AddressPrefix 10 . 1 . 1 . 0 / 27 $vnet1 = Set-AzVirtualNetwork -VirtualNetwork $vnet1 Create VPN gateway in VNet1 $gwpip = New-AzPublicIpAddress -Name VNet1-GW-IP -ResourceGroupName $rg -Location 'North Europe' -AllocationMethod Dynamic $gwsubnet = Get-AzVirtualNetworkSubnetConfig -Name 'GatewaySubnet' -VirtualNetwork $vnet1 $gwipconf = New-AzVirtualNetworkGatewayIpConfig -Name GwIPConf -Subnet $gwsubnet -PublicIpAddress $gwpip $vnet1gw = New-AzVirtualNetworkGateway -Name VNet1-GW -ResourceGroupName $rg -Location 'North Europe' -IpConfigurations $gwipconf -GatewayType Vpn -VpnType RouteBased -GatewaySku VpnGw1 Create gateway subnets in VNet2 and VNet3 az network vnet subnet create --name GatewaySubnet --vnet-name VNet1 --resource-group ExamRefRG --address-prefixes 10 .1.1.0/27 az network public-ip create --name VNet1-GW-IP --resource-group ExamRefRG --location NorthEurope az network vnet-gateway create --name VNet1-GW --resource-group ExamRefRG --gateway-type vpn --sku VpnGw1 --vpn-type RouteBased --vnet VNet1 --public-ip-addresses VNet1-GW-IP --location NorthEurope","title":"Create a VPN Gateway"},{"location":"Cloud/Azure-VPN/#create-a-vpn-gateway-and-vnet-peering","text":"Create gateway subnets in VNet2 and VNet3 $vnet2 = Get-AzVirtualNetwork -Name VNet2 -ResourceGroupName ExamRefRG $vnet2 . Subnets += New-AzVirtualNetworkSubnetConfig -Name GatewaySubnet -AddressPrefix 10 . 2 . 1 . 0 / 27 $vnet2 = Set-AzVirtualNetwork -VirtualNetwork $vnet2 $vnet3 = Get-AzVirtualNetwork -Name VNet3 -ResourceGroupName ExamRefRG $vnet3 . Subnets += New-AzVirtualNetworkSubnetConfig -Name GatewaySubnet -AddressPrefix 10 . 3 . 1 . 0 / 27 $vnet3 = Set-AzVirtualNetwork -VirtualNetwork $vnet3 Create VPN gateway in VNet2 $gwpip2 = New-AzPublicIpAddress -Name VNet2-GW-IP -ResourceGroupName ExamRefRG -Location $vnet2 . Location -AllocationMethod Dynamic $gwsubnet2 = Get-AzVirtualNetworkSubnetConfig -Name 'GatewaySubnet' -VirtualNetwork $vnet2 $gwipconf2 = New-AzVirtualNetworkGatewayIpConfig -Name GwIPConf2 -Subnet $gwsubnet2 -PublicIpAddress $gwpip2 $vnet2gw = New-AzVirtualNetworkGateway -Name VNet2-GW -ResourceGroupNAme ExamRefR -Location $vnet2 . Location -IpConfigurations $gwipconf2 -GatewayType Vpn -VpnType RouteBased -GatewaySku VpnGw1 Create VPN gateway in VNet3 $gwpip3 = New-AzPublicIpAddress -Name VNet3-GW-IP -ResourceGroupName ExamRefR -Location $vnet3 . Location -AllocationMethod Dynamic $gwsubnet3 = Get-AzVirtualNetworkSubnetConfig -Name 'GatewaySubnet' -VirtualNetwork $vnet3 $gwipconf3 = New-AzVirtualNetworkGatewayIpConfig -Name GwIPConf3 -Subnet $gwsubnet3 -PublicIpAddress $gwpip3 $vnet3gw = New-AzVirtualNetworkGateway -Name VNet3-GW -ResourceGroupNAme ExamRefRG -Location $vnet3 . Location -IpConfigurations $gwipconf3 -GatewayType Vpn -VpnType RouteBased -GatewaySku VpnGw1 Create connections New-AzVirtualNetworkGatewayConnection -Name VNet2-to-VNet3 -ResourceGroupName ExamRefRG -Location $vnet2 . Location -VirtualNetworkGateway1 $vnet2gw -VirtualNetworkGateway2 $vnet3gw -ConnectionType VNet2VNet -SharedKey \"secretkey123\" New-AzVirtualNetworkGatewayConnection -Name VNet3-to-VNet2 -ResourceGroupName ExamRefRG -Location $vnet3 . Location -VirtualNetworkGateway1 $vnet3gw -VirtualNetworkGateway2 $vnet2gw -ConnectionType VNet2VNet -SharedKey \"secretkey123\" Create gateway subnets in VNet2 and VNet3 az network vnet subnet create --name GatewaySubnet --vnet-name VNet2 --resource-group ExamRefRG --address-prefixes 10 .2.1.0/27 az network vnet subnet create --name GatewaySubnet --vnet-name VNet3 --resource-group ExamRefRG --address-prefixes 10 .3.1.0/27 Create public IP addresses for use by VPN gateways az network public-ip create --name VNet2-GW-IP --resource-group ExamRefRG --location NorthEurope az network public-ip create --name VNet3-GW-IP --resource-group ExamRefRG --location WestEurope Create VPN gateways in VNet2 and VNet 3 az network vnet-gateway create --name VNet2-GW --resource-group ExamRefRG --gateway-type vpn --sku VpnGw1 --vpn-type RouteBased --vnet VNet2 --public-ip-addresses VNet2-GW-IP --location NorthEurope az network vnet-gateway create --name VNet3-GW --resource-group ExamRefRG --gateway-type vpn --sku VpnGw1 --vpn-type RouteBased --vnet VNet3 --public-ip-addresses VNet3-GW-IP --location WestEurope Create connections between VPN gateways az network vpn-connection create --name VNet2-to-VNet3 --resource-group ExamRefRG --vnet-gateway1 VNet2-GW --vnet-gateway2 VNet3-GW --shared-key secretkey123 --location NorthEurope az network vpn-connection create --name VNet3-to-VNet2 --resource-group ExamRefRG --vnet-gateway1 VNet3-GW --vnet-gateway2 VNet2-GW --shared-key secretkey123 --location WestEurope","title":"Create a VPN gateway and VNet peering"},{"location":"Cloud/Azure-VPN/#use-vpn-troubleshoot","text":"Get the Network Watcher resource $nw = Get-AzResource | Where ResourceType -eq Microsoft . Network / networkWatchers -and Location -eq WestEurope $networkWatcher = Get-AzNetworkWatcher -Name $nw . Name -ResourceGroupName $nw . ResourceGroupName Get the connection to troubleshoot $connection = Get-AzVirtualNetworkGatewayConnection -Name Vnet1-to-Vnet2 -ResourceGroupName ExamRefRG Start VPN Troubleshoot Start-AzNetworkWatcherResourceTroubleshooting -NetworkWatcher $networkWatcher -TargetResourceId $connection . Id -StorageId $sa . Id -StoragePath \" $( $sa . PrimaryEndpoints . Blob )$( $sc . name ) \" Create a storage account and container for logs az storage account create --name examrefstorage --location westeurope --resource-group ExamRefRG --sku Standard_LRS az storage account keys list --resource-group ExamRefRG --account-name examrefstorage az storage container create --account-name examrefstorage --account-key { storageAccountKey } --name logs Start VPN Troubleshoot az network watcher troubleshooting start --resource-group ExamRefRG --resource Vnet1-to-Vnet2 --resource-type vpnConnection --storage-account examrefstorage --storage-path https://examrefstorage.blob.core.windows.net/logs --output json","title":"Use VPN Troubleshoot"},{"location":"Cloud/Azure-VPN/#create-s2s-vpn","text":"AZ-103: 395 $lgwip = 53.50.123.195 $key = \"abc123\" $lgw = New-AzLocalNetworkGateway -ResourceGroupName $g -Name $n -Location $l -GatewayIpAddress $lgwip -AddressPrefix \"10.5.0.0/16\" $vgw = Get-AzVirtualNetworkGateway -ResourceGroupNAme -Name New-AzVirtualNetworkGatewayConnection -ResourceGroupName $g -Name $n -Location $l -VirtualNetworkGateway1 $vgw -LocalNetworkGateway2 $lgw -ConnectionType IPsec -SharedKey $key az network local-gateway create --gateway-ip-address $lgwip --name LocalNetGW --resource-group ExamRefRG --local-address-prefixes 10 .5.0.0/16 az network vpn-connection create --name OnPremConnection --resource-group ExamRefRG --vnet-gateway1 VPNGW1 --location WestEurope --shared-key $key --local-gateway2 LocalNetGW Sources VPN Gateway design Connect Azure VPN gateways to multiple on-premises policy-based VPN devices About VPN Gateway configuration settings Highly available cross-premises and VNet-to-VNet connectivity ExpressRoute connectivity models Connect a computer to a virtual network using P2S and RADIUS authentication: PowerShell","title":"Create S2S VPN"},{"location":"Cloud/Kusto/","text":"ADX Azure Data Explorer (ADX) has two architectural elements - Data Management - Engine ADX does not hold large tables in a single table, rather it automatically shards them into Extents Syntax In Kusto documentation T typically refers to the Table being queried, i.e. T | where Predicate where <> is equivalent to != SecurityEvent | where Level <> 8 | where EventID==4672 project Select columns to include, rename, or drop T | project X=C, // Rename column C to X A=2*B, // Calculate a new column A from the old B C=strcat(\"-\",tostring(C)), // Calculate a new column C from the old C B=2*B // Calculate a new column B from the old B case StormEvents | extend label = case ( DamageProperty < 1000, \"Storm\", DamageProperty > 1000 and DamageProperty < 10000, \"Disaster\", \"Catastrophe\" ) | summarize count() by label Examples SecurityEvent The SecurityEvent table provided as part of the Log Analytics workspace trainig dataset contains event viewer logs typical of what a security analyst would analyze, with the following columns: TimeGenerated Account AccountType (Machine or User) Computer EventSourceName Channel CommandLine Find logons, producing number of logins per Computer for computers with names beginning with \"App\" SecurityEvent | where TimeGenerated between ( ago ( 14 d ).. ago ( 7 d )) | where EventID == 4624 | where Computer startswith \"App\" | summarize count () by Computer SecurityEvent | where EventID == 4688 | summarize count () by CommandLine , Computer Movies id title director year length_minutes 1 Toy Story John Lasseter 1995 81 2 A Bug's Life John Lasseter 1998 95 3 Toy Story 2 John Lasseter 1999 93 4 Monsters, Inc. Pete Docter 2001 92 5 Finding Nemo Andrew Stanton 2003 107 6 The Incredibles Brad Bird 2004 116 7 Cars John Lasseter 2006 117 8 Ratatouille Brad Bird 2007 115 9 WALL-E Andrew Stanton 2008 104 10 Up Pete Docter 2009 101 11 Toy Story 3 Lee Unkrich 2010 103 12 Cars 2 John Lasseter 2011 120 13 Brave Brenda Chapman 2012 102 14 Monsters University Dan Scanlon 2013 110 Find the title of each film Movies | project title Number of reporting computers each hour AZ-103: 53 Heartbeat | summarize dcount(ComputerIP) by bin(TimeGenerated, 1h) | render timechart List top 10 VMs with most error events over the past day MeasureUp Event | where (EventLevelName == \"Error\") | where (TimeGenerated > ago(1days)) | summarize ErrorCount = count() by Computer | top 10 by ErrorCount desc Render a SQL query as KQL EXPLAIN SELECT name FROM greeks ; Count instances of a value movies | summarize movies_directed = count() by director Create a new column dynamically from others movies | extend age = 2020 - year | project name , age ; Hide secrets from the queries log print h\"Hello world!\"; .show Tasks Kusto clusters can be provisioned and Kusto databases created and manipulated using both PowerShell and Azure CLI. The Azure CLI kusto module will not be supported after 01/01/2021. az extension add -n kusto Create cluster Azure PowerShell New-AzKustoCluster -ResourceGroupName testrg -Name testnewkustocluster -Location 'East US' -SkuName Standard_D11_v2 -SkuTier Standard -EnableDoubleEncryption true Azure CLI az kusto cluster create --name --resource-group --sku az kusto database create Create table Connect to database #connect cluster('jasper.eastus').database('test'); Create table .create table starships (Name:string, Registry:string, Class:string, Crew:int32) Ingest data . ingest into table T h 'https://raw.githubusercontent.com/jasper-zanjani/dogfood/master/csv/greeks.csv' with ( ignoreFirstRecord = true ) Alternatively, define a new datatable inline let starships = datatable ( Name : string , Class : string , Registry : string , Crew : int ) [ \"USS Enterprise\" , \"Constitution\" , \"NCC-1701\" , 203 , \"USS Constitution\" , \"Constitution\" , \"NCC-1700\" , 204 , \"USS Defiant\" , \"Defiant\" , \"NX-74205\" , 50 , \"USS Voyager\" , \"Intrepid\" , \"NCC-74656\" , 141 , \"USS Enterprise\" , \"Galaxy\" , \"NCC-1701-D\" , 6000 , \"USS Reliant\" , \"Miranda\" , \"NCC-1864\" , 35 ]; Search for a word starships | search \"enterprise\" ; search in ( SecurityEvent ) \"Cryptographic\" | take 10 ; Dates Datetime values support a menagerie of functions print datetime ( 2015 - 01 - 01 ) # 2015 - 01 - 01 00 : 00 : 00 .0000 print format_datetime ( datetime ( 2015 - 01 - 01 ), \"yyyy\" ) # 2015 Dynamically calculated columns Concatenate values from other columns. StormEvents | project EpisodeId , where_storm = strcat ( EventType , \" in \" , State ); Export data . export async to sql [ 'dbo.StormEventTypeTable' ] Sources Azure Data Explorer documentation How to start with Microsoft Azure Data Explorer KQL quick reference SQL to Kusto cheat sheet Kusto.Explorer Azure Sentinel webinar parts 1, 2 , 3 KQL syntax: count , take","title":"Kusto"},{"location":"Cloud/Kusto/#adx","text":"Azure Data Explorer (ADX) has two architectural elements - Data Management - Engine ADX does not hold large tables in a single table, rather it automatically shards them into Extents","title":"ADX"},{"location":"Cloud/Kusto/#syntax","text":"In Kusto documentation T typically refers to the Table being queried, i.e. T | where Predicate","title":"Syntax"},{"location":"Cloud/Kusto/#where","text":"<> is equivalent to != SecurityEvent | where Level <> 8 | where EventID==4672","title":"where"},{"location":"Cloud/Kusto/#project","text":"Select columns to include, rename, or drop T | project X=C, // Rename column C to X A=2*B, // Calculate a new column A from the old B C=strcat(\"-\",tostring(C)), // Calculate a new column C from the old C B=2*B // Calculate a new column B from the old B","title":"project"},{"location":"Cloud/Kusto/#case","text":"StormEvents | extend label = case ( DamageProperty < 1000, \"Storm\", DamageProperty > 1000 and DamageProperty < 10000, \"Disaster\", \"Catastrophe\" ) | summarize count() by label","title":"case"},{"location":"Cloud/Kusto/#examples","text":"","title":"Examples"},{"location":"Cloud/Kusto/#securityevent","text":"The SecurityEvent table provided as part of the Log Analytics workspace trainig dataset contains event viewer logs typical of what a security analyst would analyze, with the following columns: TimeGenerated Account AccountType (Machine or User) Computer EventSourceName Channel CommandLine Find logons, producing number of logins per Computer for computers with names beginning with \"App\" SecurityEvent | where TimeGenerated between ( ago ( 14 d ).. ago ( 7 d )) | where EventID == 4624 | where Computer startswith \"App\" | summarize count () by Computer SecurityEvent | where EventID == 4688 | summarize count () by CommandLine , Computer","title":"SecurityEvent"},{"location":"Cloud/Kusto/#movies","text":"id title director year length_minutes 1 Toy Story John Lasseter 1995 81 2 A Bug's Life John Lasseter 1998 95 3 Toy Story 2 John Lasseter 1999 93 4 Monsters, Inc. Pete Docter 2001 92 5 Finding Nemo Andrew Stanton 2003 107 6 The Incredibles Brad Bird 2004 116 7 Cars John Lasseter 2006 117 8 Ratatouille Brad Bird 2007 115 9 WALL-E Andrew Stanton 2008 104 10 Up Pete Docter 2009 101 11 Toy Story 3 Lee Unkrich 2010 103 12 Cars 2 John Lasseter 2011 120 13 Brave Brenda Chapman 2012 102 14 Monsters University Dan Scanlon 2013 110 Find the title of each film Movies | project title Number of reporting computers each hour AZ-103: 53 Heartbeat | summarize dcount(ComputerIP) by bin(TimeGenerated, 1h) | render timechart List top 10 VMs with most error events over the past day MeasureUp Event | where (EventLevelName == \"Error\") | where (TimeGenerated > ago(1days)) | summarize ErrorCount = count() by Computer | top 10 by ErrorCount desc Render a SQL query as KQL EXPLAIN SELECT name FROM greeks ; Count instances of a value movies | summarize movies_directed = count() by director Create a new column dynamically from others movies | extend age = 2020 - year | project name , age ; Hide secrets from the queries log print h\"Hello world!\"; .show","title":"Movies"},{"location":"Cloud/Kusto/#tasks","text":"Kusto clusters can be provisioned and Kusto databases created and manipulated using both PowerShell and Azure CLI. The Azure CLI kusto module will not be supported after 01/01/2021. az extension add -n kusto Create cluster Azure PowerShell New-AzKustoCluster -ResourceGroupName testrg -Name testnewkustocluster -Location 'East US' -SkuName Standard_D11_v2 -SkuTier Standard -EnableDoubleEncryption true Azure CLI az kusto cluster create --name --resource-group --sku az kusto database create Create table Connect to database #connect cluster('jasper.eastus').database('test'); Create table .create table starships (Name:string, Registry:string, Class:string, Crew:int32) Ingest data . ingest into table T h 'https://raw.githubusercontent.com/jasper-zanjani/dogfood/master/csv/greeks.csv' with ( ignoreFirstRecord = true ) Alternatively, define a new datatable inline let starships = datatable ( Name : string , Class : string , Registry : string , Crew : int ) [ \"USS Enterprise\" , \"Constitution\" , \"NCC-1701\" , 203 , \"USS Constitution\" , \"Constitution\" , \"NCC-1700\" , 204 , \"USS Defiant\" , \"Defiant\" , \"NX-74205\" , 50 , \"USS Voyager\" , \"Intrepid\" , \"NCC-74656\" , 141 , \"USS Enterprise\" , \"Galaxy\" , \"NCC-1701-D\" , 6000 , \"USS Reliant\" , \"Miranda\" , \"NCC-1864\" , 35 ]; Search for a word starships | search \"enterprise\" ; search in ( SecurityEvent ) \"Cryptographic\" | take 10 ;","title":"Tasks"},{"location":"Cloud/Kusto/#dates","text":"Datetime values support a menagerie of functions print datetime ( 2015 - 01 - 01 ) # 2015 - 01 - 01 00 : 00 : 00 .0000 print format_datetime ( datetime ( 2015 - 01 - 01 ), \"yyyy\" ) # 2015","title":"Dates"},{"location":"Cloud/Kusto/#dynamically-calculated-columns","text":"Concatenate values from other columns. StormEvents | project EpisodeId , where_storm = strcat ( EventType , \" in \" , State ); Export data . export async to sql [ 'dbo.StormEventTypeTable' ] Sources Azure Data Explorer documentation How to start with Microsoft Azure Data Explorer KQL quick reference SQL to Kusto cheat sheet Kusto.Explorer Azure Sentinel webinar parts 1, 2 , 3 KQL syntax: count , take","title":"Dynamically calculated columns"},{"location":"Cloud/Tasks/","text":"Tasks \ud83d\udee0\ufe0f Administration Display subscription ID Get-AzSubscription az account show \ud83d\udda5\ufe0f CLI Initialize CLI utility gcloud init IAM Add guest user New-AzureADMSInvitation -InvitedUserEmailAddress $EMAIL -SendInvitationMessage $True -InviteRedirectUrl \"http://myapps.onmicrosoft.com\" Assign a role # At the organization level gcloud organizations add-iam-policy-binding $ORG_ID --member = \"user: $EMAIL \" --role = \"roles/compute.xpnAdmin\" # At the folder level gcloud beta resource-manager-folders add-iam-policy-binding $FOLDER_ID --member = \"user: $EMAIL \" --role = \"roles/compute.xpnAdmin\" \ud83d\udcb0 Cost management To view resource quotas for a subscription, go to the subscription in Azure Portal and open the Usage + quotas blade. From there you can select resources and then click the Request Increase button. View current usage of vCPU quotas Get-AzVMUsage View current usage of storage service Get-AzStorageUsage Create a budget To create a budget, open Cost Management + Billing , then Subscriptions , select a subscription, then click Budgets . Then click + Add , which produces a Create budget blade. The created budget can be seen in the Budgets blade. PowerShell commands used with budgets: Get-AzResourceGroup retrieve Resource Group object Set-AzResourceGroup apply a tag to a resource group with no preexisting tags .Tags method that retrieves Tag collection from a resource group .Add() method used to add tags to a resource group that already has tags. Monitoring VM extension Set-AzVMExtension -ResourceGroupName ExamRefRG -Location \"West Europe\" -VMName VM1 -Name networkWatcherAgent -Publisher Microsoft . Azure . NetworkWatcher -Type NetworkWatcherAgentWindows -TypeHandlerVersion 1 . 4 az vm extension set --vm-name VM1 --resource-group ExamRefRG --publisher Microsoft.Azure.NetworkWatcher --version 1 .4 --name NetworkWatcherAgentWindows --extension-instance-name NetworkWatcherAgent Start packet capture $nw = Get-AzResource | Where ResourceType -eq \"Microsoft.Network/networkWatchers\" -and Location -eq \"WestEurope\" $networkWatcher = Get-AzNetworkWatcher -Name $nw . Name -ResourceGroupName $nw . ResourceGroupName $storageAccount = Get-AzStorageAccount -Name examref-storage -ResourceGroupName ExamRefRG $filter1 = New-AzPacketCaptureFilterConfig -Protocol TCP -RemoteIPAddress \"1.1.1.1-255.255.255.255\" -LocalIPAddress \"10.0.0.3\" -LocalPort \"1-65535\" -RemotePort \"20;80;443\" $filter2 = New-AzPacketCaptureFilterConfig -Protocol UDP $vm = Get-AzVM ` -Name VM1 -ResourceGroupName ExamRefRG New-AzNetworkWatcherPacketCapture -NetworkWatcher $networkWatcher -TargetVirtualMachineId $vm . Id -PacketCaptureName \"PacketCaptureTest\" -StorageAccountId $storageAccount . id -TimeLimitInSeconds 60 -Filter $filter1 , $filter2 filter = '[ { \"protocol\": \"TCP\", \"remoteIPAddress\": \"1.1.1.1-255.255.255.255\", \"localIPAddress\":\"10.0.0.3\", \"remotePort\":\"20\" } ]' az network watcher packet-capture create --name PacketCaptureTest2 --resource-group ExamRefRG --vm VM1 --time-limit 300 --storage-account examref-storage --filters $filter Check status of packet capture Get-AzNetworkWatcherPacketCapture -NetworkWatcher $networkWatcher -PacketCaptureName \"PacketCaptureTest\" az network watcher packet-capture show-status --name PacketCaptureTest --location WestEurope Stop packet capture Stop-AzNetworkWatcherPacketCapture -NetworkWatcher $networkWatcher -PacketCaptureName \"PacketCaptureTest\" az network watcher packet-capture stop --name PacketCaptureTest --location WestEurope Use IP Flow Verify to test outbound connectivity from source VM and port to destination. If any configured filtering rules block traffic between the endpoints, it will return the name of the offending NSG. Test-AzNetworkWatcherIPFlow az network watcher test-ip-flow Next Hop Get-AzNetworkWatcherNextHop az network watcher show-next-hop Use Network Topology Get-AzNetworkWatcherTopology az network watcher show-topology Capture SFTP traffic $r = Get-AzResource | where ResourceType -eq \"Microsoft.Network/networkWatchers\" -and Location -eq \"EastUS\" $nw = Get-AzNetworkWatcher -Name $r . Name -ResourceGroupName $r . ResourceGroupName $s = Get-AzStorageAccount -ResourceGroupName \"Diagnostics-RG\" -Name \"Diagnostics-Storage\" $filter = New-AzPacketCaptureFilterConfig -Protocol TCP -RemoteIPAddress \"1.1.1.1-255.255.255.255\" -LocalIPAddress \"10.0.0.4\" -LocalPort \"1-65535\" -RemotePort \"22\" New-AzNetworkWatcherPacketCapture -NetworkWatcher $nw -TargetVirtualMachineId $vm . ID -PacketCaptureName \"Capture SFTP traffic\" -StorageAccountId $s . Id -TimeLimitInSeconds 60 -Filter $filter Policy Assign a policy $scope = '/subscriptions/$subscriptionID' $policyparam = '{ \"tagName\" : { \"value\": \"Environment\" }, \"tagValue\": { \"value\" : \"Production\" } }' $assignment = New-AzPolicyAssignment -Name 'append-environment-tag' -DisplayName 'Append Environment Tag' -Scope $scope -PolicyDefinition $definition -PolicyParameter $policyparam Remove policy assignment and definition Remove-AzPolicyAssignment -Id $assignment . ResourceId Remove-AzPolicyDefinition -Id $definition . ResourceId Create a policy definition Azure Portal (All Services) > Policy > Definitions: Both builtin and custom policies can be managed here. New-AzPolicyDefinition -Name 'appendEnvironmentTag' -DisplayName 'Append Environment Tag' -Policy 'AppendDefaultTag.json' -Parameter 'AppendDefaultTagParams.json' az policy definition create --name 'allowedVMs' --description 'Only allow virtual machines in the defined SKUs' --mode ALL --rules '{...}' --params '{...}' Apply policy to a scope az policy assignment create --policy allowedVMs --name 'deny-non-compliant-vms' --scope '/subscriptions/<Subscription ID>' -p Delete policy assignment az policy assignment delete --name deny-non-compliant-vms Resources Create resource group New-AzGroup -Location $location -Name $rgName az group create -l $location -n $rgName Register resource provider in subscription az provider register --namespace 'Microsoft.PolicyInsights' Move resources $webapp = Get-AzResource -ResourceGroupName OldRG -ResourceName ExampleSite $plan = Get-AzResource -ResourceGroupName OldRG -ResourceName ExamplePlan Move-AzResource -DestinationResourceGroupName NewRG -ResourceId $webapp . ResourceId , $plan . ResourceId webapp = $( az resource show -g OldRG -n ExampleSite --resource-type \"Microsoft.Web/sites\" --query id --output tsv ) plan = $( az resource show -g OldRG -n ExamplePlan --resource-type \"Microsoft.Web/serverfarms\" --query id --output tsv ) az resource move --destination-group newgroup --ids $webapp $plan Create lock on a resource New-AzResourceLock -LockName LockSite -LockLevel CanNotDelete -ResourceGroupName $rg -ResourceName $r -ResourceType Microsoft . Web / sites az lock create --name LockSite --lock-type CanNotDelete --resource-group $rg --resource-name $r --resource-type Microsoft.Web/sites Create lock on a resource group New-AzResourceLock -LockName LockGroup -LockLevel CanNotDelete -ResourceGroupName $rg az lock create --name LockGroup --lock-type CanNotDelete --resource-group $rg Display resource lock Get-AzResourceLock -ResourceName $r -ResourceType Microsoft . Web / sites -ResourceGroupName $rg az lock list --resource-group $rg --resource-name $r --namespace Microsoft.Web --resource-type sites --parent \"\" Delete resource lock $lockId = ( Get-AzResourceLock -ResourceGroupName $rg -ResourceName $r -ResourceType Microsoft . Web / sites ). LockId Remove-AzResourceLock -LockId $lockId lockid = $( az lock show --name LockSite --resource-group $rg --resource-type Microsoft.Web/sites --resource-name $r --output tsv --query id ) az lock delete --ids $lockid Sources Manage Azure Resource Manager resource groups by using Azure PowerShell Manage Azure Resource Manager resource groups by using Azure CLI Resource providers Lock resources to prevent unexpected changes AZ-103: 1.3 , p. 76 Tags List all resources by tag ( Get-AzResource -Tag @{ CostCode = \"1001\" }). Name # List all resources by tag name, with no value ( Get-AzResource -TagName CostCode ). Name az resource list --tag Dept = Finance List resource groups by tag ( Get-AzResourceGroup -Tag @{ CostCode = \"1001\" }). ResourceGroupName az group list --tag CostCode = 1001 Enumerate a resource's tags $r = Get-AzResource -Name $resourceName -ResourceGroup rg Get-AzTag -ResourceId $r . id # Resource group $rg = Get-AzResourceGroup -Name $rgName Get-AzTag -ResourceId $rg . ResourceId # Subscription $s = ( Get-AzSubscription -SubscriptionName \"Example Subscription\" ). Id Get-AzTag -ResourceId \"/subscriptions/$s\" az resource show -n $resourceName -g $rgName --query tags # Resource group az group show -n $rgName --query tags Tag resource $r = Get-AzResource -ResourceName hrvm1 -ResourceGroupName rg $r . Tags . Add ( \"Owner\" , \"user@contoso.com\" ) Set-AzResource -Tag $r . Tags -ResourceId $r . ResourceId -Force Resource group $tags = @{ \"Dept\" = \"Finance\" ; \"Status\" = \"Normal\" } $rg = Get-AzResourceGroup -Name demoGroup New-AzTag -ResourceId $rg . ResourceId -tag $tags $tags = ( Get-AzResourceGroup -Name rg ). Tags $tags . Add ( \"Owner\" , \"user@contoso.com\" ) Set-AzResourceGroup -Tag $tags -Name rg jsonrtag = $( az group show -n rg --query tags ) rt = $( echo $jsonrtag | tr -d '\"{},' | sed 's/: /=/g' ) az group update -n rg --tags $rt Owner = user@contoso.com Remove specific tags $tags = @{ \"Project\" = \"ECommerce\" ; \"Team\" = \"Web\" } Update-AzTag -ResourceId $resource . id -Tag $tags -Operation Delete Remove all tags $s = ( Get-AzSubscription -SubscriptionName \"Example Subscription\" ). Id Remove-AzTag -ResourceId \"/subscriptions/$s\" # Alternatively Set-AzResourceGroup -Tag @{} -Name rg Apply tags to resource, overwriting $tags = @{ \"Dept\" = \"Finance\" ; \"Status\" = \"Normal\" } New-AzTag -ResourceId $resource . id -Tag $tags Set-AzResource -ResourceId $r . ResourceId -Tag @{ CostCode = \"1001\" ; Environment = \"Production\" } -Force az resource tag --tags 'Dept=IT' 'Environment=Test' -g $rgName -n examplevnet --resource-type \"Microsoft.Network/virtualNetworks\" Apply tags to resource group Set-AzResourceGroup -Name rg -Tag @{ CostCode = 1001 ; Environment = Production } az group update -n $rgName --tags 'Environment=Test' 'Dept=IT' # Alternatively az group update -n $rgName --set tags.Environment = Production tags.CostCode = 1001 Compute IaaS Create a VM ( src ) gcloud compute instances create instance-1 --zone-uscentral1-a PaaS Deploy app in current working directory. gcloud app deploy View the deployed app gcloud app browse app.yaml allows configuration of the app in several ways runtime : python37 In Azure, multiple web applications are organized under an App Service Plan resource. So if no such app service plan exists, it must be created. $p = New-AzAppServicePlan -Name $n -ResourceGroupName $g -Location $l -Tier \"Basic\" -NumberofWorkers 2 -WorkerSize \"Small\" New-AzWebApp -Name $n -Location $l -ResourceGroupName $g -AppServicePlan $p az appservice plan create -g $g -n $p --is-linux az webapp create -n $n -g $g --plan $p Containers Create a new source repository These steps require: Cloud SDK and Git to be installed A GCP project with billing and the Cloud Source Repositories API enabled #Create a new repository gcloud source repos create hello-world #Clone it locally gcloud source repos clone hello-world # Create scripts, then add, commit and push them as usual. git commit -am \"Initial\" git push origin master Create container registry New-AzContainerRegistry -ResourceGroupName $rg -Name $registry -Sku \"Basic\" -EnableAdminUser az acr create --name $registry --resource-group $rg --sku Basic --admin-enabled true \u2693 Kubernetes Create Kubernetes cluster New-AzAKS -ResourceGroupName $g -Name $n -NodeCount 2 -NetworkPlugin azure -NodeVmSetType VirtualMachineScaleSets -WindowsProfileAdminUserName azureuser -WindowsProfileAdminUserPassword $Password -KubernetesVersion 1 . 16 . 7 # PowerShell does not offer an option to generate SSH keys for access to the cluster; `ssh-keygen` must be used. - Create a Windows Server container on an AKS cluster az aks create -g $g -n $n --node-count 2 --network-plugin azure --vm-set-type VirtualMachineScaleSets --windows-admin-username azureuser --windows-admin-password $PASSWORD --generate-ssh-keys --enable-addons monitoring - Create a Windows Server container on an AKS cluster Add a pool of nodes New-AzAksNodePool -ResourceGroupName $rgName -Name npwin -ClusterName $clusterName -OsType Windows -KubernetesVersion 1 . 16 . 7 az aks nodepool add -g $g -n $n --cluster-name $clusterName --os-type Windows --node-count 1 Persistent volume claim apiVersion : v1 kind : PersistentVolumeClaim metadata : name : azure-managed-disk spec : accessModes : - ReadWriteOnce storageClassName : managed-premium resources : requests : storage : 5Gi - Source Provision Azure Disk Standard kind : StorageClass apiVersion : storage.k8s.io/v1 metadata : name : managed-disk-forapp provisioner : kubernetes.io/azure-disk reclaimPolicy : Retain parameters : storageaccounttype : default kind : Managed Premium kind : StorageClass apiVersion : storage.k8s.io/v1 metadata : name : managed-disk-forapp provisioner : kubernetes.io/azure-disk reclaimPolicy : Retain parameters : storageaccounttype : Premium_LRS kind : Managed Functions Deploy gcloud functions deploy hello_get --runtime python37 --trigger-http Test gcloud functions describe hello_get Storage Create storage account Azure Portal Click Create a resouce , then Storage , then Storage account . Choose a globally unique name for the account, containing lower-case characters and digits only. New-AzStorageAccount -ResourceGroupName ExamRefRG -Name mystorage112300 -SkuName Standard_LRS -Location WestUS -Kind StorageV2 -AccessTier Hot az storage account create --name $accountName --resource-group $resourceGroup -location $location --sku $sku Change access tier of storage account === \"Azure PowerShell ```powershell Set-AzStorageAccount -ResourceGroupName RG -Name $accountName -AccessTier Cool -Force ``` Change replication mode of storage account Set-AzStorageAccount -ResourceGroupName $resourceGroup -Name $accountName -SkuName $type Renew storage account keys === \"Azure ```powershell New-AzStorageAccountKey ``` az storage account keys renew Create Azure Key Vault New-AzKeyVault -VaultName $vaultName -ResourceGroupName $g -Location $location $key = Add-AzKeyVaultKey -VaultName $vaultName -Name $keyName -Destination 'Software' $storageKey = Get-AzStorageAccountKey -ResourceGroupName $g -Name $storageAccount $secretvalue = ConvertTo-SecureString $storageKey [ 0 ]. Value -AsPlainText -Force $secret = Set-AzKeyVaultSecret -VaultName $vaultName -Name $secretName -SecretValue $secretvalue az keyvault create --name $vaultName --resource-group $g --location $location az keyvault key create --vault-name \" $vaultName \" --name $keyName --protection \"software\" az keyvault secret set --vault-name \" $vaultName \" --name \" $secretName \" --value \" $secretValue \" Create key in Azure Key Vault $key = Add-AzKeyVaultKey -VaultName $vaultName -Name $keyName -Destination 'Software' $storageKey = Get-AzStorageAccountKey -ResourceGroupName $g -Name $storageAccount $secretvalue = ConvertTo-SecureString $storageKey [ 0 ]. Value -AsPlainText -Force $secret = Set-AzKeyVaultSecret -VaultName $vaultName -Name $secretName -SecretValue $secretvalue az keyvault key create --vault-name $vaultName --name $keyName --protection \"software\" az keyvault secret set --vault-name $vaultName --name $secretName --value $secretValue Create Azure sync group Specify name of sync group in dialog after creating an Azure File Sync Change storage class $STORAGE_CLASS can be multi_regional , regional , nearline , or coldline gsutil rewrite -s $STORAGE_CLASS gs:// $PATH_TO_OBJECT File shares Deploy Azure File Sync # Create Storage Sync Service $storageSync = New-AzStorageSyncService -ResourceGroupName $g -Name $storageSyncName -Location $l # Create Azure File Share $storageKey = Get-AzStorageAccountKey -ResourceGroupName $g -Name $storageAccount $context = New-AzStorageContext -StorageAccountName $storageAccount -StorageAccountKey $storageKey . Value [ 0 ] New-AzStorageShare -Name $shareName -Context $context # Creating a Storage Sync Service resource is only possible in PowerShell or Portal constring = $( az storage account show-connection-string -n $storageAccountName ) az storage share create --name $shareName --quota 2048 --connection-string $constring Create sync group $syncgroup = New-AzStorageSyncGroup -Name $syncgroupname -ParentObject $storageSync Create cloud endpoint New-AzStorageSyncCloudEndpoint -Name $shareName -ParentObject $syncgroup -StorageAccountResourceId $storageAccount . Id -AzureFileShareName $shareName Network access Display the status of the default NetworkRule for a storage account Get-AzStorageAccountNetworkRuleSet -ResourceGroupName $rgName -AccountName $n | Select-Object DefaultAction az storage account show - $rgName -n $n --query networkRuleSet.defaultAction Set default rule Update-AzStorageAccountNetworkRuleSet -ResourceGroupName $g -Name $n -DefaultAction Deny Update-AzStorageAccountNetworkRuleSet -ResourceGroupName $g -Name $n -DefaultAction Allow az storage account update -g $g -n $n --default-action Deny az storage account update -g $g -n $n --default-action Allow Networking Create virtual network with a specific prefix and subnet $subnet = New-AzVirtualNetworkSubnetConfig -Name $subnetName -AddressPrefix \"10.0.0.0/24\" $vnet = New-AzVirtualNetwork -Name $name -ResourceGroupName $rgName -Location $l -AddressPrefix \"10.0.0.0/16\" -Subnet $subnet az network vnet create -g $rgName -n $name --address-prefix \"10.0.0.0/16\" --subnet-name $subnetName --subnet-prefix \"10.0.0.0/24\" gcloud networks create $name --subnet-mode = custom gcloud beta compute networks subnets create $subnetName --network = $name --region = $l --range = \"10.0.0.0/16\" --enable-private-ip-google-access --enable-flow-logs Create peering Add-AzVirtualNetworkPeering -Name 'peering1' -VirtualNetwork $net1 -RemoteVirtualNetworkId $net2 . Id Add-AzVirtualNetworkPeering -Name 'peering2' -VirtualNetwork $net2 -RemoteVirtualNetworkId $net1 . Id az network vnet peering create -n 'peering1' -g $g --vnet-name net1 --allow-vnet-access --remote-vnet net2 az network vnet peering create -n 'peering2' -g $g --vnet-name net2 --allow-vnet-access --remote-vnet net1 gcloud compute networks peerings create \"peering1\" --network net1 --peer-project $p --peer-network net2 --auto-create-routes gcloud compute networks peerings create \"peering2\" --network net1 --peer-project $p --peer-network net1 --auto-create-routes Check peering Get-AzVirtualNetworkPeering -ResourceGroupName $rg -VirtualNetworkName $vnetName az network vnet peering list --resource-group $rg --vnet-name VNet1 az network vnet peering list --resource-group $rg --vnet-name VNet2 User-defined routes # Create the route table resource $routeTable = New-AzRouteTable -Name $routeTableName -ResourceGroupName ExamRefRG # Add a route to route table object Add-AzRouteConfig -RouteTable $routeTable -Name $routeName -AddressPrefix 10 . 3 . 0 . 0 / 16 -NextHopType VirtualAppliance -NextHopIpAddress 10 . 2 . 20 . 4 Set-AzRouteTable -RouteTable $routeTable # Associate route table with subnet Set-AzVirtualNetworkSubnetConfig -VirtualNetwork $vnet -Name Default -AddressPrefix $subnet . AddressPrefix -RouteTable $routeTable # Commit changes Set-AzVirtualNetwork -VirtualNetwork $vnet # Get effective routes for a NIC Get-AzEffectiveRouteTable -NetworkInterfaceName $nicName -ResourceGroupName $rgName # Create route table resource az network route-table create --name $routeTableName --resource-group $rgName # Add route to route table az network route-table route create --resource-group $rgName --route-table-name $routeTableName --name $routeName --address-prefix 10 .3.0.0/16 --next-hop-type VirtualAppliance --next-hop-ip-address 10 .2.20.4 # Associate route table with subnet az network vnet subnet update --name defualt --vnet-name Vnet1 --resource-group $rgName --route-table rt # Get effective routes for NIC az network nic show-effective-route-table --name $nicName --resource-group $rgName Create NSG $nsgRules = @() $nsgRules += New-AzNetworkSecurityRuleConfig -Name \"AllowingWinRMHTTP\" -Description \"To Enable PowerShell Remote Access\" -Access Allow -Protocol Tcp -Direction Inbound -Priority 103 -SourceAddressPrefix Internet -SourcePortRange * -DestinationAddressPrefix * -DestinationPortRange 5985 $nsgRules += New-AzNetworkSecurityRuleConfig -Name \"AllowingWinRMHTTPS\" -Description \"To Enable PowerShell Remote Access\" -Access Allow -Protocol Tcp -Direction Inbound -Priority 104 -SourceAddressPrefix Internet -SourcePortRange * -DestinationAddressPrefix * -DestinationPortRange 5986 $nsg = New-AzNetworkSecurityGroup -Name \"wscore-nsg\" -ResourceGroupName \"RG\" -Location \"East US\" -SecurityRules $nsgRules View rules Get-AzEffectiveNetworkSecurityGroup -NetworkInterfaceName $nicName -ResourceGroupName $rgName az network nic list-effective-nsg --name $nicName --resource-group $rgName Create Bastion Connecting to a VM requires at least Reader role privileges on the VM, its NIC, and on the Bastion itself. New-AzBastion -ResourceGroupName $rgName -Name $n -PublicIpAddress $pip -VirtualNetwork $vnet az network bastion create -g $rgName -n $n -l $l --public-ip-address $pip --vnet-name $vnetName Create virtual appliance IP forwarding must be enabled on the VM's NIC, then applications installed on the VM can begin accepting packets destined for other IP addresses. CDN Create new profile Azure Portal Click Create a resource Click Web Click CDN , opening the CDN profile blade Specify name for the profile, name of the resource group, region, and pricing tier. Click Create AZ-103: p. 140 Create endpoint Azure Portal Add an endpoint to a CDN profile (Portal) 1. Open the CDN Profile 2. Click + Endpoint button 3. Specify unique name, configuration for origin settings such as type, host header, and origin port for HTTP and HTTPS. 4. Click Add button AZ-103: p. 141 Publish content in a CDN endpoint Azure Portal Create a new CDN profile Add an endpoint to the profile DNS Create DNS zone New-AzDnsZone -Name examref . com -ResourceGroupName ExamRefRG az network dns zone create --name examref.com --resource-group ExamRefRG Create empty A record New-AzDnsRecordSet -Name www -RecordType A -ZoneName examref . com -ResourceGroupName ExamRefRG -Ttl 3600 -DnsRecords ( New-AzDnsRecordConfig -IPv4Address \"1.2.3.4\" ) az network dns record-set a create --name www --zone-name examref.com --resource-group ExamRefRG --ttl 3600 Create multiple records $records = @() $records += New-AzDnsRecordConfig -IPv4Address \"1.2.3.4\" $records += New-AzDnsRecordConfig -IPv4Address \"5.6.7.8\" New-AzDnsRecordSet -Name \"@\" -RecordType A -ZoneName examref . com -ResourceGroupName ExamRefRG -Ttl 3600 -DnsRecords $records az network dns record-set a add-record --record-set-name www --zone-name examref.com --resource-group ExamRefRG --ipv4-address 1 .2.3.4 az network dns record-set a add-record --record-set-name www --zone-name examref.com --resource-group ExamRefRG --ipv4-address 5 .6.7.8 Remove record $recordset = Get-AzDnsRecordSet -Name www -RecordType A -ZoneName examref . com -ResourceGroupName ExamRefRG Add-AzdnsRecordConfig -RecordSet $recordset -IPv4Address \"5.6.7.8\" Remove-AzDnsRecordConfig -RecordSet $recordset -IPv4Address \"1.2.3.4\" Set-AzDnsRecordSet -RecordSet $recordset az network dns record-set a remove-record --record-set-name www --zone-name examref.com --resource-group ExamRefRG --ipv4-address 1 .2.3.4 Read records Get-AzDnsRecordSet -ZoneName examref . com -ResourceGroupName ExamRefRG az network dns record-set list --zone-name examref.com --resource-group ExamRefRG -o table Create a virtual network with custom DNS settings New-AzVirtualNetwork -Name VNet1 -ResourceGroupName $rgName -Location $location -AddressPrefix 10 . 1 . 0 . 0 / 16 -Subnet ( New-AzVirtualNetworkSubnetConfig -Name Default -AddressPrefix 10 . 1 . 0 . 0 / 24 ) -DNSServer 10 . 0 . 0 . 4 , 10 . 0 . 0 . 5 az network vnet create --name VNet1 --resource-group $rgName --address-prefixes 10 .0.0.0/16 --dns-servers 10 .0.0.4 10 .0.0.5 Modify the DNS server configuration of an existing VNET $vnet = Get-AzVirtualNetwork -Name $vnetName -ResourceGroupName $rgName $vnet . DhcpOptions . DnsServers . Clear () $vnet . DhcpOptions . DnsServers . Add ( \"10.10.200.1\" ) $vnet . DhcpOptions . DnsServers . Add ( \"10.10.200.2\" ) Set-AzVirtualNetwork -VirtualNetwork $vnet az network vnet update --name $vnetName --resource-group $rgName --dns-servers 10 .10.200.1 10 .10.200.2 Restart the VMs in the VNet to pick up the DNS change $vm = Get-AzVM -Name VNet1-VM -ResourceGroupName ExamRefRG Restart-AzVM -ID $vm . Id Update the DNS settings on a NIC $nic = Get-AzNetworkInterface -Name VM1-NIC -ResourceGroupName ExamRefRG $nic . DnsSettings . DnsServers . Clear () $nic . DnsSettings . DnsServers . Add ( \"8.8.8.8\" ) $nic . DnsSettings . DnsServers . Add ( \"8.8.4.4\" ) Commit the DNS change, causing the VM to restart Set-AzNetworkInterface -NetworkInterface $nic Remove custom DNS servers from a VNET az network vnet update --name VNet1 --resource-group ExamRefRG --remove DHCPOptions.DNSServers Set custom DNS servers on a NIC az network nic update --name VM1-NIC --resource-group ExamRefRG --dns-servers 8 .8.8.8 8 .8.4.4 Load balancing Create public load balancer Creating a load balancer in PowerShell requires defining objects which are all passed to New-AzLoadBalancer as objects: - Frontend IP - Public Ip Address resource (if public) - Private IP address specified as a string (if internal) - Backend address pool - Health probe - Load balancing rule By contrast, in Azure CLI, the load balancer can be defined first with az network lb create before adding a probe and rule, passing the name of the load balancer to --lb-name . $publicIP = New-AzPublicIpAddress -Name ExamRefLB-IP -ResourceGroupName $g -Location $location -AllocationMethod Static $frontendIP = New-AzLoadBalancerFrontendIpConfig -Name frontend -PublicIpAddress $publicIP $beAddressPool = New-AzLoadBalancerBackendAddressPoolConfig -Name backend $healthProbe = New-AzLoadBalancerProbeConfig -Name -RequestPath '/' -Protocol http -Port 80 $lbrule = New-AzLoadBalancerRuleConfig -Name -FrontendIpConfiguration $frontendIP -BackendAddressPool $beAddressPool -Probe $healthProbe -Protocol Tcp -FrontendPort 80 -BackendPort 80 $lb = New-AzLoadBalancer -ResourceGroupName -Name -Location -FrontendIpConfiguration $frontendIP -LoadBalancingRule $lbrule -BackendAddressPool $beAddressPool -Probe $healthProbe az network public-ip create --name ExamRefLB-IP --resource-group ExamRefRG --location --allocation-method Static az network lb create --name ExamRefLB --resource-group ExamRefRG --location --backend-pool-name backend --frontend-ip-name frontend --public-ip-address ExamRefLB-IP az network lb probe create --resource-group ExamRefRG --name HealthProbe --lb-name ExamRefLB --protocol http --port 80 --path / --interval 5 --threshold az network lb rule create --name ExamRefRule --lb-name ExamRefLB --resource-group ExamRefRG --protocol Tcp --frontend-port 80 --backend-port 80 --frontend-ip-name ExamRefFrontEnd --backend-pool-name backend --probe-name HealthProbe","title":"Tasks"},{"location":"Cloud/Tasks/#tasks","text":"","title":"Tasks"},{"location":"Cloud/Tasks/#administration","text":"Display subscription ID Get-AzSubscription az account show","title":"\ud83d\udee0&#xfe0f; Administration"},{"location":"Cloud/Tasks/#cli","text":"Initialize CLI utility gcloud init","title":"\ud83d\udda5&#xfe0f; CLI"},{"location":"Cloud/Tasks/#iam","text":"Add guest user New-AzureADMSInvitation -InvitedUserEmailAddress $EMAIL -SendInvitationMessage $True -InviteRedirectUrl \"http://myapps.onmicrosoft.com\" Assign a role # At the organization level gcloud organizations add-iam-policy-binding $ORG_ID --member = \"user: $EMAIL \" --role = \"roles/compute.xpnAdmin\" # At the folder level gcloud beta resource-manager-folders add-iam-policy-binding $FOLDER_ID --member = \"user: $EMAIL \" --role = \"roles/compute.xpnAdmin\"","title":"IAM"},{"location":"Cloud/Tasks/#cost-management","text":"To view resource quotas for a subscription, go to the subscription in Azure Portal and open the Usage + quotas blade. From there you can select resources and then click the Request Increase button. View current usage of vCPU quotas Get-AzVMUsage View current usage of storage service Get-AzStorageUsage Create a budget To create a budget, open Cost Management + Billing , then Subscriptions , select a subscription, then click Budgets . Then click + Add , which produces a Create budget blade. The created budget can be seen in the Budgets blade. PowerShell commands used with budgets: Get-AzResourceGroup retrieve Resource Group object Set-AzResourceGroup apply a tag to a resource group with no preexisting tags .Tags method that retrieves Tag collection from a resource group .Add() method used to add tags to a resource group that already has tags.","title":"\ud83d\udcb0 Cost management"},{"location":"Cloud/Tasks/#monitoring","text":"VM extension Set-AzVMExtension -ResourceGroupName ExamRefRG -Location \"West Europe\" -VMName VM1 -Name networkWatcherAgent -Publisher Microsoft . Azure . NetworkWatcher -Type NetworkWatcherAgentWindows -TypeHandlerVersion 1 . 4 az vm extension set --vm-name VM1 --resource-group ExamRefRG --publisher Microsoft.Azure.NetworkWatcher --version 1 .4 --name NetworkWatcherAgentWindows --extension-instance-name NetworkWatcherAgent Start packet capture $nw = Get-AzResource | Where ResourceType -eq \"Microsoft.Network/networkWatchers\" -and Location -eq \"WestEurope\" $networkWatcher = Get-AzNetworkWatcher -Name $nw . Name -ResourceGroupName $nw . ResourceGroupName $storageAccount = Get-AzStorageAccount -Name examref-storage -ResourceGroupName ExamRefRG $filter1 = New-AzPacketCaptureFilterConfig -Protocol TCP -RemoteIPAddress \"1.1.1.1-255.255.255.255\" -LocalIPAddress \"10.0.0.3\" -LocalPort \"1-65535\" -RemotePort \"20;80;443\" $filter2 = New-AzPacketCaptureFilterConfig -Protocol UDP $vm = Get-AzVM ` -Name VM1 -ResourceGroupName ExamRefRG New-AzNetworkWatcherPacketCapture -NetworkWatcher $networkWatcher -TargetVirtualMachineId $vm . Id -PacketCaptureName \"PacketCaptureTest\" -StorageAccountId $storageAccount . id -TimeLimitInSeconds 60 -Filter $filter1 , $filter2 filter = '[ { \"protocol\": \"TCP\", \"remoteIPAddress\": \"1.1.1.1-255.255.255.255\", \"localIPAddress\":\"10.0.0.3\", \"remotePort\":\"20\" } ]' az network watcher packet-capture create --name PacketCaptureTest2 --resource-group ExamRefRG --vm VM1 --time-limit 300 --storage-account examref-storage --filters $filter Check status of packet capture Get-AzNetworkWatcherPacketCapture -NetworkWatcher $networkWatcher -PacketCaptureName \"PacketCaptureTest\" az network watcher packet-capture show-status --name PacketCaptureTest --location WestEurope Stop packet capture Stop-AzNetworkWatcherPacketCapture -NetworkWatcher $networkWatcher -PacketCaptureName \"PacketCaptureTest\" az network watcher packet-capture stop --name PacketCaptureTest --location WestEurope Use IP Flow Verify to test outbound connectivity from source VM and port to destination. If any configured filtering rules block traffic between the endpoints, it will return the name of the offending NSG. Test-AzNetworkWatcherIPFlow az network watcher test-ip-flow Next Hop Get-AzNetworkWatcherNextHop az network watcher show-next-hop Use Network Topology Get-AzNetworkWatcherTopology az network watcher show-topology Capture SFTP traffic $r = Get-AzResource | where ResourceType -eq \"Microsoft.Network/networkWatchers\" -and Location -eq \"EastUS\" $nw = Get-AzNetworkWatcher -Name $r . Name -ResourceGroupName $r . ResourceGroupName $s = Get-AzStorageAccount -ResourceGroupName \"Diagnostics-RG\" -Name \"Diagnostics-Storage\" $filter = New-AzPacketCaptureFilterConfig -Protocol TCP -RemoteIPAddress \"1.1.1.1-255.255.255.255\" -LocalIPAddress \"10.0.0.4\" -LocalPort \"1-65535\" -RemotePort \"22\" New-AzNetworkWatcherPacketCapture -NetworkWatcher $nw -TargetVirtualMachineId $vm . ID -PacketCaptureName \"Capture SFTP traffic\" -StorageAccountId $s . Id -TimeLimitInSeconds 60 -Filter $filter","title":"Monitoring"},{"location":"Cloud/Tasks/#policy","text":"Assign a policy $scope = '/subscriptions/$subscriptionID' $policyparam = '{ \"tagName\" : { \"value\": \"Environment\" }, \"tagValue\": { \"value\" : \"Production\" } }' $assignment = New-AzPolicyAssignment -Name 'append-environment-tag' -DisplayName 'Append Environment Tag' -Scope $scope -PolicyDefinition $definition -PolicyParameter $policyparam Remove policy assignment and definition Remove-AzPolicyAssignment -Id $assignment . ResourceId Remove-AzPolicyDefinition -Id $definition . ResourceId Create a policy definition Azure Portal (All Services) > Policy > Definitions: Both builtin and custom policies can be managed here. New-AzPolicyDefinition -Name 'appendEnvironmentTag' -DisplayName 'Append Environment Tag' -Policy 'AppendDefaultTag.json' -Parameter 'AppendDefaultTagParams.json' az policy definition create --name 'allowedVMs' --description 'Only allow virtual machines in the defined SKUs' --mode ALL --rules '{...}' --params '{...}' Apply policy to a scope az policy assignment create --policy allowedVMs --name 'deny-non-compliant-vms' --scope '/subscriptions/<Subscription ID>' -p Delete policy assignment az policy assignment delete --name deny-non-compliant-vms","title":"Policy"},{"location":"Cloud/Tasks/#resources","text":"Create resource group New-AzGroup -Location $location -Name $rgName az group create -l $location -n $rgName Register resource provider in subscription az provider register --namespace 'Microsoft.PolicyInsights' Move resources $webapp = Get-AzResource -ResourceGroupName OldRG -ResourceName ExampleSite $plan = Get-AzResource -ResourceGroupName OldRG -ResourceName ExamplePlan Move-AzResource -DestinationResourceGroupName NewRG -ResourceId $webapp . ResourceId , $plan . ResourceId webapp = $( az resource show -g OldRG -n ExampleSite --resource-type \"Microsoft.Web/sites\" --query id --output tsv ) plan = $( az resource show -g OldRG -n ExamplePlan --resource-type \"Microsoft.Web/serverfarms\" --query id --output tsv ) az resource move --destination-group newgroup --ids $webapp $plan Create lock on a resource New-AzResourceLock -LockName LockSite -LockLevel CanNotDelete -ResourceGroupName $rg -ResourceName $r -ResourceType Microsoft . Web / sites az lock create --name LockSite --lock-type CanNotDelete --resource-group $rg --resource-name $r --resource-type Microsoft.Web/sites Create lock on a resource group New-AzResourceLock -LockName LockGroup -LockLevel CanNotDelete -ResourceGroupName $rg az lock create --name LockGroup --lock-type CanNotDelete --resource-group $rg Display resource lock Get-AzResourceLock -ResourceName $r -ResourceType Microsoft . Web / sites -ResourceGroupName $rg az lock list --resource-group $rg --resource-name $r --namespace Microsoft.Web --resource-type sites --parent \"\" Delete resource lock $lockId = ( Get-AzResourceLock -ResourceGroupName $rg -ResourceName $r -ResourceType Microsoft . Web / sites ). LockId Remove-AzResourceLock -LockId $lockId lockid = $( az lock show --name LockSite --resource-group $rg --resource-type Microsoft.Web/sites --resource-name $r --output tsv --query id ) az lock delete --ids $lockid Sources Manage Azure Resource Manager resource groups by using Azure PowerShell Manage Azure Resource Manager resource groups by using Azure CLI Resource providers Lock resources to prevent unexpected changes AZ-103: 1.3 , p. 76","title":"Resources"},{"location":"Cloud/Tasks/#tags","text":"List all resources by tag ( Get-AzResource -Tag @{ CostCode = \"1001\" }). Name # List all resources by tag name, with no value ( Get-AzResource -TagName CostCode ). Name az resource list --tag Dept = Finance List resource groups by tag ( Get-AzResourceGroup -Tag @{ CostCode = \"1001\" }). ResourceGroupName az group list --tag CostCode = 1001 Enumerate a resource's tags $r = Get-AzResource -Name $resourceName -ResourceGroup rg Get-AzTag -ResourceId $r . id # Resource group $rg = Get-AzResourceGroup -Name $rgName Get-AzTag -ResourceId $rg . ResourceId # Subscription $s = ( Get-AzSubscription -SubscriptionName \"Example Subscription\" ). Id Get-AzTag -ResourceId \"/subscriptions/$s\" az resource show -n $resourceName -g $rgName --query tags # Resource group az group show -n $rgName --query tags Tag resource $r = Get-AzResource -ResourceName hrvm1 -ResourceGroupName rg $r . Tags . Add ( \"Owner\" , \"user@contoso.com\" ) Set-AzResource -Tag $r . Tags -ResourceId $r . ResourceId -Force Resource group $tags = @{ \"Dept\" = \"Finance\" ; \"Status\" = \"Normal\" } $rg = Get-AzResourceGroup -Name demoGroup New-AzTag -ResourceId $rg . ResourceId -tag $tags $tags = ( Get-AzResourceGroup -Name rg ). Tags $tags . Add ( \"Owner\" , \"user@contoso.com\" ) Set-AzResourceGroup -Tag $tags -Name rg jsonrtag = $( az group show -n rg --query tags ) rt = $( echo $jsonrtag | tr -d '\"{},' | sed 's/: /=/g' ) az group update -n rg --tags $rt Owner = user@contoso.com Remove specific tags $tags = @{ \"Project\" = \"ECommerce\" ; \"Team\" = \"Web\" } Update-AzTag -ResourceId $resource . id -Tag $tags -Operation Delete Remove all tags $s = ( Get-AzSubscription -SubscriptionName \"Example Subscription\" ). Id Remove-AzTag -ResourceId \"/subscriptions/$s\" # Alternatively Set-AzResourceGroup -Tag @{} -Name rg Apply tags to resource, overwriting $tags = @{ \"Dept\" = \"Finance\" ; \"Status\" = \"Normal\" } New-AzTag -ResourceId $resource . id -Tag $tags Set-AzResource -ResourceId $r . ResourceId -Tag @{ CostCode = \"1001\" ; Environment = \"Production\" } -Force az resource tag --tags 'Dept=IT' 'Environment=Test' -g $rgName -n examplevnet --resource-type \"Microsoft.Network/virtualNetworks\" Apply tags to resource group Set-AzResourceGroup -Name rg -Tag @{ CostCode = 1001 ; Environment = Production } az group update -n $rgName --tags 'Environment=Test' 'Dept=IT' # Alternatively az group update -n $rgName --set tags.Environment = Production tags.CostCode = 1001","title":"Tags"},{"location":"Cloud/Tasks/#compute","text":"","title":"Compute"},{"location":"Cloud/Tasks/#iaas","text":"Create a VM ( src ) gcloud compute instances create instance-1 --zone-uscentral1-a","title":"IaaS"},{"location":"Cloud/Tasks/#paas","text":"Deploy app in current working directory. gcloud app deploy View the deployed app gcloud app browse app.yaml allows configuration of the app in several ways runtime : python37 In Azure, multiple web applications are organized under an App Service Plan resource. So if no such app service plan exists, it must be created. $p = New-AzAppServicePlan -Name $n -ResourceGroupName $g -Location $l -Tier \"Basic\" -NumberofWorkers 2 -WorkerSize \"Small\" New-AzWebApp -Name $n -Location $l -ResourceGroupName $g -AppServicePlan $p az appservice plan create -g $g -n $p --is-linux az webapp create -n $n -g $g --plan $p","title":"PaaS"},{"location":"Cloud/Tasks/#containers","text":"Create a new source repository These steps require: Cloud SDK and Git to be installed A GCP project with billing and the Cloud Source Repositories API enabled #Create a new repository gcloud source repos create hello-world #Clone it locally gcloud source repos clone hello-world # Create scripts, then add, commit and push them as usual. git commit -am \"Initial\" git push origin master Create container registry New-AzContainerRegistry -ResourceGroupName $rg -Name $registry -Sku \"Basic\" -EnableAdminUser az acr create --name $registry --resource-group $rg --sku Basic --admin-enabled true","title":"Containers"},{"location":"Cloud/Tasks/#kubernetes","text":"Create Kubernetes cluster New-AzAKS -ResourceGroupName $g -Name $n -NodeCount 2 -NetworkPlugin azure -NodeVmSetType VirtualMachineScaleSets -WindowsProfileAdminUserName azureuser -WindowsProfileAdminUserPassword $Password -KubernetesVersion 1 . 16 . 7 # PowerShell does not offer an option to generate SSH keys for access to the cluster; `ssh-keygen` must be used. - Create a Windows Server container on an AKS cluster az aks create -g $g -n $n --node-count 2 --network-plugin azure --vm-set-type VirtualMachineScaleSets --windows-admin-username azureuser --windows-admin-password $PASSWORD --generate-ssh-keys --enable-addons monitoring - Create a Windows Server container on an AKS cluster Add a pool of nodes New-AzAksNodePool -ResourceGroupName $rgName -Name npwin -ClusterName $clusterName -OsType Windows -KubernetesVersion 1 . 16 . 7 az aks nodepool add -g $g -n $n --cluster-name $clusterName --os-type Windows --node-count 1 Persistent volume claim apiVersion : v1 kind : PersistentVolumeClaim metadata : name : azure-managed-disk spec : accessModes : - ReadWriteOnce storageClassName : managed-premium resources : requests : storage : 5Gi - Source Provision Azure Disk Standard kind : StorageClass apiVersion : storage.k8s.io/v1 metadata : name : managed-disk-forapp provisioner : kubernetes.io/azure-disk reclaimPolicy : Retain parameters : storageaccounttype : default kind : Managed Premium kind : StorageClass apiVersion : storage.k8s.io/v1 metadata : name : managed-disk-forapp provisioner : kubernetes.io/azure-disk reclaimPolicy : Retain parameters : storageaccounttype : Premium_LRS kind : Managed","title":"\u2693 Kubernetes"},{"location":"Cloud/Tasks/#functions","text":"Deploy gcloud functions deploy hello_get --runtime python37 --trigger-http Test gcloud functions describe hello_get","title":"Functions"},{"location":"Cloud/Tasks/#storage","text":"Create storage account Azure Portal Click Create a resouce , then Storage , then Storage account . Choose a globally unique name for the account, containing lower-case characters and digits only. New-AzStorageAccount -ResourceGroupName ExamRefRG -Name mystorage112300 -SkuName Standard_LRS -Location WestUS -Kind StorageV2 -AccessTier Hot az storage account create --name $accountName --resource-group $resourceGroup -location $location --sku $sku Change access tier of storage account === \"Azure PowerShell ```powershell Set-AzStorageAccount -ResourceGroupName RG -Name $accountName -AccessTier Cool -Force ``` Change replication mode of storage account Set-AzStorageAccount -ResourceGroupName $resourceGroup -Name $accountName -SkuName $type Renew storage account keys === \"Azure ```powershell New-AzStorageAccountKey ``` az storage account keys renew Create Azure Key Vault New-AzKeyVault -VaultName $vaultName -ResourceGroupName $g -Location $location $key = Add-AzKeyVaultKey -VaultName $vaultName -Name $keyName -Destination 'Software' $storageKey = Get-AzStorageAccountKey -ResourceGroupName $g -Name $storageAccount $secretvalue = ConvertTo-SecureString $storageKey [ 0 ]. Value -AsPlainText -Force $secret = Set-AzKeyVaultSecret -VaultName $vaultName -Name $secretName -SecretValue $secretvalue az keyvault create --name $vaultName --resource-group $g --location $location az keyvault key create --vault-name \" $vaultName \" --name $keyName --protection \"software\" az keyvault secret set --vault-name \" $vaultName \" --name \" $secretName \" --value \" $secretValue \" Create key in Azure Key Vault $key = Add-AzKeyVaultKey -VaultName $vaultName -Name $keyName -Destination 'Software' $storageKey = Get-AzStorageAccountKey -ResourceGroupName $g -Name $storageAccount $secretvalue = ConvertTo-SecureString $storageKey [ 0 ]. Value -AsPlainText -Force $secret = Set-AzKeyVaultSecret -VaultName $vaultName -Name $secretName -SecretValue $secretvalue az keyvault key create --vault-name $vaultName --name $keyName --protection \"software\" az keyvault secret set --vault-name $vaultName --name $secretName --value $secretValue Create Azure sync group Specify name of sync group in dialog after creating an Azure File Sync Change storage class $STORAGE_CLASS can be multi_regional , regional , nearline , or coldline gsutil rewrite -s $STORAGE_CLASS gs:// $PATH_TO_OBJECT","title":"Storage"},{"location":"Cloud/Tasks/#file-shares","text":"Deploy Azure File Sync # Create Storage Sync Service $storageSync = New-AzStorageSyncService -ResourceGroupName $g -Name $storageSyncName -Location $l # Create Azure File Share $storageKey = Get-AzStorageAccountKey -ResourceGroupName $g -Name $storageAccount $context = New-AzStorageContext -StorageAccountName $storageAccount -StorageAccountKey $storageKey . Value [ 0 ] New-AzStorageShare -Name $shareName -Context $context # Creating a Storage Sync Service resource is only possible in PowerShell or Portal constring = $( az storage account show-connection-string -n $storageAccountName ) az storage share create --name $shareName --quota 2048 --connection-string $constring Create sync group $syncgroup = New-AzStorageSyncGroup -Name $syncgroupname -ParentObject $storageSync Create cloud endpoint New-AzStorageSyncCloudEndpoint -Name $shareName -ParentObject $syncgroup -StorageAccountResourceId $storageAccount . Id -AzureFileShareName $shareName","title":"File shares"},{"location":"Cloud/Tasks/#network-access","text":"Display the status of the default NetworkRule for a storage account Get-AzStorageAccountNetworkRuleSet -ResourceGroupName $rgName -AccountName $n | Select-Object DefaultAction az storage account show - $rgName -n $n --query networkRuleSet.defaultAction Set default rule Update-AzStorageAccountNetworkRuleSet -ResourceGroupName $g -Name $n -DefaultAction Deny Update-AzStorageAccountNetworkRuleSet -ResourceGroupName $g -Name $n -DefaultAction Allow az storage account update -g $g -n $n --default-action Deny az storage account update -g $g -n $n --default-action Allow","title":"Network access"},{"location":"Cloud/Tasks/#networking","text":"Create virtual network with a specific prefix and subnet $subnet = New-AzVirtualNetworkSubnetConfig -Name $subnetName -AddressPrefix \"10.0.0.0/24\" $vnet = New-AzVirtualNetwork -Name $name -ResourceGroupName $rgName -Location $l -AddressPrefix \"10.0.0.0/16\" -Subnet $subnet az network vnet create -g $rgName -n $name --address-prefix \"10.0.0.0/16\" --subnet-name $subnetName --subnet-prefix \"10.0.0.0/24\" gcloud networks create $name --subnet-mode = custom gcloud beta compute networks subnets create $subnetName --network = $name --region = $l --range = \"10.0.0.0/16\" --enable-private-ip-google-access --enable-flow-logs Create peering Add-AzVirtualNetworkPeering -Name 'peering1' -VirtualNetwork $net1 -RemoteVirtualNetworkId $net2 . Id Add-AzVirtualNetworkPeering -Name 'peering2' -VirtualNetwork $net2 -RemoteVirtualNetworkId $net1 . Id az network vnet peering create -n 'peering1' -g $g --vnet-name net1 --allow-vnet-access --remote-vnet net2 az network vnet peering create -n 'peering2' -g $g --vnet-name net2 --allow-vnet-access --remote-vnet net1 gcloud compute networks peerings create \"peering1\" --network net1 --peer-project $p --peer-network net2 --auto-create-routes gcloud compute networks peerings create \"peering2\" --network net1 --peer-project $p --peer-network net1 --auto-create-routes Check peering Get-AzVirtualNetworkPeering -ResourceGroupName $rg -VirtualNetworkName $vnetName az network vnet peering list --resource-group $rg --vnet-name VNet1 az network vnet peering list --resource-group $rg --vnet-name VNet2 User-defined routes # Create the route table resource $routeTable = New-AzRouteTable -Name $routeTableName -ResourceGroupName ExamRefRG # Add a route to route table object Add-AzRouteConfig -RouteTable $routeTable -Name $routeName -AddressPrefix 10 . 3 . 0 . 0 / 16 -NextHopType VirtualAppliance -NextHopIpAddress 10 . 2 . 20 . 4 Set-AzRouteTable -RouteTable $routeTable # Associate route table with subnet Set-AzVirtualNetworkSubnetConfig -VirtualNetwork $vnet -Name Default -AddressPrefix $subnet . AddressPrefix -RouteTable $routeTable # Commit changes Set-AzVirtualNetwork -VirtualNetwork $vnet # Get effective routes for a NIC Get-AzEffectiveRouteTable -NetworkInterfaceName $nicName -ResourceGroupName $rgName # Create route table resource az network route-table create --name $routeTableName --resource-group $rgName # Add route to route table az network route-table route create --resource-group $rgName --route-table-name $routeTableName --name $routeName --address-prefix 10 .3.0.0/16 --next-hop-type VirtualAppliance --next-hop-ip-address 10 .2.20.4 # Associate route table with subnet az network vnet subnet update --name defualt --vnet-name Vnet1 --resource-group $rgName --route-table rt # Get effective routes for NIC az network nic show-effective-route-table --name $nicName --resource-group $rgName Create NSG $nsgRules = @() $nsgRules += New-AzNetworkSecurityRuleConfig -Name \"AllowingWinRMHTTP\" -Description \"To Enable PowerShell Remote Access\" -Access Allow -Protocol Tcp -Direction Inbound -Priority 103 -SourceAddressPrefix Internet -SourcePortRange * -DestinationAddressPrefix * -DestinationPortRange 5985 $nsgRules += New-AzNetworkSecurityRuleConfig -Name \"AllowingWinRMHTTPS\" -Description \"To Enable PowerShell Remote Access\" -Access Allow -Protocol Tcp -Direction Inbound -Priority 104 -SourceAddressPrefix Internet -SourcePortRange * -DestinationAddressPrefix * -DestinationPortRange 5986 $nsg = New-AzNetworkSecurityGroup -Name \"wscore-nsg\" -ResourceGroupName \"RG\" -Location \"East US\" -SecurityRules $nsgRules View rules Get-AzEffectiveNetworkSecurityGroup -NetworkInterfaceName $nicName -ResourceGroupName $rgName az network nic list-effective-nsg --name $nicName --resource-group $rgName Create Bastion Connecting to a VM requires at least Reader role privileges on the VM, its NIC, and on the Bastion itself. New-AzBastion -ResourceGroupName $rgName -Name $n -PublicIpAddress $pip -VirtualNetwork $vnet az network bastion create -g $rgName -n $n -l $l --public-ip-address $pip --vnet-name $vnetName Create virtual appliance IP forwarding must be enabled on the VM's NIC, then applications installed on the VM can begin accepting packets destined for other IP addresses.","title":"Networking"},{"location":"Cloud/Tasks/#cdn","text":"Create new profile Azure Portal Click Create a resource Click Web Click CDN , opening the CDN profile blade Specify name for the profile, name of the resource group, region, and pricing tier. Click Create AZ-103: p. 140 Create endpoint Azure Portal Add an endpoint to a CDN profile (Portal) 1. Open the CDN Profile 2. Click + Endpoint button 3. Specify unique name, configuration for origin settings such as type, host header, and origin port for HTTP and HTTPS. 4. Click Add button AZ-103: p. 141 Publish content in a CDN endpoint Azure Portal Create a new CDN profile Add an endpoint to the profile","title":"CDN"},{"location":"Cloud/Tasks/#dns","text":"Create DNS zone New-AzDnsZone -Name examref . com -ResourceGroupName ExamRefRG az network dns zone create --name examref.com --resource-group ExamRefRG Create empty A record New-AzDnsRecordSet -Name www -RecordType A -ZoneName examref . com -ResourceGroupName ExamRefRG -Ttl 3600 -DnsRecords ( New-AzDnsRecordConfig -IPv4Address \"1.2.3.4\" ) az network dns record-set a create --name www --zone-name examref.com --resource-group ExamRefRG --ttl 3600 Create multiple records $records = @() $records += New-AzDnsRecordConfig -IPv4Address \"1.2.3.4\" $records += New-AzDnsRecordConfig -IPv4Address \"5.6.7.8\" New-AzDnsRecordSet -Name \"@\" -RecordType A -ZoneName examref . com -ResourceGroupName ExamRefRG -Ttl 3600 -DnsRecords $records az network dns record-set a add-record --record-set-name www --zone-name examref.com --resource-group ExamRefRG --ipv4-address 1 .2.3.4 az network dns record-set a add-record --record-set-name www --zone-name examref.com --resource-group ExamRefRG --ipv4-address 5 .6.7.8 Remove record $recordset = Get-AzDnsRecordSet -Name www -RecordType A -ZoneName examref . com -ResourceGroupName ExamRefRG Add-AzdnsRecordConfig -RecordSet $recordset -IPv4Address \"5.6.7.8\" Remove-AzDnsRecordConfig -RecordSet $recordset -IPv4Address \"1.2.3.4\" Set-AzDnsRecordSet -RecordSet $recordset az network dns record-set a remove-record --record-set-name www --zone-name examref.com --resource-group ExamRefRG --ipv4-address 1 .2.3.4 Read records Get-AzDnsRecordSet -ZoneName examref . com -ResourceGroupName ExamRefRG az network dns record-set list --zone-name examref.com --resource-group ExamRefRG -o table Create a virtual network with custom DNS settings New-AzVirtualNetwork -Name VNet1 -ResourceGroupName $rgName -Location $location -AddressPrefix 10 . 1 . 0 . 0 / 16 -Subnet ( New-AzVirtualNetworkSubnetConfig -Name Default -AddressPrefix 10 . 1 . 0 . 0 / 24 ) -DNSServer 10 . 0 . 0 . 4 , 10 . 0 . 0 . 5 az network vnet create --name VNet1 --resource-group $rgName --address-prefixes 10 .0.0.0/16 --dns-servers 10 .0.0.4 10 .0.0.5 Modify the DNS server configuration of an existing VNET $vnet = Get-AzVirtualNetwork -Name $vnetName -ResourceGroupName $rgName $vnet . DhcpOptions . DnsServers . Clear () $vnet . DhcpOptions . DnsServers . Add ( \"10.10.200.1\" ) $vnet . DhcpOptions . DnsServers . Add ( \"10.10.200.2\" ) Set-AzVirtualNetwork -VirtualNetwork $vnet az network vnet update --name $vnetName --resource-group $rgName --dns-servers 10 .10.200.1 10 .10.200.2 Restart the VMs in the VNet to pick up the DNS change $vm = Get-AzVM -Name VNet1-VM -ResourceGroupName ExamRefRG Restart-AzVM -ID $vm . Id Update the DNS settings on a NIC $nic = Get-AzNetworkInterface -Name VM1-NIC -ResourceGroupName ExamRefRG $nic . DnsSettings . DnsServers . Clear () $nic . DnsSettings . DnsServers . Add ( \"8.8.8.8\" ) $nic . DnsSettings . DnsServers . Add ( \"8.8.4.4\" ) Commit the DNS change, causing the VM to restart Set-AzNetworkInterface -NetworkInterface $nic Remove custom DNS servers from a VNET az network vnet update --name VNet1 --resource-group ExamRefRG --remove DHCPOptions.DNSServers Set custom DNS servers on a NIC az network nic update --name VM1-NIC --resource-group ExamRefRG --dns-servers 8 .8.8.8 8 .8.4.4","title":"DNS"},{"location":"Cloud/Tasks/#load-balancing","text":"Create public load balancer Creating a load balancer in PowerShell requires defining objects which are all passed to New-AzLoadBalancer as objects: - Frontend IP - Public Ip Address resource (if public) - Private IP address specified as a string (if internal) - Backend address pool - Health probe - Load balancing rule By contrast, in Azure CLI, the load balancer can be defined first with az network lb create before adding a probe and rule, passing the name of the load balancer to --lb-name . $publicIP = New-AzPublicIpAddress -Name ExamRefLB-IP -ResourceGroupName $g -Location $location -AllocationMethod Static $frontendIP = New-AzLoadBalancerFrontendIpConfig -Name frontend -PublicIpAddress $publicIP $beAddressPool = New-AzLoadBalancerBackendAddressPoolConfig -Name backend $healthProbe = New-AzLoadBalancerProbeConfig -Name -RequestPath '/' -Protocol http -Port 80 $lbrule = New-AzLoadBalancerRuleConfig -Name -FrontendIpConfiguration $frontendIP -BackendAddressPool $beAddressPool -Probe $healthProbe -Protocol Tcp -FrontendPort 80 -BackendPort 80 $lb = New-AzLoadBalancer -ResourceGroupName -Name -Location -FrontendIpConfiguration $frontendIP -LoadBalancingRule $lbrule -BackendAddressPool $beAddressPool -Probe $healthProbe az network public-ip create --name ExamRefLB-IP --resource-group ExamRefRG --location --allocation-method Static az network lb create --name ExamRefLB --resource-group ExamRefRG --location --backend-pool-name backend --frontend-ip-name frontend --public-ip-address ExamRefLB-IP az network lb probe create --resource-group ExamRefRG --name HealthProbe --lb-name ExamRefLB --protocol http --port 80 --path / --interval 5 --threshold az network lb rule create --name ExamRefRule --lb-name ExamRefLB --resource-group ExamRefRG --protocol Tcp --frontend-port 80 --backend-port 80 --frontend-ip-name ExamRefFrontEnd --backend-pool-name backend --probe-name HealthProbe","title":"Load balancing"},{"location":"Coding/6502/","text":"The 6502 processor is an 8-bit CPU that was used in many computers and consoles, including the NES and SNES. Specifications Address bus: 16 bit Emulation Component C++ Memory struct Memory The first 256B of memory is referred to as the ($0000-$00FF). Memory range Description 0x0000-0x00FF Zero Page 0x0100-0x01FF Stack Registers # Register Size Description PC Program Counter 16 bit pointer to the next instruction SP Stack pointer 8 bit holds low 8 bits of the next free location on the stack A Accumulator 8 bit used for all arithmetic operations (except for incrementation and decrementation) X X register 8 bit available for holding counter or offset values for memory access. It also has the special function of copying or changing the value of SP. Y Y register 8 bits available for holding counter or offset values for memory access. Opcodes All opcodes are 1 byte in size, and their operands are 0, 1, or 2 bytes Opcode Code Description CMP ( src ) LDA Load from ZP to A STA $85 Store accumulator in memory ( src ) LDA LDA #$33 ; Load 69 into A STA Exapunks Instruction Description LINK Move GRAB COPY","title":"6502"},{"location":"Coding/6502/#specifications","text":"Address bus: 16 bit","title":"Specifications"},{"location":"Coding/6502/#emulation","text":"Component C++ Memory struct","title":"Emulation"},{"location":"Coding/6502/#memory","text":"The first 256B of memory is referred to as the ($0000-$00FF). Memory range Description 0x0000-0x00FF Zero Page 0x0100-0x01FF Stack","title":"Memory"},{"location":"Coding/6502/#registers","text":"# Register Size Description PC Program Counter 16 bit pointer to the next instruction SP Stack pointer 8 bit holds low 8 bits of the next free location on the stack A Accumulator 8 bit used for all arithmetic operations (except for incrementation and decrementation) X X register 8 bit available for holding counter or offset values for memory access. It also has the special function of copying or changing the value of SP. Y Y register 8 bits available for holding counter or offset values for memory access.","title":"Registers"},{"location":"Coding/6502/#opcodes","text":"All opcodes are 1 byte in size, and their operands are 0, 1, or 2 bytes Opcode Code Description CMP ( src ) LDA Load from ZP to A STA $85 Store accumulator in memory ( src )","title":"Opcodes"},{"location":"Coding/6502/#lda","text":"LDA #$33 ; Load 69 into A","title":"LDA"},{"location":"Coding/6502/#sta","text":"","title":"STA"},{"location":"Coding/6502/#exapunks","text":"Instruction Description LINK Move GRAB COPY","title":"Exapunks"},{"location":"Coding/C%23/","text":"C# To-do Sort out Events section, in particular the example cited Develop C# implementation of CSV parser. This appears to be harder than it should be, apparently because the object returned by the CsvReader.GetRecords<T>() method is an IEnumerable which is one of the confusing points of the UWP course. YouTube tutorials do not seem helpful.. Variables Parsing Data types can be used as static classes, exposing a TryParse method. Int32 parsedInt = Int32 . TryParse ( rawInt ); DateTime parsedDate = DateTime . TryParse ( rawDate ); String Specify a verbatim literal string by prepending @ , which disables escape characters and forces interpretation of backslashes literally: string filePath = @\"C:\\televisions\\sony\\bravia.txt\" ; Specify a formatted string by prepending a $ int n ; string s = $ \"{n} is a number\" ; Standard numeric format strings are used to format common numeric types. They take the form of a character (i.e. C for currency, N for number) followed by a number. They can be passed as arguments to the ToString method of the literal or in the placeholder of a formatted string after : . Console . WriteLine ( $ \"{123.456789:C }\" ); // $123.46 Console . WriteLine ( 123.456789d . ToString ( \"C\" )); // $123.46 A precision specifier can define the number of fractional digits after the decimal separator. Console . WriteLine ( $ \"{123.456789:C3 }\" ); // $123.457 Empty space can be added to either side of the value to create evenly spaced output by placing a number after a comma (positive for right-alignment, negative for left-alignment): Console . WriteLine ( $ \"{123.456789, 15}\" ); Casting Because real numbers are stored as double s by default, in order to assign to a float variable you must append f to the literal: float num = 3.14f ; A similar logic pertains for integers to be declared as doubles : double num = 3d ; decimal data type literals, which have 28-29 significant digits, can be declared with the m suffix decimal num = 123.4567890123456789 m char literals can be encoded in Unicode: char umlaut = '\\ u00F6 ' ; Variables can be explicitly cast to some other data types by placing the new data type in parentheses before the value: int pi = ( int ) System . Math . PI ; This casting won't work with string , which can be cast by using the Convert type or parsed using the data type's Parse method. The differense is that using Convert will return a 0 if the value is null while Parse will throw an exception. w = \"5\" ; int wConverted = System . Convert . ToInt32 ( w ); int wParsed = int . Parse ( w ); Collection Arrays Arrays are declared differently from built-in arrays in C++. C# int [] primes = { 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 }; C++ int primes [ 10 ] { 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 } An empty array must still have its size declared int [] primes = new int [ 10 ]; An unnamed array: new [] { 1 , 2 , 3 }; Arrays can be traversed with a foreach loop, but the elements can not be changed.: foreach ( var i in container ) { // ... } Arrays can be copied with the Clone() and Copy() methods: see ArrayCloning . Arrays can be reversed in place with the Reverse method. int [] primes = { 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 }; Array . Reverse ( primes ); LINQ Language Integrated Query (LINQ) refers to a C# library that facilitates querying of collections. These are exposed as extension methods : methods that are available on already existing queryable types This means extension methods are exposedon existing collection types like Array and List because they are derived from IEnumerable<T> , and thus need no modification to serve as a LINQ data source. Linq methods are available in two semantically identical syntaxes: query syntax and method syntax (also lambda syntax). Query syntax is meant to be more intuitive for developers familiar with SQL. Method syntax allows method chaining. Query syntax int [] numbers = { 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 }; var evens = from n in nums where n % 2 == 0 orderby n descending select n ; Method syntax int [] numbers = { 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 }; var evens = nums . Where ( n => n % 2 == 0 ). OrderByDescending ( n => n ); Python numbers = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] evens = [ n for n in numbers if n % 2 == 0 ] list ( reversed ( evens )) Notably, unlike loop structures in C#, LINQ methods can work on unordered collections like Dictionaries. ObservableCollection The ObservableCollection class is used to define collections that provide notifications to data bindings when items are added or removed. As such, it is used in GUI programming... DateTime DateTimeOffset is preferred over DateTime because it includes an offset value that indicates the timezone. Parsing DateTime is a class that exposes several static methods of parsing raw values. All of them have overloads that accept CultureInfo objects (implementing IFormatProvider ) which can affect parsing of ambiguous dates. Parse() will attempt to parse a string and raise a FormatException if unable to do so. ParseExact() requires an exact string template and requires a CultureInfo object as well. Parse() string rawDate = \"07/04/1776\" ; try { DateTime parsedDate = DateTime . Parse ( rawDate ); } catch ( FormatException ) { Console . WriteLine ( \"Unparsable!\" ) } Console . WriteLine ( parsedDate . ToLongDateString ()); // => \"July 4, 1776\" Specifying culture string rawDate = \"07/04/1776\" ; try { DateTime parsedDate = DateTime . Parse ( rawDate , CultureInfo . GetCultureInfo ( \"en-GB\" )); } catch ( FormatException ) { Console . WriteLine ( \"Unparsable!\" ) } Console . WriteLine ( parsedDate . ToLongDateString ()); // => \"April 7, 1776\" ParseExact() string rawDate = \"07/04/1776\" ; try { DateTime parsedDate = DateTime . ParseExact ( rawDate , \"M/d/yyyy\" , CultureInfo . InvariantCulture ); } catch ( FormatException ) { Console . WriteLine ( \"Unparsable!\" ) } Console . WriteLine ( parsedDate . ToLongDateString ()); // => \"July 4, 1776\" TryParse() returns no value, but takes an out parameter. It does not throw an exception if the date is unparsable, but rather outputs the default date January 1, 1 AD. The overload that accepts a Culture object also requires a DateTimeStyles object. TryParse() string rawDate = \"07/04/1776\" ; DateTime parsedDate ; DateTime . TryParse ( rawDate , out parsedDate ); Console . WriteLine ( parsedDate . ToLongDateString ()); // => \"July 4, 1776\" Specifying culture string rawDate = \"07/04/1776\" ; DateTime parsedDate ; DateTime . TryParse ( rawDate , CultureInfo . GetCultureInfo ( \"en-GB\" ), DateTimeStyles . None , out parsedDate ); Console . WriteLine ( parsedDate . ToLongDateString ()); // => \"April 7, 1776 ParseExact() TryParseExact() Timezones TimeZoneInfo includes static methods that can access system timezones. DateTime does not include timezone information, so it must be specified at runtime. TimeZoneInfo sidneyTimeZone = TimeZoneInfo . FindSystemTimeZoneById ( \"E. Australia Standard Time\" ); var sydneyTime = TimeZoneInfo . ConvertTime ( DateTime . Now , sydneyTimeZone ); Enumerating all system timezones. foreach ( var timeZone in TimeZoneInfo . GetSystemTimeZones ()) { Console . WriteLine ( timeZone . GetUtcOffset ()); } Methods Lambda A lambda expression can have two forms, both of which use the lambda declaration operator => Expression lambda ( input - parameters ) => expression Statement lambda ( input - parameters ) => { statements } Anonymous event handlers can be reformulated as lambdas to reduce code complexity. SubmitButton . Click += delegate ( object sender , EventArgs e ) { MessageBox . Show ( \"Button Clicked\" ); } // Using a (statement) lambda: SubmitButton . Click += ( s , e ) => MessageBox . Show ( \"Button Clicked\" ); ref ref allows variables that are normally passed by value to be passed by reference. Pass by value Integers are normally passed by value, so number will not change static void Main () { int number = 0 ; plusOne ( number ); } static void plusOne ( int n ) { n ++; } Pass by reference Now number will increment by one because it is being passed by reference. static void Main () { int number = 0 ; plusOne ( ref number ); } static void plusOne ( ref int n ) { n ++; } Returning a value Alternatively, number can be reassigned a variable if the method is refactored to return the new value. static void Main () { int number = 0 ; number = plusOne ( number ); } static int plusOne ( ref int n ) { n ++; return n ; } out out allows a method to assign a value to a variable that has no value yet. It can be used to return multiple values. static void Main () { double n = 5 ; double nSquared ; square ( n , out nSquared ); Console . WriteLine ( $ \"{n} ^ 2 = {nSquared}\" ); } static void square ( double x , out double y ) { y = System . Math . Pow ( x , 2 ); } out is prominently used in the TryParse method. params params allows you to process a variable number of similarly-typed arguments in the method signature. This collection of arguments is abstracted as an array. static void method ( int [] args ) { foreach ( int el in args ) { Console . WriteLine ( el ); } } If the arguments to be accepted are themselves arrays, then you must define an array of arrays ( ex. ). This technique may not have worked in previous versions of C#. static void method ( int [][] args ) { foreach ( var array in args ){ foreach ( int el in array ) { Console . WriteLine ( el ); } } } Delegates Delegates are a functional programming feature in C# that facilitate loose coupling. They allow a function to be abstracted so that updated logic can be implemented without incurring technical debt. Delegates take the form of a method signature using the delegate keyword. One or more methods implementing the delegate can be formulated which do not reference the delegate in any way, shape, or form, except for the fact that their method signature matches that specified by the delegate. Where the method is to be used, instead of calling the method directly, the delegate is instantiated like an object, but the name of the specific method that implements the delegate is passed as a parameter. The instantiated delegate can then be called, which passes the parameters to the method. This results in looser coupling because when changing implementation, only the parameter specifying the improved method needs to be adjusted, and the delegate ensures that the same pattern of parameters is enforced at compile-time. Delegates can be used for messaging in .NET and especially to tie events to event handlers, but they are no longer used as much as Func<T,TResult> and Action<T> . Initial implementation public delegate void InformationNeeded ( int n , string s ); static void Main () { InformationNeeded form = new InformationNeeded ( SimpleReport ) // ... form ( 2 , \"kiwi\" ); form ( 3 , \"jackfruit\" ); } void SimpleReport ( int m , string t ) { Console . WriteLine ( $ \"int: {m}, string: {t}\" ); } Improved implementation public delegate void InformationNeeded ( int n , string s ); static void Main () { InformationNeeded form = new InformationNeeded ( BetterReport ) // ... form ( 2 , \"kiwi\" ); form ( 3 , \"jackfruit\" ); } void BetterReport ( int m , string t ) { Console . WriteLine ( $ \"There are {m} items of type {t}\" ); } Events Events signal the occurrence of an action or notification. They are raised or fired (invoked) by the publisher and received by the event handler or subscriber . They represent a syntactic sugar over the delegate structure, which is used in the background as the pipeline to connect publisher and handler. The simplest way to define an event, using the builtin EventHandler type, is as follows: public event EventHandler Occurrence ; In actuality, EventHandler is itself a wrapper around a delegate, and any delegate can be wrapped by the event by the delegate's name as the event's data type: public delegate void InformationNeeded ( int n , string s ); class Form { public event InformationNeeded FormEvent ; } But because the event structure requires an object reference, the simplest implementation for raising an event is more involved. This is because the Main entry-point for C# programs is static, and not an object instance. The event must be defined within a class that is then instantiated. The event is implemented in an event handler that is a method within the same class that defines the event. After first checking if the event is null (abbreviated syntax using the null-conditional member access operator is equivalent) the event object is called. Here, the event handler is called by the constructor itself. namespace SimpleEvent { class Program { static void Main ( string [] args ) { TriggeringEvent eventTrigger = new TriggeringEvent (); } public class TriggeringEvent { public event EventHandler Event ; public TriggeringEvent () { OnEvent ( this , EventArgs . Empty ); } protected virtual void OnEvent ( object s , EventArgs e ) { var newEvent = Event as EventHandler ; if ( newEvent != null ) { newEvent ( this , EventArgs . Empty ); } // Null-conditional operator available since C# 6: // newEvent?.Invoke(this, EventArgs.Empty); } } } } If the method signature of the event handler is made public , then the event can be raised externally and called like any other method, and a slightly simpler example can be constructed. namespace SimpleEvent { class Program { static void Main ( string [] args ) { TriggeringEvent eventTrigger = new TriggeringEvent (); eventTrigger . OnEvent ( eventTrigger , EventArgs . Empty ); } public class TriggeringEvent { public event EventHandler Event ; public virtual void OnEvent ( object s , EventArgs e ) { Console . WriteLine ( \"OnEvent\" ); var newEvent = Event as EventHandler ; newEvent ?. Invoke ( this , EventArgs . Empty ); } } } } Conventionally, however, the event handler is not made public, but defined using the protected virtual void method signature. Event wiring refers to the process of adding subscribers to an event. In implementation, this involves adding the subscribers to the invocation list of the delegate that is used to tie the event to event handler. Event += EventSubscriber ; In actuality, this syntax uses delegate inference , where the compiler automatically determines the correct delegate to use. The fuller syntax avoiding the use of this feature would be Event += new EventHandler ( EventSubscriber ); The event is then fired by calling it, but this can only occur from within the type in which it is defined. So it has to be fired from within another of that type's methods. Anonymous methods and lambdas can also be used after the += operator: Event += ( s , e ) => Console . WriteLine ( \"Subscribing to event!\" ); In this example , adapted from a Pluralsight course, the OnMissionAccomplished and OnMissionStatusReport event handlers send two different types of events, respectively: MissionStatusReport and MissionAccomplished . Even though both of these events are EventHandler types, they are actually events. Async The async modifier is used to construct asynchronous code. By convention, asynchronous methods are named with \"Async\" to distinguish them. The await keyword marks the variable containing the result. public int Addition () { var a = SlowMethodOne (); var b = SlowMethodTwo (); return a + b ; } public async Task < int > AdditionAsync () { var a = SlowMethodOneAsync (); var b = SlowMethodTwoAsync (); return await a + await b ; } Return types used for async include: - Task - Task<T> - Void should generally be avoided with the exception of event handlers Async does not create new threads by default, so it is only suitable for UI and IO-bound methods, not CPU-bound methods. Here async is used to return an enumerable collection of Customer objects from the file IO system . public class CustomerDataProvider { private static readonly string _customersFileName = \"customers.json\" ; private static readonly StorageFolder _localFolder = ApplicationData . Current . LocalFolder ; public async Task < IEnumerable < Customer >> LoadCustomersAsync () { var storageFile = await _localFolder . TryGetItemAsync ( _customersFileName ) as StorageFile ; List < Customer > customerList = null ; // ... } } Here threads are used to handle JSON files and data on application load in the data provider for a GUI application LoadCustomersAsync public async Task < IEnumerable < Customer >> LoadCustomersAsync () { } SaveCustomersAsync public async Task SaveCustomersAsync ( IEnumerable < Customer > customers ) { } Exceptions Exceptions expose Message and StackTrace attributes that can be inspected for further information (ref. ExceptionHandling ) Member access operators are syntactic sugars that allow operations to be performed without exception handling. Operator Name Description => Lambda declaration operator ?. Null-conditional member access operator Applies the operation to its operand only if it evaluates to non-null, otherwise it returns null ?[] Null-conditional element access operator Applies the operation to its operand only if it evaluates to non-null, otherwise it returns null ?? Null-coalescing operator Returns value of left-hand operand if non-null, otherwise returns result of right-hand operand. ??= Null-coalescing assignment operator Assigns the value of the right-hand operand to the left-hand operand, only if the left-hand operand evaluates to null . Classes Access modifiers Classes can be declared with various access modifiers that affect the compiler's behavior. These are intended to prevent what would be runtime errors by turning them into compile-time errors, improving code quality. - static prevents instantiation - abstract indicates the class is to be completed in a derived class. Every method marked as abstract has to be implemented in the derived class, and the class has to be marked with abstract as well. - sealed prevents inheritance - partial allows the same class to be defined across multiple files Constructor If not defined, the compiler will provide a default constructor. A constructor can be overloaded by using the this keyword in the constructor's signature after a colon, as if invoking the second constructor: using System ; class Car { public string brand { get ; set ; } public Car () : this ( \"Ford\" ) { } public Car ( string brand ) { this . brand = brand ; } } class Program { static void Main ( string [] args ) { Car ford = new Car (); Console . WriteLine ( ford . brand ); // => Ford } } Properties A property protects the data of a private variable (\"field\") by implementing getter and setter accessor functions. These allow data validation or other logic to be performed when the variable is changed. The private variable being protected is called the backing store . By convention, properties have identifiers in title case. The identifier for the backing field of a property is conventionally the same as the property, except lowercase or prepended with an underscore. In the set accessor, the keyword value is used for the argument passed in. private string name ; // field public string Name // property { get { return name ; } set { this . name = value ; } } A common shorthand was introduced in C# 3 called automatically implemented properties , where the p public string Name { get ; set ; } The set accessor uses an implicit parameter value , whose value is the type of the property. class Person { private string _name ; // the name field public string Name // the Name property { get => _name ; set => _name = value ; } } Data validation for setter accessor: public class Date { private int _month = 7 ; // Backing store public int Month { get => _month ; set { if (( value > 0 ) && ( value < 13 )) { _month = value ; } } } } A property can be made read-only by simply removing the setter. An access modifier can also be applied to only one or the other of the accessors to enforce encapsulation. This can make the property read-only externally while still allowing the class's own logic to change the property's value: private set { if (( value > 0 ) && ( value < 13 )) { _month = value ; } } Similarly, fields can be modified with the readonly access modifier. This will prevent the variable from being changed in external code as well as in any internal methods. Readonly fields can only be set by the constructor or variable initializers. Static classes Classes marked with static are not instantiated. An example is the System.Console class, which is never instantiated even though its methods are available for use. This structure is called a singleton and is useful as a container for assorted utilities. Methods marked with static are independent of the class instance itself, and as such do not have access to fields that are not const . Polymorphism Modifiers like abstract , virtual , and override allow derived classes to implement logic that builds upon that of a base class. virtual allows you to declare methods and properties in a base class which can be overriden in a derived class. virtual cannot be used with static , abstract , private , or override . abstract is similar, except that the class itself must also be marked as an abstract class, preventing instantiation of the base class. Instead of defining a base function, only the signature is declared. In both cases, override is used to mark the implementation in the derived class. Base class ( abstract ) abstract class Shape { public abstract double GetArea (); } class Shape { public virtual double GetArea () { return ; } } Derived class class Rectangle : Shape { public double Length { get ; set ; } public double Width { get ; set ; } public Rectangle () { Length = 2 ; Width = 3 ; } public override double GetArea () { return Length * Width ; } } Derived class class Circle : Shape { public double Radius { get ; set ; } public Circle () { Radius = 3 ; } public override double GetArea () { return System . Math . PI * System . Math . Pow ( Radius , 2 ); } } Interfaces Interfaces can be used to break up dependencies and implement the dependency inversion principle . This principle holds that components should be dependent on abstractions, and not on implementations. ( src ) Interfaces contain property and method definitions that must be implemented in derived classes, and as such are similar in concept to abstract classes. Like abstract classes, an interface may not be instantiated. Unlike abstract classes, the override keyword is not used on classes that implement interfaces, and access modifiers are not acceptable for interface members. Also unlike abstract classes, smplementation of interface members is mandatory. And although a derived class can only inherit from a single base class, there is no limit on the number of interfaces that a derived class can inherit from. Interface identifiers conventionally with the capital I . Interface interface IAnimal { void AnimalSound (); } Implementation class Pig : IAnimal { public void AnimalSound () { Console . WriteLine ( \"Oink\" ); } } Notably, a commonly encountered interface is IEnumerable because both Lists and Arrays implement it. So methods that iterate over either Lists or Arrays typically use IEnumerable to accept either data type. Attributes Attributes appear to resemble Python decorators because like decorators appear on the line preceding a function or class definition, but they appear to be used for something else. Attributes in C# are used to adjust the function of code in a variety of ways. ObsoleteAttribute will produce a compiler warning or error (preventing compilation entirely) when deprecated code is being used. Warning [Obsolete(\"Don't use this class anymore, instead use ...\")] class Cow { } static void Main () { Cow betsy = new Cow (); } Error [Obsolete(\"Don't use this class anymore\", true)] class Cow { } static void Main () { Cow betsy = new Cow (); } A family of attributes exist to assist debugging. DebuggerStepThrough DebuggerStepThrough can decorate certain methods to be stepped through or skipped while debugging. This is useful for situations where only some properties of a class have to be debugged. This allows more controlled debugging than using the \"Step over properties and operators\" setting in Debugging Options. ( src ) using System.Diagnostics ; struct Cow { public string Name { [ DebuggerStepThrough ] get { return \"Bessy\" ; } } public int Weight { get { return 5 ; } } } static void Main () { Cow betsy = new Cow (); } DebuggerDisplay DebuggerDisplayAttribute allows an object's state to be formatted to be more understandable in the debugger's watch window. ( src ) using System.Diagnostics ; [DebuggerDisplay(\"{Name} weighs {Weight} lbs\")] struct Cow { public string Name ; public int Weight ; } class Program { static void Main ( string [] args ) { Console . WriteLine ( \"Hello World!\" ); // This triggers instantiation of the attribute typeof ( Program ). GetCustomAttributes ( false ); Cow betsy = new Cow { Name = \"Betsy\" , Weight = 1000 }; Console . WriteLine ( $ \"{betsy.Name} weighs {betsy.Weight} lbs\" ); } } The CallerMemberNameAttribute can be added to string parameters in functions that are meant to process the name of the function calling them. This avoids the verbosity of placing nameof() on every invocation. ( src ) using System.ComponentModel ; using System.Runtime.CompilerServices ; namespace WiredBrainCoffee.UWP.Models { public class Customer : INotifyPropertyChanged { private string firstName ; public string FirstName { get => firstName ; set { firstName = value ; OnPropertyChanged ( nameof ( FirstName )); } } public string LastName { get ; set ; } public bool IsCoffeeDrinker { get ; set ; } public event PropertyChangedEventHandler PropertyChanged ; private void OnPropertyChanged ([ CallerMemberName ] string propertyName = null ) { PropertyChanged ?. Invoke ( this , new PropertyChangedEventArgs ( propertyName )); } } } In XAML, the ContentPropertyAttribute attribute is used to define whether or not a control accepts a default Content property field using Windows.UI.Xaml ; using Windows.UI.Xaml.Controls ; using Windows.UI.Xaml.Markup ; [ContentProperty(Name = nameof(Customer))] public sealed partial class CustomerDetailControl : UserControl { public CustomerDetailControl () { this . InitializeComponent (); } } Files Streams A Stream is an abstraction of a backing store , or sequence of bytes, which can be a file, an input/output device, a websocket, or an inter-process communication pipe. The Stream class itself is an abstract base class that can't be instantiated. FileStream is the concrete class that uses files as its backing store. Streams can support seeking, although network streams do not support seeking. This can be checked by calling the stream's boolean CanSeek property. Stream implements IDisposable , which means it can be disposed indirectly by being placed in a using block. A bit bucket is a stream with no backing store and is implemented as Stream.Null . Testing Tests are usually organized in a separate project that is linked to the project containing the system under test (SUT). Visual Studio has a built-in test-runner, but the dotnet CLI utility also allows the entire test suite to be executed from the command-line. dotnet test .NET supports several test frameworks. xUnit In xUnit , tests are organized into public classes, and test cases are composed by individual methods on this class, decorated with the Fact attribute. Test assertions are made with static Assert method calls. ( src ) public class TestCases { [Fact] public void TestCase () { Assert . Equal ( 2 + 2 , 4 ); } } Assertions that an exception must be thrown are generic method calls typed to the specific exception. public class StarshipDeploymentShould { [Fact] public void ThrowOnNullValidator () { var sut = new StarshipDeployment ( null ); Assert . Throws < ArgumentNullException >( sut ); } } Test fixtures can be formed on properties of the main test class. They must be initialized with the test class's constructor. Xunit public class DeskBookerRequestProcessorTests { public DeskBookerRequestProcessor processor { get ; set ; } public DeskBookerRequestProcessorTests () { processor = new DeskBookerRequestProcessor (); } [Fact] public void ShouldReturnDeskBookerResultWithRequestValues () { var request = new DeskBookerRequest { FirstName = \"Thomas\" , LastName = \"Huber\" , Email = \"thomas@huber.com\" , Date = new DateTime ( 2020 , 1 , 28 ) }; var result = processor . BookDesk ( request ); Assert . NotNull ( result ); Assert . Equal ( request . FirstName , result . FirstName ); Assert . Equal ( request . LastName , result . LastName ); Assert . Equal ( request . Email , result . Email ); Assert . Equal ( request . Date , result . Date ); } [Fact] public void ShouldThrowExceptionIfRequestIsNull () { var exception = Assert . Throws < ArgumentNullException >(() => processor . BookDesk ( null )); Assert . Equal ( \"request\" , exception . ParamName ); } } Classes under test namespace DeskBooker.Core.Processor { public class DeskBookingResult { public string FirstName { get ; set ; } public string LastName { get ; set ; } public DateTime Date { get ; set ; } } public class DeskBookingRequest { public string FirstName { get ; set ; } public string LastName { get ; set ; } public DateTime Date { get ; set ; } } public class DeskBookingRequestProcessor { public DeskBookingResult BookDesk ( DeskBookingRequest request ) { return new DeskBookingResult { FirstName = request . FirstName , LastName = request . LastName , Date = request . Date , }; } } } In this example, PersonProcessor is a public class whose constructor takes an ISqlDataAccess data provider by means of dependency injection. The LoadData method is setup, and the mocked object is instantiated with the Create method call. The mock will inject the mock data provider, which returns a List<PersonModel> . using ( var mock = AutoMock . GetLoose ()) { mock . Mock < ISqliteDataAccess >() . Setup ( x => x . LoadData < PersonModel >( \"SELECT * FROM Person\" )) . Returns ( GetSamplePeople ()); var sut = mock . Create < PersonProcessor >(); var expected = GetSamplePeople (); var actual = sut . LoadPeople (); Assert . True ( actual != null ); Assert . Equal ( actual . Count , expected . Count ); } The Theory attribute decorates a parameterized test and commonly appears in conjunction with InlineData attributes that contain the parameter values. using System ; using Xunit ; using System.Linq ; namespace MathTests { public class MathWorks { [Theory] [InlineData(2,2)] [InlineData(3,3,3)] [InlineData(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)] public void Addition ( params int [] ops ) { int loopsum = 0 ; foreach ( int i in ops ) { loopsum += i ; } int linqsum = ops . Sum (); Assert . Equal ( loopsum , linqsum ); } } } The xUnit test-runner can be modified using a JSON file named xunit.runner.json. This file must be copied to the output directory by selecting \"Copy if newer\" in the file's properties. This example will display the method names only, rather than the fully-qualified dotted name with namespace and class. { \"methodDisplay\" : \"method\" } Moq Moq (\"mock-you\") is an open-source mocking library available as a NuGet package. Mock objects are generics that take the abstract base class or interface used by the mocked object (see provider pattern ). Naturally, this means the concrete objects they are replacing must also be implementing those interfaces. There are two mock modes, strict and loose . By default, mock objects are loose, which means they will return default type values and not throw any exceptions to methods that have not been setup. Loose (default) var mock = new Mock < IMockTarget >(); Strict var mock = new Mock < IMockTarget >( MockBehavior . Strict ); Mock properties require setup. mock . Setup ( x => x . Property ). Returns ( \"Hello, world!\" ); Methods of mock objects also require setup using an identical syntax. Concrete arguments can be provided, but preferable is using argument matching . In argument matching, It.IsAny<T> is used like a type declaration to fill the place of any concrete variable used as an argument. ( src ) Argument matching mock . Setup ( x => x . IsValid ( It . IsAny < string >())). Returns ( true ); Concrete mock . Setup ( x => x . IsValid ( \"Hello, world!\" )). Returns ( true ); The mock object exposes an Object property that can be used to test assertions against properties of the mocked object. Assert . Equal ( mock . Object . Property , value ) A mock object's Verify is used to verify that a mocked method was called by the system under test. Verification is specific to the parameters of the mocked method call, and argument matching is available just as it is for setting up mocked methods. Here, the mocked validator, which is passed in to the SUT by dependency injection, must make a call to the validator's Evaluate() method. If the call is removed, the test will fail (\"Expected invocation on the mock at least once, but was never performed...\"). An overload of the Verify method also allows a custom error message to be specified. Another overload can ensure that the mocked method was not called, by passing Times.None after the lambda. The Times struct exposes other members like AtLeastOnce and Between that can specify any imaginable number or range of invocations. Test public class StarshipDeploymentShould { [Theory] [InlineData(\"Betelgeuse\")] public void EvaluateStarship ( string destination ) { var mockValidator = new Mock < IStarshipValidator >(); mockValidator . Setup ( x => x . Evaluate ()). Returns ( true ); var mockStarship = new Mock < IStarship >(); var sut = new StarshipDeployment ( mockValidator . Object as IStarshipValidator ); sut . Deploy ( mockStarship . Object as Starship , destination ); mockValidator . Verify ( x => x . Evaluate ()); } } Custom error message public class StarshipDeploymentShould { [Theory] [InlineData(\"Betelgeuse\")] public void EvaluateStarship ( string destination ) { var mockValidator = new Mock < IStarshipValidator >(); mockValidator . Setup ( x => x . Evaluate ()). Returns ( true ); var mockStarship = new Mock < IStarship >(); var sut = new StarshipDeployment ( mockValidator . Object as IStarshipValidator ); sut . Deploy ( mockStarship . Object as Starship , destination ); mockValidator . Verify ( x => x . Evaluate (), \"Starships should be validated\" ); } } SUT public class StarshipDeployment { public IStarshipValidator StarshipValidator { get ; set ; } public StarshipDeployment ( IStarshipValidator validator ) { StarshipValidator = validator ?? throw new ArgumentNullException ( nameof ( validator )); } public bool ValidateDestination ( string destination ) { return destination . Length > 1 ? true : false ; } public StarshipMission Deploy ( Starship starship , string destination ) { bool destinationValidated = ValidateDestination ( destination ); bool starshipValidated = StarshipValidator . Evaluate (); return destinationValidated && starshipValidated ? new StarshipMission { Starship = starship as Starship , Destination = destination } : throw new ArgumentException (); } } A mocked method can also be setup to throw an exception with the Throw<Exception>() method, a generic method that takes an Exception type. Application design Test-driven development and the requirement to be able to mock data providers has a strong influence on application architecture. Instead of tightly coupling models with a particular data provider (such as an hardcoding, an in-memory database, or parsing a file), the recommended pattern is dependency injection . A data provider that implements an interface is passed as an argument to the controller or viewmodel upon entry. For example, a DataProvider class is used to provide a list of integers on application load implements public interface IDataProvider { IEnumerable < int > LoadAsync (); } public class DataProvider : IDataProvider { async public List < int > LoadAsync () { return await List < int > { 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 , 29 , 31 }; } } A mocked data provider also implementing that interface can then be used in testing. .NET The .NET ecosystem has 3 runtimes , all of which implement the .NET Standard Library and rest on common build tools , languages, and runtime components .NET Framework released in 2002, making it the oldest runtime, and runs only on Windows. Two major components: Common Language Runtime (CLR) runs managed code and performs garbage collection .NET Framework Class Library (also called the Base Class Library ) is composed of many classes, interfaces, and value types .NET Core is cross-platform, open-source, and optimized for performance. Its application host is dotnet.exe Core Common Language Runtime (CoreCLR) is more lightweight than that of .NET Framework, but implements Just-In Time compilation .NET Core Class Library is smaller than (and actually a subset of) that of .NET Framework Mono for Xamarin is used for mobile platforms like IOS, Android, and OS X .NET Standard is a specification of which APIs are available across all these runtimes. It evolved from Portable Class Libraries (PCL) and will eventually replace them. .NET's package manager is NuGet . An assembly can be compiled to EXE or DLL. dotnet Install dotnet-format dotnet tool install -g dotnet-format Install the ASP.NET scaffolding engine dotnet tool install -g dotnet-aspnet-codegenerator Create a new xUnit project named tests dotnet new xunit -n tests Add a project file to a solution dotnet sln add ./Project/Project.csproj Add a project reference to a project dotnet add reference ./path/to/Project.csproj Install a NuGet package and add a PackageReference in the project file Moq dotnet add package Moq System.CommandLine dotnet add package System.CommandLine Run the dotnet try web server that supports .NET Interactive-style markdown: ```cs --source-file ./Program.cs --project ./project.csproj ``` Project files Project files are XML files that describe various metadata to the dotnet compiler. The root node is Project which has two subnodes that collect various information about the project: PropertyGroup can contain various elements that affect project settings: RootNamespace specifies the namespace that contains the Main() method for console applications TargetFramework specifies the targeted CLR framework: net5.0 , netcoreapp3.1 , etc LangVersion C# version: 9.0 , etc Nullable Enable nullable reference types ItemGroup contains references to NuGet packages ( PackageReference ) and other projects ( ProjectReference ). LangVersion <PropertyGroup> <LangVersion> preview </LangVersion> </PropertyGroup> Nullable <PropertyGroup> <Nullable> enable </Nullable> </PropertyGroup> ItemGroup <ItemGroup> <ProjectReference Include= \"/path/to/OtherProject.csproj\" /> <PackageReference Include= \"xunit\" Version= \"2.4.0\" /> </ItemGroup> Adding a reference to another project is also easily accomplished from the command-line. dotnet add project /path/to/OtherProject.csproj Packages NuGet is the official package manager for .NET. NuGet packages required for any project were stored in a XML packages.config file. But projects that use PackageReference may store that information in /obj/project.assets.json. System.CommandLine Prior to System.CommandLine, it had been up to the developer to build a custom solution resolving command-line arguments as an array of strings. Although .NET includes several earlier attempts at solving this problem, none had emerged as a default solution. Similar to Python's argparse , the CommandLine library allows you to construct a RootCommand object that accepts definitions of argument and options. Here, an argument is required: C# using System ; using System.CommandLine ; using System.CommandLine.Invocation ; namespace CommandLine { class Program { static int Main ( string [] args ) { var cmd = new RootCommand { new Argument < string >( \"name\" ) }; cmd . Handler = CommandHandler . Create < string >( HandleGreeting ); return cmd . Invoke ( args ); } static void HandleGreeting ( string name = \"world\" ) { Console . WriteLine ( $ \"Hello, {name}\" ); } } } Python import argparse def get_args (): parser = argparse . ArgumentParser ( description = \"Say hello\" ) parser . add_argument ( dest = \"name\" , metavar = \"name\" , default = \"World\" , help = \"Name to greet\" ) return parser . parse_args () def main (): args = get_args () print ( f \"Hello, { args . name } !\" ) if __name__ == \"__main__\" : main () Here, the greeting can be specified with an optional parameter C# using System ; using System.CommandLine ; using System.CommandLine.Invocation ; namespace CommandLine { class Program { static int Main ( string [] args ) { var cmd = new RootCommand { new Argument < string >( \"name\" ), //, \"Your name\"), new Option < string? >( new [] { \"--greeting\" , \"-g\" }, \"The greeting to use\" ), }; cmd . Handler = CommandHandler . Create < string , string? >( HandleGreeting ); return cmd . Invoke ( args ); } static void HandleGreeting ( string? greeting , string name ) { Console . WriteLine ( $ \"{greeting}, {name}\" ); } } } Python import argparse def get_args (): parser = argparse . ArgumentParser ( description = \"Say hello\" ) parser . add_argument ( dest = \"name\" , metavar = \"name\" , default = \"World\" , help = \"Name to greet\" ) parser . add_argument ( \"--greeting\" , \"-g\" , dest = \"greeting\" , default = \"Hello\" , help = \"Greeting to use\" ) return parser . parse_args () def main (): args = get_args () print ( f \" { args . greeting } , { args . name } !\" ) if __name__ == \"__main__\" : main () Documentation C# supports documentation comments that can be exported to an XML file, which can then be imported into a static site generator (especially DocFX). Visual Studio can be set to export these comments upon build. SDKs DynamoDB To develop a .NET application using DynamoDB, add the AWSSDK.DynamoDBv2 NuGet package. The AWS Explorer, part of the AWS Toolkit for Visual Studio extension, is also useful for setting up a new table. A user with programmatic access, including an Access Key and Secret Key, is necessary to use the toolkit. ( src ) Both .NET Core and .NET Framework are supported as target frameworks, but .NET Core uses exclusively asynchronous operations. A service client object is formed by instantiating AmazonDynamoDBClient . The exposed method PutItemAsync is used to save an item to a table as a PutItemRequest object. The item itself is provided as a Dictionary in the Item key, but the Dictionary's values are AttributeValue objects, formed with a magic key that determines the data type of the value. String new AttributeValue { S = \"Hello, world!\" } Integer new AttributeValue { N = \"3\" } Boolean new AttributeValue { BOOL = true } List new AttributeValue { L = new List < AttributeValue > { new AttributeValue { S = \"Socrates\" }, new AttributeValue { S = \"Plato\" }, new AttributeValue { S = \"Aristotle\" }, }} using Amazon.DynamoDBv2 ; namespace DynamoDBDemo { public class LowLevelSample { public static async Task ExecuteAsync () { using ( IAmazonDynamoDB ddbClient = new AmazonDynamoDBClient () { await ddbClient . PutItemAsync ( new PutItemRequest { TableName = \"Users\" , Item = new Dictionary < string , AttributeValue > { { \"Id\" , new AttributeValue { S = \"john@doe.com\" } }, { \"String\" , new AttributeValue { /* ... */ } } } }) }) } } } Concurrency Asynchronous programming Consuming APIs: HttpClient Multithreading The Task Parallel Library offers a high-level way to set up multiple threads. A Task represents an asynchronous operation. Task.Run() queues the work passed as the action to run on a different thread in the thread pool. Task.Run<T>() represents an asynchronous operation that returns a specific value type. Task . Run ( () => { // ... }); Objects in other threads will be inaccessible without using an object like Dispatcher in WPF Task . Run ( () => { Dispatcher . Invoke (() => { // ... }); }); To avoid blocking, we can make it asynchronous private async void Search_Click ( object sender , RoutedEventArgs e ) { await Task . Run () => { // ... Dispatcher . Invoke (() => { // ... } } } \ud83d\udcd8 Glossary Assembly A collection of types and resources that are built to work together and form a logical unit of functionality and which form the building blocks of .NET applications. Module A portable executable file (DLL or EXE) consisting of one or more classes and interfaces. Although multiple modules can theoretically compose a single assembly, in practice an assembly and module can be considered one and the same for most .NET applications. Provider pattern A favored development model in .NET, and a form of dependency injection where a class is passed as an argument to another class that uses it for some purpose. The key is that the provider must derive from an abstract base class or an interface to support mocks in unit testing.","title":"C&#35;"},{"location":"Coding/C%23/#c","text":"","title":"C&#35;"},{"location":"Coding/C%23/#to-do","text":"Sort out Events section, in particular the example cited Develop C# implementation of CSV parser. This appears to be harder than it should be, apparently because the object returned by the CsvReader.GetRecords<T>() method is an IEnumerable which is one of the confusing points of the UWP course. YouTube tutorials do not seem helpful..","title":"To-do"},{"location":"Coding/C%23/#variables","text":"","title":"Variables"},{"location":"Coding/C%23/#parsing","text":"Data types can be used as static classes, exposing a TryParse method. Int32 parsedInt = Int32 . TryParse ( rawInt ); DateTime parsedDate = DateTime . TryParse ( rawDate );","title":"Parsing"},{"location":"Coding/C%23/#string","text":"Specify a verbatim literal string by prepending @ , which disables escape characters and forces interpretation of backslashes literally: string filePath = @\"C:\\televisions\\sony\\bravia.txt\" ; Specify a formatted string by prepending a $ int n ; string s = $ \"{n} is a number\" ; Standard numeric format strings are used to format common numeric types. They take the form of a character (i.e. C for currency, N for number) followed by a number. They can be passed as arguments to the ToString method of the literal or in the placeholder of a formatted string after : . Console . WriteLine ( $ \"{123.456789:C }\" ); // $123.46 Console . WriteLine ( 123.456789d . ToString ( \"C\" )); // $123.46 A precision specifier can define the number of fractional digits after the decimal separator. Console . WriteLine ( $ \"{123.456789:C3 }\" ); // $123.457 Empty space can be added to either side of the value to create evenly spaced output by placing a number after a comma (positive for right-alignment, negative for left-alignment): Console . WriteLine ( $ \"{123.456789, 15}\" );","title":"String"},{"location":"Coding/C%23/#casting","text":"Because real numbers are stored as double s by default, in order to assign to a float variable you must append f to the literal: float num = 3.14f ; A similar logic pertains for integers to be declared as doubles : double num = 3d ; decimal data type literals, which have 28-29 significant digits, can be declared with the m suffix decimal num = 123.4567890123456789 m char literals can be encoded in Unicode: char umlaut = '\\ u00F6 ' ; Variables can be explicitly cast to some other data types by placing the new data type in parentheses before the value: int pi = ( int ) System . Math . PI ; This casting won't work with string , which can be cast by using the Convert type or parsed using the data type's Parse method. The differense is that using Convert will return a 0 if the value is null while Parse will throw an exception. w = \"5\" ; int wConverted = System . Convert . ToInt32 ( w ); int wParsed = int . Parse ( w );","title":"Casting"},{"location":"Coding/C%23/#collection","text":"","title":"Collection"},{"location":"Coding/C%23/#arrays","text":"Arrays are declared differently from built-in arrays in C++. C# int [] primes = { 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 }; C++ int primes [ 10 ] { 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 } An empty array must still have its size declared int [] primes = new int [ 10 ]; An unnamed array: new [] { 1 , 2 , 3 }; Arrays can be traversed with a foreach loop, but the elements can not be changed.: foreach ( var i in container ) { // ... } Arrays can be copied with the Clone() and Copy() methods: see ArrayCloning . Arrays can be reversed in place with the Reverse method. int [] primes = { 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 }; Array . Reverse ( primes );","title":"Arrays"},{"location":"Coding/C%23/#linq","text":"Language Integrated Query (LINQ) refers to a C# library that facilitates querying of collections. These are exposed as extension methods : methods that are available on already existing queryable types This means extension methods are exposedon existing collection types like Array and List because they are derived from IEnumerable<T> , and thus need no modification to serve as a LINQ data source. Linq methods are available in two semantically identical syntaxes: query syntax and method syntax (also lambda syntax). Query syntax is meant to be more intuitive for developers familiar with SQL. Method syntax allows method chaining. Query syntax int [] numbers = { 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 }; var evens = from n in nums where n % 2 == 0 orderby n descending select n ; Method syntax int [] numbers = { 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 }; var evens = nums . Where ( n => n % 2 == 0 ). OrderByDescending ( n => n ); Python numbers = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] evens = [ n for n in numbers if n % 2 == 0 ] list ( reversed ( evens )) Notably, unlike loop structures in C#, LINQ methods can work on unordered collections like Dictionaries.","title":"LINQ"},{"location":"Coding/C%23/#observablecollection","text":"The ObservableCollection class is used to define collections that provide notifications to data bindings when items are added or removed. As such, it is used in GUI programming...","title":"ObservableCollection"},{"location":"Coding/C%23/#datetime","text":"DateTimeOffset is preferred over DateTime because it includes an offset value that indicates the timezone.","title":"DateTime"},{"location":"Coding/C%23/#parsing_1","text":"DateTime is a class that exposes several static methods of parsing raw values. All of them have overloads that accept CultureInfo objects (implementing IFormatProvider ) which can affect parsing of ambiguous dates. Parse() will attempt to parse a string and raise a FormatException if unable to do so. ParseExact() requires an exact string template and requires a CultureInfo object as well. Parse() string rawDate = \"07/04/1776\" ; try { DateTime parsedDate = DateTime . Parse ( rawDate ); } catch ( FormatException ) { Console . WriteLine ( \"Unparsable!\" ) } Console . WriteLine ( parsedDate . ToLongDateString ()); // => \"July 4, 1776\" Specifying culture string rawDate = \"07/04/1776\" ; try { DateTime parsedDate = DateTime . Parse ( rawDate , CultureInfo . GetCultureInfo ( \"en-GB\" )); } catch ( FormatException ) { Console . WriteLine ( \"Unparsable!\" ) } Console . WriteLine ( parsedDate . ToLongDateString ()); // => \"April 7, 1776\" ParseExact() string rawDate = \"07/04/1776\" ; try { DateTime parsedDate = DateTime . ParseExact ( rawDate , \"M/d/yyyy\" , CultureInfo . InvariantCulture ); } catch ( FormatException ) { Console . WriteLine ( \"Unparsable!\" ) } Console . WriteLine ( parsedDate . ToLongDateString ()); // => \"July 4, 1776\" TryParse() returns no value, but takes an out parameter. It does not throw an exception if the date is unparsable, but rather outputs the default date January 1, 1 AD. The overload that accepts a Culture object also requires a DateTimeStyles object. TryParse() string rawDate = \"07/04/1776\" ; DateTime parsedDate ; DateTime . TryParse ( rawDate , out parsedDate ); Console . WriteLine ( parsedDate . ToLongDateString ()); // => \"July 4, 1776\" Specifying culture string rawDate = \"07/04/1776\" ; DateTime parsedDate ; DateTime . TryParse ( rawDate , CultureInfo . GetCultureInfo ( \"en-GB\" ), DateTimeStyles . None , out parsedDate ); Console . WriteLine ( parsedDate . ToLongDateString ()); // => \"April 7, 1776 ParseExact() TryParseExact()","title":"Parsing"},{"location":"Coding/C%23/#timezones","text":"TimeZoneInfo includes static methods that can access system timezones. DateTime does not include timezone information, so it must be specified at runtime. TimeZoneInfo sidneyTimeZone = TimeZoneInfo . FindSystemTimeZoneById ( \"E. Australia Standard Time\" ); var sydneyTime = TimeZoneInfo . ConvertTime ( DateTime . Now , sydneyTimeZone ); Enumerating all system timezones. foreach ( var timeZone in TimeZoneInfo . GetSystemTimeZones ()) { Console . WriteLine ( timeZone . GetUtcOffset ()); }","title":"Timezones"},{"location":"Coding/C%23/#methods","text":"","title":"Methods"},{"location":"Coding/C%23/#lambda","text":"A lambda expression can have two forms, both of which use the lambda declaration operator => Expression lambda ( input - parameters ) => expression Statement lambda ( input - parameters ) => { statements } Anonymous event handlers can be reformulated as lambdas to reduce code complexity. SubmitButton . Click += delegate ( object sender , EventArgs e ) { MessageBox . Show ( \"Button Clicked\" ); } // Using a (statement) lambda: SubmitButton . Click += ( s , e ) => MessageBox . Show ( \"Button Clicked\" );","title":"Lambda"},{"location":"Coding/C%23/#ref","text":"ref allows variables that are normally passed by value to be passed by reference. Pass by value Integers are normally passed by value, so number will not change static void Main () { int number = 0 ; plusOne ( number ); } static void plusOne ( int n ) { n ++; } Pass by reference Now number will increment by one because it is being passed by reference. static void Main () { int number = 0 ; plusOne ( ref number ); } static void plusOne ( ref int n ) { n ++; } Returning a value Alternatively, number can be reassigned a variable if the method is refactored to return the new value. static void Main () { int number = 0 ; number = plusOne ( number ); } static int plusOne ( ref int n ) { n ++; return n ; }","title":"ref"},{"location":"Coding/C%23/#out","text":"out allows a method to assign a value to a variable that has no value yet. It can be used to return multiple values. static void Main () { double n = 5 ; double nSquared ; square ( n , out nSquared ); Console . WriteLine ( $ \"{n} ^ 2 = {nSquared}\" ); } static void square ( double x , out double y ) { y = System . Math . Pow ( x , 2 ); } out is prominently used in the TryParse method.","title":"out"},{"location":"Coding/C%23/#params","text":"params allows you to process a variable number of similarly-typed arguments in the method signature. This collection of arguments is abstracted as an array. static void method ( int [] args ) { foreach ( int el in args ) { Console . WriteLine ( el ); } } If the arguments to be accepted are themselves arrays, then you must define an array of arrays ( ex. ). This technique may not have worked in previous versions of C#. static void method ( int [][] args ) { foreach ( var array in args ){ foreach ( int el in array ) { Console . WriteLine ( el ); } } }","title":"params"},{"location":"Coding/C%23/#delegates","text":"Delegates are a functional programming feature in C# that facilitate loose coupling. They allow a function to be abstracted so that updated logic can be implemented without incurring technical debt. Delegates take the form of a method signature using the delegate keyword. One or more methods implementing the delegate can be formulated which do not reference the delegate in any way, shape, or form, except for the fact that their method signature matches that specified by the delegate. Where the method is to be used, instead of calling the method directly, the delegate is instantiated like an object, but the name of the specific method that implements the delegate is passed as a parameter. The instantiated delegate can then be called, which passes the parameters to the method. This results in looser coupling because when changing implementation, only the parameter specifying the improved method needs to be adjusted, and the delegate ensures that the same pattern of parameters is enforced at compile-time. Delegates can be used for messaging in .NET and especially to tie events to event handlers, but they are no longer used as much as Func<T,TResult> and Action<T> . Initial implementation public delegate void InformationNeeded ( int n , string s ); static void Main () { InformationNeeded form = new InformationNeeded ( SimpleReport ) // ... form ( 2 , \"kiwi\" ); form ( 3 , \"jackfruit\" ); } void SimpleReport ( int m , string t ) { Console . WriteLine ( $ \"int: {m}, string: {t}\" ); } Improved implementation public delegate void InformationNeeded ( int n , string s ); static void Main () { InformationNeeded form = new InformationNeeded ( BetterReport ) // ... form ( 2 , \"kiwi\" ); form ( 3 , \"jackfruit\" ); } void BetterReport ( int m , string t ) { Console . WriteLine ( $ \"There are {m} items of type {t}\" ); }","title":"Delegates"},{"location":"Coding/C%23/#events","text":"Events signal the occurrence of an action or notification. They are raised or fired (invoked) by the publisher and received by the event handler or subscriber . They represent a syntactic sugar over the delegate structure, which is used in the background as the pipeline to connect publisher and handler. The simplest way to define an event, using the builtin EventHandler type, is as follows: public event EventHandler Occurrence ; In actuality, EventHandler is itself a wrapper around a delegate, and any delegate can be wrapped by the event by the delegate's name as the event's data type: public delegate void InformationNeeded ( int n , string s ); class Form { public event InformationNeeded FormEvent ; } But because the event structure requires an object reference, the simplest implementation for raising an event is more involved. This is because the Main entry-point for C# programs is static, and not an object instance. The event must be defined within a class that is then instantiated. The event is implemented in an event handler that is a method within the same class that defines the event. After first checking if the event is null (abbreviated syntax using the null-conditional member access operator is equivalent) the event object is called. Here, the event handler is called by the constructor itself. namespace SimpleEvent { class Program { static void Main ( string [] args ) { TriggeringEvent eventTrigger = new TriggeringEvent (); } public class TriggeringEvent { public event EventHandler Event ; public TriggeringEvent () { OnEvent ( this , EventArgs . Empty ); } protected virtual void OnEvent ( object s , EventArgs e ) { var newEvent = Event as EventHandler ; if ( newEvent != null ) { newEvent ( this , EventArgs . Empty ); } // Null-conditional operator available since C# 6: // newEvent?.Invoke(this, EventArgs.Empty); } } } } If the method signature of the event handler is made public , then the event can be raised externally and called like any other method, and a slightly simpler example can be constructed. namespace SimpleEvent { class Program { static void Main ( string [] args ) { TriggeringEvent eventTrigger = new TriggeringEvent (); eventTrigger . OnEvent ( eventTrigger , EventArgs . Empty ); } public class TriggeringEvent { public event EventHandler Event ; public virtual void OnEvent ( object s , EventArgs e ) { Console . WriteLine ( \"OnEvent\" ); var newEvent = Event as EventHandler ; newEvent ?. Invoke ( this , EventArgs . Empty ); } } } } Conventionally, however, the event handler is not made public, but defined using the protected virtual void method signature. Event wiring refers to the process of adding subscribers to an event. In implementation, this involves adding the subscribers to the invocation list of the delegate that is used to tie the event to event handler. Event += EventSubscriber ; In actuality, this syntax uses delegate inference , where the compiler automatically determines the correct delegate to use. The fuller syntax avoiding the use of this feature would be Event += new EventHandler ( EventSubscriber ); The event is then fired by calling it, but this can only occur from within the type in which it is defined. So it has to be fired from within another of that type's methods. Anonymous methods and lambdas can also be used after the += operator: Event += ( s , e ) => Console . WriteLine ( \"Subscribing to event!\" ); In this example , adapted from a Pluralsight course, the OnMissionAccomplished and OnMissionStatusReport event handlers send two different types of events, respectively: MissionStatusReport and MissionAccomplished . Even though both of these events are EventHandler types, they are actually events.","title":"Events"},{"location":"Coding/C%23/#async","text":"The async modifier is used to construct asynchronous code. By convention, asynchronous methods are named with \"Async\" to distinguish them. The await keyword marks the variable containing the result. public int Addition () { var a = SlowMethodOne (); var b = SlowMethodTwo (); return a + b ; } public async Task < int > AdditionAsync () { var a = SlowMethodOneAsync (); var b = SlowMethodTwoAsync (); return await a + await b ; } Return types used for async include: - Task - Task<T> - Void should generally be avoided with the exception of event handlers Async does not create new threads by default, so it is only suitable for UI and IO-bound methods, not CPU-bound methods. Here async is used to return an enumerable collection of Customer objects from the file IO system . public class CustomerDataProvider { private static readonly string _customersFileName = \"customers.json\" ; private static readonly StorageFolder _localFolder = ApplicationData . Current . LocalFolder ; public async Task < IEnumerable < Customer >> LoadCustomersAsync () { var storageFile = await _localFolder . TryGetItemAsync ( _customersFileName ) as StorageFile ; List < Customer > customerList = null ; // ... } } Here threads are used to handle JSON files and data on application load in the data provider for a GUI application LoadCustomersAsync public async Task < IEnumerable < Customer >> LoadCustomersAsync () { } SaveCustomersAsync public async Task SaveCustomersAsync ( IEnumerable < Customer > customers ) { }","title":"Async"},{"location":"Coding/C%23/#exceptions","text":"Exceptions expose Message and StackTrace attributes that can be inspected for further information (ref. ExceptionHandling ) Member access operators are syntactic sugars that allow operations to be performed without exception handling. Operator Name Description => Lambda declaration operator ?. Null-conditional member access operator Applies the operation to its operand only if it evaluates to non-null, otherwise it returns null ?[] Null-conditional element access operator Applies the operation to its operand only if it evaluates to non-null, otherwise it returns null ?? Null-coalescing operator Returns value of left-hand operand if non-null, otherwise returns result of right-hand operand. ??= Null-coalescing assignment operator Assigns the value of the right-hand operand to the left-hand operand, only if the left-hand operand evaluates to null .","title":"Exceptions"},{"location":"Coding/C%23/#classes","text":"","title":"Classes"},{"location":"Coding/C%23/#access-modifiers","text":"Classes can be declared with various access modifiers that affect the compiler's behavior. These are intended to prevent what would be runtime errors by turning them into compile-time errors, improving code quality. - static prevents instantiation - abstract indicates the class is to be completed in a derived class. Every method marked as abstract has to be implemented in the derived class, and the class has to be marked with abstract as well. - sealed prevents inheritance - partial allows the same class to be defined across multiple files","title":"Access modifiers"},{"location":"Coding/C%23/#constructor","text":"If not defined, the compiler will provide a default constructor. A constructor can be overloaded by using the this keyword in the constructor's signature after a colon, as if invoking the second constructor: using System ; class Car { public string brand { get ; set ; } public Car () : this ( \"Ford\" ) { } public Car ( string brand ) { this . brand = brand ; } } class Program { static void Main ( string [] args ) { Car ford = new Car (); Console . WriteLine ( ford . brand ); // => Ford } }","title":"Constructor"},{"location":"Coding/C%23/#properties","text":"A property protects the data of a private variable (\"field\") by implementing getter and setter accessor functions. These allow data validation or other logic to be performed when the variable is changed. The private variable being protected is called the backing store . By convention, properties have identifiers in title case. The identifier for the backing field of a property is conventionally the same as the property, except lowercase or prepended with an underscore. In the set accessor, the keyword value is used for the argument passed in. private string name ; // field public string Name // property { get { return name ; } set { this . name = value ; } } A common shorthand was introduced in C# 3 called automatically implemented properties , where the p public string Name { get ; set ; } The set accessor uses an implicit parameter value , whose value is the type of the property. class Person { private string _name ; // the name field public string Name // the Name property { get => _name ; set => _name = value ; } } Data validation for setter accessor: public class Date { private int _month = 7 ; // Backing store public int Month { get => _month ; set { if (( value > 0 ) && ( value < 13 )) { _month = value ; } } } } A property can be made read-only by simply removing the setter. An access modifier can also be applied to only one or the other of the accessors to enforce encapsulation. This can make the property read-only externally while still allowing the class's own logic to change the property's value: private set { if (( value > 0 ) && ( value < 13 )) { _month = value ; } } Similarly, fields can be modified with the readonly access modifier. This will prevent the variable from being changed in external code as well as in any internal methods. Readonly fields can only be set by the constructor or variable initializers.","title":"Properties"},{"location":"Coding/C%23/#static-classes","text":"Classes marked with static are not instantiated. An example is the System.Console class, which is never instantiated even though its methods are available for use. This structure is called a singleton and is useful as a container for assorted utilities. Methods marked with static are independent of the class instance itself, and as such do not have access to fields that are not const .","title":"Static classes"},{"location":"Coding/C%23/#polymorphism","text":"Modifiers like abstract , virtual , and override allow derived classes to implement logic that builds upon that of a base class. virtual allows you to declare methods and properties in a base class which can be overriden in a derived class. virtual cannot be used with static , abstract , private , or override . abstract is similar, except that the class itself must also be marked as an abstract class, preventing instantiation of the base class. Instead of defining a base function, only the signature is declared. In both cases, override is used to mark the implementation in the derived class. Base class ( abstract ) abstract class Shape { public abstract double GetArea (); } class Shape { public virtual double GetArea () { return ; } } Derived class class Rectangle : Shape { public double Length { get ; set ; } public double Width { get ; set ; } public Rectangle () { Length = 2 ; Width = 3 ; } public override double GetArea () { return Length * Width ; } } Derived class class Circle : Shape { public double Radius { get ; set ; } public Circle () { Radius = 3 ; } public override double GetArea () { return System . Math . PI * System . Math . Pow ( Radius , 2 ); } }","title":"Polymorphism"},{"location":"Coding/C%23/#interfaces","text":"Interfaces can be used to break up dependencies and implement the dependency inversion principle . This principle holds that components should be dependent on abstractions, and not on implementations. ( src ) Interfaces contain property and method definitions that must be implemented in derived classes, and as such are similar in concept to abstract classes. Like abstract classes, an interface may not be instantiated. Unlike abstract classes, the override keyword is not used on classes that implement interfaces, and access modifiers are not acceptable for interface members. Also unlike abstract classes, smplementation of interface members is mandatory. And although a derived class can only inherit from a single base class, there is no limit on the number of interfaces that a derived class can inherit from. Interface identifiers conventionally with the capital I . Interface interface IAnimal { void AnimalSound (); } Implementation class Pig : IAnimal { public void AnimalSound () { Console . WriteLine ( \"Oink\" ); } } Notably, a commonly encountered interface is IEnumerable because both Lists and Arrays implement it. So methods that iterate over either Lists or Arrays typically use IEnumerable to accept either data type.","title":"Interfaces"},{"location":"Coding/C%23/#attributes","text":"Attributes appear to resemble Python decorators because like decorators appear on the line preceding a function or class definition, but they appear to be used for something else. Attributes in C# are used to adjust the function of code in a variety of ways. ObsoleteAttribute will produce a compiler warning or error (preventing compilation entirely) when deprecated code is being used. Warning [Obsolete(\"Don't use this class anymore, instead use ...\")] class Cow { } static void Main () { Cow betsy = new Cow (); } Error [Obsolete(\"Don't use this class anymore\", true)] class Cow { } static void Main () { Cow betsy = new Cow (); } A family of attributes exist to assist debugging. DebuggerStepThrough DebuggerStepThrough can decorate certain methods to be stepped through or skipped while debugging. This is useful for situations where only some properties of a class have to be debugged. This allows more controlled debugging than using the \"Step over properties and operators\" setting in Debugging Options. ( src ) using System.Diagnostics ; struct Cow { public string Name { [ DebuggerStepThrough ] get { return \"Bessy\" ; } } public int Weight { get { return 5 ; } } } static void Main () { Cow betsy = new Cow (); } DebuggerDisplay DebuggerDisplayAttribute allows an object's state to be formatted to be more understandable in the debugger's watch window. ( src ) using System.Diagnostics ; [DebuggerDisplay(\"{Name} weighs {Weight} lbs\")] struct Cow { public string Name ; public int Weight ; } class Program { static void Main ( string [] args ) { Console . WriteLine ( \"Hello World!\" ); // This triggers instantiation of the attribute typeof ( Program ). GetCustomAttributes ( false ); Cow betsy = new Cow { Name = \"Betsy\" , Weight = 1000 }; Console . WriteLine ( $ \"{betsy.Name} weighs {betsy.Weight} lbs\" ); } } The CallerMemberNameAttribute can be added to string parameters in functions that are meant to process the name of the function calling them. This avoids the verbosity of placing nameof() on every invocation. ( src ) using System.ComponentModel ; using System.Runtime.CompilerServices ; namespace WiredBrainCoffee.UWP.Models { public class Customer : INotifyPropertyChanged { private string firstName ; public string FirstName { get => firstName ; set { firstName = value ; OnPropertyChanged ( nameof ( FirstName )); } } public string LastName { get ; set ; } public bool IsCoffeeDrinker { get ; set ; } public event PropertyChangedEventHandler PropertyChanged ; private void OnPropertyChanged ([ CallerMemberName ] string propertyName = null ) { PropertyChanged ?. Invoke ( this , new PropertyChangedEventArgs ( propertyName )); } } } In XAML, the ContentPropertyAttribute attribute is used to define whether or not a control accepts a default Content property field using Windows.UI.Xaml ; using Windows.UI.Xaml.Controls ; using Windows.UI.Xaml.Markup ; [ContentProperty(Name = nameof(Customer))] public sealed partial class CustomerDetailControl : UserControl { public CustomerDetailControl () { this . InitializeComponent (); } }","title":"Attributes"},{"location":"Coding/C%23/#files","text":"","title":"Files"},{"location":"Coding/C%23/#streams","text":"A Stream is an abstraction of a backing store , or sequence of bytes, which can be a file, an input/output device, a websocket, or an inter-process communication pipe. The Stream class itself is an abstract base class that can't be instantiated. FileStream is the concrete class that uses files as its backing store. Streams can support seeking, although network streams do not support seeking. This can be checked by calling the stream's boolean CanSeek property. Stream implements IDisposable , which means it can be disposed indirectly by being placed in a using block. A bit bucket is a stream with no backing store and is implemented as Stream.Null .","title":"Streams"},{"location":"Coding/C%23/#testing","text":"Tests are usually organized in a separate project that is linked to the project containing the system under test (SUT). Visual Studio has a built-in test-runner, but the dotnet CLI utility also allows the entire test suite to be executed from the command-line. dotnet test .NET supports several test frameworks.","title":"Testing"},{"location":"Coding/C%23/#xunit","text":"In xUnit , tests are organized into public classes, and test cases are composed by individual methods on this class, decorated with the Fact attribute. Test assertions are made with static Assert method calls. ( src ) public class TestCases { [Fact] public void TestCase () { Assert . Equal ( 2 + 2 , 4 ); } } Assertions that an exception must be thrown are generic method calls typed to the specific exception. public class StarshipDeploymentShould { [Fact] public void ThrowOnNullValidator () { var sut = new StarshipDeployment ( null ); Assert . Throws < ArgumentNullException >( sut ); } } Test fixtures can be formed on properties of the main test class. They must be initialized with the test class's constructor. Xunit public class DeskBookerRequestProcessorTests { public DeskBookerRequestProcessor processor { get ; set ; } public DeskBookerRequestProcessorTests () { processor = new DeskBookerRequestProcessor (); } [Fact] public void ShouldReturnDeskBookerResultWithRequestValues () { var request = new DeskBookerRequest { FirstName = \"Thomas\" , LastName = \"Huber\" , Email = \"thomas@huber.com\" , Date = new DateTime ( 2020 , 1 , 28 ) }; var result = processor . BookDesk ( request ); Assert . NotNull ( result ); Assert . Equal ( request . FirstName , result . FirstName ); Assert . Equal ( request . LastName , result . LastName ); Assert . Equal ( request . Email , result . Email ); Assert . Equal ( request . Date , result . Date ); } [Fact] public void ShouldThrowExceptionIfRequestIsNull () { var exception = Assert . Throws < ArgumentNullException >(() => processor . BookDesk ( null )); Assert . Equal ( \"request\" , exception . ParamName ); } } Classes under test namespace DeskBooker.Core.Processor { public class DeskBookingResult { public string FirstName { get ; set ; } public string LastName { get ; set ; } public DateTime Date { get ; set ; } } public class DeskBookingRequest { public string FirstName { get ; set ; } public string LastName { get ; set ; } public DateTime Date { get ; set ; } } public class DeskBookingRequestProcessor { public DeskBookingResult BookDesk ( DeskBookingRequest request ) { return new DeskBookingResult { FirstName = request . FirstName , LastName = request . LastName , Date = request . Date , }; } } } In this example, PersonProcessor is a public class whose constructor takes an ISqlDataAccess data provider by means of dependency injection. The LoadData method is setup, and the mocked object is instantiated with the Create method call. The mock will inject the mock data provider, which returns a List<PersonModel> . using ( var mock = AutoMock . GetLoose ()) { mock . Mock < ISqliteDataAccess >() . Setup ( x => x . LoadData < PersonModel >( \"SELECT * FROM Person\" )) . Returns ( GetSamplePeople ()); var sut = mock . Create < PersonProcessor >(); var expected = GetSamplePeople (); var actual = sut . LoadPeople (); Assert . True ( actual != null ); Assert . Equal ( actual . Count , expected . Count ); } The Theory attribute decorates a parameterized test and commonly appears in conjunction with InlineData attributes that contain the parameter values. using System ; using Xunit ; using System.Linq ; namespace MathTests { public class MathWorks { [Theory] [InlineData(2,2)] [InlineData(3,3,3)] [InlineData(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)] public void Addition ( params int [] ops ) { int loopsum = 0 ; foreach ( int i in ops ) { loopsum += i ; } int linqsum = ops . Sum (); Assert . Equal ( loopsum , linqsum ); } } } The xUnit test-runner can be modified using a JSON file named xunit.runner.json. This file must be copied to the output directory by selecting \"Copy if newer\" in the file's properties. This example will display the method names only, rather than the fully-qualified dotted name with namespace and class. { \"methodDisplay\" : \"method\" }","title":"xUnit"},{"location":"Coding/C%23/#moq","text":"Moq (\"mock-you\") is an open-source mocking library available as a NuGet package. Mock objects are generics that take the abstract base class or interface used by the mocked object (see provider pattern ). Naturally, this means the concrete objects they are replacing must also be implementing those interfaces. There are two mock modes, strict and loose . By default, mock objects are loose, which means they will return default type values and not throw any exceptions to methods that have not been setup. Loose (default) var mock = new Mock < IMockTarget >(); Strict var mock = new Mock < IMockTarget >( MockBehavior . Strict ); Mock properties require setup. mock . Setup ( x => x . Property ). Returns ( \"Hello, world!\" ); Methods of mock objects also require setup using an identical syntax. Concrete arguments can be provided, but preferable is using argument matching . In argument matching, It.IsAny<T> is used like a type declaration to fill the place of any concrete variable used as an argument. ( src ) Argument matching mock . Setup ( x => x . IsValid ( It . IsAny < string >())). Returns ( true ); Concrete mock . Setup ( x => x . IsValid ( \"Hello, world!\" )). Returns ( true ); The mock object exposes an Object property that can be used to test assertions against properties of the mocked object. Assert . Equal ( mock . Object . Property , value ) A mock object's Verify is used to verify that a mocked method was called by the system under test. Verification is specific to the parameters of the mocked method call, and argument matching is available just as it is for setting up mocked methods. Here, the mocked validator, which is passed in to the SUT by dependency injection, must make a call to the validator's Evaluate() method. If the call is removed, the test will fail (\"Expected invocation on the mock at least once, but was never performed...\"). An overload of the Verify method also allows a custom error message to be specified. Another overload can ensure that the mocked method was not called, by passing Times.None after the lambda. The Times struct exposes other members like AtLeastOnce and Between that can specify any imaginable number or range of invocations. Test public class StarshipDeploymentShould { [Theory] [InlineData(\"Betelgeuse\")] public void EvaluateStarship ( string destination ) { var mockValidator = new Mock < IStarshipValidator >(); mockValidator . Setup ( x => x . Evaluate ()). Returns ( true ); var mockStarship = new Mock < IStarship >(); var sut = new StarshipDeployment ( mockValidator . Object as IStarshipValidator ); sut . Deploy ( mockStarship . Object as Starship , destination ); mockValidator . Verify ( x => x . Evaluate ()); } } Custom error message public class StarshipDeploymentShould { [Theory] [InlineData(\"Betelgeuse\")] public void EvaluateStarship ( string destination ) { var mockValidator = new Mock < IStarshipValidator >(); mockValidator . Setup ( x => x . Evaluate ()). Returns ( true ); var mockStarship = new Mock < IStarship >(); var sut = new StarshipDeployment ( mockValidator . Object as IStarshipValidator ); sut . Deploy ( mockStarship . Object as Starship , destination ); mockValidator . Verify ( x => x . Evaluate (), \"Starships should be validated\" ); } } SUT public class StarshipDeployment { public IStarshipValidator StarshipValidator { get ; set ; } public StarshipDeployment ( IStarshipValidator validator ) { StarshipValidator = validator ?? throw new ArgumentNullException ( nameof ( validator )); } public bool ValidateDestination ( string destination ) { return destination . Length > 1 ? true : false ; } public StarshipMission Deploy ( Starship starship , string destination ) { bool destinationValidated = ValidateDestination ( destination ); bool starshipValidated = StarshipValidator . Evaluate (); return destinationValidated && starshipValidated ? new StarshipMission { Starship = starship as Starship , Destination = destination } : throw new ArgumentException (); } } A mocked method can also be setup to throw an exception with the Throw<Exception>() method, a generic method that takes an Exception type.","title":"Moq"},{"location":"Coding/C%23/#application-design","text":"Test-driven development and the requirement to be able to mock data providers has a strong influence on application architecture. Instead of tightly coupling models with a particular data provider (such as an hardcoding, an in-memory database, or parsing a file), the recommended pattern is dependency injection . A data provider that implements an interface is passed as an argument to the controller or viewmodel upon entry. For example, a DataProvider class is used to provide a list of integers on application load implements public interface IDataProvider { IEnumerable < int > LoadAsync (); } public class DataProvider : IDataProvider { async public List < int > LoadAsync () { return await List < int > { 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 , 29 , 31 }; } } A mocked data provider also implementing that interface can then be used in testing.","title":"Application design"},{"location":"Coding/C%23/#net","text":"The .NET ecosystem has 3 runtimes , all of which implement the .NET Standard Library and rest on common build tools , languages, and runtime components .NET Framework released in 2002, making it the oldest runtime, and runs only on Windows. Two major components: Common Language Runtime (CLR) runs managed code and performs garbage collection .NET Framework Class Library (also called the Base Class Library ) is composed of many classes, interfaces, and value types .NET Core is cross-platform, open-source, and optimized for performance. Its application host is dotnet.exe Core Common Language Runtime (CoreCLR) is more lightweight than that of .NET Framework, but implements Just-In Time compilation .NET Core Class Library is smaller than (and actually a subset of) that of .NET Framework Mono for Xamarin is used for mobile platforms like IOS, Android, and OS X .NET Standard is a specification of which APIs are available across all these runtimes. It evolved from Portable Class Libraries (PCL) and will eventually replace them. .NET's package manager is NuGet . An assembly can be compiled to EXE or DLL.","title":".NET"},{"location":"Coding/C%23/#dotnet","text":"Install dotnet-format dotnet tool install -g dotnet-format Install the ASP.NET scaffolding engine dotnet tool install -g dotnet-aspnet-codegenerator Create a new xUnit project named tests dotnet new xunit -n tests Add a project file to a solution dotnet sln add ./Project/Project.csproj Add a project reference to a project dotnet add reference ./path/to/Project.csproj Install a NuGet package and add a PackageReference in the project file Moq dotnet add package Moq System.CommandLine dotnet add package System.CommandLine Run the dotnet try web server that supports .NET Interactive-style markdown: ```cs --source-file ./Program.cs --project ./project.csproj ```","title":"dotnet"},{"location":"Coding/C%23/#project-files","text":"Project files are XML files that describe various metadata to the dotnet compiler. The root node is Project which has two subnodes that collect various information about the project: PropertyGroup can contain various elements that affect project settings: RootNamespace specifies the namespace that contains the Main() method for console applications TargetFramework specifies the targeted CLR framework: net5.0 , netcoreapp3.1 , etc LangVersion C# version: 9.0 , etc Nullable Enable nullable reference types ItemGroup contains references to NuGet packages ( PackageReference ) and other projects ( ProjectReference ). LangVersion <PropertyGroup> <LangVersion> preview </LangVersion> </PropertyGroup> Nullable <PropertyGroup> <Nullable> enable </Nullable> </PropertyGroup> ItemGroup <ItemGroup> <ProjectReference Include= \"/path/to/OtherProject.csproj\" /> <PackageReference Include= \"xunit\" Version= \"2.4.0\" /> </ItemGroup> Adding a reference to another project is also easily accomplished from the command-line. dotnet add project /path/to/OtherProject.csproj","title":"Project files"},{"location":"Coding/C%23/#packages","text":"NuGet is the official package manager for .NET. NuGet packages required for any project were stored in a XML packages.config file. But projects that use PackageReference may store that information in /obj/project.assets.json.","title":"Packages"},{"location":"Coding/C%23/#systemcommandline","text":"Prior to System.CommandLine, it had been up to the developer to build a custom solution resolving command-line arguments as an array of strings. Although .NET includes several earlier attempts at solving this problem, none had emerged as a default solution. Similar to Python's argparse , the CommandLine library allows you to construct a RootCommand object that accepts definitions of argument and options. Here, an argument is required: C# using System ; using System.CommandLine ; using System.CommandLine.Invocation ; namespace CommandLine { class Program { static int Main ( string [] args ) { var cmd = new RootCommand { new Argument < string >( \"name\" ) }; cmd . Handler = CommandHandler . Create < string >( HandleGreeting ); return cmd . Invoke ( args ); } static void HandleGreeting ( string name = \"world\" ) { Console . WriteLine ( $ \"Hello, {name}\" ); } } } Python import argparse def get_args (): parser = argparse . ArgumentParser ( description = \"Say hello\" ) parser . add_argument ( dest = \"name\" , metavar = \"name\" , default = \"World\" , help = \"Name to greet\" ) return parser . parse_args () def main (): args = get_args () print ( f \"Hello, { args . name } !\" ) if __name__ == \"__main__\" : main () Here, the greeting can be specified with an optional parameter C# using System ; using System.CommandLine ; using System.CommandLine.Invocation ; namespace CommandLine { class Program { static int Main ( string [] args ) { var cmd = new RootCommand { new Argument < string >( \"name\" ), //, \"Your name\"), new Option < string? >( new [] { \"--greeting\" , \"-g\" }, \"The greeting to use\" ), }; cmd . Handler = CommandHandler . Create < string , string? >( HandleGreeting ); return cmd . Invoke ( args ); } static void HandleGreeting ( string? greeting , string name ) { Console . WriteLine ( $ \"{greeting}, {name}\" ); } } } Python import argparse def get_args (): parser = argparse . ArgumentParser ( description = \"Say hello\" ) parser . add_argument ( dest = \"name\" , metavar = \"name\" , default = \"World\" , help = \"Name to greet\" ) parser . add_argument ( \"--greeting\" , \"-g\" , dest = \"greeting\" , default = \"Hello\" , help = \"Greeting to use\" ) return parser . parse_args () def main (): args = get_args () print ( f \" { args . greeting } , { args . name } !\" ) if __name__ == \"__main__\" : main ()","title":"System.CommandLine  "},{"location":"Coding/C%23/#documentation","text":"C# supports documentation comments that can be exported to an XML file, which can then be imported into a static site generator (especially DocFX). Visual Studio can be set to export these comments upon build.","title":"Documentation"},{"location":"Coding/C%23/#sdks","text":"","title":"SDKs"},{"location":"Coding/C%23/#dynamodb","text":"To develop a .NET application using DynamoDB, add the AWSSDK.DynamoDBv2 NuGet package. The AWS Explorer, part of the AWS Toolkit for Visual Studio extension, is also useful for setting up a new table. A user with programmatic access, including an Access Key and Secret Key, is necessary to use the toolkit. ( src ) Both .NET Core and .NET Framework are supported as target frameworks, but .NET Core uses exclusively asynchronous operations. A service client object is formed by instantiating AmazonDynamoDBClient . The exposed method PutItemAsync is used to save an item to a table as a PutItemRequest object. The item itself is provided as a Dictionary in the Item key, but the Dictionary's values are AttributeValue objects, formed with a magic key that determines the data type of the value. String new AttributeValue { S = \"Hello, world!\" } Integer new AttributeValue { N = \"3\" } Boolean new AttributeValue { BOOL = true } List new AttributeValue { L = new List < AttributeValue > { new AttributeValue { S = \"Socrates\" }, new AttributeValue { S = \"Plato\" }, new AttributeValue { S = \"Aristotle\" }, }} using Amazon.DynamoDBv2 ; namespace DynamoDBDemo { public class LowLevelSample { public static async Task ExecuteAsync () { using ( IAmazonDynamoDB ddbClient = new AmazonDynamoDBClient () { await ddbClient . PutItemAsync ( new PutItemRequest { TableName = \"Users\" , Item = new Dictionary < string , AttributeValue > { { \"Id\" , new AttributeValue { S = \"john@doe.com\" } }, { \"String\" , new AttributeValue { /* ... */ } } } }) }) } } }","title":"DynamoDB"},{"location":"Coding/C%23/#concurrency","text":"","title":"Concurrency"},{"location":"Coding/C%23/#asynchronous-programming","text":"Consuming APIs: HttpClient","title":"Asynchronous programming"},{"location":"Coding/C%23/#multithreading","text":"The Task Parallel Library offers a high-level way to set up multiple threads. A Task represents an asynchronous operation. Task.Run() queues the work passed as the action to run on a different thread in the thread pool. Task.Run<T>() represents an asynchronous operation that returns a specific value type. Task . Run ( () => { // ... }); Objects in other threads will be inaccessible without using an object like Dispatcher in WPF Task . Run ( () => { Dispatcher . Invoke (() => { // ... }); }); To avoid blocking, we can make it asynchronous private async void Search_Click ( object sender , RoutedEventArgs e ) { await Task . Run () => { // ... Dispatcher . Invoke (() => { // ... } } }","title":"Multithreading"},{"location":"Coding/C%23/#glossary","text":"Assembly A collection of types and resources that are built to work together and form a logical unit of functionality and which form the building blocks of .NET applications. Module A portable executable file (DLL or EXE) consisting of one or more classes and interfaces. Although multiple modules can theoretically compose a single assembly, in practice an assembly and module can be considered one and the same for most .NET applications. Provider pattern A favored development model in .NET, and a form of dependency injection where a class is passed as an argument to another class that uses it for some purpose. The key is that the provider must derive from an abstract base class or an interface to support mocks in unit testing.","title":"\ud83d\udcd8 Glossary"},{"location":"Coding/C%2B%2B/","text":"Preprocessing Preprocessing is the phase of executing preprocessing directives in a source file, which are then removed from the resulting translation unit , which combines the pure C or C++ code of a source file with all its included header files. The translation unit is then compiled into an object file , and it is the linker that then forms linkages between object files to produce an executable module. #define #include The #define directive specifies a macro which can define a text replacement to occur in code before it is compiled. Macros are considered a holdover from C, and other constructs like variable templates and function templates are considered better suited in C++. In practice, #define statements are most commonly used to handle header files. Here, any instance of \" PI \" in the source code will be replaced by the string of digits \"3.14159265\", but it will not be replaced if it forms part of an identifier or appears in a string literal or comment. #define PI 3.14159265 If a substitution string isn't specified then any instance of the identifier will simply be removed. #define break Macros may not span multiple lines without escaped line breaks, but during preprocessing these are removed and the substitution is made inline. #define PI 3.14\\ 159265 Function-like macros are possible because the preprocessor can recognize a function call in the macro identifier and replace its arguments intelligently. Here any invocation of the MAX() function call will have its arguments incorporated into the substitution statement. #define MAX(A, B) A >= B ? A : B All the lines following #ifndef ( #if !defined ) will be kept in the file as long as the identifier \"MYHEADER_H\" has not already been defined. This common is called an #include guard . #ifndef MYHEADER_H #define MYHEADER_H // ... #endif Variables Since C++14 , you can separate digits in a long integer with a single quote to make it more readable. int num { 12'345 }; // 12,345 Hexadecimal literals are prefixed with \"0x\" and octals with \"0\": int hex { 0xabcdef }; int oct { 0567 }; constexpr is used in some situations I can't figure out yet. static constexpr u32 MAX_MEM = 1024 * 64 ; size_t is a type alias defined in the Standard Library (in the cstddef header). It is an alias for an unsigned integer type. Initialization A braced initializer refers to placing the initial value of a variable in braces. This is a novel style of initialization introduced in C++11 . Its main advantage is that it will raise a compile-time error if the compiler has to perform a narrowing conversion of the value to match the declared type. int apples { 15 }; Older but equally valid ways of initializing variables: int oranges = 12 ; int kiwis ( 13 ); // \"functional notation\" Zero initialization refers to initializing a variable with empty braces. It works for any fundamental type, and numeric types initialize to zero. int grapes {}; // 0 Sequences, like this array class can be efficiently zero-initialized; the braces can contain any number of values up to the declared size of the array (remaining values will be zero-initialized). #include <array> array < int , 5 > myIntArray {}; Multi-dimensional arrays can be initialized with nested initializers: int myNums [ 2 ][ 3 ] { { 1 , 2 , 3 }, { 4 , 5 , 6 } }; Pointers Smart pointers (also called managed pointers ) are pointers that manage their own memory. They were introduced in C++11, and there is no longer a reason to use the earlier raw pointers . In older versions of C++, memory leaks were common because the programmer had to remember to release memory allocated dynamically from the heap/free store using the delete keyword. These older pointers are now called raw pointers The most commonly used smart pointer is unique_ptr : others include shared_ptr and weak_ptr . Only a single unique_ptr<T> can point to any memory address, unless ownership is transfered with move() . Since C++14, it is recommended to create unique pointers using makeunique<T>() . auto pdbl = make_unique < dbouble > (); When variables are declared with an asterisk * appended to the datatype or prepended to the identifier, the variable becomes a pointer to that type. Pointer identifiers usually begin with \"p\", a convention known as Hungarian notation . The size of pointers corresponds to the address space of available memory (4 bytes for 32-bit architectures, and 8 bytes for 64-bit). // The following statements are equivalent. long * pnum {}; long * pnum { nullptr }; void * is known as \"pointer to void type\", meaning variables defined as such are pointers to data of an unspecified type, making it similar to var in C#. The address-of operator & obtains the address of a variable. The address-of operator typically also occurs with the indirection operator or dereference operator (also * ) to access the data pointed to by a pointer. Using a dereferenced pointer is the same as using the variable to which it points. long num { 12345L }; long * pnum { & num }; long newnum { * pnum + 1 }; When a pointer points to an object with methods, like a vector<T> container , the indirect member selection operator ( -> ) can be used to access the methods. // The following statements are equivalent. auto * pdata { new std :: vector < int > {}}; std :: vector < int > data ; auto * pdata = & data ; // The following statements are equivalent. ( * pdata ). push_back ( 66 ); pdata -> push_back ( 66 ); Pointers to classes can be recast with the following syntax: Animal * ptr = & kitty ; (( Cat * ) ptr ) -> chaseMouse (); // newer, safer syntax ( reinterpret_cast < Cat > ( ptr )) -> chaseMouse (); Containers Containers are a type of data structure used to contain elements for various purposes. They are deeply tied to algorithms through iterators . Two array-like data structures defined in the Standard Library that are more typically used are array<T,N> and vector<T> Arrays An array is a variable that represents a contiguous sequence of memory locations, each storing an item of data of the same data type, each of which are called elements . Arrays must be declared with a constant integer expression that is fixed at compile time. Built-in arrays in C++ are inherited from C. int primes [ 10 ] { 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 } Sequence containers The two most common sequence containers are array<T,N> and vector<T> . All sequence containers expose several of a family of related member functions: Member function vector array list forward_list deque front() \u2705 \u2705 \u2705 \u2705 \u2705 back() \u2705 \u2705 \u2705 \u274c \u2705 push_front() \u274c \u274c \u2705 \u2705 \u2705 pop_front() \u274c \u274c \u2705 \u2705 \u2705 push_back() \u2705 \u274c \u2705 \u274c \u2705 pop_back() \u2705 \u274c \u2705 \u274c \u2705 insert() \u2705 \u274c \u2705 \u2705 \u2705 erase() \u2705 \u274c \u2705 \u2705 \u2705 array<T,N> (also \" array class \") is a fixed sequence defined with two template parameters to create an array of N elements of type T . Here it is zero initialized with an empty braced initializer . #include <array> array < int , 5 > myIntArray {}; Other methods: - fill() : Set every element of the array to the same value - size() : Return the number of elements as a type size_t - at() : Access an element at a given index but testing for a valid range. Safer than using the built-in index method. Vectors are sequential containers with typed elements like the array class , but are not limited to fixed sizes. The push_back() method is similar to a Python List.append() . Other methods like front() , back() , and pop_back() can be used to manipulate the vector. #include <vector> vector < int > vals ; The insert() method takes two arguments, one is an iterator , here provided by yet another vector method - begin() , and the content to be inserted. This code will insert the string at index 2. #include <iostream> #include <vector> #include <string> using namespace std ; int main () { vector < string > family ; family . push_back ( \"Plato\" ); family . push_back ( \"Aristotle\" ); family . push_back ( \"Socrates\" ); family . push_back ( \"Pythagoras\" ); family . push_back ( \"Aristarchos\" ); family . insert ( family . begin () + 2 , \"Dgiapusccu\" ); family . pop_back (); for ( string i : family ) { cout << i << endl ; } return 0 ; } A forward_list<T> is an implementation of the singly-linked list and is rarely used. A list<T> is an implementation of the doubly-linked list and is rarely used. The double-ended queue (deque) exposes push_Front() and push_back() methods. Container adapters A stack<T> implements last-in first-out (LIFO) semantics. Stacks support push and pop methods. A queue<T> implements first-in first-out (FIFO) semantics. Associate containers Standard iterators There are three types of iterator supported by containers in the Standard Library: - random access iterator support the widest variety of operations: vector<T> , array<T,N> , and deque<T> - forward iterators do not support decrement operations (\"going backwards\"), and this describes the operation of a forward_list<T> - bidirectional iterators support both increment and decrement operations, but cannot jump more than one value, describing the operation of a list<T> stack<T> , queue<T> , and priority_queue<T> do not have iterators whatsoever. Containers in the Standard Library expose a begin() member function, which is the most commonly used iterator. std :: vector < char > letters { 'a' , 'b' , 'c' , 'd' , 'e' } auto iter { letters . begin ()}; // Specifying type explicitly std :: vector :: iterator iter { letters . begin ()}; At a deep level, iterators are pointers, so dereferencing them produces the element of the container being iterated. std :: cout << * iter << std :: endl ; // a The container can now be traversed by incrementing and (sometimes) decrementing the iterator. ++ iter ; std :: cout << * iter << std :: endl ; // b A string can be reversed using the rbegin() and rend() iterators: string name { \"Lorem ipsum...\" }; string reverse ( name . rbegin (), name . rend ()); Maps A syntactic sugar has been available since C++17: for ([ x , y ] : coords ) { std :: cout << x << y << endl ; } It is equivalent to: for ( std :: pair < int , int > el : coords ) { std :: cout << el . first << el . second << endl ; } The pair type has two public fields: first second ## Math #include <math.h> using namespace std ; int main () { int num { 2 }; cout << pow ( num , 2 ) << endl ; } Classes New data types in C++ are created as classes , which can be composites of member variables of other types and member functions , allowing complex and intuitive models to be created. The three primary principles of OOP are: Encapsulation : member variables and functions are packaged together Data hiding preserves the integrity of an object Inheritance allows one type to define another. Polymorphism (in C++ implemented by calling member functions using a pointer or reference) allow behavior of base classes to be exposed from objects of derived classes Member variables Classes can contain member variables that are public or private (\" access specifiers \"), but it is best practice to make variables private while implementing accessor functions (getters and setters): - Hiding data preserves the integrity of objects - Loose coupling facilitates future change in codebase - Extra code can be injected for logging or data validation - Debuggers can set breakpoints on these getters and setters class Box { private : double length { 1 , 0 }; double width { 1 , 0 }; double height { 1 , 0 }; public : double volume () { return length * width * height ; } }; Initialization A member initializer list can be used to initialize fields more efficiently than explicit assignment. Box :: Box ( double lv , double wv , double hv ) : length { lv }, width { wv }, height { hv } {} This technique must have an expression in braces, even if they are empty. // Compiler error Box :: Box ( double lv , double wv , double hv ) : length { lv }, width { wv }, height { hv } Notably, this technique doesn't appear to work in derived class constructors. class Animal { public : std :: string _name {}; Animal ( std :: string n ) : _name { n } {} } class Dog : public Animal { public : Dog ( std :: string n , std :: string b ) : Animal ( n ) { _breed = b ;}; } A class constructor is called whenever a new instance of the class is defined. It always has the same name as the class itself and has no return data type because it returns no data. class Box { private : // ... public : Box ( double l , double w , double h ) { length = l ; width = w ; height = h ; } }; If a constructor isn't defined, the compiler will supply a default default constructor when an object is instantiated without initial values. To define a default constructor: Box () = default ; A destructor is a special member of a class executed to deal with cleanup upon use of the delete operator. A class can have only one destructor, and if one isn't defined then the compiler provides a default destructor that does nothing. The name of the destructor for a class is always the class name prefixed with a tilde, and similar to a constructor it cannot have a return type or parameters. ~ Box () = default ; Box ::~ Box () = default ; // when defined outside the class Base class destructors should always be declared as virtual . Access When variables of class types are instantiated with the const keyword, they are called const objects , and none of their member variables can be altered (member variables of const objects become immutable). The compiler will throw an error when attempting to invoke methods of const objects unless they are identified as const member functions by using the const keyword in the signature after the identifier (\"attribute\"?): class Box { double volume () const { /* ... */ } double getLength () const { return length ; } double getWidth () const { return width ; } double getHeight () const { return height ; } } public , private , and protected are access specifiers that determine how a member variables and functions can be accessed from the outside. When inheriting from a base class, an access specifier can also be used to determine how accessible that base class's members are within the derived class. class Dog : public Animal { // ... } - When the base class specifier is public , inherited members are unchanged - When the base class specifier is protected , inherited public and protected members become protected - When the base class specifier is private, all inherited members become private. A friend is a function to which a class grants access to its private internals. They may be useful in rare situations where a single function needs access to the internals of different kinds of objects. Inheritance When creating subclasses, you must remember: - Private variables must be placed in the protected access specifier so that they are accessible to child classes. - The base class access specifier must allow access to the base class's private variables ( public or protected ) - Parent class must have a default constructor class Animal { protected : // not `private:`! std :: string _name ; public : Animal () = default ; } class Dog : public Animal { /* ... */ } // Polymorphism Polymorphism in C++ refers to the practice of invoking a base class's member function rather than the derived class. Because the compiler performs early binding by default, a pointer typed to a base class but initialized to a derived class will invoke the base class's member function. Late binding can be used to force the pointer to use the derived class's member function even when the type of the pointer is the base class. This is done by using the virtual keyword on the base class's member function. Classes with virtual functions are called abstract classes and may not be instantiated. Abstract classes that are made of only virtual functions are called interfaces . class Base { public : virtual void doStuff () { /* ... */ } } class Derived : public Base { public : void doStuff () { /* ... */ } } int main () { Derived derived {}; Base * pointer = & derived ; // a pointer to an abstract class **may** be used pointer -> doStuff (); } Derived classes must then override this virtual function with the override keyword. class Derived : public Base { override doStuff () // ... } Enumerations Enumerations can be specified with enum . Without specifying a value, each element of the enum is given a successively greater integer value starting with 0, like the indexes of an array (an ordinal value ). enum Choices { A , B , C , D } These elements can be specified with or without the scope resolution operator :: cout << A ; // 0 cout << Choices :: A ; // 0 Templates Templates are used to have the compiler generate code automatically for a given data type. This is to avoid highly repetitive overloaded function definitions which only differ in parameter lists. The template and typename keywords define a template. The placeholder \"T\" represents the data type that will be replaced by a specific type by the compiler. template < typename T > T larger ( T a , T b ) { return a > b ? a : b ; } More than one data type can be used for the parameters, but in that case the return type must be explicitly specified: template < typename T1 , typename T2 > bool larger ( T1 a , T2 b ) { return a > b ; } Control flow The choices in a switch statement are called cases . You can only switch on constant expressions that can be evaluated at compile-time, typically literals but excluding strings. Each case must be followed by a break statement to prevent fallthrough , except for the default case . switch ( choice ) { case 1 : // ... break ; case 2 : // ... break ; default : // ... } Since C++11, the range-based for-loop is available, which works very similar to a Python for-in loop: for ( string num : nums ) { cout << num << endl ; } Functions Function prototypes , defining the function header (return data type, function name, and parameter list), describe a function sufficiently for the compiler to be able to compile calls to it and are required before using a function if the function declaration doesn't precede all the locations where it's called. #include <iostream> using namespace std ; // Without this prototype, there is a compile-time error. void printSomething (); int main () { printSomething (); return 0 ; } void printSomething () { cout << \"something...\" << endl ; } Passing by reference allows variables to be changed in-place and works by using the reference to the argument. Passing by value is the default parameter passing scheme , which works by actually copying the argument. int func ( int a ) { // Pass by value } int func ( int & a ) { // Pass by reference } Recursion Recursion requires a base case and at least one recursive case . The call stack is a stack data structure that figures prominently in recursive computing.## Home Project Description JamoftheMonthProject.cpp CLI application that calculates how much the user owes based on selected subscription tier and units purchased TicTacToe.cpp RPGCharGen.cpp Multiple classes using inheritance, virtual member functions, enums Task Description Reverse a string ...## Iterators An iterator is a classical and widespread design pattern that allows a wide variety of container-like objects to be traversed by exposing a uniform interface. However, loops based on iterators should only be used if access to the iterator is needed for advanced processing in the loop body. A range-based for loop is the recommended way to iterate over all elements of a container. Standard iterators begin() end() rbegin() Memory Memory leaks can be detected using valgrind . Namespaces A namespace is a block that attaches an extra name to every entity name that is declared or defined within it. The qualified name of each entity is the namespace name followed by the scope resolution operator :: followed by the basic entity name. Namespaces can be used to partition large codebases into logical groupings to avoid name clashes. If a namespace isn't defined, the global namespace , where entities have no namespace name attached, applies by default. You can define a namespace using the namespace keyword. namespace foo { // ... } Namespaces can be nested.. namespace outer { namespace inner { void foo () { // ... } } } outer :: inner :: foo () Namespace aliases can be formed: namespace outin = outer :: inner ; outin :: foo () The using keyword allows you to reference any name from a namespace without qualifying it. using namespace std ; It can also be used specify a type alias , where an alternative name is used to refer to an existing data type. using BigOnes = unsigned long long ; typedef unsigned long long BigOnes ; // Older, less intuitive syntax Operators Each operator is associated with a particular named function. Operators can be overloaded by implementing that function. bool Rectangle :: operator == ( const Rectangle & other ) const { return _length == other . _length && _width == other . _width ; } Header files Topic Header file array <array> deque <deque> exception <exception> map <map> Mathematical functions <math.h> queue <queue> stack <stack> vector <vector> Smart pointers <memory> Applications gtkmm gtkmm (historically \"GTK--\") is a C++ wrapper for an underlying GTK code base written in C. Compared to Qt , another GUI library, gtkmm uses more modern and native C++ features. In Ubuntu , installing the development environment is done with the gnome-devel metapackage: sudo apt install gnome-devel NES emulator Courses C++ Standard Template Library in Practice /# Topic Video Projects 01.01 The Course Overview 01.02 Templates Introduction to the STL TemplateSTL.cpp 01.03 General Concepts ExceptionSTL.cpp 01.04 Utilities - Common Utilities StringSTL.cpp 01.05 Utilities - Regex RegexSTL.cpp 01.06 Project - Bitcoin Exchange Program BTCX.cpp 01.07 Project - Coding 01.08 Project - Custom Writer Function 01.09 Review 02.01 Understanding Containers 02.02 Vectors 02.03 Standard Array 02.04 Lists 02.05 Stacks and Queues 02.06 Maps and Multimaps - Overview 02.07 Maps - Coding 02.08 Multimaps - Coding 02.09 Sets and Multisets 02.10 Project 02.11 Review 03.01 Iterators 03.02 Input Iterators 03.03 Output Iterators 03.04 Forward Iterators 03.05 Bidirectional Iterators 03.06 Random Access Iterators 03.07 Auxiliary Iterator Functions 03.08 Iterator Adaptors 03.09 Writing Generic Functions for Iterators 03.10 User - Defined Iterators 03.11 Project 03.12 Review 04.01 Introduction to Algorithms 04.02 Sequence Algorithms - for_each 04.03 Sequence Algorithms - equals 04.04 Copying 04.05 Moving 04.06 Removing 04.07 Sorting and Gathering - std::sort 04.08 Sorting and Gathering - std::partial_sort algorithm 04.09 Sorting and Gathering - std::partition 04.10 Sorting and Gathering - std::partition_copy 04.11 Searching and Finding - std::find 04.12 Sorting and Gathering - std::find_first_of, std::adjacent_find 04.13 Sorting and Gathering - std::search 04.14 Sorting and Gathering - std::binary_search 04.15 Counting 05.01 Replacing and Transforming - std::replace 05.02 Replacing and Transforming - std::replace_copy 05.03 Replacing and Transforming - equals 05.04 Swapping 05.05 Rotating 05.06 Randomizing 05.07 Permutations 05.08 Sampling 05.09 Min 05.10 Max 05.11 Clamp 05.12 Fill and Generate 05.13 Numeric Algorithms - std::accumulate 05.14 Numeric Algorithms - std::partial_sum and std::adjacent_difference 05.15 Numeric Algorithms - std::gcd, and std::lcm 05.16 Numeric Algorithms - std::inner_product and std::iota 05.17 Review 06.01 Basic Architecture of the I/O Stream Library 06.02 Console I/O - Interact with a User 06.03 Console I/O - Read Input 06.04 File I/O 06.05 String Streams 06.06 Manipulators and Formatters 06.07 Stream States 06.08 Low Level I/O 06.09 Overloading Stream Operators 06.10 Project - Overview 06.11 Project - Classes and structures 06.12 Project - Implementation 06.13 Review 07.01 Unique Pointers 07.02 Shared Pointers 07.03 Allocators 07.04 Defining an Allocator 07.05 Uninitialized Memory 07.06 Review 08.01 Introduction to Threading 08.02 Creating Threads 08.03 Locks 08.04 Shared Locks 08.05 Atomic Values 08.06 Async 08.07 Condition Variables 08.08 Project 08.09 Review 09.01 Concepts 09.02 Modules 09.03 Coroutines 09.04 Course Review Complete C++ Developer Course /# Topic Video Projects 1.1 Section Overview 1.2 Getting Started on Windows with Visual Studio Integrated Development Environment (IDE) 1.3 Getting Started on macOS or Linux with CodeBlocks IDE 1.4 Getting Started with macOS Catalina or Higher with Visual Studio Code 1.5 Finding Answers to Your Questions 2.1 Section Overview 2.2 Saying \"Hello\" to C++ 2.3 Variables and Data Types - Part 1 2.4 Variables and Data Types - Part Two 2.5 Variables and Data Types - Part Three 2.6 Comments 2.7 Arithmetic Operators 2.8 Relational Operators 2.9 Logical Operators 2.10 Symbolic Constants and Naming Conventions 2.11 User Input 2.12 Project - Average of Three 2.13 Project - MadLibs Clone 2.14 Section Wrap-Up 3.1 Section Overview 3.2 Introduction to Control Statements 3.3 Selection Control Statements 3.4 Repetition Control Statements 3.5 The Break and Continue Statements 3.6 Random Numbers 3.7 Project - Jam of the Month Club 3.8 Project - Odds and Evens 3.9 Project - Guess the Number 3.10 Section Wrap-Up 4.1 Section Overview 4.2 Built-in Arrays 4.3 The Array Class 4.4 The Vector Class 4.5 Multi-Dimensional Arrays 4.6 Project - Array Data 4.7 Project - Vector Data 4.8 Project - Parallel Arrays/Vectors 4.9 Section Wrap-Up 5.1 Section Overview 5.2 Function Prototypes and Definitions FunctionFun.cpp 5.3 Function Return Types and Parameters 5.4 Parameter Passing: Pass-by-Value and Pass-by-Reference PassingSchemes.cpp 5.5 Variable Scope and Lifetime ScopeFun 5.6 Function Overloading 5.7 The <cmath> Library 5.8 Recursion 5.9 Project - Return the Product of Three Parameters 5.10 Project - Return the Sum of Built-in Array Elements 5.11 Project - Return the Sum of Array Object Elements 5.12 Project - Retrieve the Sum of Array Object Elements by Reference 5.13 Project - Tic-Tac-Toe (ADVANCED) 5.14 Section Wrap-Up 6.1 Section Overview 6.2 Basics of Object Oriented Programming (OOP) 6.3 Encapsulation: Data Members and Member Functions 6.4 Separate Compilation 6.5 Constructors and Destructors 6.6 A Rectangle Class Rectangle.cpp 6.7 A Book Class 6.8 Project - A Bank Account Class 6.9 Project - A Pizza Class 6.10 Project - A Circle Class 6.11 Section Wrap-Up 7.1 Section Overview 7.2 Exceptions and the Exception Hierarchy 7.3 Logic Errors 7.4 Runtime Errors and Throwing Exceptions 7.5 Rethrowing Exceptions 7.6 Custom Exceptions 7.7 Basic Testing and Debugging 7.8 Project - Throwing and Handling an Out_of_Range Exception 7.9 Project - Creating and Using Your Own Exception 7.10 Section Wrap-Up 8.1 Section Overview 8.2 Introduction to Pointers 8.3 Dynamic Memory - Part 1 8.4 Dynamic Memory (- Part 2 8.5 Const Correctness 8.6 Project - Dynamically Creating Rectangles 8.7 Project - Dynamically Creating Circles 8.8 Section Wrap-Up 9.1 Section Overview 9.2 Sequential File Output 9.3 Sequential File Input 9.4 More File Input/ Output (I/O) 9.5 Project - Reading Data from File and Printing Statistics 9.6 Project - Dynamically Creating Rectangles from File 9.7 Project - Shopping Item File 9.8 Section Wrap-Up 10.1 Section Overview 10.2 Inheritance - Part 1 10.3 Inheritance - Part 2) 10.4 Polymorphism and Late Binding 10.5 Enumerated Types This video explains enumerated types. EnumFun.cpp 10.6 Project - Derived Cat Class Cat.cpp 10.7 Project \u2013 Role Playing Game (RPG) Player Character Creation RPGCharGen 10.8 Section Wrap-Up 11.1 Section Overview 11.2 Templates - Standard Template Library (STL) TemplateFun.cpp 11.3 queue<T> stack<T> deque<T> Standard Template Library (STL) - Part 1) DequeFun.cpp, StackFun.cpp, QueueProject.cpp 11.4 map<T> Standard Template Library (STL) - Part 2 ContactsFun.cpp, AlgorithmFun.cpp 11.5 unique_ptr<T> Smart Pointers SmartPointerFun.cpp, Car.cpp 11.6 Friend functions Friend Functions and Friend Classes FriendFunctions.cpp 11.7 Operator Overloading OverloadingFun.cpp, Rectangle.h 11.8 map<T> Project - Dictionary of Terms DictionaryProject.cpp 11.9 Project - Aliens Aliens.cpp 11.10 Section Wrap-Up RPGCharGen cpp #include <iostream> #include \"RPGCharGen.h\" using namespace std ; int main () { Warrior w { \"Doofus McGroober\" , Race :: HUMAN }; cout << \"Player name: \" << w . getName () << endl ; cout << \"Player HP: \" << w . getHp () << endl ; cout << \"Player MP: \" << w . getMp () << endl ; cout << \"Player race: \" << w . getRace () << endl ; w . attack (); Priest m { \"Brother Tolkien\" , Race :: ELF }; cout << \"Player name: \" << m . getName () << endl ; cout << \"Player HP: \" << m . getHp () << endl ; cout << \"Player MP: \" << m . getMp () << endl ; cout << \"Player race: \" << m . getRace () << endl ; m . attack (); Mage n { \"Smart Frodo\" , Race :: DWARF }; cout << \"Player name: \" << n . getName () << endl ; cout << \"Player HP: \" << n . getHp () << endl ; cout << \"Player MP: \" << n . getMp () << endl ; cout << \"Player race: \" << n . getRace () << endl ; n . attack (); return 0 ; } h #if !defined(RPGCHARGEN_H) #define RPGCHARGEN_H #include <string> enum Race { HUMAN , ELF , DWARF }; class Player { protected : std :: string _name { \"Johnny Bravo\" }; Race _race { Race :: HUMAN }; int _hp { 100 }; int _mp { 100 }; public : Player ( std :: string n , Race r , int hp , int mp ) : _name { n }, _race { r }, _hp ( hp ), _mp ( mp ) {} virtual std :: string attack () = 0 ; int getHp () { return _hp ; } int getMp () { return _mp ; } std :: string getRace () { switch ( _race ) { case 0 : return \"human\" ; break ; case 1 : return \"elf\" ; break ; case 2 : return \"dwarf\" ; break ; default : return \"none\" ; break ; } } std :: string getName () { return _name ; } void setHp ( int n ) { _hp = n ; } void setMp ( int n ) { _mp = n ; } void setName ( std :: string s ) { _name = s ; } void setRace ( Race r ) { _race = r ;} }; class Warrior : public Player { public : Warrior ( std :: string n , Race r ) : Player ( n , r , 200 , 0 ) {} std :: string attack () { return \"I will destroy you with my sword, foul demon!\" ;} }; class Priest : public Player { public : Priest ( std :: string n , Race r ) : Player ( n , r , 100 , 200 ) {} std :: string attack () { return \"Taste the wrath of the Two True Gods!\" ;} }; class Mage : public Player { public : Mage ( std :: string n , Race r ) : Player ( n , r , 150 , 150 ) {} std :: string attack () { return \"You are overmatched by my esoteric artifices!\" ;} }; #endif // RPGCHARGEN_H","title":"C++"},{"location":"Coding/C%2B%2B/#preprocessing","text":"Preprocessing is the phase of executing preprocessing directives in a source file, which are then removed from the resulting translation unit , which combines the pure C or C++ code of a source file with all its included header files. The translation unit is then compiled into an object file , and it is the linker that then forms linkages between object files to produce an executable module. #define #include The #define directive specifies a macro which can define a text replacement to occur in code before it is compiled. Macros are considered a holdover from C, and other constructs like variable templates and function templates are considered better suited in C++. In practice, #define statements are most commonly used to handle header files. Here, any instance of \" PI \" in the source code will be replaced by the string of digits \"3.14159265\", but it will not be replaced if it forms part of an identifier or appears in a string literal or comment. #define PI 3.14159265 If a substitution string isn't specified then any instance of the identifier will simply be removed. #define break Macros may not span multiple lines without escaped line breaks, but during preprocessing these are removed and the substitution is made inline. #define PI 3.14\\ 159265 Function-like macros are possible because the preprocessor can recognize a function call in the macro identifier and replace its arguments intelligently. Here any invocation of the MAX() function call will have its arguments incorporated into the substitution statement. #define MAX(A, B) A >= B ? A : B All the lines following #ifndef ( #if !defined ) will be kept in the file as long as the identifier \"MYHEADER_H\" has not already been defined. This common is called an #include guard . #ifndef MYHEADER_H #define MYHEADER_H // ... #endif","title":"Preprocessing"},{"location":"Coding/C%2B%2B/#variables","text":"Since C++14 , you can separate digits in a long integer with a single quote to make it more readable. int num { 12'345 }; // 12,345 Hexadecimal literals are prefixed with \"0x\" and octals with \"0\": int hex { 0xabcdef }; int oct { 0567 }; constexpr is used in some situations I can't figure out yet. static constexpr u32 MAX_MEM = 1024 * 64 ; size_t is a type alias defined in the Standard Library (in the cstddef header). It is an alias for an unsigned integer type.","title":"Variables"},{"location":"Coding/C%2B%2B/#initialization","text":"A braced initializer refers to placing the initial value of a variable in braces. This is a novel style of initialization introduced in C++11 . Its main advantage is that it will raise a compile-time error if the compiler has to perform a narrowing conversion of the value to match the declared type. int apples { 15 }; Older but equally valid ways of initializing variables: int oranges = 12 ; int kiwis ( 13 ); // \"functional notation\" Zero initialization refers to initializing a variable with empty braces. It works for any fundamental type, and numeric types initialize to zero. int grapes {}; // 0 Sequences, like this array class can be efficiently zero-initialized; the braces can contain any number of values up to the declared size of the array (remaining values will be zero-initialized). #include <array> array < int , 5 > myIntArray {}; Multi-dimensional arrays can be initialized with nested initializers: int myNums [ 2 ][ 3 ] { { 1 , 2 , 3 }, { 4 , 5 , 6 } };","title":"Initialization"},{"location":"Coding/C%2B%2B/#pointers","text":"Smart pointers (also called managed pointers ) are pointers that manage their own memory. They were introduced in C++11, and there is no longer a reason to use the earlier raw pointers . In older versions of C++, memory leaks were common because the programmer had to remember to release memory allocated dynamically from the heap/free store using the delete keyword. These older pointers are now called raw pointers The most commonly used smart pointer is unique_ptr : others include shared_ptr and weak_ptr . Only a single unique_ptr<T> can point to any memory address, unless ownership is transfered with move() . Since C++14, it is recommended to create unique pointers using makeunique<T>() . auto pdbl = make_unique < dbouble > (); When variables are declared with an asterisk * appended to the datatype or prepended to the identifier, the variable becomes a pointer to that type. Pointer identifiers usually begin with \"p\", a convention known as Hungarian notation . The size of pointers corresponds to the address space of available memory (4 bytes for 32-bit architectures, and 8 bytes for 64-bit). // The following statements are equivalent. long * pnum {}; long * pnum { nullptr }; void * is known as \"pointer to void type\", meaning variables defined as such are pointers to data of an unspecified type, making it similar to var in C#. The address-of operator & obtains the address of a variable. The address-of operator typically also occurs with the indirection operator or dereference operator (also * ) to access the data pointed to by a pointer. Using a dereferenced pointer is the same as using the variable to which it points. long num { 12345L }; long * pnum { & num }; long newnum { * pnum + 1 }; When a pointer points to an object with methods, like a vector<T> container , the indirect member selection operator ( -> ) can be used to access the methods. // The following statements are equivalent. auto * pdata { new std :: vector < int > {}}; std :: vector < int > data ; auto * pdata = & data ; // The following statements are equivalent. ( * pdata ). push_back ( 66 ); pdata -> push_back ( 66 ); Pointers to classes can be recast with the following syntax: Animal * ptr = & kitty ; (( Cat * ) ptr ) -> chaseMouse (); // newer, safer syntax ( reinterpret_cast < Cat > ( ptr )) -> chaseMouse ();","title":"Pointers"},{"location":"Coding/C%2B%2B/#containers","text":"Containers are a type of data structure used to contain elements for various purposes. They are deeply tied to algorithms through iterators . Two array-like data structures defined in the Standard Library that are more typically used are array<T,N> and vector<T>","title":"Containers"},{"location":"Coding/C%2B%2B/#arrays","text":"An array is a variable that represents a contiguous sequence of memory locations, each storing an item of data of the same data type, each of which are called elements . Arrays must be declared with a constant integer expression that is fixed at compile time. Built-in arrays in C++ are inherited from C. int primes [ 10 ] { 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 }","title":"Arrays"},{"location":"Coding/C%2B%2B/#sequence-containers","text":"The two most common sequence containers are array<T,N> and vector<T> . All sequence containers expose several of a family of related member functions: Member function vector array list forward_list deque front() \u2705 \u2705 \u2705 \u2705 \u2705 back() \u2705 \u2705 \u2705 \u274c \u2705 push_front() \u274c \u274c \u2705 \u2705 \u2705 pop_front() \u274c \u274c \u2705 \u2705 \u2705 push_back() \u2705 \u274c \u2705 \u274c \u2705 pop_back() \u2705 \u274c \u2705 \u274c \u2705 insert() \u2705 \u274c \u2705 \u2705 \u2705 erase() \u2705 \u274c \u2705 \u2705 \u2705 array<T,N> (also \" array class \") is a fixed sequence defined with two template parameters to create an array of N elements of type T . Here it is zero initialized with an empty braced initializer . #include <array> array < int , 5 > myIntArray {}; Other methods: - fill() : Set every element of the array to the same value - size() : Return the number of elements as a type size_t - at() : Access an element at a given index but testing for a valid range. Safer than using the built-in index method. Vectors are sequential containers with typed elements like the array class , but are not limited to fixed sizes. The push_back() method is similar to a Python List.append() . Other methods like front() , back() , and pop_back() can be used to manipulate the vector. #include <vector> vector < int > vals ; The insert() method takes two arguments, one is an iterator , here provided by yet another vector method - begin() , and the content to be inserted. This code will insert the string at index 2. #include <iostream> #include <vector> #include <string> using namespace std ; int main () { vector < string > family ; family . push_back ( \"Plato\" ); family . push_back ( \"Aristotle\" ); family . push_back ( \"Socrates\" ); family . push_back ( \"Pythagoras\" ); family . push_back ( \"Aristarchos\" ); family . insert ( family . begin () + 2 , \"Dgiapusccu\" ); family . pop_back (); for ( string i : family ) { cout << i << endl ; } return 0 ; } A forward_list<T> is an implementation of the singly-linked list and is rarely used. A list<T> is an implementation of the doubly-linked list and is rarely used. The double-ended queue (deque) exposes push_Front() and push_back() methods.","title":"Sequence containers"},{"location":"Coding/C%2B%2B/#container-adapters","text":"A stack<T> implements last-in first-out (LIFO) semantics. Stacks support push and pop methods. A queue<T> implements first-in first-out (FIFO) semantics.","title":"Container adapters"},{"location":"Coding/C%2B%2B/#associate-containers","text":"","title":"Associate containers"},{"location":"Coding/C%2B%2B/#standard-iterators","text":"There are three types of iterator supported by containers in the Standard Library: - random access iterator support the widest variety of operations: vector<T> , array<T,N> , and deque<T> - forward iterators do not support decrement operations (\"going backwards\"), and this describes the operation of a forward_list<T> - bidirectional iterators support both increment and decrement operations, but cannot jump more than one value, describing the operation of a list<T> stack<T> , queue<T> , and priority_queue<T> do not have iterators whatsoever. Containers in the Standard Library expose a begin() member function, which is the most commonly used iterator. std :: vector < char > letters { 'a' , 'b' , 'c' , 'd' , 'e' } auto iter { letters . begin ()}; // Specifying type explicitly std :: vector :: iterator iter { letters . begin ()}; At a deep level, iterators are pointers, so dereferencing them produces the element of the container being iterated. std :: cout << * iter << std :: endl ; // a The container can now be traversed by incrementing and (sometimes) decrementing the iterator. ++ iter ; std :: cout << * iter << std :: endl ; // b A string can be reversed using the rbegin() and rend() iterators: string name { \"Lorem ipsum...\" }; string reverse ( name . rbegin (), name . rend ());","title":"Standard iterators"},{"location":"Coding/C%2B%2B/#maps","text":"A syntactic sugar has been available since C++17: for ([ x , y ] : coords ) { std :: cout << x << y << endl ; } It is equivalent to: for ( std :: pair < int , int > el : coords ) { std :: cout << el . first << el . second << endl ; } The pair type has two public fields: first second ## Math #include <math.h> using namespace std ; int main () { int num { 2 }; cout << pow ( num , 2 ) << endl ; }","title":"Maps"},{"location":"Coding/C%2B%2B/#classes","text":"New data types in C++ are created as classes , which can be composites of member variables of other types and member functions , allowing complex and intuitive models to be created. The three primary principles of OOP are: Encapsulation : member variables and functions are packaged together Data hiding preserves the integrity of an object Inheritance allows one type to define another. Polymorphism (in C++ implemented by calling member functions using a pointer or reference) allow behavior of base classes to be exposed from objects of derived classes","title":"Classes"},{"location":"Coding/C%2B%2B/#member-variables","text":"Classes can contain member variables that are public or private (\" access specifiers \"), but it is best practice to make variables private while implementing accessor functions (getters and setters): - Hiding data preserves the integrity of objects - Loose coupling facilitates future change in codebase - Extra code can be injected for logging or data validation - Debuggers can set breakpoints on these getters and setters class Box { private : double length { 1 , 0 }; double width { 1 , 0 }; double height { 1 , 0 }; public : double volume () { return length * width * height ; } };","title":"Member variables"},{"location":"Coding/C%2B%2B/#initialization_1","text":"A member initializer list can be used to initialize fields more efficiently than explicit assignment. Box :: Box ( double lv , double wv , double hv ) : length { lv }, width { wv }, height { hv } {} This technique must have an expression in braces, even if they are empty. // Compiler error Box :: Box ( double lv , double wv , double hv ) : length { lv }, width { wv }, height { hv } Notably, this technique doesn't appear to work in derived class constructors. class Animal { public : std :: string _name {}; Animal ( std :: string n ) : _name { n } {} } class Dog : public Animal { public : Dog ( std :: string n , std :: string b ) : Animal ( n ) { _breed = b ;}; } A class constructor is called whenever a new instance of the class is defined. It always has the same name as the class itself and has no return data type because it returns no data. class Box { private : // ... public : Box ( double l , double w , double h ) { length = l ; width = w ; height = h ; } }; If a constructor isn't defined, the compiler will supply a default default constructor when an object is instantiated without initial values. To define a default constructor: Box () = default ; A destructor is a special member of a class executed to deal with cleanup upon use of the delete operator. A class can have only one destructor, and if one isn't defined then the compiler provides a default destructor that does nothing. The name of the destructor for a class is always the class name prefixed with a tilde, and similar to a constructor it cannot have a return type or parameters. ~ Box () = default ; Box ::~ Box () = default ; // when defined outside the class Base class destructors should always be declared as virtual .","title":"Initialization"},{"location":"Coding/C%2B%2B/#access","text":"When variables of class types are instantiated with the const keyword, they are called const objects , and none of their member variables can be altered (member variables of const objects become immutable). The compiler will throw an error when attempting to invoke methods of const objects unless they are identified as const member functions by using the const keyword in the signature after the identifier (\"attribute\"?): class Box { double volume () const { /* ... */ } double getLength () const { return length ; } double getWidth () const { return width ; } double getHeight () const { return height ; } } public , private , and protected are access specifiers that determine how a member variables and functions can be accessed from the outside. When inheriting from a base class, an access specifier can also be used to determine how accessible that base class's members are within the derived class. class Dog : public Animal { // ... } - When the base class specifier is public , inherited members are unchanged - When the base class specifier is protected , inherited public and protected members become protected - When the base class specifier is private, all inherited members become private. A friend is a function to which a class grants access to its private internals. They may be useful in rare situations where a single function needs access to the internals of different kinds of objects.","title":"Access"},{"location":"Coding/C%2B%2B/#inheritance","text":"When creating subclasses, you must remember: - Private variables must be placed in the protected access specifier so that they are accessible to child classes. - The base class access specifier must allow access to the base class's private variables ( public or protected ) - Parent class must have a default constructor class Animal { protected : // not `private:`! std :: string _name ; public : Animal () = default ; } class Dog : public Animal { /* ... */ } //","title":"Inheritance"},{"location":"Coding/C%2B%2B/#polymorphism","text":"Polymorphism in C++ refers to the practice of invoking a base class's member function rather than the derived class. Because the compiler performs early binding by default, a pointer typed to a base class but initialized to a derived class will invoke the base class's member function. Late binding can be used to force the pointer to use the derived class's member function even when the type of the pointer is the base class. This is done by using the virtual keyword on the base class's member function. Classes with virtual functions are called abstract classes and may not be instantiated. Abstract classes that are made of only virtual functions are called interfaces . class Base { public : virtual void doStuff () { /* ... */ } } class Derived : public Base { public : void doStuff () { /* ... */ } } int main () { Derived derived {}; Base * pointer = & derived ; // a pointer to an abstract class **may** be used pointer -> doStuff (); } Derived classes must then override this virtual function with the override keyword. class Derived : public Base { override doStuff () // ... }","title":"Polymorphism"},{"location":"Coding/C%2B%2B/#enumerations","text":"Enumerations can be specified with enum . Without specifying a value, each element of the enum is given a successively greater integer value starting with 0, like the indexes of an array (an ordinal value ). enum Choices { A , B , C , D } These elements can be specified with or without the scope resolution operator :: cout << A ; // 0 cout << Choices :: A ; // 0","title":"Enumerations"},{"location":"Coding/C%2B%2B/#templates","text":"Templates are used to have the compiler generate code automatically for a given data type. This is to avoid highly repetitive overloaded function definitions which only differ in parameter lists. The template and typename keywords define a template. The placeholder \"T\" represents the data type that will be replaced by a specific type by the compiler. template < typename T > T larger ( T a , T b ) { return a > b ? a : b ; } More than one data type can be used for the parameters, but in that case the return type must be explicitly specified: template < typename T1 , typename T2 > bool larger ( T1 a , T2 b ) { return a > b ; }","title":"Templates"},{"location":"Coding/C%2B%2B/#control-flow","text":"The choices in a switch statement are called cases . You can only switch on constant expressions that can be evaluated at compile-time, typically literals but excluding strings. Each case must be followed by a break statement to prevent fallthrough , except for the default case . switch ( choice ) { case 1 : // ... break ; case 2 : // ... break ; default : // ... } Since C++11, the range-based for-loop is available, which works very similar to a Python for-in loop: for ( string num : nums ) { cout << num << endl ; }","title":"Control flow"},{"location":"Coding/C%2B%2B/#functions","text":"Function prototypes , defining the function header (return data type, function name, and parameter list), describe a function sufficiently for the compiler to be able to compile calls to it and are required before using a function if the function declaration doesn't precede all the locations where it's called. #include <iostream> using namespace std ; // Without this prototype, there is a compile-time error. void printSomething (); int main () { printSomething (); return 0 ; } void printSomething () { cout << \"something...\" << endl ; } Passing by reference allows variables to be changed in-place and works by using the reference to the argument. Passing by value is the default parameter passing scheme , which works by actually copying the argument. int func ( int a ) { // Pass by value } int func ( int & a ) { // Pass by reference }","title":"Functions"},{"location":"Coding/C%2B%2B/#recursion","text":"Recursion requires a base case and at least one recursive case . The call stack is a stack data structure that figures prominently in recursive computing.## Home Project Description JamoftheMonthProject.cpp CLI application that calculates how much the user owes based on selected subscription tier and units purchased TicTacToe.cpp RPGCharGen.cpp Multiple classes using inheritance, virtual member functions, enums Task Description Reverse a string ...## Iterators An iterator is a classical and widespread design pattern that allows a wide variety of container-like objects to be traversed by exposing a uniform interface. However, loops based on iterators should only be used if access to the iterator is needed for advanced processing in the loop body. A range-based for loop is the recommended way to iterate over all elements of a container. Standard iterators begin() end() rbegin()","title":"Recursion"},{"location":"Coding/C%2B%2B/#memory","text":"Memory leaks can be detected using valgrind .","title":"Memory"},{"location":"Coding/C%2B%2B/#namespaces","text":"A namespace is a block that attaches an extra name to every entity name that is declared or defined within it. The qualified name of each entity is the namespace name followed by the scope resolution operator :: followed by the basic entity name. Namespaces can be used to partition large codebases into logical groupings to avoid name clashes. If a namespace isn't defined, the global namespace , where entities have no namespace name attached, applies by default. You can define a namespace using the namespace keyword. namespace foo { // ... } Namespaces can be nested.. namespace outer { namespace inner { void foo () { // ... } } } outer :: inner :: foo () Namespace aliases can be formed: namespace outin = outer :: inner ; outin :: foo () The using keyword allows you to reference any name from a namespace without qualifying it. using namespace std ; It can also be used specify a type alias , where an alternative name is used to refer to an existing data type. using BigOnes = unsigned long long ; typedef unsigned long long BigOnes ; // Older, less intuitive syntax","title":"Namespaces"},{"location":"Coding/C%2B%2B/#operators","text":"Each operator is associated with a particular named function. Operators can be overloaded by implementing that function. bool Rectangle :: operator == ( const Rectangle & other ) const { return _length == other . _length && _width == other . _width ; }","title":"Operators"},{"location":"Coding/C%2B%2B/#header-files","text":"Topic Header file array <array> deque <deque> exception <exception> map <map> Mathematical functions <math.h> queue <queue> stack <stack> vector <vector> Smart pointers <memory>","title":"Header files"},{"location":"Coding/C%2B%2B/#applications","text":"","title":"Applications"},{"location":"Coding/C%2B%2B/#gtkmm","text":"gtkmm (historically \"GTK--\") is a C++ wrapper for an underlying GTK code base written in C. Compared to Qt , another GUI library, gtkmm uses more modern and native C++ features. In Ubuntu , installing the development environment is done with the gnome-devel metapackage: sudo apt install gnome-devel","title":"gtkmm"},{"location":"Coding/C%2B%2B/#nes-emulator","text":"","title":"NES emulator"},{"location":"Coding/C%2B%2B/#courses","text":"","title":"Courses"},{"location":"Coding/C%2B%2B/#c-standard-template-library-in-practice","text":"/# Topic Video Projects 01.01 The Course Overview 01.02 Templates Introduction to the STL TemplateSTL.cpp 01.03 General Concepts ExceptionSTL.cpp 01.04 Utilities - Common Utilities StringSTL.cpp 01.05 Utilities - Regex RegexSTL.cpp 01.06 Project - Bitcoin Exchange Program BTCX.cpp 01.07 Project - Coding 01.08 Project - Custom Writer Function 01.09 Review 02.01 Understanding Containers 02.02 Vectors 02.03 Standard Array 02.04 Lists 02.05 Stacks and Queues 02.06 Maps and Multimaps - Overview 02.07 Maps - Coding 02.08 Multimaps - Coding 02.09 Sets and Multisets 02.10 Project 02.11 Review 03.01 Iterators 03.02 Input Iterators 03.03 Output Iterators 03.04 Forward Iterators 03.05 Bidirectional Iterators 03.06 Random Access Iterators 03.07 Auxiliary Iterator Functions 03.08 Iterator Adaptors 03.09 Writing Generic Functions for Iterators 03.10 User - Defined Iterators 03.11 Project 03.12 Review 04.01 Introduction to Algorithms 04.02 Sequence Algorithms - for_each 04.03 Sequence Algorithms - equals 04.04 Copying 04.05 Moving 04.06 Removing 04.07 Sorting and Gathering - std::sort 04.08 Sorting and Gathering - std::partial_sort algorithm 04.09 Sorting and Gathering - std::partition 04.10 Sorting and Gathering - std::partition_copy 04.11 Searching and Finding - std::find 04.12 Sorting and Gathering - std::find_first_of, std::adjacent_find 04.13 Sorting and Gathering - std::search 04.14 Sorting and Gathering - std::binary_search 04.15 Counting 05.01 Replacing and Transforming - std::replace 05.02 Replacing and Transforming - std::replace_copy 05.03 Replacing and Transforming - equals 05.04 Swapping 05.05 Rotating 05.06 Randomizing 05.07 Permutations 05.08 Sampling 05.09 Min 05.10 Max 05.11 Clamp 05.12 Fill and Generate 05.13 Numeric Algorithms - std::accumulate 05.14 Numeric Algorithms - std::partial_sum and std::adjacent_difference 05.15 Numeric Algorithms - std::gcd, and std::lcm 05.16 Numeric Algorithms - std::inner_product and std::iota 05.17 Review 06.01 Basic Architecture of the I/O Stream Library 06.02 Console I/O - Interact with a User 06.03 Console I/O - Read Input 06.04 File I/O 06.05 String Streams 06.06 Manipulators and Formatters 06.07 Stream States 06.08 Low Level I/O 06.09 Overloading Stream Operators 06.10 Project - Overview 06.11 Project - Classes and structures 06.12 Project - Implementation 06.13 Review 07.01 Unique Pointers 07.02 Shared Pointers 07.03 Allocators 07.04 Defining an Allocator 07.05 Uninitialized Memory 07.06 Review 08.01 Introduction to Threading 08.02 Creating Threads 08.03 Locks 08.04 Shared Locks 08.05 Atomic Values 08.06 Async 08.07 Condition Variables 08.08 Project 08.09 Review 09.01 Concepts 09.02 Modules 09.03 Coroutines 09.04 Course Review","title":"C++ Standard Template Library in Practice"},{"location":"Coding/C%2B%2B/#complete-c-developer-course","text":"/# Topic Video Projects 1.1 Section Overview 1.2 Getting Started on Windows with Visual Studio Integrated Development Environment (IDE) 1.3 Getting Started on macOS or Linux with CodeBlocks IDE 1.4 Getting Started with macOS Catalina or Higher with Visual Studio Code 1.5 Finding Answers to Your Questions 2.1 Section Overview 2.2 Saying \"Hello\" to C++ 2.3 Variables and Data Types - Part 1 2.4 Variables and Data Types - Part Two 2.5 Variables and Data Types - Part Three 2.6 Comments 2.7 Arithmetic Operators 2.8 Relational Operators 2.9 Logical Operators 2.10 Symbolic Constants and Naming Conventions 2.11 User Input 2.12 Project - Average of Three 2.13 Project - MadLibs Clone 2.14 Section Wrap-Up 3.1 Section Overview 3.2 Introduction to Control Statements 3.3 Selection Control Statements 3.4 Repetition Control Statements 3.5 The Break and Continue Statements 3.6 Random Numbers 3.7 Project - Jam of the Month Club 3.8 Project - Odds and Evens 3.9 Project - Guess the Number 3.10 Section Wrap-Up 4.1 Section Overview 4.2 Built-in Arrays 4.3 The Array Class 4.4 The Vector Class 4.5 Multi-Dimensional Arrays 4.6 Project - Array Data 4.7 Project - Vector Data 4.8 Project - Parallel Arrays/Vectors 4.9 Section Wrap-Up 5.1 Section Overview 5.2 Function Prototypes and Definitions FunctionFun.cpp 5.3 Function Return Types and Parameters 5.4 Parameter Passing: Pass-by-Value and Pass-by-Reference PassingSchemes.cpp 5.5 Variable Scope and Lifetime ScopeFun 5.6 Function Overloading 5.7 The <cmath> Library 5.8 Recursion 5.9 Project - Return the Product of Three Parameters 5.10 Project - Return the Sum of Built-in Array Elements 5.11 Project - Return the Sum of Array Object Elements 5.12 Project - Retrieve the Sum of Array Object Elements by Reference 5.13 Project - Tic-Tac-Toe (ADVANCED) 5.14 Section Wrap-Up 6.1 Section Overview 6.2 Basics of Object Oriented Programming (OOP) 6.3 Encapsulation: Data Members and Member Functions 6.4 Separate Compilation 6.5 Constructors and Destructors 6.6 A Rectangle Class Rectangle.cpp 6.7 A Book Class 6.8 Project - A Bank Account Class 6.9 Project - A Pizza Class 6.10 Project - A Circle Class 6.11 Section Wrap-Up 7.1 Section Overview 7.2 Exceptions and the Exception Hierarchy 7.3 Logic Errors 7.4 Runtime Errors and Throwing Exceptions 7.5 Rethrowing Exceptions 7.6 Custom Exceptions 7.7 Basic Testing and Debugging 7.8 Project - Throwing and Handling an Out_of_Range Exception 7.9 Project - Creating and Using Your Own Exception 7.10 Section Wrap-Up 8.1 Section Overview 8.2 Introduction to Pointers 8.3 Dynamic Memory - Part 1 8.4 Dynamic Memory (- Part 2 8.5 Const Correctness 8.6 Project - Dynamically Creating Rectangles 8.7 Project - Dynamically Creating Circles 8.8 Section Wrap-Up 9.1 Section Overview 9.2 Sequential File Output 9.3 Sequential File Input 9.4 More File Input/ Output (I/O) 9.5 Project - Reading Data from File and Printing Statistics 9.6 Project - Dynamically Creating Rectangles from File 9.7 Project - Shopping Item File 9.8 Section Wrap-Up 10.1 Section Overview 10.2 Inheritance - Part 1 10.3 Inheritance - Part 2) 10.4 Polymorphism and Late Binding 10.5 Enumerated Types This video explains enumerated types. EnumFun.cpp 10.6 Project - Derived Cat Class Cat.cpp 10.7 Project \u2013 Role Playing Game (RPG) Player Character Creation RPGCharGen 10.8 Section Wrap-Up 11.1 Section Overview 11.2 Templates - Standard Template Library (STL) TemplateFun.cpp 11.3 queue<T> stack<T> deque<T> Standard Template Library (STL) - Part 1) DequeFun.cpp, StackFun.cpp, QueueProject.cpp 11.4 map<T> Standard Template Library (STL) - Part 2 ContactsFun.cpp, AlgorithmFun.cpp 11.5 unique_ptr<T> Smart Pointers SmartPointerFun.cpp, Car.cpp 11.6 Friend functions Friend Functions and Friend Classes FriendFunctions.cpp 11.7 Operator Overloading OverloadingFun.cpp, Rectangle.h 11.8 map<T> Project - Dictionary of Terms DictionaryProject.cpp 11.9 Project - Aliens Aliens.cpp 11.10 Section Wrap-Up","title":"Complete C++ Developer Course"},{"location":"Coding/C%2B%2B/#rpgchargen","text":"cpp #include <iostream> #include \"RPGCharGen.h\" using namespace std ; int main () { Warrior w { \"Doofus McGroober\" , Race :: HUMAN }; cout << \"Player name: \" << w . getName () << endl ; cout << \"Player HP: \" << w . getHp () << endl ; cout << \"Player MP: \" << w . getMp () << endl ; cout << \"Player race: \" << w . getRace () << endl ; w . attack (); Priest m { \"Brother Tolkien\" , Race :: ELF }; cout << \"Player name: \" << m . getName () << endl ; cout << \"Player HP: \" << m . getHp () << endl ; cout << \"Player MP: \" << m . getMp () << endl ; cout << \"Player race: \" << m . getRace () << endl ; m . attack (); Mage n { \"Smart Frodo\" , Race :: DWARF }; cout << \"Player name: \" << n . getName () << endl ; cout << \"Player HP: \" << n . getHp () << endl ; cout << \"Player MP: \" << n . getMp () << endl ; cout << \"Player race: \" << n . getRace () << endl ; n . attack (); return 0 ; } h #if !defined(RPGCHARGEN_H) #define RPGCHARGEN_H #include <string> enum Race { HUMAN , ELF , DWARF }; class Player { protected : std :: string _name { \"Johnny Bravo\" }; Race _race { Race :: HUMAN }; int _hp { 100 }; int _mp { 100 }; public : Player ( std :: string n , Race r , int hp , int mp ) : _name { n }, _race { r }, _hp ( hp ), _mp ( mp ) {} virtual std :: string attack () = 0 ; int getHp () { return _hp ; } int getMp () { return _mp ; } std :: string getRace () { switch ( _race ) { case 0 : return \"human\" ; break ; case 1 : return \"elf\" ; break ; case 2 : return \"dwarf\" ; break ; default : return \"none\" ; break ; } } std :: string getName () { return _name ; } void setHp ( int n ) { _hp = n ; } void setMp ( int n ) { _mp = n ; } void setName ( std :: string s ) { _name = s ; } void setRace ( Race r ) { _race = r ;} }; class Warrior : public Player { public : Warrior ( std :: string n , Race r ) : Player ( n , r , 200 , 0 ) {} std :: string attack () { return \"I will destroy you with my sword, foul demon!\" ;} }; class Priest : public Player { public : Priest ( std :: string n , Race r ) : Player ( n , r , 100 , 200 ) {} std :: string attack () { return \"Taste the wrath of the Two True Gods!\" ;} }; class Mage : public Player { public : Mage ( std :: string n , Race r ) : Player ( n , r , 150 , 150 ) {} std :: string attack () { return \"You are overmatched by my esoteric artifices!\" ;} }; #endif // RPGCHARGEN_H","title":"RPGCharGen"},{"location":"Coding/Courses/","text":"Courses The Complete C# Masterclass /# Video Topic Projects 01.01 Welcome and a brief introduction to the Course 01.02 Guide Lecture - How to install Visual Studio 01.03 Guide lecture - Creating a project in Visual Studio 01.04 Your first C# program 02.01 What is a variable and what is its relationship with the data types IntegerDataTypes 02.02 The \"numbers\" data type - Integers 02.03 The \"numbers with a decimal point data types - float, double, decimal FloatingPointDataTypes 02.04 The \"Yes or No\" data types - booleans Boolean 02.05 The \"single symbol\" dat atypes - characters Characters 02.06 The \"information as text\" data types - strings Strings 02.07 Collections of information from a specific data type - arrays Arrays 02.08 Some cool, useful tricks with strings StringTricks 02.09 Transforming any data type into a string - allows you to use string methods 02.10 The 3 different ways to build strings 02.11 The 3 different ways to convert one data type to another 03.01 Write vs WriteLine, when to use which? WriteandWriteLine 03.04 Accepting single character inputs from the Console - Read method ReadingCharacter 03.05 Accepting string inputs from the Console - ReadLine method ReadLine 03.06 Accepting inputs as keys from the Console - ReadKey ReadKey 03.07 Changing the color of the text and the background of the text in the Console ConsoleColors 03.08 Changing cursor settings in the Console - Size, Visibility, Position CursorSettings 03.09 Controlling the size of the Console window - WindowSize, BufferSize and more ConsoleSize 04.01 Arithmetic Operators 04.02 Assignment Operators 04.03 Comparison Operators 04.04 Logical Operators && , || 04.05 Ternary Operator 06.02 Practicing while loops MiniWhileGame 06.04 The for loops and their common uses ForLoops 06.05 Practicing for loops MenuWhile 06.06 foreach loop ForeachLoops 07.05 Methods with variable number of arguments params ListExamples 07.08 Methods with ref and out arguments ref , out RefAndOut 08.01 Introduction to one-dimensional arrays IntroductionToArrays 08.02 Outputting arrays string.Join() OutputtingArrays 08.03 Correctly cloning arrays Array.Clone() ArrayCloning 08.04 Reversing arrays Array.Reverse() 08.05 Bubble sorting algorithm BubbleSort 08.09 Lists List<T> 08.10 Practice working with List<T> s List<T> ListExamples 09.01 Introduction to multidimensional arrays 11.01 Introduction to exception handling try catch 11.02 Catching multiple exceptions 11.03 Inspecting caught exception string.Substring() MultipleExceptions 11.04 finally block finally TryCatchFinally 12.01 Introduction to object-oriented programming 12.02 Creating a basic class RPGCharGen 12.03 Fields and properties - the variables of a class RPGCharGen 12.04 Methods - the actions of a class RPGCharGen 12.05 Constructors - the builders of a class RPGCharGen 12.06 Namespaces and files - structuring your project RPGCharGen 13.02 Controlling the accessors of a property - read, write, and read-write properties RPGCharGen 13.03 Implementing validation in properties RPGCharGen 13.04 Validation and exceptions throw RPGCharGen 13.05 Properties and fields - when to use which RPGCharGen 14.01 this this RPGCharGen 14.03 Overloading constructors RPGCharGen 14.04 Chaining constructors RPGCharGen 15.01 public and private access modifiers public , private RPGCharGen 15.02 internal and protected access modifiers internal , protected RPGCharGen 16.01 Static fields and properties RPGCharGen 16.02 const and readonly 16.03 Static methods 16.04 Static classes 16.05 enum 17.01 Introduction to Inheritance Learn C# By Building Applications /# Video Topic Projects 02.12 Static vs. non-static ArrayCloning int [] primes = { 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 , 29 , 31 , 37 , 41 , 43 , }; //int[] primesClone = (int[])primes.Clone(); int [] primesClone = new int [ primes . Length ]; Array . Copy ( primes , primesClone , primes . Length ); primesClone [ primesClone . Length - 1 ] = 47 ; foreach ( var i in primesClone ) { Console . WriteLine ( i ); } Arrays int [] numbers = new int [ 10 ]; for ( int i = 0 ; i < numbers . Length ; i ++) { System . Console . WriteLine ( numbers [ i ]); } string [] fruits = { \"apple\" , \"banana\" , \"jackfruit\" , \"kiwi\" , \"mango\" }; for ( int i = 0 ; i < fruits . Length ; i ++) { System . Console . WriteLine ( fruits [ i ]); } Boolean int firstNumber = 4 ; int secondNumber = 6 ; bool isSmaller = firstNumber < secondNumber ; bool isTheCookieJarFull = false ; bool isTheCookieJarEmpty = ! isTheCookieJarFull ; Characters System . Console . InputEncoding = System . Text . Encoding . UTF8 ; System . Console . OutputEncoding = System . Text . Encoding . UTF8 ; char x = 'x' ; System . Console . WriteLine ( x ); char plus = '\\ u002b ' ; System . Console . WriteLine ( plus ); char umlaut = '\\ u00F6 ' ; System . Console . WriteLine ( umlaut ); ConsoleColors Console . ForegroundColor = ConsoleColor . Green ; Console . WriteLine ( \"Once upon a midnight dreary\" ); Console . ForegroundColor = ConsoleColor . Cyan ; Console . WriteLine ( \"While I pondered weak and weary\" ); Console . ResetColor (); Console . WriteLine ( \"Over many a quaint and curious volume of forgotten lore,\" ); ConsoleSize Console . WindowHeight = 20 ; Console . WindowWidth = 20 ; Console . SetWindowSize ( 30 , 30 ); Console . BufferHeight = 40 ; Console . BufferWidth = 40 ; Console . WindowLeft = 10 ; Console . WindowTop = 10 ; Console . SetWindowPosition ( 10 , 10 ); CursorSettings Console . Title = \"Terminal\" ; Console . CursorVisible = true ; Console . CursorSize = 50 ; Console . SetCursorPosition ( 20 , 10 ); ExceptionHandling Console . WriteLine ( \"'q' to quit\" ); List < int > primes = new List < int >(); while ( true ) { string input = Console . ReadLine (); try { primes . Add ( Int32 . Parse ( input )); } catch ( FormatException ex ) { if ( input . ToLower () == \"q\" ) { break ; } else { Console . WriteLine ( ex . Message ); //string stacktrace = ex.StackTrace; //string filename = stacktrace.Substring(stacktrace.IndexOf(':') - 1); //Console.WriteLine(filename); } } } Console . WriteLine ( String . Join ( \" \" , primes )); FloatingPointDataTypes float floatNum = 13.14321365431f ; string output = $ \"Floating point numbers have a maximum precision of 8 digits: {floatNum}\" ; System . Console . WriteLine ( output ); float radius = 3.5f ; double area = System . Math . PI * System . Math . Pow ( radius , 2d ); System . Console . WriteLine ( $ \"A circle with a radius of {radius} has an are of {area}.\" ); float floatMax = float . MaxValue ; float floatMin = float . MinValue ; System . Console . WriteLine ( $ \"float ranges from {floatMin} to {floatMax}\" ); double doubleMax = double . MaxValue ; double doubleMin = double . MinValue ; System . Console . WriteLine ( $ \"double ranges from {doubleMin} to {doubleMax}\" ); decimal decimalMax = decimal . MaxValue ; decimal decimalMin = decimal . MinValue ; System . Console . WriteLine ( $ \"decimal ranges from {decimalMin} to {decimalMax}\" ); ForeachLoops int [] primes = { 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 }; foreach ( var item in primes ) { Console . WriteLine ( item ); } ForLoops int [] primes = { 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 }; for ( int i = 0 ; i < primes . Length ; i ++) { primes [ i ] = 2 * primes [ i ]; Console . WriteLine ( primes [ i ]); } IntegerDataTypes int intMax = int . MaxValue ; int intMin = int . MinValue ; string output = $ \"int ranges from {intMin} to {intMax}\" ; Console . WriteLine ( output ); uint uintMin = uint . MinValue ; uint uintMax = uint . MaxValue ; output = $ \"uint ranges from {uintMin} to {uintMax}\" ; Console . WriteLine ( output ); byte byteMin = byte . MinValue ; byte byteMax = byte . MaxValue ; output = $ \"byte ranges from {byteMin} to {byteMax}\" ; Console . WriteLine ( output ); long longMin = long . MinValue ; long longMax = long . MaxValue ; output = $ \"long ranges from {longMin} to {longMax}\" ; Console . WriteLine ( output ); ulong ulongMin = ulong . MinValue ; ulong ulongMax = ulong . MaxValue ; output = $ \"ulong ranges from {ulongMin} to {ulongMax}\" ; Console . WriteLine ( output ); ListExamples static void Main ( string [] args ) { int [] arr1 = { 0 , 1 , 2 , 3 , 4 }; int [] arr2 = { 5 , 6 , 7 , 8 , 9 }; List < int > list = new List < int >(); list = splat ( arr1 , arr2 ); foreach ( var el in list ) { Console . WriteLine ( el ); } } static List < int > splat ( params int [][] arg ) { List < int > output = new List < int >(); foreach ( int [] arr in arg ) { output . AddRange ( arr ); } return output ; } MiniWhileGame static void Main ( string [] args ) { int mage = 30 ; int warrior = 40 ; while ( mage > 0 && warrior > 0 ) { warrior -= mage_attack (); mage -= warrior_attack (); } Console . WriteLine ( $ \"Conflict ends with Mage having {mage} points and Warrior having {warrior}!\" ); } private static int warrior_attack () { var r = new System . Random (); int result = r . Next ( 1 , 3 ); Console . WriteLine ( $ \"Warrior attacks Mage, dealing {result} damage!\" ); return result ; } private static int mage_attack () { var r = new System . Random (); int result = r . Next ( 7 , 9 ); Console . WriteLine ( $ \"Mage attacks Warrior, dealing {result} damage!\" ); return result ; } MenuWhile static void Main ( string [] args ) { string [] fruits = { \"Apple\" , \"Banana\" , \"Date\" , \"Grape\" , \"Jackfruit\" , \"Kiwi\" , \"Lime\" , \"Orange\" , \"Peach\" , \"Strawberry\" }; string [] menu = { \"Add New Item\" , \"Edit Item\" , \"Remove Item\" , \"View All Items\" , \"Exit\" }; int choice ; do { choice = Menu ( menu ); switch ( choice ) { case 1 : for ( int i = 0 ; i < fruits . Length ; i ++) { if ( fruits [ i ] == null ) { Console . Write ( \"Please add a new fruit: \" ); fruits [ i ] = Console . ReadLine (); break ; } } break ; case 2 : int chosenFruit = 0 ; do { Console . Write ( $ \"Edit fruit number (1 to {fruits.Length}): \" ); try { chosenFruit = Convert . ToInt32 ( Console . ReadLine ()); } catch { InvalidInput (); } } while ( chosenFruit == 0 ); Console . Write ( $ \"Enter new value for the {fruits[chosenFruit - 1]}: \" ); fruits [ chosenFruit - 1 ] = Console . ReadLine (); break ; case 3 : break ; case 4 : Console . WriteLine ( \"Current fruits: \" ); for ( int i = 0 ; i < fruits . Length ; i ++) { if ( fruits [ i ] != null ) { Console . WriteLine ( fruits [ i ]); } } break ; case 5 : break ; default : InvalidInput (); break ; } } while ( true ); } private static int Menu ( string [] menu ) { Console . WriteLine ( '\\n' ); for ( var i = 0 ; i < menu . Length ; i ++) { Console . WriteLine ( \"{0}. {1}\" , i + 1 , menu [ i ]); } Console . Write ( \"Your choice: \" ); string input = Console . ReadLine (); try { int output = Int32 . Parse ( input ); return output ; } catch ( FormatException ) { InvalidInput (); return - 1 ; } catch ( OverflowException ) { InvalidInput (); return - 1 ; } } private static void InvalidInput () { Console . ForegroundColor = ConsoleColor . Red ; Console . WriteLine ( \"Invalid input!\" ); Console . ResetColor (); } OutputtingArrays int [] primes = { 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 , 29 , 31 , 37 , 41 , 43 , 47 }; Console . WriteLine ( $ \"{primes.Length} total primes\" ); List < int > lessThanTwenty = new List < int >(); List < int > moreThanTwenty = new List < int >(); foreach ( int i in primes ) { if ( i < 20 ) { lessThanTwenty . Add ( i ); } else { moreThanTwenty . Add ( i ); } } Console . WriteLine ( $ \"Primes < 20:\\n {string.Join(\" , \", lessThanTwenty)}\" ); Console . WriteLine ( $ \"Primes >= 20:\\n {string.Join(\" , \", moreThanTwenty)}\" ); ReadingCharacter Console . Write ( \"How old are you? \" ); int age ; int . TryParse ( Console . ReadLine (), out age ) ; Console . WriteLine ( $ \"You are {age} years old (allegedly).\" ); ReadKey Console . WriteLine ( $ \"\\n\\nKey pressed: {key.Key}\" ); Console . WriteLine ( $ \"Key as char: {key.KeyChar}\" ); Console . WriteLine ( $ \"Modifiers: {key.Modifiers}\" ); ReadLine Console . Write ( \"Input the drive letter: \" ); string driveLetter = Console . ReadLine (); Console . Write ( \"Input the folder path: \" ); string folderPath = Console . ReadLine (); Console . Write ( \"Input the file name: \" ); string fileName = Console . ReadLine (); Console . WriteLine ( $ \"{driveLetter}:\\\\{folderPath}\\\\{fileName}.exe\" ); RefAndOut static void Main ( string [] args ) { int number = 0 ; Console . WriteLine ( number ); IncreaseByOne ( ref number ); Console . WriteLine ( number ); } static void IncreaseByOne ( ref int n ) { n ++; } static void Main () { double n = 5 ; double nSquared ; square ( n , out nSquared ); Console . WriteLine ( $ \"{n} ^ 2 = {nSquared}\" ); } static void square ( double x , out double y ) { y = System . Math . Pow ( x , 2 ); } RPGCharGen Strings string username = \"admin\" ; System . Console . WriteLine ( username [ 0 ]); // impossible, strings are immutable username [ 0 ] = 'A' ; StringTricks string fruitJuice = \"Strawberry juice\" ; string separator = new string ( '-' , fruitJuice . Length ); System . Console . WriteLine ( fruitJuice ); System . Console . WriteLine ( separator ); System . Console . WriteLine ( fruitJuice . Contains ( \"j\" )); System . Console . WriteLine ( fruitJuice . IndexOf ( \"r\" )); System . Console . WriteLine ( fruitJuice . LastIndexOf ( \"r\" )); System . Console . WriteLine ( fruitJuice . ToUpper (). Contains ( \"J\" )); System . Console . WriteLine ( fruitJuice . ToUpper (). IndexOf ( \"RR\" )); System . Console . WriteLine ( fruitJuice . ToUpper (). LastIndexOf ( \"RR\" )); TryCatchFinally StreamWriter sw = null ; try { sw = File . CreateText ( Directory . GetCurrentDirectory () + @\"/test.txt\" ); int number = int . Parse ( Console . ReadLine ()); int dividend = 5 / number ; sw . Write ( number ); } catch ( FormatException ex ) { Console . WriteLine ( ex . Message ); } catch ( DivideByZeroException ex ) { Console . WriteLine ( ex . Message ); } finally { sw . Close (); } WriteAndWriteLine string heading = \"Protein Intake Week: 1\" ; string underline = new string ( '=' , heading . Length ); double num1 = 80.885570 ; double num2 = 94.564645 ; double num3 = 78.678931 ; double num4 = 88.66654 ; double num5 = 88.6466 ; double num6 = 76.777 ; double num7 = 91.85759 ; double sum = num1 + num2 + num3 + num4 + num5 + num6 + num7 ; double [] array = { num1 , num2 , num3 , num4 , num5 , num6 , num7 }; System . Console . WriteLine ( \"|{0}|\" , heading ); System . Console . WriteLine ( \"|{0}|\" , underline ); foreach ( double i in array ) { System . Console . WriteLine ( $ \"|{i, 22:N2}|\" ); } System . Console . WriteLine ( \"|{0}|\" , underline ); System . Console . WriteLine ( \"|Total: {0, 15:N2}|\" , sum ); Test-Driven Development in C# Create a Red Unit Test Mocking with Moq and xUnit","title":"Courses"},{"location":"Coding/Courses/#courses","text":"","title":"Courses"},{"location":"Coding/Courses/#the-complete-c-masterclass","text":"/# Video Topic Projects 01.01 Welcome and a brief introduction to the Course 01.02 Guide Lecture - How to install Visual Studio 01.03 Guide lecture - Creating a project in Visual Studio 01.04 Your first C# program 02.01 What is a variable and what is its relationship with the data types IntegerDataTypes 02.02 The \"numbers\" data type - Integers 02.03 The \"numbers with a decimal point data types - float, double, decimal FloatingPointDataTypes 02.04 The \"Yes or No\" data types - booleans Boolean 02.05 The \"single symbol\" dat atypes - characters Characters 02.06 The \"information as text\" data types - strings Strings 02.07 Collections of information from a specific data type - arrays Arrays 02.08 Some cool, useful tricks with strings StringTricks 02.09 Transforming any data type into a string - allows you to use string methods 02.10 The 3 different ways to build strings 02.11 The 3 different ways to convert one data type to another 03.01 Write vs WriteLine, when to use which? WriteandWriteLine 03.04 Accepting single character inputs from the Console - Read method ReadingCharacter 03.05 Accepting string inputs from the Console - ReadLine method ReadLine 03.06 Accepting inputs as keys from the Console - ReadKey ReadKey 03.07 Changing the color of the text and the background of the text in the Console ConsoleColors 03.08 Changing cursor settings in the Console - Size, Visibility, Position CursorSettings 03.09 Controlling the size of the Console window - WindowSize, BufferSize and more ConsoleSize 04.01 Arithmetic Operators 04.02 Assignment Operators 04.03 Comparison Operators 04.04 Logical Operators && , || 04.05 Ternary Operator 06.02 Practicing while loops MiniWhileGame 06.04 The for loops and their common uses ForLoops 06.05 Practicing for loops MenuWhile 06.06 foreach loop ForeachLoops 07.05 Methods with variable number of arguments params ListExamples 07.08 Methods with ref and out arguments ref , out RefAndOut 08.01 Introduction to one-dimensional arrays IntroductionToArrays 08.02 Outputting arrays string.Join() OutputtingArrays 08.03 Correctly cloning arrays Array.Clone() ArrayCloning 08.04 Reversing arrays Array.Reverse() 08.05 Bubble sorting algorithm BubbleSort 08.09 Lists List<T> 08.10 Practice working with List<T> s List<T> ListExamples 09.01 Introduction to multidimensional arrays 11.01 Introduction to exception handling try catch 11.02 Catching multiple exceptions 11.03 Inspecting caught exception string.Substring() MultipleExceptions 11.04 finally block finally TryCatchFinally 12.01 Introduction to object-oriented programming 12.02 Creating a basic class RPGCharGen 12.03 Fields and properties - the variables of a class RPGCharGen 12.04 Methods - the actions of a class RPGCharGen 12.05 Constructors - the builders of a class RPGCharGen 12.06 Namespaces and files - structuring your project RPGCharGen 13.02 Controlling the accessors of a property - read, write, and read-write properties RPGCharGen 13.03 Implementing validation in properties RPGCharGen 13.04 Validation and exceptions throw RPGCharGen 13.05 Properties and fields - when to use which RPGCharGen 14.01 this this RPGCharGen 14.03 Overloading constructors RPGCharGen 14.04 Chaining constructors RPGCharGen 15.01 public and private access modifiers public , private RPGCharGen 15.02 internal and protected access modifiers internal , protected RPGCharGen 16.01 Static fields and properties RPGCharGen 16.02 const and readonly 16.03 Static methods 16.04 Static classes 16.05 enum 17.01 Introduction to Inheritance","title":"The Complete C# Masterclass"},{"location":"Coding/Courses/#learn-c-by-building-applications","text":"/# Video Topic Projects 02.12 Static vs. non-static","title":"Learn C# By Building Applications"},{"location":"Coding/Courses/#arraycloning","text":"int [] primes = { 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 , 29 , 31 , 37 , 41 , 43 , }; //int[] primesClone = (int[])primes.Clone(); int [] primesClone = new int [ primes . Length ]; Array . Copy ( primes , primesClone , primes . Length ); primesClone [ primesClone . Length - 1 ] = 47 ; foreach ( var i in primesClone ) { Console . WriteLine ( i ); }","title":"ArrayCloning"},{"location":"Coding/Courses/#arrays","text":"int [] numbers = new int [ 10 ]; for ( int i = 0 ; i < numbers . Length ; i ++) { System . Console . WriteLine ( numbers [ i ]); } string [] fruits = { \"apple\" , \"banana\" , \"jackfruit\" , \"kiwi\" , \"mango\" }; for ( int i = 0 ; i < fruits . Length ; i ++) { System . Console . WriteLine ( fruits [ i ]); }","title":"Arrays"},{"location":"Coding/Courses/#boolean","text":"int firstNumber = 4 ; int secondNumber = 6 ; bool isSmaller = firstNumber < secondNumber ; bool isTheCookieJarFull = false ; bool isTheCookieJarEmpty = ! isTheCookieJarFull ;","title":"Boolean"},{"location":"Coding/Courses/#characters","text":"System . Console . InputEncoding = System . Text . Encoding . UTF8 ; System . Console . OutputEncoding = System . Text . Encoding . UTF8 ; char x = 'x' ; System . Console . WriteLine ( x ); char plus = '\\ u002b ' ; System . Console . WriteLine ( plus ); char umlaut = '\\ u00F6 ' ; System . Console . WriteLine ( umlaut );","title":"Characters"},{"location":"Coding/Courses/#consolecolors","text":"Console . ForegroundColor = ConsoleColor . Green ; Console . WriteLine ( \"Once upon a midnight dreary\" ); Console . ForegroundColor = ConsoleColor . Cyan ; Console . WriteLine ( \"While I pondered weak and weary\" ); Console . ResetColor (); Console . WriteLine ( \"Over many a quaint and curious volume of forgotten lore,\" );","title":"ConsoleColors"},{"location":"Coding/Courses/#consolesize","text":"Console . WindowHeight = 20 ; Console . WindowWidth = 20 ; Console . SetWindowSize ( 30 , 30 ); Console . BufferHeight = 40 ; Console . BufferWidth = 40 ; Console . WindowLeft = 10 ; Console . WindowTop = 10 ; Console . SetWindowPosition ( 10 , 10 );","title":"ConsoleSize"},{"location":"Coding/Courses/#cursorsettings","text":"Console . Title = \"Terminal\" ; Console . CursorVisible = true ; Console . CursorSize = 50 ; Console . SetCursorPosition ( 20 , 10 );","title":"CursorSettings"},{"location":"Coding/Courses/#exceptionhandling","text":"Console . WriteLine ( \"'q' to quit\" ); List < int > primes = new List < int >(); while ( true ) { string input = Console . ReadLine (); try { primes . Add ( Int32 . Parse ( input )); } catch ( FormatException ex ) { if ( input . ToLower () == \"q\" ) { break ; } else { Console . WriteLine ( ex . Message ); //string stacktrace = ex.StackTrace; //string filename = stacktrace.Substring(stacktrace.IndexOf(':') - 1); //Console.WriteLine(filename); } } } Console . WriteLine ( String . Join ( \" \" , primes ));","title":"ExceptionHandling"},{"location":"Coding/Courses/#floatingpointdatatypes","text":"float floatNum = 13.14321365431f ; string output = $ \"Floating point numbers have a maximum precision of 8 digits: {floatNum}\" ; System . Console . WriteLine ( output ); float radius = 3.5f ; double area = System . Math . PI * System . Math . Pow ( radius , 2d ); System . Console . WriteLine ( $ \"A circle with a radius of {radius} has an are of {area}.\" ); float floatMax = float . MaxValue ; float floatMin = float . MinValue ; System . Console . WriteLine ( $ \"float ranges from {floatMin} to {floatMax}\" ); double doubleMax = double . MaxValue ; double doubleMin = double . MinValue ; System . Console . WriteLine ( $ \"double ranges from {doubleMin} to {doubleMax}\" ); decimal decimalMax = decimal . MaxValue ; decimal decimalMin = decimal . MinValue ; System . Console . WriteLine ( $ \"decimal ranges from {decimalMin} to {decimalMax}\" );","title":"FloatingPointDataTypes"},{"location":"Coding/Courses/#foreachloops","text":"int [] primes = { 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 }; foreach ( var item in primes ) { Console . WriteLine ( item ); }","title":"ForeachLoops"},{"location":"Coding/Courses/#forloops","text":"int [] primes = { 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 }; for ( int i = 0 ; i < primes . Length ; i ++) { primes [ i ] = 2 * primes [ i ]; Console . WriteLine ( primes [ i ]); }","title":"ForLoops"},{"location":"Coding/Courses/#integerdatatypes","text":"int intMax = int . MaxValue ; int intMin = int . MinValue ; string output = $ \"int ranges from {intMin} to {intMax}\" ; Console . WriteLine ( output ); uint uintMin = uint . MinValue ; uint uintMax = uint . MaxValue ; output = $ \"uint ranges from {uintMin} to {uintMax}\" ; Console . WriteLine ( output ); byte byteMin = byte . MinValue ; byte byteMax = byte . MaxValue ; output = $ \"byte ranges from {byteMin} to {byteMax}\" ; Console . WriteLine ( output ); long longMin = long . MinValue ; long longMax = long . MaxValue ; output = $ \"long ranges from {longMin} to {longMax}\" ; Console . WriteLine ( output ); ulong ulongMin = ulong . MinValue ; ulong ulongMax = ulong . MaxValue ; output = $ \"ulong ranges from {ulongMin} to {ulongMax}\" ; Console . WriteLine ( output );","title":"IntegerDataTypes"},{"location":"Coding/Courses/#listexamples","text":"static void Main ( string [] args ) { int [] arr1 = { 0 , 1 , 2 , 3 , 4 }; int [] arr2 = { 5 , 6 , 7 , 8 , 9 }; List < int > list = new List < int >(); list = splat ( arr1 , arr2 ); foreach ( var el in list ) { Console . WriteLine ( el ); } } static List < int > splat ( params int [][] arg ) { List < int > output = new List < int >(); foreach ( int [] arr in arg ) { output . AddRange ( arr ); } return output ; }","title":"ListExamples"},{"location":"Coding/Courses/#miniwhilegame","text":"static void Main ( string [] args ) { int mage = 30 ; int warrior = 40 ; while ( mage > 0 && warrior > 0 ) { warrior -= mage_attack (); mage -= warrior_attack (); } Console . WriteLine ( $ \"Conflict ends with Mage having {mage} points and Warrior having {warrior}!\" ); } private static int warrior_attack () { var r = new System . Random (); int result = r . Next ( 1 , 3 ); Console . WriteLine ( $ \"Warrior attacks Mage, dealing {result} damage!\" ); return result ; } private static int mage_attack () { var r = new System . Random (); int result = r . Next ( 7 , 9 ); Console . WriteLine ( $ \"Mage attacks Warrior, dealing {result} damage!\" ); return result ; }","title":"MiniWhileGame"},{"location":"Coding/Courses/#menuwhile","text":"static void Main ( string [] args ) { string [] fruits = { \"Apple\" , \"Banana\" , \"Date\" , \"Grape\" , \"Jackfruit\" , \"Kiwi\" , \"Lime\" , \"Orange\" , \"Peach\" , \"Strawberry\" }; string [] menu = { \"Add New Item\" , \"Edit Item\" , \"Remove Item\" , \"View All Items\" , \"Exit\" }; int choice ; do { choice = Menu ( menu ); switch ( choice ) { case 1 : for ( int i = 0 ; i < fruits . Length ; i ++) { if ( fruits [ i ] == null ) { Console . Write ( \"Please add a new fruit: \" ); fruits [ i ] = Console . ReadLine (); break ; } } break ; case 2 : int chosenFruit = 0 ; do { Console . Write ( $ \"Edit fruit number (1 to {fruits.Length}): \" ); try { chosenFruit = Convert . ToInt32 ( Console . ReadLine ()); } catch { InvalidInput (); } } while ( chosenFruit == 0 ); Console . Write ( $ \"Enter new value for the {fruits[chosenFruit - 1]}: \" ); fruits [ chosenFruit - 1 ] = Console . ReadLine (); break ; case 3 : break ; case 4 : Console . WriteLine ( \"Current fruits: \" ); for ( int i = 0 ; i < fruits . Length ; i ++) { if ( fruits [ i ] != null ) { Console . WriteLine ( fruits [ i ]); } } break ; case 5 : break ; default : InvalidInput (); break ; } } while ( true ); } private static int Menu ( string [] menu ) { Console . WriteLine ( '\\n' ); for ( var i = 0 ; i < menu . Length ; i ++) { Console . WriteLine ( \"{0}. {1}\" , i + 1 , menu [ i ]); } Console . Write ( \"Your choice: \" ); string input = Console . ReadLine (); try { int output = Int32 . Parse ( input ); return output ; } catch ( FormatException ) { InvalidInput (); return - 1 ; } catch ( OverflowException ) { InvalidInput (); return - 1 ; } } private static void InvalidInput () { Console . ForegroundColor = ConsoleColor . Red ; Console . WriteLine ( \"Invalid input!\" ); Console . ResetColor (); }","title":"MenuWhile"},{"location":"Coding/Courses/#outputtingarrays","text":"int [] primes = { 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 , 29 , 31 , 37 , 41 , 43 , 47 }; Console . WriteLine ( $ \"{primes.Length} total primes\" ); List < int > lessThanTwenty = new List < int >(); List < int > moreThanTwenty = new List < int >(); foreach ( int i in primes ) { if ( i < 20 ) { lessThanTwenty . Add ( i ); } else { moreThanTwenty . Add ( i ); } } Console . WriteLine ( $ \"Primes < 20:\\n {string.Join(\" , \", lessThanTwenty)}\" ); Console . WriteLine ( $ \"Primes >= 20:\\n {string.Join(\" , \", moreThanTwenty)}\" );","title":"OutputtingArrays"},{"location":"Coding/Courses/#readingcharacter","text":"Console . Write ( \"How old are you? \" ); int age ; int . TryParse ( Console . ReadLine (), out age ) ; Console . WriteLine ( $ \"You are {age} years old (allegedly).\" );","title":"ReadingCharacter"},{"location":"Coding/Courses/#readkey","text":"Console . WriteLine ( $ \"\\n\\nKey pressed: {key.Key}\" ); Console . WriteLine ( $ \"Key as char: {key.KeyChar}\" ); Console . WriteLine ( $ \"Modifiers: {key.Modifiers}\" );","title":"ReadKey"},{"location":"Coding/Courses/#readline","text":"Console . Write ( \"Input the drive letter: \" ); string driveLetter = Console . ReadLine (); Console . Write ( \"Input the folder path: \" ); string folderPath = Console . ReadLine (); Console . Write ( \"Input the file name: \" ); string fileName = Console . ReadLine (); Console . WriteLine ( $ \"{driveLetter}:\\\\{folderPath}\\\\{fileName}.exe\" );","title":"ReadLine"},{"location":"Coding/Courses/#refandout","text":"static void Main ( string [] args ) { int number = 0 ; Console . WriteLine ( number ); IncreaseByOne ( ref number ); Console . WriteLine ( number ); } static void IncreaseByOne ( ref int n ) { n ++; } static void Main () { double n = 5 ; double nSquared ; square ( n , out nSquared ); Console . WriteLine ( $ \"{n} ^ 2 = {nSquared}\" ); } static void square ( double x , out double y ) { y = System . Math . Pow ( x , 2 ); }","title":"RefAndOut"},{"location":"Coding/Courses/#rpgchargen","text":"","title":"RPGCharGen"},{"location":"Coding/Courses/#strings","text":"string username = \"admin\" ; System . Console . WriteLine ( username [ 0 ]); // impossible, strings are immutable username [ 0 ] = 'A' ;","title":"Strings"},{"location":"Coding/Courses/#stringtricks","text":"string fruitJuice = \"Strawberry juice\" ; string separator = new string ( '-' , fruitJuice . Length ); System . Console . WriteLine ( fruitJuice ); System . Console . WriteLine ( separator ); System . Console . WriteLine ( fruitJuice . Contains ( \"j\" )); System . Console . WriteLine ( fruitJuice . IndexOf ( \"r\" )); System . Console . WriteLine ( fruitJuice . LastIndexOf ( \"r\" )); System . Console . WriteLine ( fruitJuice . ToUpper (). Contains ( \"J\" )); System . Console . WriteLine ( fruitJuice . ToUpper (). IndexOf ( \"RR\" )); System . Console . WriteLine ( fruitJuice . ToUpper (). LastIndexOf ( \"RR\" ));","title":"StringTricks"},{"location":"Coding/Courses/#trycatchfinally","text":"StreamWriter sw = null ; try { sw = File . CreateText ( Directory . GetCurrentDirectory () + @\"/test.txt\" ); int number = int . Parse ( Console . ReadLine ()); int dividend = 5 / number ; sw . Write ( number ); } catch ( FormatException ex ) { Console . WriteLine ( ex . Message ); } catch ( DivideByZeroException ex ) { Console . WriteLine ( ex . Message ); } finally { sw . Close (); }","title":"TryCatchFinally"},{"location":"Coding/Courses/#writeandwriteline","text":"string heading = \"Protein Intake Week: 1\" ; string underline = new string ( '=' , heading . Length ); double num1 = 80.885570 ; double num2 = 94.564645 ; double num3 = 78.678931 ; double num4 = 88.66654 ; double num5 = 88.6466 ; double num6 = 76.777 ; double num7 = 91.85759 ; double sum = num1 + num2 + num3 + num4 + num5 + num6 + num7 ; double [] array = { num1 , num2 , num3 , num4 , num5 , num6 , num7 }; System . Console . WriteLine ( \"|{0}|\" , heading ); System . Console . WriteLine ( \"|{0}|\" , underline ); foreach ( double i in array ) { System . Console . WriteLine ( $ \"|{i, 22:N2}|\" ); } System . Console . WriteLine ( \"|{0}|\" , underline ); System . Console . WriteLine ( \"|Total: {0, 15:N2}|\" , sum );","title":"WriteAndWriteLine"},{"location":"Coding/Courses/#test-driven-development-in-c","text":"","title":"Test-Driven Development in C#"},{"location":"Coding/Courses/#create-a-red-unit-test","text":"","title":"Create a Red Unit Test"},{"location":"Coding/Courses/#mocking-with-moq-and-xunit","text":"","title":"Mocking with Moq and xUnit"},{"location":"Coding/DnD/","text":"\ud83d\udc32\ufe0f Dungeons & Dragons Ability scores Six ability scores define the capabilities of every character and monster in the game. Each of these are obtained by rolling 4d6, then discarding the die with the lowest value. Strength Dexterity Constitution Intelligence Wisdom Charisma Alternatively, 27 points can be spent according to the following table to arbitrarily determine ability scores. Ability score Cost 8 0 9 1 10 2 11 3 12 4 13 5 14 7 15 9 Finally, the following set of scores can be used if either of the two methods above are unacceptable: 15, 14, 13, 12, 10, and 8. Ability modifiers Ability modifiers apply to each score according to its value. These modifiers are equivalent to finding the floor of half of the score's difference with ten (i.e. (13-10) // 2 = 1, but (7-10) // 2 = -2) Ability score Modifier Ability Score Modifier Ability Score Modifier 1 -5 12-13 +1 22-23 +6 2-3 -4 14-15 +2 24-25 +7 4-5 -3 16-17 +3 26-27 +8 6-7 -2 18-19 +4 28-29 +9 8-9 -1 20-21 +5 30 +10 10-11 0 Class A class describes a character's vocation, talents, and combat style. Class features are capabilities unique to each class Proficiencies in armor, weapons, saving throws, and other items define a character's talents","title":"\ud83d\udc32&#xFE0F;  Dungeons & Dragons"},{"location":"Coding/DnD/#dungeons-dragons","text":"","title":"\ud83d\udc32&#xFE0F;  Dungeons &amp; Dragons"},{"location":"Coding/DnD/#ability-scores","text":"Six ability scores define the capabilities of every character and monster in the game. Each of these are obtained by rolling 4d6, then discarding the die with the lowest value. Strength Dexterity Constitution Intelligence Wisdom Charisma Alternatively, 27 points can be spent according to the following table to arbitrarily determine ability scores. Ability score Cost 8 0 9 1 10 2 11 3 12 4 13 5 14 7 15 9 Finally, the following set of scores can be used if either of the two methods above are unacceptable: 15, 14, 13, 12, 10, and 8.","title":"Ability scores"},{"location":"Coding/DnD/#ability-modifiers","text":"Ability modifiers apply to each score according to its value. These modifiers are equivalent to finding the floor of half of the score's difference with ten (i.e. (13-10) // 2 = 1, but (7-10) // 2 = -2) Ability score Modifier Ability Score Modifier Ability Score Modifier 1 -5 12-13 +1 22-23 +6 2-3 -4 14-15 +2 24-25 +7 4-5 -3 16-17 +3 26-27 +8 6-7 -2 18-19 +4 28-29 +9 8-9 -1 20-21 +5 30 +10 10-11 0","title":"Ability modifiers"},{"location":"Coding/DnD/#class","text":"A class describes a character's vocation, talents, and combat style. Class features are capabilities unique to each class Proficiencies in armor, weapons, saving throws, and other items define a character's talents","title":"Class"},{"location":"Coding/Dogfood/","text":"\ud83d\udc36 Dogfood data A problem I've encountered is that when it comes to anything to do with software, learning is doing. But there isn't really much play data easily available. Some tutorials go to the trouble of providing files for download, and I'm sure anyone reading this already has a favorite API or JSON repo where they go to get stuff. But the situation is not ideal, and I've never encountered a body of data in any learning materials I've encountered that compelled me to come back to it time and again. So I made it raven These .csv files have proven useful to me as sample datasets to use while learning the Unix filters grep , sed , and awk . I hope whoever finds them finds them just as valuable. \ud83e\udde0 Greek philosophers Very small dataset featuring all your favorite Ancients: Name City Date of birth Socrates Athens 470 BC Plato Athens 428 BC Aristotle Stagira 384 BC Euclid Alexandria 325 BC Pythagoras Samos 570 BC \ud83d\udea2 Ships of the line A somewhat larger, more complicated dataset featuring some of history's battliest battleships: Name Country Displacement Length Beam Commissioned date Yamato Japan 65027 256 38.9 16 December 1941 USS Enterprise United States of America 19800 251.4 33.4 12 May 1938 Bismarck Germany 41700 251 36 24 August 1940 HMS Dreadnought United Kingdom 18120 160.6 25 2 December 1906 USS Iowa United States of America 46000 270.43 32.97 22 February 1943 HMS Vanguard United Kingdom 45200 248.2 32.9 12 May 1946 \ud83d\ude80 Starships Ships famous, to varying degrees, for plying the inky black. If these interest you, check out my Starships repo too. Name Class Registry Crew USS Enterprise Constitution NCC-1701 203 USS Constitution Constitution NCC-1700 204 USS Defiant Defiant NX-74205 50 USS Voyager Intrepid NCC-74656 141 USS Enterprise Galaxy NCC-1701-D 6000 USS Reliant Miranda NCC-1864 35 \ud83e\uddd4 Mathematicians An expanded assortment of smartypants from a different era. The third and largest dataset, with two date fields and one full of semicolon-delimited subfields ripe for parsing: Name Surname Date of birth Date of death Concepts Carl Gauss 30 April 1777 23 February 1855 Gaussian elimination; Gauss\u2013Jordan elimination; Gauss\u2013Seidel method; Gauss's cyclotomic formula; Gauss's lemma; Gaussian binomial coefficient; Gauss transformation; Gauss\u2013Bodenmiller theorem; Gauss\u2013Bolyai\u2013Lobachevsky space; Gauss\u2013Bonnet theorem; Generalized Gauss\u2013Bonnet theorem; Braid theory; Gauss\u2013Codazzi equations; Gauss\u2013Manin connection; Newton line; Gauss's area formula; Gauss's lemma; Gauss map; Gaussian curvature; Gauss circle problem; Gauss\u2013Kuzmin\u2013Wirsing constant; Gauss's constant; Gauss's digamma theorem; Gauss's generalization of Wilson's theorem; Gauss's lemma; Gauss map; Gaussian moat; Gauss class number problem; Gauss's multiplication formula George Berkeley 12 March 1685 14 January 1753 Gottfried Leibniz 1 July 1646 14 November 1716 Calculus; Monads; Best of all possible worlds; Pre-established harmony; Identity of indiscernibles; Matrix (mathematics); Leibniz integral rule; Principle of sufficient reason; Notation for differentiation; Product rule; Vis viva; Boolean algebra; Salva veritate; Stepped reckoner; Symbolic logic; Semiotics; Analysis situs; Law of Continuity; Transcendental law of homogeneity; Ars combinatoria; Calculus ratiocinator; Leibniz's notation; Characteristica universalis; Problem of why there is anything at all; Pluralistic idealism; Metaphysical dynamism; Relationism; Apperception; A priori/a posteriori distinction Immanuel Kant 22 April 1724 12 February 1804 Kantianism; Kantian ethics; Neo-Kantianism Isaac Newton 25 December 1642 20 March 1726 \"Gauss\u2013Newton algorithm; Newton\u2013Cotes formulas; Newton\u2013Okounkov body; Newton\u2013Pepys problem; Newton fractal; Newton's identities; Newton's inequalities; Newton's method; Newton's method in optimization; Newton's notation; Newton polygon; Newton polynomial; Newton series; Newton's theorem about ovals; Truncated Newton method; bucket argument; Newton's cannonball; Universal gravitational constant; Newton's cradle; Newton disc; Newton\u2013Cartan theory; Newton\u2013Euler equations; Newton's law of cooling; Newton's laws of motion; Newton's law of universal gravitation; Newton's metal; Newton's reflector; Newton's rings; Rotating spheres; Newton scale; Newton's sphere theorem John Locke 29 August 1632 28 October 1704 Labor theory of property; Social contract; State of nature Joseph Fourier 21 March 1768 16 May 1830 Fourier series; Fourier transform; Fourier's law of conduction; Fourier-Motzkin elimination Joseph-Louis Lagrange 25 January 1736 10 April 1813 Lagrangian analysis; Lagrangian coordinates; Lagrangian derivative; Lagrangian drifter; Lagrangian foliation; Lagrangian Grassmannian; Lagrangian intersection Floer homology; Lagrangian mechanics; Lagrangian (field theory); Lagrangian system; Lagrangian mixing; Lagrangian point; Lagrangian relaxation; Lagrangian submanifold; Lagrangian subspace; Nonlocal Lagrangian; Proca lagrangian; Special Lagrangian submanifold; Euler\u2013Lagrange equation; Green\u2013Lagrange strain; Lagrange bracket; Lagrange\u2013d'Alembert principle; Lagrange error bound; Lagrange form; Lagrange interpolation; Lagrange invariant; Lagrange inversion theorem; Lagrange multiplier; Lagrange number; Lagrange point colonization; Lagrange polynomial; Lagrange property; Lagrange reversion theorem; Lagrange resolvent; Lagrange spectrum; Lagrange stream function; Lagrange's approximation theorem; Lagrange's formula (disambiguation); Lagrange's identity (disambiguation); Lagrange's theorem (group theory); Lagrange's theorem (number theory); Lagrange's four-square theorem; Lagrange's trigonometric identities Pierre-Simon Laplace 23 March 1749 5 March 1827 Bayesian inference; Bayesian probability; Laplace's equation; Laplacian; Laplace transform; Inverse Laplace transform; Laplace distribution; Laplace's demon; Laplace expansion; Young\u2013Laplace equation; Laplace number; Laplace limit; Laplace invariant; Laplace principle; Laplace's principle of insufficient reason; Laplace's method; Laplace expansion; Laplace force; Laplace filter; Laplace functional; Laplacian matrix; Laplace motion; Laplace plane; Laplace pressure; Laplace resonance; Laplace's spherical harmonics; Laplace smoothing; Laplace expansion; Laplace expansion; Laplace-Bayes estimator; Laplace\u2013Stieltjes transform; Laplace\u2013Runge\u2013Lenz vector; Nebular hypothesis Ren\u00e9 Descartes 31 March 1596 11 February 1650 Cartesian circle; Cartesian coordinate system; Cartesian diagram; Cartesian diver; Cartesian morphism; Cartesian plane; Cartesian product; Cartesian product of graphs; Cartesian theater; Cartesian tree; Descartes' rule of signs; Descartes' theorem (4 tangent circles); Descartes' theorem (on total angular defect); Folium of Descartes","title":"\ud83d\udc36 Dogfood data"},{"location":"Coding/Dogfood/#dogfood-data","text":"A problem I've encountered is that when it comes to anything to do with software, learning is doing. But there isn't really much play data easily available. Some tutorials go to the trouble of providing files for download, and I'm sure anyone reading this already has a favorite API or JSON repo where they go to get stuff. But the situation is not ideal, and I've never encountered a body of data in any learning materials I've encountered that compelled me to come back to it time and again. So I made it raven These .csv files have proven useful to me as sample datasets to use while learning the Unix filters grep , sed , and awk . I hope whoever finds them finds them just as valuable.","title":"\ud83d\udc36 Dogfood data"},{"location":"Coding/Dogfood/#greek-philosophers","text":"Very small dataset featuring all your favorite Ancients: Name City Date of birth Socrates Athens 470 BC Plato Athens 428 BC Aristotle Stagira 384 BC Euclid Alexandria 325 BC Pythagoras Samos 570 BC","title":"\ud83e\udde0 Greek philosophers"},{"location":"Coding/Dogfood/#ships-of-the-line","text":"A somewhat larger, more complicated dataset featuring some of history's battliest battleships: Name Country Displacement Length Beam Commissioned date Yamato Japan 65027 256 38.9 16 December 1941 USS Enterprise United States of America 19800 251.4 33.4 12 May 1938 Bismarck Germany 41700 251 36 24 August 1940 HMS Dreadnought United Kingdom 18120 160.6 25 2 December 1906 USS Iowa United States of America 46000 270.43 32.97 22 February 1943 HMS Vanguard United Kingdom 45200 248.2 32.9 12 May 1946","title":"\ud83d\udea2 Ships of the line"},{"location":"Coding/Dogfood/#starships","text":"Ships famous, to varying degrees, for plying the inky black. If these interest you, check out my Starships repo too. Name Class Registry Crew USS Enterprise Constitution NCC-1701 203 USS Constitution Constitution NCC-1700 204 USS Defiant Defiant NX-74205 50 USS Voyager Intrepid NCC-74656 141 USS Enterprise Galaxy NCC-1701-D 6000 USS Reliant Miranda NCC-1864 35","title":"\ud83d\ude80 Starships"},{"location":"Coding/Dogfood/#mathematicians","text":"An expanded assortment of smartypants from a different era. The third and largest dataset, with two date fields and one full of semicolon-delimited subfields ripe for parsing: Name Surname Date of birth Date of death Concepts Carl Gauss 30 April 1777 23 February 1855 Gaussian elimination; Gauss\u2013Jordan elimination; Gauss\u2013Seidel method; Gauss's cyclotomic formula; Gauss's lemma; Gaussian binomial coefficient; Gauss transformation; Gauss\u2013Bodenmiller theorem; Gauss\u2013Bolyai\u2013Lobachevsky space; Gauss\u2013Bonnet theorem; Generalized Gauss\u2013Bonnet theorem; Braid theory; Gauss\u2013Codazzi equations; Gauss\u2013Manin connection; Newton line; Gauss's area formula; Gauss's lemma; Gauss map; Gaussian curvature; Gauss circle problem; Gauss\u2013Kuzmin\u2013Wirsing constant; Gauss's constant; Gauss's digamma theorem; Gauss's generalization of Wilson's theorem; Gauss's lemma; Gauss map; Gaussian moat; Gauss class number problem; Gauss's multiplication formula George Berkeley 12 March 1685 14 January 1753 Gottfried Leibniz 1 July 1646 14 November 1716 Calculus; Monads; Best of all possible worlds; Pre-established harmony; Identity of indiscernibles; Matrix (mathematics); Leibniz integral rule; Principle of sufficient reason; Notation for differentiation; Product rule; Vis viva; Boolean algebra; Salva veritate; Stepped reckoner; Symbolic logic; Semiotics; Analysis situs; Law of Continuity; Transcendental law of homogeneity; Ars combinatoria; Calculus ratiocinator; Leibniz's notation; Characteristica universalis; Problem of why there is anything at all; Pluralistic idealism; Metaphysical dynamism; Relationism; Apperception; A priori/a posteriori distinction Immanuel Kant 22 April 1724 12 February 1804 Kantianism; Kantian ethics; Neo-Kantianism Isaac Newton 25 December 1642 20 March 1726 \"Gauss\u2013Newton algorithm; Newton\u2013Cotes formulas; Newton\u2013Okounkov body; Newton\u2013Pepys problem; Newton fractal; Newton's identities; Newton's inequalities; Newton's method; Newton's method in optimization; Newton's notation; Newton polygon; Newton polynomial; Newton series; Newton's theorem about ovals; Truncated Newton method; bucket argument; Newton's cannonball; Universal gravitational constant; Newton's cradle; Newton disc; Newton\u2013Cartan theory; Newton\u2013Euler equations; Newton's law of cooling; Newton's laws of motion; Newton's law of universal gravitation; Newton's metal; Newton's reflector; Newton's rings; Rotating spheres; Newton scale; Newton's sphere theorem John Locke 29 August 1632 28 October 1704 Labor theory of property; Social contract; State of nature Joseph Fourier 21 March 1768 16 May 1830 Fourier series; Fourier transform; Fourier's law of conduction; Fourier-Motzkin elimination Joseph-Louis Lagrange 25 January 1736 10 April 1813 Lagrangian analysis; Lagrangian coordinates; Lagrangian derivative; Lagrangian drifter; Lagrangian foliation; Lagrangian Grassmannian; Lagrangian intersection Floer homology; Lagrangian mechanics; Lagrangian (field theory); Lagrangian system; Lagrangian mixing; Lagrangian point; Lagrangian relaxation; Lagrangian submanifold; Lagrangian subspace; Nonlocal Lagrangian; Proca lagrangian; Special Lagrangian submanifold; Euler\u2013Lagrange equation; Green\u2013Lagrange strain; Lagrange bracket; Lagrange\u2013d'Alembert principle; Lagrange error bound; Lagrange form; Lagrange interpolation; Lagrange invariant; Lagrange inversion theorem; Lagrange multiplier; Lagrange number; Lagrange point colonization; Lagrange polynomial; Lagrange property; Lagrange reversion theorem; Lagrange resolvent; Lagrange spectrum; Lagrange stream function; Lagrange's approximation theorem; Lagrange's formula (disambiguation); Lagrange's identity (disambiguation); Lagrange's theorem (group theory); Lagrange's theorem (number theory); Lagrange's four-square theorem; Lagrange's trigonometric identities Pierre-Simon Laplace 23 March 1749 5 March 1827 Bayesian inference; Bayesian probability; Laplace's equation; Laplacian; Laplace transform; Inverse Laplace transform; Laplace distribution; Laplace's demon; Laplace expansion; Young\u2013Laplace equation; Laplace number; Laplace limit; Laplace invariant; Laplace principle; Laplace's principle of insufficient reason; Laplace's method; Laplace expansion; Laplace force; Laplace filter; Laplace functional; Laplacian matrix; Laplace motion; Laplace plane; Laplace pressure; Laplace resonance; Laplace's spherical harmonics; Laplace smoothing; Laplace expansion; Laplace expansion; Laplace-Bayes estimator; Laplace\u2013Stieltjes transform; Laplace\u2013Runge\u2013Lenz vector; Nebular hypothesis Ren\u00e9 Descartes 31 March 1596 11 February 1650 Cartesian circle; Cartesian coordinate system; Cartesian diagram; Cartesian diver; Cartesian morphism; Cartesian plane; Cartesian product; Cartesian product of graphs; Cartesian theater; Cartesian tree; Descartes' rule of signs; Descartes' theorem (4 tangent circles); Descartes' theorem (on total angular defect); Folium of Descartes","title":"\ud83e\uddd4 Mathematicians"},{"location":"Coding/GUI/","text":"\ud83d\udda5\ufe0f GUI frameworks comparison Although XAML widgets can declare their own headers, in Tkinter this is implemented as separate LabelFrame widgets, meant to contain controls. Tkinter widgets must be slaved in a manner which ultimately leads to the root object initialized by Tk() , conceptually similar to the Window root node in XAML. To refer to UI elements, XAML prefers the term control whereas tkinter and other frameworks prefer widget . Elements that can contain other elements and are used to visually organize the application are known as layout panels . WinUI control tkinter widget Button Button Checkbox Checkbutton DatePicker DateEntry TextBox Entry ListView Widgets Date picker XAML <Window x:Class= \"EmployeeManager.WinUI.MainWindow\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:EmployeeManager.WinUI\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" > <DatePicker Header= \"Entry date\" /> </Window> tkinter import tkinter as tk from tkinter.ttk import LabelFrame from tkcalendar import DateEntry window = tk . Tk () frame = LabelFrame ( window , text = \"Entry date: \" ) frame . pack () DateEntry ( frame ) . pack () tk . mainloop () Examples Hello, World! gtk import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk win = Gtk . Window () win . connect ( \"destroy\" , Gtk . main_quit ) win . show_all () Gtk . main () wx import wx app = wx . App () frame = wx . Frame ( parent = None , title = 'Hello World' ) frame . Show () app . MainLoop () Empty window Python pip install PyGObject import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk window = Gtk . Window ( title = \"Hello, world!\" ) window . connect ( \"destroy\" , Gtk . main_quit ) window . show_all () Gtk . main () Vala src int main ( string [] args ) { Gtk . init ( ref args ); var window = new Gtk . Window (); window . title = \"Hello, world!\" ; window . border_width = 10 ; window . window_position = Gtk . WindowPosition . CENTER ; window . set_default_size ( 200 , 200 ); window . destroy . connect ( Gtk . main_quit ); window . show_all () Gtk . main (); return 0 ; } Empty window (OOP) Python import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk class HelloWorld ( Gtk . Window ): def __init__ ( self ): super () . __init__ ( self , title = \"Hello, world!\" ) window = HelloWorld (); Vala There are two constructors in Vala, one named after the class and the second named with the keyword construct . The class constructor is only invoked upon the first instantiation. The construct constructor is invoked with every new object instantiation. public class HelloWorld : Gtk . Application { public HelloWorld () { Object ( application_id : \"com.github.alecaddd.test\" , flags : GLib . ApplicationFlags . FLAGS_NONE ) } protected override activate () { var window = new Gtk . ApplicationWindow ( this ); window . title = \"Hello, world!\" ; window . border_width = 10 ; window . window_position = Gtk . WindowPosition . CENTER ; window . set_default_size ( 200 , 200 ); window . show_all (); } public static int main ( string [] args ) { var app = new HelloWorld (); return app . run ( args ); } } tkinter main.py import tkinter as tk from tkinter.ttk import Button from tkinter.ttk import Labelframe from tkinter.ttk import Label from tkinter.ttk import Entry from tkinter.ttk import Checkbutton # from tkinter.ttk import import tkcalendar LabelFrame = Labelframe window = tk . Tk () window . title ( \"Wired Brain Coffee\" ) main = Labelframe ( window ) main . pack () headline = Label ( main , text = \"Wired Brain Coffee\" ) headline . grid ( row = 0 , column = 0 , columnspan = 2 ) sidebar = LabelFrame ( main ) sidebar . grid ( row = 1 , column = 0 ) refresh = Button ( sidebar , text = \"Refresh\" ) refresh . pack () mainarea = LabelFrame ( main ) mainarea . grid ( row = 1 , column = 1 ) firstname_frame = Labelframe ( mainarea , text = \"First name\" ) firstname_frame . pack () firstname_textbox = Entry ( firstname_frame ) . pack () # firstname_textbox.pack(expand='yes', fill='both') dateentry_frame = Labelframe ( mainarea , text = \"Entry date\" ) dateentry_frame . pack () dateentry_picker = tkcalendar . DateEntry ( dateentry_frame ) dateentry_picker . pack () jobrole_frame = Labelframe ( mainarea , text = \"Job Role\" ) jobrole_frame . pack () jobrole_textbox = Entry ( jobrole_frame ) . pack () # jobrole_textbox.pack(expand='yes', fill='both') iscoffeedrinker = Checkbutton ( mainarea , text = \"Is coffee drinker?\" ) . pack () # iscoffeedrinker.pack() window . resizable = ( True , True ) window . mainloop ()","title":"\ud83d\udda5&#xFE0F; GUI frameworks comparison"},{"location":"Coding/GUI/#gui-frameworks-comparison","text":"Although XAML widgets can declare their own headers, in Tkinter this is implemented as separate LabelFrame widgets, meant to contain controls. Tkinter widgets must be slaved in a manner which ultimately leads to the root object initialized by Tk() , conceptually similar to the Window root node in XAML. To refer to UI elements, XAML prefers the term control whereas tkinter and other frameworks prefer widget . Elements that can contain other elements and are used to visually organize the application are known as layout panels . WinUI control tkinter widget Button Button Checkbox Checkbutton DatePicker DateEntry TextBox Entry ListView","title":"\ud83d\udda5&#xFE0F; GUI frameworks comparison"},{"location":"Coding/GUI/#widgets","text":"","title":"Widgets"},{"location":"Coding/GUI/#date-picker","text":"XAML <Window x:Class= \"EmployeeManager.WinUI.MainWindow\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:EmployeeManager.WinUI\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" > <DatePicker Header= \"Entry date\" /> </Window> tkinter import tkinter as tk from tkinter.ttk import LabelFrame from tkcalendar import DateEntry window = tk . Tk () frame = LabelFrame ( window , text = \"Entry date: \" ) frame . pack () DateEntry ( frame ) . pack () tk . mainloop ()","title":"Date picker"},{"location":"Coding/GUI/#examples","text":"","title":"Examples"},{"location":"Coding/GUI/#hello-world","text":"gtk import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk win = Gtk . Window () win . connect ( \"destroy\" , Gtk . main_quit ) win . show_all () Gtk . main () wx import wx app = wx . App () frame = wx . Frame ( parent = None , title = 'Hello World' ) frame . Show () app . MainLoop ()","title":"Hello, World!"},{"location":"Coding/GUI/#empty-window","text":"Python pip install PyGObject import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk window = Gtk . Window ( title = \"Hello, world!\" ) window . connect ( \"destroy\" , Gtk . main_quit ) window . show_all () Gtk . main () Vala src int main ( string [] args ) { Gtk . init ( ref args ); var window = new Gtk . Window (); window . title = \"Hello, world!\" ; window . border_width = 10 ; window . window_position = Gtk . WindowPosition . CENTER ; window . set_default_size ( 200 , 200 ); window . destroy . connect ( Gtk . main_quit ); window . show_all () Gtk . main (); return 0 ; }","title":"Empty window"},{"location":"Coding/GUI/#empty-window-oop","text":"Python import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk class HelloWorld ( Gtk . Window ): def __init__ ( self ): super () . __init__ ( self , title = \"Hello, world!\" ) window = HelloWorld (); Vala There are two constructors in Vala, one named after the class and the second named with the keyword construct . The class constructor is only invoked upon the first instantiation. The construct constructor is invoked with every new object instantiation. public class HelloWorld : Gtk . Application { public HelloWorld () { Object ( application_id : \"com.github.alecaddd.test\" , flags : GLib . ApplicationFlags . FLAGS_NONE ) } protected override activate () { var window = new Gtk . ApplicationWindow ( this ); window . title = \"Hello, world!\" ; window . border_width = 10 ; window . window_position = Gtk . WindowPosition . CENTER ; window . set_default_size ( 200 , 200 ); window . show_all (); } public static int main ( string [] args ) { var app = new HelloWorld (); return app . run ( args ); } }","title":"Empty window (OOP)"},{"location":"Coding/GUI/#tkinter","text":"main.py import tkinter as tk from tkinter.ttk import Button from tkinter.ttk import Labelframe from tkinter.ttk import Label from tkinter.ttk import Entry from tkinter.ttk import Checkbutton # from tkinter.ttk import import tkcalendar LabelFrame = Labelframe window = tk . Tk () window . title ( \"Wired Brain Coffee\" ) main = Labelframe ( window ) main . pack () headline = Label ( main , text = \"Wired Brain Coffee\" ) headline . grid ( row = 0 , column = 0 , columnspan = 2 ) sidebar = LabelFrame ( main ) sidebar . grid ( row = 1 , column = 0 ) refresh = Button ( sidebar , text = \"Refresh\" ) refresh . pack () mainarea = LabelFrame ( main ) mainarea . grid ( row = 1 , column = 1 ) firstname_frame = Labelframe ( mainarea , text = \"First name\" ) firstname_frame . pack () firstname_textbox = Entry ( firstname_frame ) . pack () # firstname_textbox.pack(expand='yes', fill='both') dateentry_frame = Labelframe ( mainarea , text = \"Entry date\" ) dateentry_frame . pack () dateentry_picker = tkcalendar . DateEntry ( dateentry_frame ) dateentry_picker . pack () jobrole_frame = Labelframe ( mainarea , text = \"Job Role\" ) jobrole_frame . pack () jobrole_textbox = Entry ( jobrole_frame ) . pack () # jobrole_textbox.pack(expand='yes', fill='both') iscoffeedrinker = Checkbutton ( mainarea , text = \"Is coffee drinker?\" ) . pack () # iscoffeedrinker.pack() window . resizable = ( True , True ) window . mainloop ()","title":"tkinter"},{"location":"Coding/Perl/","text":"\ud83e\uddaa Perl Perl6 offers an interactive shell, but previous versions needed a specialized command to be run through the interpreter General syntax - Semicolons terminate lines - Whitespace is irrelevant, except inside strings - Enclosing function arguments in parentheses is optional Inline execution of code Compare similar syntax for sed (MP:17, YUG:614) perl6 -e 'code' Enable warning messages perl6 -w Request an implicit input-reading loop that stores records in $_ perl6 -n Request an implicit input-reading loop that stores records in $_ and automatically prints that variable after optional processing of its contents perl6 -p -l :automatically insert an output record separator at end of the output of print -0digits define the character that marks the end of an input record, using octal digits Shebang #!/usr/bin/env perl` Perl variables are of three types, associated with 3 corresponding sigils which begin the identifiers - Scalars: $ - Arrays: @ - Hashes (associative arrays): % my declares and initializes a variable. Variables can be typed by placing a type between my and the identifier. my $animal = \"camel\" my Str $animal = \"camel\" Predefined variables $. # current line number $_ # conventionally used as a default pattern space for searches; is already initialized but not defined; does **not** function like in bash $[ # current array base subscript (0 by default) $/ # input line separator (newline by default) @ARGV # arguments passed from the command-line; index 0 is the first **additional** argument passed subsequent to the name of the script Sources: Python to Perl6 - nutshell Scalar Operations $pbj = 'peanut butter'.' and '.'jelly' string concatenation was performed with . operator in Perl5 (YUG:617) $pbj = 'peanut butter'~' and '~'jelly' string concatenation now performed with ~ operator in Perl6 print '*' x 40 x operator repeats the string (YUG:617) Array my @array = (element, element, element) | my @array = element, element, elmeent initialize arrays by (optionally) enclosing elements in parentheses (not brackets) @array[n] retrieve element at (0-based) index {n} $matrix[0]->[0] arrow or infix operator can also be used to dereference array refrences $#array return number of the last subscript in the array (effectively length-1) @colors = ( 'red' , 'green' , 'yellow' , 'orange' ); ( $c [ 0 ], $c [ 1 ], $c [ 3 ], $c [ 5 ]) = @colors ; arrays can be declared by initializing constituent elements (array slicing) (PBX:85) Operations @list=(2..10); assign a range of numbers (arr-rng) (PBX:81) @letters=( 'A' .. 'Z' ); assign a range of letters (PBX:82) my @biglist = |@smalllist, |@littlelist array unpacking is done using the | prefix operator, allowing for concatenation Hash Hashes are key-value pairs my %fruits = (apple => red, ...) declare a hash %fruits{'apple'} values are obtained by referencing the key in curly braces (vice brackets) (p6) Functions defined return 1 if the variable passed as argument has a value and null if it does not die mesg print {mesg} and exit; used to implement error-handling join (YUG...) print print to STDOUT, but with no ending string specified prompt take input from STDIN read filehandle, variable, n read {n} bytes into {variable} from {filehandle} (deprecated in Perl6) (PBX:96) say print to STDOUT, but with a newline at the end (Perl6) undef undefine a defined variable, releasing the memory allocated for it Arrays pop @array remove and return last element of {array} push @array, elements append {elements} to {array} shift @array remove and return first element of {array} splice @array, index, n[, elements] remove {n} {elements} from {array}, starting at {index}; or, add {elements} to {array} at {index} unshift @array, elements prepend elements to an array (on the left side) Strings chomp remove last character from a scalar or last character from each word in an array if and only if that character is the input line separator chop remove last character from a scalar or last character from each word in an array; intended for use in removing newlines s/pattern/substitution/g ; substitution command works similar to sed tr/regex1/regex2/ ; replace instances of {regex1} with {regex2}; like replacing lowercase letters with uppercase split /delim/ Hashes keys(%hash) ; return an array of the keys of {hash} values(%hash) ; return an array of the values of {hash} Filehandles <> null filehandle (YUG:618) INFILE filehandle used when opening a file for input (YUG:632) OUTFILE filehandle used when opening a file for writing STDIN deprecated in Perl6 (PBX:96) Special literals These have been deprecated in Perl6 (PBX:51) __LINE__ (Perl5) | $?LINE (Perl6) current line number __FILE__ (Perl5) | $?FILE (Perl6) current filename __END__ logical end of the script (\\004 in Unix and \\032 in Windows) __DATA__ special filehandle __PACKAGE__ (Perl5) | $?PACKAGE (Perl6) current package; default package is main Control flow Loops for @array { action } | deprecated: foreach $var (@arr) { statements} foreach loops have been replaced by for in Perl6 for @array { .print } | deprecated: for @array { print } here the print function is working as a method on $_ , the default variable for @array1 Z @array2 zip up elements of two separate arrays (Source)[https://perl6advent.wordpress.com/2009/12/07/day-7-looping-for-fun-and-profit/] Documentation Implemented using markup language called \"pod\", which uses directives distinguishable by the = character =begin pod start documentation =end pod end documentation Glossary lvalue any value that can be \"assigned to\", and which represents a named region of storage (PBX:71) array slice when the elements of one array are assigned values from another (PBX:84) Object-oriented programming instance variables declared with has keyword: has $.name; class attributes are declard with my keyword, then a method is declared to allow it to be referenced Math Mathematical consonants have their own keywords in Perl6: pi , e , and tau (2*pi) Other topics rename function in Debian The Rename Command","title":"\ud83e\uddaa Perl"},{"location":"Coding/Perl/#perl","text":"Perl6 offers an interactive shell, but previous versions needed a specialized command to be run through the interpreter General syntax - Semicolons terminate lines - Whitespace is irrelevant, except inside strings - Enclosing function arguments in parentheses is optional Inline execution of code Compare similar syntax for sed (MP:17, YUG:614) perl6 -e 'code' Enable warning messages perl6 -w Request an implicit input-reading loop that stores records in $_ perl6 -n Request an implicit input-reading loop that stores records in $_ and automatically prints that variable after optional processing of its contents perl6 -p -l :automatically insert an output record separator at end of the output of print -0digits define the character that marks the end of an input record, using octal digits Shebang #!/usr/bin/env perl` Perl variables are of three types, associated with 3 corresponding sigils which begin the identifiers - Scalars: $ - Arrays: @ - Hashes (associative arrays): % my declares and initializes a variable. Variables can be typed by placing a type between my and the identifier. my $animal = \"camel\" my Str $animal = \"camel\" Predefined variables $. # current line number $_ # conventionally used as a default pattern space for searches; is already initialized but not defined; does **not** function like in bash $[ # current array base subscript (0 by default) $/ # input line separator (newline by default) @ARGV # arguments passed from the command-line; index 0 is the first **additional** argument passed subsequent to the name of the script Sources: Python to Perl6 - nutshell","title":"\ud83e\uddaa Perl"},{"location":"Coding/Perl/#scalar","text":"","title":"Scalar"},{"location":"Coding/Perl/#operations","text":"$pbj = 'peanut butter'.' and '.'jelly' string concatenation was performed with . operator in Perl5 (YUG:617) $pbj = 'peanut butter'~' and '~'jelly' string concatenation now performed with ~ operator in Perl6 print '*' x 40 x operator repeats the string (YUG:617)","title":"Operations"},{"location":"Coding/Perl/#array","text":"my @array = (element, element, element) | my @array = element, element, elmeent initialize arrays by (optionally) enclosing elements in parentheses (not brackets) @array[n] retrieve element at (0-based) index {n} $matrix[0]->[0] arrow or infix operator can also be used to dereference array refrences $#array return number of the last subscript in the array (effectively length-1) @colors = ( 'red' , 'green' , 'yellow' , 'orange' ); ( $c [ 0 ], $c [ 1 ], $c [ 3 ], $c [ 5 ]) = @colors ; arrays can be declared by initializing constituent elements (array slicing) (PBX:85)","title":"Array"},{"location":"Coding/Perl/#operations_1","text":"@list=(2..10); assign a range of numbers (arr-rng) (PBX:81) @letters=( 'A' .. 'Z' ); assign a range of letters (PBX:82) my @biglist = |@smalllist, |@littlelist array unpacking is done using the | prefix operator, allowing for concatenation","title":"Operations"},{"location":"Coding/Perl/#hash","text":"Hashes are key-value pairs my %fruits = (apple => red, ...) declare a hash %fruits{'apple'} values are obtained by referencing the key in curly braces (vice brackets) (p6)","title":"Hash"},{"location":"Coding/Perl/#functions","text":"defined return 1 if the variable passed as argument has a value and null if it does not die mesg print {mesg} and exit; used to implement error-handling join (YUG...) print print to STDOUT, but with no ending string specified prompt take input from STDIN read filehandle, variable, n read {n} bytes into {variable} from {filehandle} (deprecated in Perl6) (PBX:96) say print to STDOUT, but with a newline at the end (Perl6) undef undefine a defined variable, releasing the memory allocated for it","title":"Functions"},{"location":"Coding/Perl/#arrays","text":"pop @array remove and return last element of {array} push @array, elements append {elements} to {array} shift @array remove and return first element of {array} splice @array, index, n[, elements] remove {n} {elements} from {array}, starting at {index}; or, add {elements} to {array} at {index} unshift @array, elements prepend elements to an array (on the left side)","title":"Arrays"},{"location":"Coding/Perl/#strings","text":"chomp remove last character from a scalar or last character from each word in an array if and only if that character is the input line separator chop remove last character from a scalar or last character from each word in an array; intended for use in removing newlines s/pattern/substitution/g ; substitution command works similar to sed tr/regex1/regex2/ ; replace instances of {regex1} with {regex2}; like replacing lowercase letters with uppercase split /delim/","title":"Strings"},{"location":"Coding/Perl/#hashes","text":"keys(%hash) ; return an array of the keys of {hash} values(%hash) ; return an array of the values of {hash}","title":"Hashes"},{"location":"Coding/Perl/#filehandles","text":"<> null filehandle (YUG:618) INFILE filehandle used when opening a file for input (YUG:632) OUTFILE filehandle used when opening a file for writing STDIN deprecated in Perl6 (PBX:96)","title":"Filehandles"},{"location":"Coding/Perl/#special-literals","text":"These have been deprecated in Perl6 (PBX:51) __LINE__ (Perl5) | $?LINE (Perl6) current line number __FILE__ (Perl5) | $?FILE (Perl6) current filename __END__ logical end of the script (\\004 in Unix and \\032 in Windows) __DATA__ special filehandle __PACKAGE__ (Perl5) | $?PACKAGE (Perl6) current package; default package is main","title":"Special literals"},{"location":"Coding/Perl/#control-flow","text":"","title":"Control flow"},{"location":"Coding/Perl/#loops","text":"for @array { action } | deprecated: foreach $var (@arr) { statements} foreach loops have been replaced by for in Perl6 for @array { .print } | deprecated: for @array { print } here the print function is working as a method on $_ , the default variable for @array1 Z @array2 zip up elements of two separate arrays (Source)[https://perl6advent.wordpress.com/2009/12/07/day-7-looping-for-fun-and-profit/]","title":"Loops"},{"location":"Coding/Perl/#documentation","text":"Implemented using markup language called \"pod\", which uses directives distinguishable by the = character =begin pod start documentation =end pod end documentation","title":"Documentation"},{"location":"Coding/Perl/#glossary","text":"lvalue any value that can be \"assigned to\", and which represents a named region of storage (PBX:71) array slice when the elements of one array are assigned values from another (PBX:84)","title":"Glossary"},{"location":"Coding/Perl/#object-oriented-programming","text":"instance variables declared with has keyword: has $.name; class attributes are declard with my keyword, then a method is declared to allow it to be referenced","title":"Object-oriented programming"},{"location":"Coding/Perl/#math","text":"Mathematical consonants have their own keywords in Perl6: pi , e , and tau (2*pi)","title":"Math"},{"location":"Coding/Perl/#other-topics","text":"","title":"Other topics"},{"location":"Coding/Perl/#rename-function-in-debian","text":"The Rename Command","title":"rename function in Debian"},{"location":"Coding/Python/","text":"\ud83d\udc0d Python Decorators Sources: Primer on Python Decorators YouTube tutorial A decorator is any function that accepts a function and returns a function. Decorators are one of the main ways that Python implements functional programming principles. Functions are first-class objects and can be passed as parameters. import logging def hello_wrapper ( name , func ): func ( f 'Hello { name } ' ) hello_wrapper ( \"world\" , func = print ) # Hello world hello_wrapper ( \"logs\" , func = logging . warning ) # WARNING:root:Hello logs with open ( 'hello.txt' , 'w' ) as f : hello_wrapper ( 'everyone!' , func = f . write ) import random def anagram ( t ): l = [ c for c in t ] random . shuffle ( l ) print ( \"\" . join ( l )) hello_wrapper ( 'Japushku' , anagram ) # eHoulhluaskpJ The function has to be passed as a reference, actually calling it will cause the wrapper function to attempt to execute the value returned by the inner function. hello_wrapper ( \"world\" , func = print ()) # Error def outer (): print ( 'Hi from the outer function' ) def inner (): print ( 'Hello from the inner function' ) inner () We can use the __name__ attribute to access a passed function's name. def hello ( func ): print ( f 'Hello { func . __name__ } ' ) hello ( outer ) # Hello outer We can also return functions, which can then be invoked def hello ( func ): print ( f 'Hello { func . __name__ } ' ) return func hello ( outer )() ''' Hi from the outer function Hello from the inner function ''' new_outer = hello ( outer ) new_outer is outer # True def wrapper ( func ): print ( f 'Before { func . __name__ } ' ) func () print ( f 'After { func . __name__ } ' ) wrapper ( outer ) ''' Before outer Hi from the outer function Hello from the inner function After outer ''' The true decorator pattern appears here, where wrapper is called the decorator and outer has been decorated . def wrapper ( func ): def _wrapper (): print ( f 'Before { func . __name__ } ' ) func () print ( f 'After { func . __name__ } ' ) return _wrapper outer = wrapper ( outer ) But the usual syntax since Python 2.4 is to place the decorator on the line above the decorated function, preceded by @ : @wrapper def outer (): print ( 'Hi from the outer function' ) def inner (): print ( 'Hello from the inner function' ) inner () _wrapper here does not accept any positional arguments, so wrapping functions that take arguments will produce a TypeError @wrapper def say_hello ( name ): print ( f 'Hello { name } !' ) # error The solution is to incorporate *args, **kwargs into the definition of the inner function, as well as the invocation of the function passed in. def wrapper ( func ): def _wrapper ( * args , ** kwargs ): print ( f 'Before { func . __name__ } ' ) func ( * args , ** kwargs ) print ( f 'After { func . __name__ } ' ) return _wrapper Returned values are not captured yet: def wrapper ( func ): def _wrapper ( * args , ** kwargs ): print ( f 'Before { func . __name__ } ' ) value = func ( * args , ** kwargs ) print ( f 'After { func . __name__ } ' ) return value return _wrapper Inspecting the decorated function's __name__ attribute reveals that it is still named _wrapper say_hello . __name__ # '_wrapper' This is also true for other attributes, including docstring. functools.wraps is a decorator factory to reassign attributes to the wrapped function. This is considered superior to the functools.update_wrapper function which is also available. def wrapper ( func ): @functools . wraps ( func ) def _wrapper ( * args , ** kwargs ): print ( f 'Before { func . __name__ } ' ) value = func ( * args , ** kwargs ) print ( f 'After { func . __name__ } ' ) return value return _wrapper This forms an ideal starting template for the creation of custom decorators. Classes Properties In the Python documentation , attributes accessed with accessor functions are called managed attributes , which makes the term equivalent to properties in C#. Three methods can be defined using the @property decorator Constructor def __init__ ( self , price ): self . _price = price Getter @property def price ( self ): return self . _price Setter @price . setter def price ( self , new_price ): if new_price > 0 : self . _price = new_price else : raise ValueError Deleter @price . deleter def price ( self ): del self . _price Class methods The @classmethod decorator prevents the interpreter from passing in the instantiated object using self , rather the class itself is passed in as the cls argument. This means that the methods decorated as such must take not self as the first argument but cls @classmethod def classmethod ( cls ): pass The @staticmethod decorator prevents the interpreter from passing any additional arguments whatsoever. The resulting method has no access to the object itself nor the class and functions like a procedurally defined function. Formatting flake8 , black , and yapf (Google) are CLI tools used to automatically format Python code. Web frameworks Django A typical Django project contains multiple apps , which are Python packages containing their own models, views, templates, and urls. A model is the single definitive source of information about your data, and contains the essential fields and behaviors of the data you're storing. Migrations are necessary when Model classes are updated. And for projects sufficiently advanced, migration scripts must be developed for any such changes. Async Server Gateway Interface (ASGI) is the spiritual successor to, and superset of, WSGI . It implements the new Python standard for asynchronous web servers and applications, which resembles that of websockets . From WSGI to ASGI WSGI is coupled tightly with the synchronous request-response model familiar from HTTP 1.1. URL patterns (stored in the urlpatterns list defined in the project-level urls.py file) can be parameterized. Here, the template <int:x> specifies an integer to be assigned to the view parameter x . from app.views import my_view urlpatterns = [ path ( '/example/<int:x>' , my_view ) ] modelform_factory can be used to automatically produce a webform from a Model class. # views.py MeetingForm = modelform_factory ( Meeting , exclude = []) This can be placed into a template using the {{ form }} template tag. Note, a {% csrf_token %} template tag must also be present for a submit button to work. { % block content % } < h1 > Plan a new meeting </ h1 > < form method = \"POST\" > < table > {{ form }} </ table > { % csrf_token % } < button type = \"submit\" > Create </ button > </ form > { % endblock content % } When the modelform_factory class has been defined, it is instantiated within the view function . This object exposes several methods: - is_valid data validation is strongly recommended for any form input - save imports the validated form data into the database def new ( request ): if request . method == 'POST' : form = MeetingForm ( request . POST ) if form . is_valid (): form . save () return redirect ( \"home\" ) else : form = MeetingForm () return render ( request , \"meetings/new.html\" , { \"form\" : form }) Template Django templates are HTML files with additional markup to signify places where data can be dynamically inserted. The data used by the views file is called the template context . Templates must be placed within the /templates folder within the app, and it is considered best practice to place templates within a nested subdirectory within it, e.g. /templates/app . Django template tags are specified beween {% .. %} and allow for interpolation of data. < ul > { % for m in meetings % } { % endfor % } </ ul > URLs can be built by using the url template tag, specifying the name of a URL urlpatterns = [ path ( '' , home , name = 'home' ) ] < a href = \"{ % u rl 'home' %}\" > Home </ a > Models Models In Django, a Model class is mapped to a database table. Each object is a record in that table. Model objects expose several attributes and methods Get all objects meetings = Meeting . objects . all () Get count of objects in database count = Meeting . objects . count () Query for a specific object meeting = Meeting . objects . get ( pk = id ) get_object_or_404 may be better for most cases meeting = get_object_or_404 ( Meeting , pk = id ) Adding a new app python manage.py startapp app Then add to settings.py in project directory INSTALLED_APPS = [ # ... 'app' , ] There appears to be much flexibility in the arrangement of input controls in a form. So long as the Submit button is child to the form element, tasks are accepted in the To-Do app. Per Bulma documentation, the field class is intended as a container for label.label s, .control s, and optional p.help text. In contrast, control is a block container meant to enhance single form controls and can only contain input , select , button , or icon elements. form .field ( method= \"POST\" , action= \"/\" ) label .label Enter something to do .control | {{form.title}} | {% csrf_token %} button .button.is-primary ( type= \"submit\" ) Submit FastAPI Variables values can be taken from the route or from query parameters following a question mark. Routes from fastapi import FastAPI starships = FastAPI () @starships . get ( \"/starships/ {registry} \" ) def get_starship ( name : str ): return { \"response\" : f \"Hello, { name } \" } Query parameters from fastapi import FastAPI starships = FastAPI () @starships . get ( \"/\" ) def get_starship ( name : str = \"world\" ): return { \"response\" : f \"Hello, { name } \" } FastAPI is notable for being able to use type hints to construct data models, which are much lighter than the object relational models used by other frameworks. FastAPI from pydantic import BaseModel class Starship ( BaseModel ): name : str registry : str crew : int Django from django.db import models class Starship ( models . Model ): name = models . CharField ( max_length = 50 ) registry = models . CharField ( max_length = 15 ) crew = models . IntegerField () Dogfood data can be incorporated by using the keyword argument unpacking or \"double splat\" operator ( ** ) data = { \"name\" : \"USS Enterprise\" , \"registry\" : \"NCC-1701\" , \"crew\" : 203 } enterprise = Starship ( ** data ) POST method definitions then can use this newly defined class to validate posted data db = [] @app . post ( \"/starships\" ) async def create_starship ( starship : Starship ): db . append ( starship ) FastAPI supports Jinja templates to serve HTML templates import fastapi from fastapi.templating import Jinja2Templates # specifies the directory where templates are to be found templates = Jinja2Templates ( \"templates\" ) api = fastapi . APIRouter () @api . get ( '/' ) def index ( request : starlette . requests . Request ): return templates . TemplateResponse ( \"helloworld.html\" , { \"request\" : request }) By default, FastAPI also exposes web applications at /docs where you can test out all the exposed API methods. FastAPI integrates with ASGI servers like Uvicorn and Hypercorn, which can run a specific web application by name from the command-line or from within the script: Shell uvicorn main:starships --port 7000 Python import uvicorn uvicorn . run ( starships , port = 7000 ) Virtual environments pipenv pipenv --python 3 .6 venv Create a virtual environment named project python -m venv project virtualenv Create a virtual environment named project using a different version of Python virtualenv -p /usr/bin/python2 project Testing Pytest is a popular testing framework preferred to unittest by many Python developers because it follows Pythonic conventions more closely. In contrast to unittest's custom methods, pytest relies on the builtin assert statement. pytest from phonebook import PhoneBook import pytest @pytest . fixture def phonebook (): phonebook = PhoneBook () yield phonebook phonebook . clear () def test_lookup_by_name ( phonebook ): phonebook . add ( \"Bob\" , \"1234\" ) assert \"1234\" == phonebook . lookup ( \"Bob\" ) def test_phonebook_contains_all_names ( phonebook ): phonebook . add ( \"Bob\" , \"1234\" ) assert \"Bob\" in phonebook . names () def test_missing_name_raises_error ( phonebook ): with pytest . raises ( KeyError ): phonebook . lookup ( \"Bob\" ) python -m pytest unittest import unittest from phonebook import PhoneBook class PhoneBookTest ( unittest . TestCase ): def test_lookup_by_name ( self ): self . phonebook . add ( \"Bob\" , \"12345\" ) number = self . phonebook . lookup ( \"Bob\" ) self . assertEqual ( \"12345\" , number ) def test_missing_name ( self ): with self . assertRaises ( KeyError ): self . phonebook . lookup ( \"missing\" ) def test_empty_phonebook_is_consistent ( self ): self . assertTrue ( self . phonebook . is_consistent ()) def setUp ( self ) -> None : self . phonebook = PhoneBook () def tearDown ( self ) -> None : self . phonebook . clear () python -m unittest Class under test import os class PhoneBook : def __init__ ( self , cache_directory = os . getcwd ()): self . numbers = {} self . filename = os . path . join ( cache_directory , \"phonebook.txt\" ) self . cache = open ( self . filename , \"w\" ) def add ( self , name , number ): self . numbers [ name ] = number def lookup ( self , name ): return self . numbers [ name ] def is_consistent ( self ): return True def names ( self ): return set ( self . numbers . keys ()) def clear ( self ): self . cache . close () os . remove ( self . filename ) Doctest A doctest is a docstring containing what looks like interactive Python sessions. Python Docs \"\"\" Return the factorial of n, an exact integer >= 0. >>> [factorial(n) for n in range(6)] [1, 1, 2, 6, 24, 120] >>> factorial(30) 265252859812191058636308480000000 >>> factorial(-1) Traceback (most recent call last): ... ValueError: n must be >= 0 Factorials of floats are OK, but the float must be an exact integer: >>> factorial(30.1) Traceback (most recent call last): ... ValueError: n must be exact integer >>> factorial(30.0) 265252859812191058636308480000000 It must also not be ridiculously large: >>> factorial(1e100) Traceback (most recent call last): ... OverflowError: n too large \"\"\" This can then be run if __name__ == '__main__' : import doctest doctest . testmod () pytest PyTest relies on the built-in assert statement. Fixtures The @pytest.fixture decorator facilitiates the creation of test fixtures. The fixture function's name is used as argument to the test case, and the value returned can be used by the logic within. ( src ) Any clean-up logic can be invoked in this fixture as well by replacing return with yield . Pytest also provides its own tmpdir test fixture for temporary directories. ( src ) Before from phonebook import PhoneBook import pytest def test_lookup_by_name ( phonebook ): phonebook = PhoneBook () phonebook . add ( \"Bob\" , \"1234\" ) assert \"1234\" == phonebook . lookup ( \"Bob\" ) def test_phonebook_contains_all_names ( phonebook ): phonebook = PhoneBook () phonebook . add ( \"Bob\" , \"1234\" ) assert \"Bob\" in phonebook . names () def test_missing_name_raises_error ( phonebook ): phonebook = PhoneBook () with pytest . raises ( KeyError ): phonebook . lookup ( \"Bob\" ) After from phonebook import PhoneBook import pytest @pytest . fixture def phonebook (): phonebook = PhoneBook () yield phonebook phonebook . clear () def test_lookup_by_name ( phonebook ): # phonebook = PhoneBook() phonebook . add ( \"Bob\" , \"1234\" ) assert \"1234\" == phonebook . lookup ( \"Bob\" ) def test_phonebook_contains_all_names ( phonebook ): # phonebook = PhoneBook() phonebook . add ( \"Bob\" , \"1234\" ) assert \"Bob\" in phonebook . names () def test_missing_name_raises_error ( phonebook ): # phonebook = PhoneBook() with pytest . raises ( KeyError ): phonebook . lookup ( \"Bob\" ) tmpdir from phonebook import PhoneBook import pytest @pytest . fixture def phonebook ( tmpdir ): phonebook = PhoneBook ( tmpdir ) return phonebook def test_lookup_by_name ( phonebook ): # phonebook = PhoneBook() phonebook . add ( \"Bob\" , \"1234\" ) assert \"1234\" == phonebook . lookup ( \"Bob\" ) def test_phonebook_contains_all_names ( phonebook ): # phonebook = PhoneBook() phonebook . add ( \"Bob\" , \"1234\" ) assert \"Bob\" in phonebook . names () def test_missing_name_raises_error ( phonebook ): # phonebook = PhoneBook() with pytest . raises ( KeyError ): phonebook . lookup ( \"Bob\" ) unittest unittest is a testing framework built into Python's Standard Library that was based on JUnit. unittest came out in 2001, when JUnit was being ported and adapted to many languages. Collectively, these frameworks were referred to as the xUnit family . unittest's method names do not follow Python conventions because it predates the PEP-8 naming standard. unittest allows you to create test classes that inherit from TestCase . Assertions Assertions are implemented in individual methods of the TestCase subclass through unittest methods like assertEqual and assertRaises , etc. Notably, TestCase subclasses must not have an __init__() constructor method defined. def test_lookup_by_name ( self ): phonebook = PhoneBook () phonebook . add ( \"Bob\" , \"12345\" ) number = phonebook . lookup ( \"Bob\" ) self . assertEqual ( \"12345\" , number ) assertRaises must be placed in a context manager. Here, the test case will run the code within the with block and check to make sure it raises the specified exception: KeyError : ( src ) def test_missing_name ( self ): fleet = Fleet () with self . assertRaises ( KeyError ): fleet . lookup ( \"bla\" ) Fixtures setUp is run before every test method, allowing a test fixture to be created to avoid repetitive code. tearDown is called after every method, which allows these resources to be released, even if the test case raises an exception. However, if it is setUp that raises the exception, then neither the test case nor tearDown will run. ( src , src ) Before import unittest from phonebook import PhoneBook class PhoneBookTest ( unittest . TestCase ): def test_lookup_by_name ( self ): phonebook = PhoneBook () phonebook . add ( \"Bob\" , \"12345\" ) number = phonebook . lookup ( \"Bob\" ) self . assertEqual ( \"12345\" , number ) def test_missing_name ( self ): phonebook = PhoneBook () with self . assertRaises ( KeyError ): phonebook . lookup ( \"missing\" ) @unittest . skip ( \"WIP\" ) def test_empty_phonebook_is_consistent ( self ): phonebook = PhoneBook () self . assertTrue ( phonebook . is_consistent ()) After import unittest from phonebook import PhoneBook class PhoneBookTest ( unittest . TestCase ): def setUp ( self ) -> None : self . phonebook = PhoneBook () def tearDown ( self ) -> None : self . phonebook . clear () def test_lookup_by_name ( self ): # phonebook = PhoneBook() self . phonebook . add ( \"Bob\" , \"12345\" ) number = self . phonebook . lookup ( \"Bob\" ) self . assertEqual ( \"12345\" , number ) def test_missing_name ( self ): # phonebook = PhoneBook() with self . assertRaises ( KeyError ): self . phonebook . lookup ( \"missing\" ) @unittest . skip ( \"WIP\" ) def test_empty_phonebook_is_consistent ( self ): # phonebook = PhoneBook() self . assertTrue ( self . phonebook . is_consistent ()) The @unittest.skip decorator will tell the test runner to skip the decorated test case ( src ) @unittest . skip ( \"WIP\" ) def test_empty_phonebook_is_consistent ( self ): phonebook = PhoneBook () self . assertTrue ( phonebook . is_consistent ()) The command line entry point is made with a call to unittest.main() , which executes the tests. ( src ) import unittest from my_sum import sum class TestSum ( unittest . TestCase ): def test_list_int ( self ): \"\"\" Test that it can sum a list of integers \"\"\" data = [ 1 , 2 , 3 ] result = sum ( data ) self . assertEqual ( result , 6 ) if __name__ == '__main__' : unittest . main () Integration tests By convention, tests are put in their own directory as sibling to the main module ( in order to be able to import it ). Integration and unit tests should be organized separately. . \u251c\u2500\u2500 project \u2502 \u2514\u2500\u2500 __init__.py \u2514\u2500\u2500 tests \u251c\u2500\u2500 integration \u2514\u2500\u2500 unit Run all integration tests within specified directory. python -m unittest discover -s tests/integration Modules When learning unfamiliar packages and importing them in a demonstration script, care must be taken that the demonstration script does not have the same name as the package being studied. If so, attempting to import the package while in an interpreter within that directory will cause the interpreter to try importing the incomplete script and not the package. When running a Python interpreter within this directory, the files \"calc\" and \"main\" can be imported as modules by specifying their names with no file extension. . \u251c\u2500\u2500 calc.py \u2514\u2500\u2500 main.py import calc # No errors import main # No errors Specifying the full filename including extension does produce an error import calc.py # Error import main.py # Error argparse The ArgumentParser object exposes an attribute that contains the value passed in from the command-line. This attribute takes its identifier from the dest keyword argument when invoking the add_argument() method. Python import argparse def get_args (): parser = argparse . ArgumentParser () parser . add_argument ( dest = 'bar' ) return parser . parse_args () def main (): args = get_args () . bar The optional value assigned to description will be displayed when running the script with the options -h or --help parser = argparse . ArgumentParser ( description = helptext ) Output usage: argparse_practice.py [-h] [-f bar] optional arguments: -h, --help show this help message and exit -f bar, --foo bar A help string can be provided as a keyword argument to help . parser . add_argument ( \"foo\" , help = \"bar\" ) A data type can be specified as an argument to type : parser . add_argument ( \"foo\" , type = int ) An alternative name for the dest value on the command-line (but which does not affect the identifier of the attribute on which the value is exposed) can be specified by metavar . parser . add_argument ( \"foo\" , metavar = \"bar\" ) The examples above used positional parameters (i.e. an argument). A named parameter (an option or flag, i.e. -h , --help , etc) requires - at the beginning of the string and values from the command-line to be passed after = or Space . add_argument supports the required keyword argument for named parameters. Note that use of the option on the command-line at all requires an argument to it, even if the option is not required itself. parser . add_argument ( '-r' , '--radius' , type = int , required = True , help = 'radius' ) A flag option can be created by defining an action keyword parameter. ( src ) parser . add_argument ( '-o' , '--on' , help = 'A boolean flag' , action = 'store_true' ) add_mutually_exclusive_group() can be used to add a group of mutually exclusive arguments. In this case, add_argument() is invoked on the new object returned by this method and not directly on the ArgumentParser() object. g = ArgumentParser . add_mutually_exclusive_group () g . add_argument ( \"-v\" , \"--verbose\" , action = \"store_true\" ) g . add_argument ( \"-q\" , \"--quiet\" , \"-s\" , \"--silent\" , action = \"store_true\" , help = 'quiet/silent mode' ) User input can be restricted by providing a value for choices , which will accept any iterable value including lists, ranges, and strings: parser . add_argument ( \"foo\" , choices = [ \"bar\" , \"baz\" ]) parser . add_argument ( \"foo\" , choices = range ( 1 , 10 )) parser . add_argument ( \"foo\" , choices = 'Hello, world!' ) # equivalent to ['H','e', ...] Sources Python documentation asyncio The asyncio module offers an implementation of coroutines which allow tasks to control context switching to implement concurrency . The await keyword is a checkpoint that indicates where it is safe for the process to go to another coroutine, allowing total control over context switching import asyncio import time counter = 0 async def func1 (): global counter while True : counter += 1 counter -= 1 await asyncio . sleep ( 0 ) async def func2 (): global counter while True : counter += 1 counter -= 1 await asyncio . sleep ( 0 ) asyncio . gather ( func1 (), func2 ()) asyncio . get_event_loop () . run_forever () async def get_users (): users = await client . do_query ( 'select * from users' ) return users async def main (): task = asyncio . create_task ( get_users ()) # ... await task asyncio . run ( main ()) Allows the joining of multiple threads. async def get_users (): users = await client . do_query ( 'select * from users' ) return users async def main (): await asyncio . gather ( get_users (), get_users (), ) asyncio . run ( main ()) async def get_users (): users = await client . do_query ( 'select * from users' ) return users asyncio . run ( get_users ()) async def main (): users = await get_users () print ( users ) asyncio . run ( main ()) Sources: Demistifying Python's Async and Await keywords azure.cosmos import azure.cosmos from azure.cosmos.partition_key import PartitionKey database = cosmos_client . create_database ( 'RetailDemo' ) container = database . create_container ( id = 'WebsiteData' , partition_key = PartitionKey ( path = '/CartID' )) print ( 'Container WebsiteData created' ) bullet bullet.Check() implements a checkbox widget: cli = bullet . Check ( prompt = \"Choose from the following items: \" , choices = [ 'pepperoni' , 'sausage' , 'green peppers' ]) bullet.Bullet() implements a radio button widget: cli = bullet . Bullet ( prompt = \"Choose from the following items: \" , choices = [ 'red' , 'white' , 'blue' ]) The resulting object then exposes a launch() method. cli . launch () click Click modifies functions using decorators whch determine the command-line arguments elements that the decorated function can see. Hello World program. Click import click @click . command () def hello (): click . echo ( 'Hello World!' ) if __name__ = '__main__' : hello () Modified Hello World import click @click . command () @click . option ( '--count' , default = 1 , help = 'number of greetings' ) @click . argument ( 'name' ) def hello ( count , name ): for x in range ( count ): click . echo ( 'Hello %s !' % name ) if __name__ == '__main__' : hello () Developing the pdfcropper tool; passing --examref changes the numbers. import click @click . command () @click . option ( '--examref' , is_flag = True ) def hello ( examref ): top , right , bottom , left = 0 , 0 , 0 , 0 if examref : top , right , bottom , left = 1 , 2 , 3 , 4 click . echo ( f 'Your numbers are: top ( { top } ), right { right } , bottom { bottom } , left { left } ' ) if __name__ == '__main__' : hello () @click.group() decorators allow nested command groups to be created. There are two ways of adding commands to command groups : - Using the group as a decorator, whereby the name of the function decorated by @click.group() is then used to decorate commands: @click . group () def group1 () pass @group1 . command () def command1 (): pass - Using the add_command method @click . group () def group1 () pass @click . command () def command1 (): pass group1 . add_command ( command1 ) For example, to imitate the nested commands available in netsh : netsh interface ip @click . group () def interface (): pass @interface . command ( 'ip' ) @click . argument ( 'args' , nargs =- 1 ) # All arguments passed in as tuple \"args\" def interface_ip ( args ): pass Docstrings of groups and commands show up as progressive help messages when they are invoked from the command-line. @click . group () def cli (): pass @click . command () def initdb (): click . echo ( 'Initialized the database' ) @click . command () def dropdb (): click . echo ( 'Dropped the database' ) cli . add_command ( initdb ) cli . add_command ( dropdb ) if __name__ == '__main__' : cli () CommandCollection flattens the structure of grouped commands so that the commands in all the contained groups appear in a single tier. It also becomes the entry-point of the script. Example from GitHub : # Three command groups cli1, cli2, and cli3 declared: @click . group () def cli1 (): pass @click . group () def cli2 (): pass @click . group () def cli3 (): pass # Three commands each belonging to a separate group @cli1 . command () def server (): pass @cli2 . command () def console (): pass @cli3 . command () def routes (): pass # CommandCollection flattens the grouped commands such that all the commands are available at once: cli = click . CommandCollection ( sources = [ cli1 , cli2 , cli3 ]) if __name__ == '__main__' : cli () collections abc provides Mapping and MutableMapping ABCs to formalize the interfaces of dict and similar types ChainMap Lookups are performed on each mapping in order Counter Holds an integer count for each key; each new key adds to the count deque : Thread-safe double-ended queue that supports most list methods namedtuple Card = namedtuple ( 'Card' ,[ 'rank' , 'suit' ]) ` City = namedtuple ( 'City' , 'Name Country Population Coordinates' . split ( ' ' )] OrderedDict: Maintains keys in insertion order UserDict: Designed to be subclassed colorama Colorama provides a set of enums that resolve to terminal codes when concatenated with strings. colorama . Fore . GREEN colorama . Style . RESET_ALL csv with open ( 'file.csv' , newline = '' ): data = [ row for row in csv . reader ( f )] csv.DictReader with open ( 'greeks.csv' ) as f : reader = csv . DictReader ( f ) for row in reader : print ( row [ 'name' ], row [ 'city' ], row [ 'dob' ]) datetime datetime . date ( 2016 , 7 , 24 ) datetime . date . today () Difference between datetime objects is a timedelta Parse strings into datetime objects datetime . strptime ( datestring , formatstring ) # Various metacharacters are defined for `strptime` datetime . datetime . strptime ( '06/30/1992' , '%m/ %d /%Y' ) discord.py pip install discord.py client = discord.Client () Client objects expose a decorator that is used for event handlers, functions named after various events: - on_ready - on_member_join - on_error - on_message @client . event async def on_ready (): print ( f ' { client . user } has connected to Discord!' ) Another decorator is exposed for in-chat commands ( commands.Bot has to be instantiated first.) @bot . command ( name = 'roll_dice' , help = 'Simulates rolling dice.' ) async def roll ( ctx , number_of_dice : int , number_of_sides : int ): dice = [ str ( random . choice ( range ( 1 , number_of_sides + 1 ))) for _ in range ( number_of_dice ) ] await ctx . send ( ', ' . join ( dice )) client . run ( token ) bot = comands . Bot ( command_prefix = '!' ) dotenv pip install -U python-dotenv Load a .env file placed in the current working directory. load_dotenv () value = os . getenv ( 'key' ) functools For higher-order functions (functions that act on or return other functions)\\ Apply function of two arguments cumulatively to the items of iterable in order to reduce it to a single value functools . reduce ( function , iterable [, initializer ]) Calculate ((((1+2) +3) +4) +5) functools . reduce ( lambda x , y : x + y , [ 1 , 2 , 3 , 4 , 5 ]) functools.reduce(lambda a,b: a*b, range(1,6)) => 120 : factorial glob Produce a list of strings glob . glob ( '*.py' ) heapq Support heaps , data objects where each node is either greater than or equal to its parent ( max-heap ) or less than or equal to its parent ( min-heap ) Create a heap from {iterable} heapq . heapify ( iterable ) Remove and return the smallest element of {heap} heapq . heappop ( heap ) Replace the smallest element of {heap} with {element} heapq . heapreplace ( heap , element ) http Start an HTTP server for the current directory python http.server itertools cycle() works like next() , but it restarts from the beginning of the iterable that is passed as argument after the last element has been reached. with open ( 'raven' ) as f : raven = [ l for l in f ] itertools . cycle ( raven ) json Parse an open file descriptor with open ( 'file.json' ) as f : data = json . load ( f ) Deserialize a JSON document rawdata that has been loaded data = json . loads ( rawdata ) Write to an open file descriptor with open ( \"file.json\" , \"w\" ) as f : json . dump ( data , f ) Specify indentation with open ( 'file.json' , 'w' ) as f : f . write ( json . dumps ( data , indent = 4 )) logging import logging def main (): logging . basicConfig ( filename = '/tmp/learn-logging.log' , level = logging . ERROR , format = ' %(asctime)s %(levelname)s : %(message)s ' ) logging . info ( \"Once upon a midnight dreary,\" ) logging . warning ( 'While I pondered weak and weary,' ) logging . error ( 'Over many a quaint and curious volume of forgotten lore,' ) if __name__ == '__main__' : main () npyscreen Widget library and application framework built on top of ncurses . Documentation ] Three main types of object compose npyscreen applications: - Application objects manage forms and other classes - Form objects form the canvas upon which widgets are arrayed - Form general-purpose - FormMutt - Widget objects are individual controls - TitleText text entry - TitleSelectOne equivalent to radio buttons - TitleDateCombo allows picking of date on a small calendar npyscreen.wrapper_basic is the main entry point import npyscreen def myFunction ( * args ): pass if __name__ == '__main__' : npyscreen . wrapper_basic ( myFunction ) print \"Blink and you missed it!\" npyscreen.Form is equivalent to the Tk() object, which is typically instantiated as win in GUI frameworks. F = npyscreen . Form ( name = 'My Test Application' ) Several important methods are key: - create() The standard constructor calls this method, which does nothing by default and is meant to be overriden in subclasses. Widgets are defined here. npyscreen.FormMutt imitates a UI layout popularized by applications like mutt , irssi , and vim , with a title bar at the top, a command line at the bottom, and a status line directly above the command line. ACTION_CONTROLLER can be defined in the FormMutt subclass as the name of a subclass of ActionControllerSimple . Commands for the application can be defined as callbacks in the create() method. self . add_action ( ident , call_back , True ) Callbacks are called with the following arguments: call_back ( command_line , control_widget_proxy , live = True ) class ActionControllerSearch ( npyscreen . ActionControllerSimple ): def create ( self ): self . add_action ( '^/.*' , self . set_search , True ) def set_search ( self , command_line , widget_proxy , live ): self . parent . value . set_filter ( command_line [ 1 :]) self . parent . wMain . values = self . parent . value . get () self . parent . wMain . display () class FmSearchActive ( npyscreen . FormMuttActiveTraditional ): ACTION_CONTROLLER = ActionControllerSearch npyscreen.NPSAppManaged is the preferred superclass to support object-oriented implementation. class MyApplication ( npyscreen . NPSAppManaged ): pass Calling run() method of application object as main entry point. run() activates the default form, which should be given an id of MAIN if __name__ == '__main__' : TestApp = MyApplication () . run () print \"All objects, baby.\" Using a try / except block to allow for well-mannered exit in case of KeyboardInterrupt (Ctrl+C) GitHub try : App () . run () except KeyboardInterrupt : sys . exit ( 0 ) There are three methods for registering a Form object with a NPSAppManaged instance; - addForm() creates a new form and returns a weakref.proxy to it - addFormClass() register a class of Form rather than an instance - registerForm() It continually displays the Form named by its NEXT_ACTIVE_FORM attribute. Use the afterEditing method to allow exiting. class myEmployeeForm ( npyscreen . Form ): def afterEditing ( self ): self . parentApp . setNextForm ( None ) numpy numpy.ndarray - 2-dimensional array - items can be fetched using the syntax a[i, j] - arrays can be sliced with syntax a[m:n, k:l] - FP:35 numpy.arange(n) build a numpy.ndarray object with numbers 0 to n-1 (FP:52) numpy.loadtxt(filename) load numbers stored in a text file (FP:53) optparse Instantiate the parser object parser = optparse . OptionParser ( usage = __doc__ . strip ()) # add an option parser . add_option ( '--timeout' ) os Execute shell command given by string. The value returned is actually the exit code, not the output of the command to STDOUT. os . system ( 'ls -la' ) Store output in a variable os . popen ( 'ls -la' ) . read () Navigate filesystem os . getcwd () os . chdir ( path ) Test for existence of a file os . path . isfile ( file ) pandas summary: open-source Python library used for data science operation: runs over NumPy - good for storing lists-of-lists (CSV) print(df) - prints it out in an easy to read tabular format DataFrame is the main object in pandas - head() , tail() - prints out the first, last several rows (5 by default) - optional numerical argument defines number of rows - describe() - numerical analyses, including count, unique, mean, etc - sort_values('field',ascending=False) pathlib Create a new pathlib object; represents a file or directory pathlib . Path ( path ) Test for existence of a file pathlib . Path . is_file ( file ) Test for existence of a directory pathlib . Path . is_dir ( dir ) Find all .py files Returns a generator pathlib . Path . glob ( '*.py' ) Open a file. This makes a file object that is automatically closed, similar to open builtin: pathlib . Path . open () Display file extension pathlib . Path . suffix () Display file size pathlib . Path . stat () . st_size pyinstaller Source: RealPython tutorial Installing PyInstaller , even in a virtual environment, will install the pyinstaller executable to $HOME/.local/bin. On Windows, it is installed to another directory within LOCALAPPDATA . pip install pyinstaller PyInstaller creates primarily 3 items: - .spec file, named after the CLI script - build/ folder, which can be ignored - dist/ folder, containing the final artifact at dist/cli/cli or dist/cli/cli.exe Several options are available hidden-import name onefile pyinstaller script.py --onefile On Windows, if PyInstaller is run from a virtual environment without necessary modules installed, they may not be available for compilation into the artifact. This does not appear to be an issue with Linux. This problem appears to be specific to certain modules, like emoji . pythonnet Docs: ? ! Developers recommend Mono version 5.20.1 Issues 939 On Ubuntu, the eoan universe repository has to be added deb https://archive.ubuntu.com/ubuntu/ eoan universe deb https://archive.ubuntu.com/ubuntu/ eoan-updates universe But I can't figure out how to add the older version, because the recommended syntax produces the error \"Unable to correct problems, you have held broken packages\" sudo apt install mono-devel = 5 .18.0.240+dfsg-3 Maybe try the tarballs on Mono's website... Or maybe there's another repo I don't know about.. apt install clang libglib2.0-dev python3-dev pip install pycparser pythonnet pip install -U setuptools random Random choice with replacement random . choice ( iterable ) Shuffle elements of an iterable in-place [FP:42] random . shuffle ( iterable ) scrapy Best used to obtain one \"stream\" of data at a time, without trying to obtain data from different pages scrapy runspider spider.py -o file.json Display HTML source of the scraped page print ( response . txt ) Get {URL} fetch ( 'url' ) Select a CSS selector # Returns a `SelectorList` response . css ( 'p' ) # Retrieve full HTML elements response . css ( 'p' ) . extract () Retrieve only the text within the element response . css ( 'p::text' ) . extract () response . css ( 'p::text' ) . extract_first () response . css ( 'p::text' ) . extract ()[ 0 ] Get the href attribute value for an anchor tag response . css ( 'a' ) . attrib [ 'href' ] Launch Scrapy shell and scrape $URL scrapy shell $URL Make a default spider named {quotes} that will be restricted to {domain} scrapy genspider quotes domain scrapy runspider scrapy1 . py Run a spider, saving scraped data to a JSON file scrapy runspider spider . py - o items . json Method which contains most of the logic of the spider, especially after the yield keyword. For multiple items, a structural basis for iteration must be found and for each iteration, data is yielded Extract URL from link using standard CSS selection techniques Add the domain name to a relative link response . urljoin () Recursively call the parse method again on the next page yield scrapy . Request ( url = next_page_url , callback = self . parse ) Scrape detail pages - parse_details would be a spider method sibling to the main parse method - if a detail page has more information than the main, then the yield keyword should be in parse_details yield scrapy . Request ( url = { url }, callback = self . parse_details ) setuptools Setuptools is for uploading to PyPi. To create self-contained executable files, use pyinstaller . PROJECT \u251c\u2500\u2500 PROJECT # Additional code files will be placed in here \u2502 \u2514\u2500\u2500 init.py \u2514\u2500\u2500 setup.py # Containing a call to `setuptools.setup()` 1 directory, 2 files setup.py from setuptools import setup setup ( name = 'funniest' , version = '0.1' , description = 'The funniest joke in the world' , url = 'http://github.com/storborg/funniest' , author = 'Flying Circus' , author_email = 'flyingcircus@example.com' , license = 'MIT' , packages = [ 'funniest' ], zip_safe = False ) If the package has dependencies, they can be added by appending a install_requires keyword argument passing an array of the module names setup ( install_requires = [ 'markdown' , ], ) Reserve the name, upload package metadata, and create the pypi.python.org webpage python setup . py register Create a source distribution, producing a tarball inside the top-level directory python setup . py sdist Upload the source distribution python setup . py sdist upload Do all the above in a single step python setup . py register sdist upload socket The socket module is Python's standard interface for the transport layer. Sockets can be classified by family AF_INET Internet AF_UNIX for UNIX sockets and type : - SOCK_STREAM TCP - SOCK_DGRAM UDP These enum values are required upon initialization of a socket object: Ortega : 25 client_socket = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) Sources: Sockets tutorial TCP server import socket with socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) as s : s . bind (( HOST , PORT )) TCP Client import socket with socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) as s : s . connect (( HOST , PORT )) UDP server import socket with socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) as s : s . bind (( HOST , PORT )) UDP client import socket msg = \"Hello, world!\" with socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) as s : s . sendto ( msg . encode (), ( HOST , PORT )) Define port on which to listen for connections. serversocket . bind (( 'localhost' , 80 )) Connect to a remote socket in one direction client_socket . connect (( 'www.packtpub.com' , 80 )) Convert a domain name into IPv4 address socket . gethostbyname ( 'packtpub.com' ) # '83.166.169.231' Defaults to localhost with no arguments s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) s . bind (( socket . gethostname (), 1234 )) Get protocol name from port number socket . getservbyport ( 80 ) # 'http' Listen to a maximum of 10 connections serversocket . listen ( 10 ) Receive bytestream from server msg = s . recv ( 1024 ) print ( msg . decode ( 'utf-8' )) sqlite3 Create a Connect connection object and employee.db (binary) if it doesn't exist conn = sqlite . connect ( 'employee.db' ) Create a Connect.Cursor object c = conn . cursor () Perform SQL commands with Connect.Cursor.execute() . Create tablename with fields field of type type ( null , integer , real , text , blob ); never use Python's native string operations (f-strings, etc) to form commands, because this method is vulnerable to SQL injection. YouTube c . execute ( '''CREATE TABLE {tablename} ( {field} {type} , {field} {type} ...)) Save changes conn . commit () Close connection conn . close () subprocess subprocess modules allows you to spawn new processes, interact with file descriptors, and obtain exit codes. The recommended approach is to use the run() function as default, which runs a CLI command with options as a list of strings and returns a CompletedProcess instance.\\ Execute shell command Unlike os.system , subprocess.run() takes a list of arguments. subprocess . run ([ 'ls' , '-l,' . '], 0) Set capture_output to True to save output, stored as property stdout of the returned object. data = subprocess . run ([ 'ls,' - l ',' . '], 0, capture_output=True) The data is stored as a bytestring , which can be decoded to a normal string. data . stdout . decode ( 'utf-8' ) This return a CompletedProcess instance with the command's output stored under the stdout property subprocess . run ([ 'ls' , '-l' , '/dev/null' ], capture_output = True ) This will raise a CalledProcessError exception because of the non-zero exit code subprocess . run ( 'exit 1' , shell = True , check = True ) sys Return site-specific directory where Python files are installed sys . prefix # /usr/local/ by default tabulate termcolor Print text in a color code termcolor . cprint ( text , color ) threading Docs counter = 0 lock = threading . RLock () def func1 (): global counter while True : with lock : counter += 1 counter -= 1 def func2 (): global counter while True : with lock : counter += 1 counter -= 1 threading . Thrad ( target = func1 ) . start () threading . Thrad ( target = func2 ) . start () counter = 0 def func1 (): global counter while True : counter += 1 counter -= 1 def func2 (): global counter while True : counter += 1 counter -= 1 threading . Thrad ( target = func1 ) . start () threading . Thrad ( target = func2 ) . start () typing As tuples, their attributes are immutable class Starship ( NamedTuple ): name : str registry : str crew : int urllib Download an RFC file from rfc-editor.org Ortega rfc_raw = urllib . request . urlopen ( url ) . read () rfc = rfc_raw . decode () weakref Weak references are references to objects which return exceptions when that object has been garbage collected Create a weak reference to {object} # A weak reference created using `ref` must be dereferenced r = weakref . ref ( obj ) r () . method () r . method () # will not work # A weak reference created using `proxy` does not need to be dereferenced: weakref . proxy ( obj ) winrm Winrm allows you to connect Linux and Windows hosts over WinRM. adamtheautomator.com Begin a WinRM session. If no errors are thrown, the session has been successfully established session = winrm . Session ( ipaddress , auth = ( username , password )) yaml from ruamel.yaml import YAML yaml = YAML () # Adjust indentation levels yaml . indent ( mapping = 4 , sequence = 6 , offset = 3 ) # Print all `data` to stdout yaml . dump ( data , sys . stdout ) with open ( \"file\" ) as f : yaml . dump ( data , f ) Resources: Introduction to YAML Glossary Method resolution order Method resolution order (MRO) is the order of base classes that are searched when using super() . It is accessed with __mro__ , which returns a tuple of base classes in order of precedence, ending in object which is the root class of all classes. ( src ) Non-interactive debugging Non-interactive debugging is the most basic form of debugging, dependent on print or log statements placed within the body of code. Type slot A type slot is any of a number of fields within each magic method, including __new__() , __init__() , and __prepare__() (which returns a dictionary-like object that's used as the local namespace for all code from the class body)","title":"\ud83d\udc0d Python"},{"location":"Coding/Python/#python","text":"","title":"\ud83d\udc0d Python"},{"location":"Coding/Python/#decorators","text":"Sources: Primer on Python Decorators YouTube tutorial A decorator is any function that accepts a function and returns a function. Decorators are one of the main ways that Python implements functional programming principles. Functions are first-class objects and can be passed as parameters. import logging def hello_wrapper ( name , func ): func ( f 'Hello { name } ' ) hello_wrapper ( \"world\" , func = print ) # Hello world hello_wrapper ( \"logs\" , func = logging . warning ) # WARNING:root:Hello logs with open ( 'hello.txt' , 'w' ) as f : hello_wrapper ( 'everyone!' , func = f . write ) import random def anagram ( t ): l = [ c for c in t ] random . shuffle ( l ) print ( \"\" . join ( l )) hello_wrapper ( 'Japushku' , anagram ) # eHoulhluaskpJ The function has to be passed as a reference, actually calling it will cause the wrapper function to attempt to execute the value returned by the inner function. hello_wrapper ( \"world\" , func = print ()) # Error def outer (): print ( 'Hi from the outer function' ) def inner (): print ( 'Hello from the inner function' ) inner () We can use the __name__ attribute to access a passed function's name. def hello ( func ): print ( f 'Hello { func . __name__ } ' ) hello ( outer ) # Hello outer We can also return functions, which can then be invoked def hello ( func ): print ( f 'Hello { func . __name__ } ' ) return func hello ( outer )() ''' Hi from the outer function Hello from the inner function ''' new_outer = hello ( outer ) new_outer is outer # True def wrapper ( func ): print ( f 'Before { func . __name__ } ' ) func () print ( f 'After { func . __name__ } ' ) wrapper ( outer ) ''' Before outer Hi from the outer function Hello from the inner function After outer ''' The true decorator pattern appears here, where wrapper is called the decorator and outer has been decorated . def wrapper ( func ): def _wrapper (): print ( f 'Before { func . __name__ } ' ) func () print ( f 'After { func . __name__ } ' ) return _wrapper outer = wrapper ( outer ) But the usual syntax since Python 2.4 is to place the decorator on the line above the decorated function, preceded by @ : @wrapper def outer (): print ( 'Hi from the outer function' ) def inner (): print ( 'Hello from the inner function' ) inner () _wrapper here does not accept any positional arguments, so wrapping functions that take arguments will produce a TypeError @wrapper def say_hello ( name ): print ( f 'Hello { name } !' ) # error The solution is to incorporate *args, **kwargs into the definition of the inner function, as well as the invocation of the function passed in. def wrapper ( func ): def _wrapper ( * args , ** kwargs ): print ( f 'Before { func . __name__ } ' ) func ( * args , ** kwargs ) print ( f 'After { func . __name__ } ' ) return _wrapper Returned values are not captured yet: def wrapper ( func ): def _wrapper ( * args , ** kwargs ): print ( f 'Before { func . __name__ } ' ) value = func ( * args , ** kwargs ) print ( f 'After { func . __name__ } ' ) return value return _wrapper Inspecting the decorated function's __name__ attribute reveals that it is still named _wrapper say_hello . __name__ # '_wrapper' This is also true for other attributes, including docstring. functools.wraps is a decorator factory to reassign attributes to the wrapped function. This is considered superior to the functools.update_wrapper function which is also available. def wrapper ( func ): @functools . wraps ( func ) def _wrapper ( * args , ** kwargs ): print ( f 'Before { func . __name__ } ' ) value = func ( * args , ** kwargs ) print ( f 'After { func . __name__ } ' ) return value return _wrapper This forms an ideal starting template for the creation of custom decorators.","title":"Decorators"},{"location":"Coding/Python/#classes","text":"","title":"Classes"},{"location":"Coding/Python/#properties","text":"In the Python documentation , attributes accessed with accessor functions are called managed attributes , which makes the term equivalent to properties in C#. Three methods can be defined using the @property decorator Constructor def __init__ ( self , price ): self . _price = price Getter @property def price ( self ): return self . _price Setter @price . setter def price ( self , new_price ): if new_price > 0 : self . _price = new_price else : raise ValueError Deleter @price . deleter def price ( self ): del self . _price","title":"Properties"},{"location":"Coding/Python/#class-methods","text":"The @classmethod decorator prevents the interpreter from passing in the instantiated object using self , rather the class itself is passed in as the cls argument. This means that the methods decorated as such must take not self as the first argument but cls @classmethod def classmethod ( cls ): pass The @staticmethod decorator prevents the interpreter from passing any additional arguments whatsoever. The resulting method has no access to the object itself nor the class and functions like a procedurally defined function.","title":"Class methods"},{"location":"Coding/Python/#formatting","text":"flake8 , black , and yapf (Google) are CLI tools used to automatically format Python code.","title":"Formatting"},{"location":"Coding/Python/#web-frameworks","text":"","title":"Web frameworks"},{"location":"Coding/Python/#django","text":"A typical Django project contains multiple apps , which are Python packages containing their own models, views, templates, and urls. A model is the single definitive source of information about your data, and contains the essential fields and behaviors of the data you're storing. Migrations are necessary when Model classes are updated. And for projects sufficiently advanced, migration scripts must be developed for any such changes. Async Server Gateway Interface (ASGI) is the spiritual successor to, and superset of, WSGI . It implements the new Python standard for asynchronous web servers and applications, which resembles that of websockets . From WSGI to ASGI WSGI is coupled tightly with the synchronous request-response model familiar from HTTP 1.1. URL patterns (stored in the urlpatterns list defined in the project-level urls.py file) can be parameterized. Here, the template <int:x> specifies an integer to be assigned to the view parameter x . from app.views import my_view urlpatterns = [ path ( '/example/<int:x>' , my_view ) ] modelform_factory can be used to automatically produce a webform from a Model class. # views.py MeetingForm = modelform_factory ( Meeting , exclude = []) This can be placed into a template using the {{ form }} template tag. Note, a {% csrf_token %} template tag must also be present for a submit button to work. { % block content % } < h1 > Plan a new meeting </ h1 > < form method = \"POST\" > < table > {{ form }} </ table > { % csrf_token % } < button type = \"submit\" > Create </ button > </ form > { % endblock content % } When the modelform_factory class has been defined, it is instantiated within the view function . This object exposes several methods: - is_valid data validation is strongly recommended for any form input - save imports the validated form data into the database def new ( request ): if request . method == 'POST' : form = MeetingForm ( request . POST ) if form . is_valid (): form . save () return redirect ( \"home\" ) else : form = MeetingForm () return render ( request , \"meetings/new.html\" , { \"form\" : form })","title":"Django"},{"location":"Coding/Python/#template","text":"Django templates are HTML files with additional markup to signify places where data can be dynamically inserted. The data used by the views file is called the template context . Templates must be placed within the /templates folder within the app, and it is considered best practice to place templates within a nested subdirectory within it, e.g. /templates/app . Django template tags are specified beween {% .. %} and allow for interpolation of data. < ul > { % for m in meetings % } { % endfor % } </ ul > URLs can be built by using the url template tag, specifying the name of a URL urlpatterns = [ path ( '' , home , name = 'home' ) ] < a href = \"{ % u rl 'home' %}\" > Home </ a >","title":"Template"},{"location":"Coding/Python/#models","text":"Models In Django, a Model class is mapped to a database table. Each object is a record in that table. Model objects expose several attributes and methods Get all objects meetings = Meeting . objects . all () Get count of objects in database count = Meeting . objects . count () Query for a specific object meeting = Meeting . objects . get ( pk = id ) get_object_or_404 may be better for most cases meeting = get_object_or_404 ( Meeting , pk = id ) Adding a new app python manage.py startapp app Then add to settings.py in project directory INSTALLED_APPS = [ # ... 'app' , ] There appears to be much flexibility in the arrangement of input controls in a form. So long as the Submit button is child to the form element, tasks are accepted in the To-Do app. Per Bulma documentation, the field class is intended as a container for label.label s, .control s, and optional p.help text. In contrast, control is a block container meant to enhance single form controls and can only contain input , select , button , or icon elements. form .field ( method= \"POST\" , action= \"/\" ) label .label Enter something to do .control | {{form.title}} | {% csrf_token %} button .button.is-primary ( type= \"submit\" ) Submit","title":"Models"},{"location":"Coding/Python/#fastapi","text":"Variables values can be taken from the route or from query parameters following a question mark. Routes from fastapi import FastAPI starships = FastAPI () @starships . get ( \"/starships/ {registry} \" ) def get_starship ( name : str ): return { \"response\" : f \"Hello, { name } \" } Query parameters from fastapi import FastAPI starships = FastAPI () @starships . get ( \"/\" ) def get_starship ( name : str = \"world\" ): return { \"response\" : f \"Hello, { name } \" } FastAPI is notable for being able to use type hints to construct data models, which are much lighter than the object relational models used by other frameworks. FastAPI from pydantic import BaseModel class Starship ( BaseModel ): name : str registry : str crew : int Django from django.db import models class Starship ( models . Model ): name = models . CharField ( max_length = 50 ) registry = models . CharField ( max_length = 15 ) crew = models . IntegerField () Dogfood data can be incorporated by using the keyword argument unpacking or \"double splat\" operator ( ** ) data = { \"name\" : \"USS Enterprise\" , \"registry\" : \"NCC-1701\" , \"crew\" : 203 } enterprise = Starship ( ** data ) POST method definitions then can use this newly defined class to validate posted data db = [] @app . post ( \"/starships\" ) async def create_starship ( starship : Starship ): db . append ( starship ) FastAPI supports Jinja templates to serve HTML templates import fastapi from fastapi.templating import Jinja2Templates # specifies the directory where templates are to be found templates = Jinja2Templates ( \"templates\" ) api = fastapi . APIRouter () @api . get ( '/' ) def index ( request : starlette . requests . Request ): return templates . TemplateResponse ( \"helloworld.html\" , { \"request\" : request }) By default, FastAPI also exposes web applications at /docs where you can test out all the exposed API methods. FastAPI integrates with ASGI servers like Uvicorn and Hypercorn, which can run a specific web application by name from the command-line or from within the script: Shell uvicorn main:starships --port 7000 Python import uvicorn uvicorn . run ( starships , port = 7000 )","title":"FastAPI"},{"location":"Coding/Python/#virtual-environments","text":"","title":"Virtual environments"},{"location":"Coding/Python/#pipenv","text":"pipenv --python 3 .6","title":"pipenv"},{"location":"Coding/Python/#venv","text":"Create a virtual environment named project python -m venv project","title":"venv"},{"location":"Coding/Python/#virtualenv","text":"Create a virtual environment named project using a different version of Python virtualenv -p /usr/bin/python2 project","title":"virtualenv"},{"location":"Coding/Python/#testing","text":"Pytest is a popular testing framework preferred to unittest by many Python developers because it follows Pythonic conventions more closely. In contrast to unittest's custom methods, pytest relies on the builtin assert statement. pytest from phonebook import PhoneBook import pytest @pytest . fixture def phonebook (): phonebook = PhoneBook () yield phonebook phonebook . clear () def test_lookup_by_name ( phonebook ): phonebook . add ( \"Bob\" , \"1234\" ) assert \"1234\" == phonebook . lookup ( \"Bob\" ) def test_phonebook_contains_all_names ( phonebook ): phonebook . add ( \"Bob\" , \"1234\" ) assert \"Bob\" in phonebook . names () def test_missing_name_raises_error ( phonebook ): with pytest . raises ( KeyError ): phonebook . lookup ( \"Bob\" ) python -m pytest unittest import unittest from phonebook import PhoneBook class PhoneBookTest ( unittest . TestCase ): def test_lookup_by_name ( self ): self . phonebook . add ( \"Bob\" , \"12345\" ) number = self . phonebook . lookup ( \"Bob\" ) self . assertEqual ( \"12345\" , number ) def test_missing_name ( self ): with self . assertRaises ( KeyError ): self . phonebook . lookup ( \"missing\" ) def test_empty_phonebook_is_consistent ( self ): self . assertTrue ( self . phonebook . is_consistent ()) def setUp ( self ) -> None : self . phonebook = PhoneBook () def tearDown ( self ) -> None : self . phonebook . clear () python -m unittest Class under test import os class PhoneBook : def __init__ ( self , cache_directory = os . getcwd ()): self . numbers = {} self . filename = os . path . join ( cache_directory , \"phonebook.txt\" ) self . cache = open ( self . filename , \"w\" ) def add ( self , name , number ): self . numbers [ name ] = number def lookup ( self , name ): return self . numbers [ name ] def is_consistent ( self ): return True def names ( self ): return set ( self . numbers . keys ()) def clear ( self ): self . cache . close () os . remove ( self . filename )","title":"Testing"},{"location":"Coding/Python/#doctest","text":"A doctest is a docstring containing what looks like interactive Python sessions. Python Docs \"\"\" Return the factorial of n, an exact integer >= 0. >>> [factorial(n) for n in range(6)] [1, 1, 2, 6, 24, 120] >>> factorial(30) 265252859812191058636308480000000 >>> factorial(-1) Traceback (most recent call last): ... ValueError: n must be >= 0 Factorials of floats are OK, but the float must be an exact integer: >>> factorial(30.1) Traceback (most recent call last): ... ValueError: n must be exact integer >>> factorial(30.0) 265252859812191058636308480000000 It must also not be ridiculously large: >>> factorial(1e100) Traceback (most recent call last): ... OverflowError: n too large \"\"\" This can then be run if __name__ == '__main__' : import doctest doctest . testmod ()","title":"Doctest"},{"location":"Coding/Python/#pytest","text":"PyTest relies on the built-in assert statement.","title":"pytest"},{"location":"Coding/Python/#fixtures","text":"The @pytest.fixture decorator facilitiates the creation of test fixtures. The fixture function's name is used as argument to the test case, and the value returned can be used by the logic within. ( src ) Any clean-up logic can be invoked in this fixture as well by replacing return with yield . Pytest also provides its own tmpdir test fixture for temporary directories. ( src ) Before from phonebook import PhoneBook import pytest def test_lookup_by_name ( phonebook ): phonebook = PhoneBook () phonebook . add ( \"Bob\" , \"1234\" ) assert \"1234\" == phonebook . lookup ( \"Bob\" ) def test_phonebook_contains_all_names ( phonebook ): phonebook = PhoneBook () phonebook . add ( \"Bob\" , \"1234\" ) assert \"Bob\" in phonebook . names () def test_missing_name_raises_error ( phonebook ): phonebook = PhoneBook () with pytest . raises ( KeyError ): phonebook . lookup ( \"Bob\" ) After from phonebook import PhoneBook import pytest @pytest . fixture def phonebook (): phonebook = PhoneBook () yield phonebook phonebook . clear () def test_lookup_by_name ( phonebook ): # phonebook = PhoneBook() phonebook . add ( \"Bob\" , \"1234\" ) assert \"1234\" == phonebook . lookup ( \"Bob\" ) def test_phonebook_contains_all_names ( phonebook ): # phonebook = PhoneBook() phonebook . add ( \"Bob\" , \"1234\" ) assert \"Bob\" in phonebook . names () def test_missing_name_raises_error ( phonebook ): # phonebook = PhoneBook() with pytest . raises ( KeyError ): phonebook . lookup ( \"Bob\" ) tmpdir from phonebook import PhoneBook import pytest @pytest . fixture def phonebook ( tmpdir ): phonebook = PhoneBook ( tmpdir ) return phonebook def test_lookup_by_name ( phonebook ): # phonebook = PhoneBook() phonebook . add ( \"Bob\" , \"1234\" ) assert \"1234\" == phonebook . lookup ( \"Bob\" ) def test_phonebook_contains_all_names ( phonebook ): # phonebook = PhoneBook() phonebook . add ( \"Bob\" , \"1234\" ) assert \"Bob\" in phonebook . names () def test_missing_name_raises_error ( phonebook ): # phonebook = PhoneBook() with pytest . raises ( KeyError ): phonebook . lookup ( \"Bob\" )","title":"Fixtures"},{"location":"Coding/Python/#unittest","text":"unittest is a testing framework built into Python's Standard Library that was based on JUnit. unittest came out in 2001, when JUnit was being ported and adapted to many languages. Collectively, these frameworks were referred to as the xUnit family . unittest's method names do not follow Python conventions because it predates the PEP-8 naming standard. unittest allows you to create test classes that inherit from TestCase .","title":"unittest"},{"location":"Coding/Python/#assertions","text":"Assertions are implemented in individual methods of the TestCase subclass through unittest methods like assertEqual and assertRaises , etc. Notably, TestCase subclasses must not have an __init__() constructor method defined. def test_lookup_by_name ( self ): phonebook = PhoneBook () phonebook . add ( \"Bob\" , \"12345\" ) number = phonebook . lookup ( \"Bob\" ) self . assertEqual ( \"12345\" , number ) assertRaises must be placed in a context manager. Here, the test case will run the code within the with block and check to make sure it raises the specified exception: KeyError : ( src ) def test_missing_name ( self ): fleet = Fleet () with self . assertRaises ( KeyError ): fleet . lookup ( \"bla\" )","title":"Assertions"},{"location":"Coding/Python/#fixtures_1","text":"setUp is run before every test method, allowing a test fixture to be created to avoid repetitive code. tearDown is called after every method, which allows these resources to be released, even if the test case raises an exception. However, if it is setUp that raises the exception, then neither the test case nor tearDown will run. ( src , src ) Before import unittest from phonebook import PhoneBook class PhoneBookTest ( unittest . TestCase ): def test_lookup_by_name ( self ): phonebook = PhoneBook () phonebook . add ( \"Bob\" , \"12345\" ) number = phonebook . lookup ( \"Bob\" ) self . assertEqual ( \"12345\" , number ) def test_missing_name ( self ): phonebook = PhoneBook () with self . assertRaises ( KeyError ): phonebook . lookup ( \"missing\" ) @unittest . skip ( \"WIP\" ) def test_empty_phonebook_is_consistent ( self ): phonebook = PhoneBook () self . assertTrue ( phonebook . is_consistent ()) After import unittest from phonebook import PhoneBook class PhoneBookTest ( unittest . TestCase ): def setUp ( self ) -> None : self . phonebook = PhoneBook () def tearDown ( self ) -> None : self . phonebook . clear () def test_lookup_by_name ( self ): # phonebook = PhoneBook() self . phonebook . add ( \"Bob\" , \"12345\" ) number = self . phonebook . lookup ( \"Bob\" ) self . assertEqual ( \"12345\" , number ) def test_missing_name ( self ): # phonebook = PhoneBook() with self . assertRaises ( KeyError ): self . phonebook . lookup ( \"missing\" ) @unittest . skip ( \"WIP\" ) def test_empty_phonebook_is_consistent ( self ): # phonebook = PhoneBook() self . assertTrue ( self . phonebook . is_consistent ()) The @unittest.skip decorator will tell the test runner to skip the decorated test case ( src ) @unittest . skip ( \"WIP\" ) def test_empty_phonebook_is_consistent ( self ): phonebook = PhoneBook () self . assertTrue ( phonebook . is_consistent ()) The command line entry point is made with a call to unittest.main() , which executes the tests. ( src ) import unittest from my_sum import sum class TestSum ( unittest . TestCase ): def test_list_int ( self ): \"\"\" Test that it can sum a list of integers \"\"\" data = [ 1 , 2 , 3 ] result = sum ( data ) self . assertEqual ( result , 6 ) if __name__ == '__main__' : unittest . main ()","title":"Fixtures"},{"location":"Coding/Python/#integration-tests","text":"By convention, tests are put in their own directory as sibling to the main module ( in order to be able to import it ). Integration and unit tests should be organized separately. . \u251c\u2500\u2500 project \u2502 \u2514\u2500\u2500 __init__.py \u2514\u2500\u2500 tests \u251c\u2500\u2500 integration \u2514\u2500\u2500 unit Run all integration tests within specified directory. python -m unittest discover -s tests/integration","title":"Integration tests"},{"location":"Coding/Python/#modules","text":"When learning unfamiliar packages and importing them in a demonstration script, care must be taken that the demonstration script does not have the same name as the package being studied. If so, attempting to import the package while in an interpreter within that directory will cause the interpreter to try importing the incomplete script and not the package. When running a Python interpreter within this directory, the files \"calc\" and \"main\" can be imported as modules by specifying their names with no file extension. . \u251c\u2500\u2500 calc.py \u2514\u2500\u2500 main.py import calc # No errors import main # No errors Specifying the full filename including extension does produce an error import calc.py # Error import main.py # Error","title":"Modules"},{"location":"Coding/Python/#argparse","text":"The ArgumentParser object exposes an attribute that contains the value passed in from the command-line. This attribute takes its identifier from the dest keyword argument when invoking the add_argument() method. Python import argparse def get_args (): parser = argparse . ArgumentParser () parser . add_argument ( dest = 'bar' ) return parser . parse_args () def main (): args = get_args () . bar The optional value assigned to description will be displayed when running the script with the options -h or --help parser = argparse . ArgumentParser ( description = helptext ) Output usage: argparse_practice.py [-h] [-f bar] optional arguments: -h, --help show this help message and exit -f bar, --foo bar A help string can be provided as a keyword argument to help . parser . add_argument ( \"foo\" , help = \"bar\" ) A data type can be specified as an argument to type : parser . add_argument ( \"foo\" , type = int ) An alternative name for the dest value on the command-line (but which does not affect the identifier of the attribute on which the value is exposed) can be specified by metavar . parser . add_argument ( \"foo\" , metavar = \"bar\" ) The examples above used positional parameters (i.e. an argument). A named parameter (an option or flag, i.e. -h , --help , etc) requires - at the beginning of the string and values from the command-line to be passed after = or Space . add_argument supports the required keyword argument for named parameters. Note that use of the option on the command-line at all requires an argument to it, even if the option is not required itself. parser . add_argument ( '-r' , '--radius' , type = int , required = True , help = 'radius' ) A flag option can be created by defining an action keyword parameter. ( src ) parser . add_argument ( '-o' , '--on' , help = 'A boolean flag' , action = 'store_true' ) add_mutually_exclusive_group() can be used to add a group of mutually exclusive arguments. In this case, add_argument() is invoked on the new object returned by this method and not directly on the ArgumentParser() object. g = ArgumentParser . add_mutually_exclusive_group () g . add_argument ( \"-v\" , \"--verbose\" , action = \"store_true\" ) g . add_argument ( \"-q\" , \"--quiet\" , \"-s\" , \"--silent\" , action = \"store_true\" , help = 'quiet/silent mode' ) User input can be restricted by providing a value for choices , which will accept any iterable value including lists, ranges, and strings: parser . add_argument ( \"foo\" , choices = [ \"bar\" , \"baz\" ]) parser . add_argument ( \"foo\" , choices = range ( 1 , 10 )) parser . add_argument ( \"foo\" , choices = 'Hello, world!' ) # equivalent to ['H','e', ...] Sources Python documentation","title":"argparse"},{"location":"Coding/Python/#asyncio","text":"The asyncio module offers an implementation of coroutines which allow tasks to control context switching to implement concurrency . The await keyword is a checkpoint that indicates where it is safe for the process to go to another coroutine, allowing total control over context switching import asyncio import time counter = 0 async def func1 (): global counter while True : counter += 1 counter -= 1 await asyncio . sleep ( 0 ) async def func2 (): global counter while True : counter += 1 counter -= 1 await asyncio . sleep ( 0 ) asyncio . gather ( func1 (), func2 ()) asyncio . get_event_loop () . run_forever () async def get_users (): users = await client . do_query ( 'select * from users' ) return users async def main (): task = asyncio . create_task ( get_users ()) # ... await task asyncio . run ( main ()) Allows the joining of multiple threads. async def get_users (): users = await client . do_query ( 'select * from users' ) return users async def main (): await asyncio . gather ( get_users (), get_users (), ) asyncio . run ( main ()) async def get_users (): users = await client . do_query ( 'select * from users' ) return users asyncio . run ( get_users ()) async def main (): users = await get_users () print ( users ) asyncio . run ( main ()) Sources: Demistifying Python's Async and Await keywords","title":"asyncio"},{"location":"Coding/Python/#azurecosmos","text":"import azure.cosmos from azure.cosmos.partition_key import PartitionKey database = cosmos_client . create_database ( 'RetailDemo' ) container = database . create_container ( id = 'WebsiteData' , partition_key = PartitionKey ( path = '/CartID' )) print ( 'Container WebsiteData created' )","title":"azure.cosmos  "},{"location":"Coding/Python/#bullet","text":"bullet.Check() implements a checkbox widget: cli = bullet . Check ( prompt = \"Choose from the following items: \" , choices = [ 'pepperoni' , 'sausage' , 'green peppers' ]) bullet.Bullet() implements a radio button widget: cli = bullet . Bullet ( prompt = \"Choose from the following items: \" , choices = [ 'red' , 'white' , 'blue' ]) The resulting object then exposes a launch() method. cli . launch ()","title":"bullet"},{"location":"Coding/Python/#click","text":"Click modifies functions using decorators whch determine the command-line arguments elements that the decorated function can see. Hello World program. Click import click @click . command () def hello (): click . echo ( 'Hello World!' ) if __name__ = '__main__' : hello () Modified Hello World import click @click . command () @click . option ( '--count' , default = 1 , help = 'number of greetings' ) @click . argument ( 'name' ) def hello ( count , name ): for x in range ( count ): click . echo ( 'Hello %s !' % name ) if __name__ == '__main__' : hello () Developing the pdfcropper tool; passing --examref changes the numbers. import click @click . command () @click . option ( '--examref' , is_flag = True ) def hello ( examref ): top , right , bottom , left = 0 , 0 , 0 , 0 if examref : top , right , bottom , left = 1 , 2 , 3 , 4 click . echo ( f 'Your numbers are: top ( { top } ), right { right } , bottom { bottom } , left { left } ' ) if __name__ == '__main__' : hello () @click.group() decorators allow nested command groups to be created. There are two ways of adding commands to command groups : - Using the group as a decorator, whereby the name of the function decorated by @click.group() is then used to decorate commands: @click . group () def group1 () pass @group1 . command () def command1 (): pass - Using the add_command method @click . group () def group1 () pass @click . command () def command1 (): pass group1 . add_command ( command1 ) For example, to imitate the nested commands available in netsh : netsh interface ip @click . group () def interface (): pass @interface . command ( 'ip' ) @click . argument ( 'args' , nargs =- 1 ) # All arguments passed in as tuple \"args\" def interface_ip ( args ): pass Docstrings of groups and commands show up as progressive help messages when they are invoked from the command-line. @click . group () def cli (): pass @click . command () def initdb (): click . echo ( 'Initialized the database' ) @click . command () def dropdb (): click . echo ( 'Dropped the database' ) cli . add_command ( initdb ) cli . add_command ( dropdb ) if __name__ == '__main__' : cli () CommandCollection flattens the structure of grouped commands so that the commands in all the contained groups appear in a single tier. It also becomes the entry-point of the script. Example from GitHub : # Three command groups cli1, cli2, and cli3 declared: @click . group () def cli1 (): pass @click . group () def cli2 (): pass @click . group () def cli3 (): pass # Three commands each belonging to a separate group @cli1 . command () def server (): pass @cli2 . command () def console (): pass @cli3 . command () def routes (): pass # CommandCollection flattens the grouped commands such that all the commands are available at once: cli = click . CommandCollection ( sources = [ cli1 , cli2 , cli3 ]) if __name__ == '__main__' : cli ()","title":"click"},{"location":"Coding/Python/#collections","text":"abc provides Mapping and MutableMapping ABCs to formalize the interfaces of dict and similar types ChainMap Lookups are performed on each mapping in order Counter Holds an integer count for each key; each new key adds to the count deque : Thread-safe double-ended queue that supports most list methods namedtuple Card = namedtuple ( 'Card' ,[ 'rank' , 'suit' ]) ` City = namedtuple ( 'City' , 'Name Country Population Coordinates' . split ( ' ' )] OrderedDict: Maintains keys in insertion order UserDict: Designed to be subclassed","title":"collections"},{"location":"Coding/Python/#colorama","text":"Colorama provides a set of enums that resolve to terminal codes when concatenated with strings. colorama . Fore . GREEN colorama . Style . RESET_ALL","title":"colorama"},{"location":"Coding/Python/#csv","text":"with open ( 'file.csv' , newline = '' ): data = [ row for row in csv . reader ( f )] csv.DictReader with open ( 'greeks.csv' ) as f : reader = csv . DictReader ( f ) for row in reader : print ( row [ 'name' ], row [ 'city' ], row [ 'dob' ])","title":"csv"},{"location":"Coding/Python/#datetime","text":"datetime . date ( 2016 , 7 , 24 ) datetime . date . today () Difference between datetime objects is a timedelta Parse strings into datetime objects datetime . strptime ( datestring , formatstring ) # Various metacharacters are defined for `strptime` datetime . datetime . strptime ( '06/30/1992' , '%m/ %d /%Y' )","title":"datetime"},{"location":"Coding/Python/#discordpy","text":"pip install discord.py client = discord.Client () Client objects expose a decorator that is used for event handlers, functions named after various events: - on_ready - on_member_join - on_error - on_message @client . event async def on_ready (): print ( f ' { client . user } has connected to Discord!' ) Another decorator is exposed for in-chat commands ( commands.Bot has to be instantiated first.) @bot . command ( name = 'roll_dice' , help = 'Simulates rolling dice.' ) async def roll ( ctx , number_of_dice : int , number_of_sides : int ): dice = [ str ( random . choice ( range ( 1 , number_of_sides + 1 ))) for _ in range ( number_of_dice ) ] await ctx . send ( ', ' . join ( dice )) client . run ( token ) bot = comands . Bot ( command_prefix = '!' )","title":"discord.py"},{"location":"Coding/Python/#dotenv","text":"pip install -U python-dotenv Load a .env file placed in the current working directory. load_dotenv () value = os . getenv ( 'key' )","title":"dotenv"},{"location":"Coding/Python/#functools","text":"For higher-order functions (functions that act on or return other functions)\\ Apply function of two arguments cumulatively to the items of iterable in order to reduce it to a single value functools . reduce ( function , iterable [, initializer ]) Calculate ((((1+2) +3) +4) +5) functools . reduce ( lambda x , y : x + y , [ 1 , 2 , 3 , 4 , 5 ]) functools.reduce(lambda a,b: a*b, range(1,6)) => 120 : factorial","title":"functools"},{"location":"Coding/Python/#glob","text":"Produce a list of strings glob . glob ( '*.py' )","title":"glob"},{"location":"Coding/Python/#heapq","text":"Support heaps , data objects where each node is either greater than or equal to its parent ( max-heap ) or less than or equal to its parent ( min-heap ) Create a heap from {iterable} heapq . heapify ( iterable ) Remove and return the smallest element of {heap} heapq . heappop ( heap ) Replace the smallest element of {heap} with {element} heapq . heapreplace ( heap , element )","title":"heapq"},{"location":"Coding/Python/#http","text":"Start an HTTP server for the current directory python http.server","title":"http"},{"location":"Coding/Python/#itertools","text":"cycle() works like next() , but it restarts from the beginning of the iterable that is passed as argument after the last element has been reached. with open ( 'raven' ) as f : raven = [ l for l in f ] itertools . cycle ( raven )","title":"itertools"},{"location":"Coding/Python/#json","text":"Parse an open file descriptor with open ( 'file.json' ) as f : data = json . load ( f ) Deserialize a JSON document rawdata that has been loaded data = json . loads ( rawdata ) Write to an open file descriptor with open ( \"file.json\" , \"w\" ) as f : json . dump ( data , f ) Specify indentation with open ( 'file.json' , 'w' ) as f : f . write ( json . dumps ( data , indent = 4 ))","title":"json"},{"location":"Coding/Python/#logging","text":"import logging def main (): logging . basicConfig ( filename = '/tmp/learn-logging.log' , level = logging . ERROR , format = ' %(asctime)s %(levelname)s : %(message)s ' ) logging . info ( \"Once upon a midnight dreary,\" ) logging . warning ( 'While I pondered weak and weary,' ) logging . error ( 'Over many a quaint and curious volume of forgotten lore,' ) if __name__ == '__main__' : main ()","title":"logging"},{"location":"Coding/Python/#npyscreen","text":"Widget library and application framework built on top of ncurses . Documentation ] Three main types of object compose npyscreen applications: - Application objects manage forms and other classes - Form objects form the canvas upon which widgets are arrayed - Form general-purpose - FormMutt - Widget objects are individual controls - TitleText text entry - TitleSelectOne equivalent to radio buttons - TitleDateCombo allows picking of date on a small calendar npyscreen.wrapper_basic is the main entry point import npyscreen def myFunction ( * args ): pass if __name__ == '__main__' : npyscreen . wrapper_basic ( myFunction ) print \"Blink and you missed it!\" npyscreen.Form is equivalent to the Tk() object, which is typically instantiated as win in GUI frameworks. F = npyscreen . Form ( name = 'My Test Application' ) Several important methods are key: - create() The standard constructor calls this method, which does nothing by default and is meant to be overriden in subclasses. Widgets are defined here. npyscreen.FormMutt imitates a UI layout popularized by applications like mutt , irssi , and vim , with a title bar at the top, a command line at the bottom, and a status line directly above the command line. ACTION_CONTROLLER can be defined in the FormMutt subclass as the name of a subclass of ActionControllerSimple . Commands for the application can be defined as callbacks in the create() method. self . add_action ( ident , call_back , True ) Callbacks are called with the following arguments: call_back ( command_line , control_widget_proxy , live = True ) class ActionControllerSearch ( npyscreen . ActionControllerSimple ): def create ( self ): self . add_action ( '^/.*' , self . set_search , True ) def set_search ( self , command_line , widget_proxy , live ): self . parent . value . set_filter ( command_line [ 1 :]) self . parent . wMain . values = self . parent . value . get () self . parent . wMain . display () class FmSearchActive ( npyscreen . FormMuttActiveTraditional ): ACTION_CONTROLLER = ActionControllerSearch npyscreen.NPSAppManaged is the preferred superclass to support object-oriented implementation. class MyApplication ( npyscreen . NPSAppManaged ): pass Calling run() method of application object as main entry point. run() activates the default form, which should be given an id of MAIN if __name__ == '__main__' : TestApp = MyApplication () . run () print \"All objects, baby.\" Using a try / except block to allow for well-mannered exit in case of KeyboardInterrupt (Ctrl+C) GitHub try : App () . run () except KeyboardInterrupt : sys . exit ( 0 ) There are three methods for registering a Form object with a NPSAppManaged instance; - addForm() creates a new form and returns a weakref.proxy to it - addFormClass() register a class of Form rather than an instance - registerForm() It continually displays the Form named by its NEXT_ACTIVE_FORM attribute. Use the afterEditing method to allow exiting. class myEmployeeForm ( npyscreen . Form ): def afterEditing ( self ): self . parentApp . setNextForm ( None )","title":"npyscreen"},{"location":"Coding/Python/#numpy","text":"numpy.ndarray - 2-dimensional array - items can be fetched using the syntax a[i, j] - arrays can be sliced with syntax a[m:n, k:l] - FP:35 numpy.arange(n) build a numpy.ndarray object with numbers 0 to n-1 (FP:52) numpy.loadtxt(filename) load numbers stored in a text file (FP:53)","title":"numpy"},{"location":"Coding/Python/#optparse","text":"Instantiate the parser object parser = optparse . OptionParser ( usage = __doc__ . strip ()) # add an option parser . add_option ( '--timeout' )","title":"optparse"},{"location":"Coding/Python/#os","text":"Execute shell command given by string. The value returned is actually the exit code, not the output of the command to STDOUT. os . system ( 'ls -la' ) Store output in a variable os . popen ( 'ls -la' ) . read () Navigate filesystem os . getcwd () os . chdir ( path ) Test for existence of a file os . path . isfile ( file )","title":"os"},{"location":"Coding/Python/#pandas","text":"summary: open-source Python library used for data science operation: runs over NumPy - good for storing lists-of-lists (CSV) print(df) - prints it out in an easy to read tabular format DataFrame is the main object in pandas - head() , tail() - prints out the first, last several rows (5 by default) - optional numerical argument defines number of rows - describe() - numerical analyses, including count, unique, mean, etc - sort_values('field',ascending=False)","title":"pandas"},{"location":"Coding/Python/#pathlib","text":"Create a new pathlib object; represents a file or directory pathlib . Path ( path ) Test for existence of a file pathlib . Path . is_file ( file ) Test for existence of a directory pathlib . Path . is_dir ( dir ) Find all .py files Returns a generator pathlib . Path . glob ( '*.py' ) Open a file. This makes a file object that is automatically closed, similar to open builtin: pathlib . Path . open () Display file extension pathlib . Path . suffix () Display file size pathlib . Path . stat () . st_size","title":"pathlib"},{"location":"Coding/Python/#pyinstaller","text":"Source: RealPython tutorial Installing PyInstaller , even in a virtual environment, will install the pyinstaller executable to $HOME/.local/bin. On Windows, it is installed to another directory within LOCALAPPDATA . pip install pyinstaller PyInstaller creates primarily 3 items: - .spec file, named after the CLI script - build/ folder, which can be ignored - dist/ folder, containing the final artifact at dist/cli/cli or dist/cli/cli.exe Several options are available hidden-import name onefile pyinstaller script.py --onefile On Windows, if PyInstaller is run from a virtual environment without necessary modules installed, they may not be available for compilation into the artifact. This does not appear to be an issue with Linux. This problem appears to be specific to certain modules, like emoji .","title":"pyinstaller"},{"location":"Coding/Python/#pythonnet","text":"Docs: ? ! Developers recommend Mono version 5.20.1 Issues 939 On Ubuntu, the eoan universe repository has to be added deb https://archive.ubuntu.com/ubuntu/ eoan universe deb https://archive.ubuntu.com/ubuntu/ eoan-updates universe But I can't figure out how to add the older version, because the recommended syntax produces the error \"Unable to correct problems, you have held broken packages\" sudo apt install mono-devel = 5 .18.0.240+dfsg-3 Maybe try the tarballs on Mono's website... Or maybe there's another repo I don't know about.. apt install clang libglib2.0-dev python3-dev pip install pycparser pythonnet pip install -U setuptools","title":"pythonnet"},{"location":"Coding/Python/#random","text":"Random choice with replacement random . choice ( iterable ) Shuffle elements of an iterable in-place [FP:42] random . shuffle ( iterable )","title":"random"},{"location":"Coding/Python/#scrapy","text":"Best used to obtain one \"stream\" of data at a time, without trying to obtain data from different pages scrapy runspider spider.py -o file.json Display HTML source of the scraped page print ( response . txt ) Get {URL} fetch ( 'url' ) Select a CSS selector # Returns a `SelectorList` response . css ( 'p' ) # Retrieve full HTML elements response . css ( 'p' ) . extract () Retrieve only the text within the element response . css ( 'p::text' ) . extract () response . css ( 'p::text' ) . extract_first () response . css ( 'p::text' ) . extract ()[ 0 ] Get the href attribute value for an anchor tag response . css ( 'a' ) . attrib [ 'href' ] Launch Scrapy shell and scrape $URL scrapy shell $URL Make a default spider named {quotes} that will be restricted to {domain} scrapy genspider quotes domain scrapy runspider scrapy1 . py Run a spider, saving scraped data to a JSON file scrapy runspider spider . py - o items . json Method which contains most of the logic of the spider, especially after the yield keyword. For multiple items, a structural basis for iteration must be found and for each iteration, data is yielded Extract URL from link using standard CSS selection techniques Add the domain name to a relative link response . urljoin () Recursively call the parse method again on the next page yield scrapy . Request ( url = next_page_url , callback = self . parse ) Scrape detail pages - parse_details would be a spider method sibling to the main parse method - if a detail page has more information than the main, then the yield keyword should be in parse_details yield scrapy . Request ( url = { url }, callback = self . parse_details )","title":"scrapy"},{"location":"Coding/Python/#setuptools","text":"Setuptools is for uploading to PyPi. To create self-contained executable files, use pyinstaller . PROJECT \u251c\u2500\u2500 PROJECT # Additional code files will be placed in here \u2502 \u2514\u2500\u2500 init.py \u2514\u2500\u2500 setup.py # Containing a call to `setuptools.setup()` 1 directory, 2 files setup.py from setuptools import setup setup ( name = 'funniest' , version = '0.1' , description = 'The funniest joke in the world' , url = 'http://github.com/storborg/funniest' , author = 'Flying Circus' , author_email = 'flyingcircus@example.com' , license = 'MIT' , packages = [ 'funniest' ], zip_safe = False ) If the package has dependencies, they can be added by appending a install_requires keyword argument passing an array of the module names setup ( install_requires = [ 'markdown' , ], ) Reserve the name, upload package metadata, and create the pypi.python.org webpage python setup . py register Create a source distribution, producing a tarball inside the top-level directory python setup . py sdist Upload the source distribution python setup . py sdist upload Do all the above in a single step python setup . py register sdist upload","title":"setuptools"},{"location":"Coding/Python/#socket","text":"The socket module is Python's standard interface for the transport layer. Sockets can be classified by family AF_INET Internet AF_UNIX for UNIX sockets and type : - SOCK_STREAM TCP - SOCK_DGRAM UDP These enum values are required upon initialization of a socket object: Ortega : 25 client_socket = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) Sources: Sockets tutorial TCP server import socket with socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) as s : s . bind (( HOST , PORT )) TCP Client import socket with socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) as s : s . connect (( HOST , PORT )) UDP server import socket with socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) as s : s . bind (( HOST , PORT )) UDP client import socket msg = \"Hello, world!\" with socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) as s : s . sendto ( msg . encode (), ( HOST , PORT )) Define port on which to listen for connections. serversocket . bind (( 'localhost' , 80 )) Connect to a remote socket in one direction client_socket . connect (( 'www.packtpub.com' , 80 )) Convert a domain name into IPv4 address socket . gethostbyname ( 'packtpub.com' ) # '83.166.169.231' Defaults to localhost with no arguments s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) s . bind (( socket . gethostname (), 1234 )) Get protocol name from port number socket . getservbyport ( 80 ) # 'http' Listen to a maximum of 10 connections serversocket . listen ( 10 ) Receive bytestream from server msg = s . recv ( 1024 ) print ( msg . decode ( 'utf-8' ))","title":"socket"},{"location":"Coding/Python/#sqlite3","text":"Create a Connect connection object and employee.db (binary) if it doesn't exist conn = sqlite . connect ( 'employee.db' ) Create a Connect.Cursor object c = conn . cursor () Perform SQL commands with Connect.Cursor.execute() . Create tablename with fields field of type type ( null , integer , real , text , blob ); never use Python's native string operations (f-strings, etc) to form commands, because this method is vulnerable to SQL injection. YouTube c . execute ( '''CREATE TABLE {tablename} ( {field} {type} , {field} {type} ...)) Save changes conn . commit () Close connection conn . close ()","title":"sqlite3"},{"location":"Coding/Python/#subprocess","text":"subprocess modules allows you to spawn new processes, interact with file descriptors, and obtain exit codes. The recommended approach is to use the run() function as default, which runs a CLI command with options as a list of strings and returns a CompletedProcess instance.\\ Execute shell command Unlike os.system , subprocess.run() takes a list of arguments. subprocess . run ([ 'ls' , '-l,' . '], 0) Set capture_output to True to save output, stored as property stdout of the returned object. data = subprocess . run ([ 'ls,' - l ',' . '], 0, capture_output=True) The data is stored as a bytestring , which can be decoded to a normal string. data . stdout . decode ( 'utf-8' ) This return a CompletedProcess instance with the command's output stored under the stdout property subprocess . run ([ 'ls' , '-l' , '/dev/null' ], capture_output = True ) This will raise a CalledProcessError exception because of the non-zero exit code subprocess . run ( 'exit 1' , shell = True , check = True )","title":"subprocess"},{"location":"Coding/Python/#sys","text":"Return site-specific directory where Python files are installed sys . prefix # /usr/local/ by default","title":"sys"},{"location":"Coding/Python/#tabulate","text":"","title":"tabulate"},{"location":"Coding/Python/#termcolor","text":"Print text in a color code termcolor . cprint ( text , color )","title":"termcolor"},{"location":"Coding/Python/#threading","text":"Docs counter = 0 lock = threading . RLock () def func1 (): global counter while True : with lock : counter += 1 counter -= 1 def func2 (): global counter while True : with lock : counter += 1 counter -= 1 threading . Thrad ( target = func1 ) . start () threading . Thrad ( target = func2 ) . start () counter = 0 def func1 (): global counter while True : counter += 1 counter -= 1 def func2 (): global counter while True : counter += 1 counter -= 1 threading . Thrad ( target = func1 ) . start () threading . Thrad ( target = func2 ) . start ()","title":"threading"},{"location":"Coding/Python/#typing","text":"As tuples, their attributes are immutable class Starship ( NamedTuple ): name : str registry : str crew : int","title":"typing"},{"location":"Coding/Python/#urllib","text":"Download an RFC file from rfc-editor.org Ortega rfc_raw = urllib . request . urlopen ( url ) . read () rfc = rfc_raw . decode ()","title":"urllib"},{"location":"Coding/Python/#weakref","text":"Weak references are references to objects which return exceptions when that object has been garbage collected Create a weak reference to {object} # A weak reference created using `ref` must be dereferenced r = weakref . ref ( obj ) r () . method () r . method () # will not work # A weak reference created using `proxy` does not need to be dereferenced: weakref . proxy ( obj )","title":"weakref"},{"location":"Coding/Python/#winrm","text":"Winrm allows you to connect Linux and Windows hosts over WinRM. adamtheautomator.com Begin a WinRM session. If no errors are thrown, the session has been successfully established session = winrm . Session ( ipaddress , auth = ( username , password ))","title":"winrm"},{"location":"Coding/Python/#yaml","text":"from ruamel.yaml import YAML yaml = YAML () # Adjust indentation levels yaml . indent ( mapping = 4 , sequence = 6 , offset = 3 ) # Print all `data` to stdout yaml . dump ( data , sys . stdout ) with open ( \"file\" ) as f : yaml . dump ( data , f ) Resources: Introduction to YAML","title":"yaml"},{"location":"Coding/Python/#glossary","text":"","title":"Glossary"},{"location":"Coding/Python/#method-resolution-order","text":"Method resolution order (MRO) is the order of base classes that are searched when using super() . It is accessed with __mro__ , which returns a tuple of base classes in order of precedence, ending in object which is the root class of all classes. ( src )","title":"Method resolution order"},{"location":"Coding/Python/#non-interactive-debugging","text":"Non-interactive debugging is the most basic form of debugging, dependent on print or log statements placed within the body of code.","title":"Non-interactive debugging"},{"location":"Coding/Python/#type-slot","text":"A type slot is any of a number of fields within each magic method, including __new__() , __init__() , and __prepare__() (which returns a dictionary-like object that's used as the local namespace for all code from the class body)","title":"Type slot"},{"location":"Coding/Testing/","text":"Testing There has been a push toward adoption of unit-testing over the past decades, so much so that the amount of test code can exceed production code by up to 10 times. Testing can help to forestall software entropy , the phenomenon whereby a software project becomes progressively more complex and disorganized. There are two popular coverage metrics that quantitatively measure the quality of a test suite: Test coverage : the portion of total production code lines executed by any test Branch coverage : the portion of total number of branches traversed by any test Unit testing A unit test quickly tests a small piece of code (or \"unit\") in isolation of others. There are two schools of unit testing which differ in their intepretation of how isolation should be achieved: London (also \"mockist\") approach emphasizes segregation of the system under test from its collaborators (dependencies) using test doubles , in particular mocks . Classical (also \"Detroit\") approach emphasizes segregation of unit tests themselves from each other, allowing them to be run independently. In classical testing, there is less emphasis on using test doubles, which are used strictly for shared dependencies AAA Conventionally, tests have a three-part structure summarized in the acronym AAA (also 3A or Given-When-Then ): Arrange : SUT and dependencies are brought to a desired state Act : methods on the SUT are called and output is captured Assert : outcome is verified Several recommendations when using this framework: Every unit test should have a single Action Avoid conditional logic in unit tests The Arrange section should be largest, but if it is too large then it should be extracted into a private method or a separate factory class. Unit tests should be loosely coupled. Placing reusable test fixtures in the test class's constructor is an anti-pattern, unless every single test method uses the fixture. Test methods should have expressive, easily understood names. \ud83d\udee0\ufe0f Tasks \ud83d\ude80 Starship C# public class StarshipShould { [Theory] [InlineData(\"USS Enterprise\",\"NCC-1701\",203)] [InlineData(\"USS Constitution\",\"NCC-1700\",204)] [InlineData(\"USS Voyager\",\"NCC-74656\",141)] [InlineData(\"USS Defiant\",\"NX-74205\",50)] [InlineData(\"USS Enterprise\",\"NCC-1701-D\",1000)] public void BeValid ( string name , string registry , int crew ) { var starship = new Starship { Name = name , Registry = registry , Crew = crew }; Assert . Equal ( starship . Name , name ); Assert . Equal ( starship . Registry , registry ); Assert . Equal ( starship . Crew , crew ); } } Python import pytest from starships import Starship , StarshipClass , Fleet @pytest . fixture def enterprise (): return Starship ( \"USS Enterprise\" , \"NCC-1701\" , StarshipClass . CONSTITUTION ) def test_lookup_by_name ( enterprise ): starfleet = Fleet () starfleet . add ( enterprise ) assert starfleet . lookup ( enterprise . name ) == enterprise \ud83d\ude80\u2714\ufe0f StarshipValidator C# public class StarshipValidatorShould { [Theory] [InlineData(\"Jean-Luc Picard\", 2305, 7, 13)] [InlineData(\"James Kirk\", 2233, 3, 22)] public void ValidateCaptainedStarships ( string n , params int [] dob ) { var mockStarship = new Mock < IStarship >(); Captain captain = new Captain ( n , new DateTime ( dob [ 0 ], dob [ 1 ], dob [ 2 ])); mockStarship . Setup ( x => x . Captain ). Returns ( captain ); StarshipValidator starshipValidator = new StarshipValidator ( mockStarship . Object ); Assert . True ( starshipValidator . IsCaptained ()); } [Theory] [InlineData(\"USS Enterprise\",\"NCC-1701\",203)] [InlineData(\"USS Constitution\",\"NCC-1700\",204)] [InlineData(\"USS Voyager\",\"NCC-74656\",141)] [InlineData(\"USS Defiant\",\"NX-74205\",50)] [InlineData(\"USS Enterprise\",\"NCC-1701-D\",1000)] public void ValidateStarshipsWithValidRegistryNumbers ( string name , string registry , int crew ) { var starship = new Starship { Name = name , Registry = registry , Crew = crew }; StarshipValidator starshipValidator = new StarshipValidator ( starship ); Assert . True ( starshipValidator . ValidateRegistry ()); } } \ud83d\ude80\ud83c\udff9 StarshipDeployment C# Test (xUnit) public class StarshipDeploymentShould { [Fact] public void ThrowOnNullValidator () { var sut = new StarshipDeployment ( null ); Assert . Throws < ArgumentNullException >( sut ); } [Theory] [InlineData(\"Betelgeuse\")] public void EvaluateStarship ( string destination ) { var mockValidator = new Mock < IStarshipValidator >(); mockValidator . Setup ( x => x . Evaluate ()). Returns ( true ); var mockStarship = new Mock < IStarship >(); var sut = new StarshipDeployment ( mockValidator . Object as IStarshipValidator ); sut . Deploy ( mockStarship . Object as Starship , destination ); mockValidator . Verify ( x => x . Evaluate ()); } } \ud83d\udc69\u200d\ud83d\ude80 Officer \ud83d\udc69\u200d\ud83d\ude80\u2714\ufe0f CaptainSelector C# public class CaptainSelectorShould { [Theory] [InlineData('B')] [InlineData('C')] [InlineData('D')] [InlineData('F')] public void OnlyAssignGoodCaptains ( char grade ) { var mockOfficer = new Mock < IOfficer >(); mockOfficer . Setup ( x => x . Grade ). Returns ( grade ); CaptainSelector captainSelector = new CaptainSelector ( mockOfficer . Object ); bool selectionResult = captainSelector . Evaluate (); Assert . False ( selectionResult ); } } \ud83d\udcd8 Glossary Mock A mock is a test double that emulates outgoing interactions, or calls the system under test makes to change the state of a dependency. Mocks include spies . Spike A spike is an experiment without tests to ensure that an idea will work. Once the spike succeeds, the spike code is thrown away and the logic is recreated following TDD, starting with tests. Stub A stub is a test double that emulates incoming interactions, or calls the system under test makes to get data from a dependency. Fakes provide a working implementation of the dependency, however one which is unsuitable for production (e.g. in-memory databases) Dummies are passed around like real implementations but never accessed or used. These are used to satisfy the parameters of a method. Test double Test double include a variety of objects that facilitate unit testing by replacing a production object, usually a data dependency. Test doubles can be classified on what type of interaction the object emulates: Mocks emulate outgoing interactions Stubs emulate incoming interactions","title":"Testing"},{"location":"Coding/Testing/#testing","text":"There has been a push toward adoption of unit-testing over the past decades, so much so that the amount of test code can exceed production code by up to 10 times. Testing can help to forestall software entropy , the phenomenon whereby a software project becomes progressively more complex and disorganized. There are two popular coverage metrics that quantitatively measure the quality of a test suite: Test coverage : the portion of total production code lines executed by any test Branch coverage : the portion of total number of branches traversed by any test","title":"Testing"},{"location":"Coding/Testing/#unit-testing","text":"A unit test quickly tests a small piece of code (or \"unit\") in isolation of others. There are two schools of unit testing which differ in their intepretation of how isolation should be achieved: London (also \"mockist\") approach emphasizes segregation of the system under test from its collaborators (dependencies) using test doubles , in particular mocks . Classical (also \"Detroit\") approach emphasizes segregation of unit tests themselves from each other, allowing them to be run independently. In classical testing, there is less emphasis on using test doubles, which are used strictly for shared dependencies","title":"Unit testing"},{"location":"Coding/Testing/#aaa","text":"Conventionally, tests have a three-part structure summarized in the acronym AAA (also 3A or Given-When-Then ): Arrange : SUT and dependencies are brought to a desired state Act : methods on the SUT are called and output is captured Assert : outcome is verified Several recommendations when using this framework: Every unit test should have a single Action Avoid conditional logic in unit tests The Arrange section should be largest, but if it is too large then it should be extracted into a private method or a separate factory class. Unit tests should be loosely coupled. Placing reusable test fixtures in the test class's constructor is an anti-pattern, unless every single test method uses the fixture. Test methods should have expressive, easily understood names.","title":"AAA"},{"location":"Coding/Testing/#tasks","text":"","title":"\ud83d\udee0&#xfe0f; Tasks"},{"location":"Coding/Testing/#starship","text":"C# public class StarshipShould { [Theory] [InlineData(\"USS Enterprise\",\"NCC-1701\",203)] [InlineData(\"USS Constitution\",\"NCC-1700\",204)] [InlineData(\"USS Voyager\",\"NCC-74656\",141)] [InlineData(\"USS Defiant\",\"NX-74205\",50)] [InlineData(\"USS Enterprise\",\"NCC-1701-D\",1000)] public void BeValid ( string name , string registry , int crew ) { var starship = new Starship { Name = name , Registry = registry , Crew = crew }; Assert . Equal ( starship . Name , name ); Assert . Equal ( starship . Registry , registry ); Assert . Equal ( starship . Crew , crew ); } } Python import pytest from starships import Starship , StarshipClass , Fleet @pytest . fixture def enterprise (): return Starship ( \"USS Enterprise\" , \"NCC-1701\" , StarshipClass . CONSTITUTION ) def test_lookup_by_name ( enterprise ): starfleet = Fleet () starfleet . add ( enterprise ) assert starfleet . lookup ( enterprise . name ) == enterprise","title":"\ud83d\ude80 Starship"},{"location":"Coding/Testing/#starshipvalidator","text":"C# public class StarshipValidatorShould { [Theory] [InlineData(\"Jean-Luc Picard\", 2305, 7, 13)] [InlineData(\"James Kirk\", 2233, 3, 22)] public void ValidateCaptainedStarships ( string n , params int [] dob ) { var mockStarship = new Mock < IStarship >(); Captain captain = new Captain ( n , new DateTime ( dob [ 0 ], dob [ 1 ], dob [ 2 ])); mockStarship . Setup ( x => x . Captain ). Returns ( captain ); StarshipValidator starshipValidator = new StarshipValidator ( mockStarship . Object ); Assert . True ( starshipValidator . IsCaptained ()); } [Theory] [InlineData(\"USS Enterprise\",\"NCC-1701\",203)] [InlineData(\"USS Constitution\",\"NCC-1700\",204)] [InlineData(\"USS Voyager\",\"NCC-74656\",141)] [InlineData(\"USS Defiant\",\"NX-74205\",50)] [InlineData(\"USS Enterprise\",\"NCC-1701-D\",1000)] public void ValidateStarshipsWithValidRegistryNumbers ( string name , string registry , int crew ) { var starship = new Starship { Name = name , Registry = registry , Crew = crew }; StarshipValidator starshipValidator = new StarshipValidator ( starship ); Assert . True ( starshipValidator . ValidateRegistry ()); } }","title":"\ud83d\ude80\u2714&#xfe0f; StarshipValidator"},{"location":"Coding/Testing/#starshipdeployment","text":"C# Test (xUnit) public class StarshipDeploymentShould { [Fact] public void ThrowOnNullValidator () { var sut = new StarshipDeployment ( null ); Assert . Throws < ArgumentNullException >( sut ); } [Theory] [InlineData(\"Betelgeuse\")] public void EvaluateStarship ( string destination ) { var mockValidator = new Mock < IStarshipValidator >(); mockValidator . Setup ( x => x . Evaluate ()). Returns ( true ); var mockStarship = new Mock < IStarship >(); var sut = new StarshipDeployment ( mockValidator . Object as IStarshipValidator ); sut . Deploy ( mockStarship . Object as Starship , destination ); mockValidator . Verify ( x => x . Evaluate ()); } }","title":"\ud83d\ude80\ud83c\udff9 StarshipDeployment"},{"location":"Coding/Testing/#officer","text":"","title":"\ud83d\udc69\u200d\ud83d\ude80 Officer"},{"location":"Coding/Testing/#captainselector","text":"C# public class CaptainSelectorShould { [Theory] [InlineData('B')] [InlineData('C')] [InlineData('D')] [InlineData('F')] public void OnlyAssignGoodCaptains ( char grade ) { var mockOfficer = new Mock < IOfficer >(); mockOfficer . Setup ( x => x . Grade ). Returns ( grade ); CaptainSelector captainSelector = new CaptainSelector ( mockOfficer . Object ); bool selectionResult = captainSelector . Evaluate (); Assert . False ( selectionResult ); } }","title":"\ud83d\udc69\u200d\ud83d\ude80\u2714&#xfe0f; CaptainSelector"},{"location":"Coding/Testing/#glossary","text":"","title":"\ud83d\udcd8 Glossary"},{"location":"Coding/Testing/#mock","text":"A mock is a test double that emulates outgoing interactions, or calls the system under test makes to change the state of a dependency. Mocks include spies .","title":"Mock"},{"location":"Coding/Testing/#spike","text":"A spike is an experiment without tests to ensure that an idea will work. Once the spike succeeds, the spike code is thrown away and the logic is recreated following TDD, starting with tests.","title":"Spike"},{"location":"Coding/Testing/#stub","text":"A stub is a test double that emulates incoming interactions, or calls the system under test makes to get data from a dependency. Fakes provide a working implementation of the dependency, however one which is unsuitable for production (e.g. in-memory databases) Dummies are passed around like real implementations but never accessed or used. These are used to satisfy the parameters of a method.","title":"Stub"},{"location":"Coding/Testing/#test-double","text":"Test double include a variety of objects that facilitate unit testing by replacing a production object, usually a data dependency. Test doubles can be classified on what type of interaction the object emulates: Mocks emulate outgoing interactions Stubs emulate incoming interactions","title":"Test double"},{"location":"Coding/Windows/","text":"Windows programming \ud83d\udcd8 Glossary C++/CX C++/CX is a legacy language projection that uses nonstandard C++ (see C++/WinRT ). Compile a C++ to a DLL ( src ) Invoke MSVC , creating: Library.dll Library.lib Library.exp Library.obj Command-line cl /w4 /ld Library.cpp Library.def Library.cpp #include \"Library.h\" #include <stdio.h> void Cluck () { printf ( \"C-style cluck! \\n \" ); } Library.h #pragma once void Cluck (); Library.def A definitions file is broken into sections, all of which are optional. EXPORTS Cluck Now the libary can be used in an application Command-line cl /W4 Application.cpp /link Library.lib Application.cpp #include \"Library.h\" int main () { Cluck (); } C++/WinRT C#/Win32 Resources: Windows APIs Everywhere in .NET C#/WinRT .NET Core 3 and .NET Framework (\"NETFX\") applications can continue to use the Microsoft.Windows.SDK.Contracts NuGet package. C#/WinRT is the WinRT language projection for C#, created after .NET5 removed WinRT projection support for C# out of the .NET compiler. This represents a decoupling of the Windows-specific APIs from .NET It is used to create C# runtime components hosted in non-.NET languages by first building interop assemblies from Windows Metadata files using cswinrt.exe . Resources: How to call WinRT APIs from .NET5 applications COM Component Object Model was developed in the late 1980s by Microsoft. The Component Object Model is a binary standard interface specification for objects. It originated in 1993 as a renaming of OLE (Object Linking and Embedding) 2.0, used by Microsoft Office at the time to link data between applications. COM objects support a collection of interfaces, most importantly IUnknown which all COM objects must implement. IUnknown includes QueryInteface() which returns pointers to the other interfaces also implemented by the COM object. It also includes AddRef() and Release() , which manage the object's lifetime. COM uses HRESULT s, 32-bit longs, to indicate success or failure. Bit 31 indicates success (0) or failure (1). With .NET interop , a failed HRESULT turns into a thrown COMException. Common HRESULT s include S_OK , S_FALSE , E_FAIL , and many other failure codes also beginning with E_ . COM objects are created by calling CoCreateInstance() . This function, which is stored in ole32.dll , searches the Registry for the given class ID, then loads the apropriate COM Server DLL file. This DLL file then creates a class factory which then creates the COM instance, which is passed back to the client as an interface pointer. ( src ) COM classes must be registered in the Registry, at minimum by mapping the class ID (a GUID) to a DLL, in the HKEY_CLASSES_ROOT hive. This is done using the regsvr32.exe utility. ( src ) Core application Core application refers to the lifecycle of a UWP application, through which Windows offers app-specific services relating to power management, security, etc and abstracts the app itself. It offers a level of control over graphical applications comparable to that available for apps as services. ( src ) DirectX DirectX is a gaming API that was created by Eric Engstrom (d. 2020), Alex St. John, and Craig Eisler in 1994 to support game development on Windows 95. Interop assembly Interop assemblies allow .NET applications to call native code. They can be distributed along with applications that reference them. Language projections like C#/WinRT produce interop assemblies composed in C# which can then be compiled into a projection assembly . Language projection A language projection (or simply \"projection\") is a native adapter that enables programming APIs in a way that is idiomatic to a given language. Framework C# C++ WinRT C#/WinRT C++/WinRT Win32 C#/Win32 ? Resources: C#/WinRT (MSDocs) Windows APIs Everywhere in .NET TFM Target Framework Monikers (TFM) are used in NuGet packages and project files to refer to the flavor of .NET targeted by an application. Only for .NET5 , Microsoft introduced new monikers that indicate the targeted OS after a hyphen, e.g. net5.0-windows , etc. These monikers will pull in the projection assemblies that are needed to access those APIs and replace earlier use of the Microsoft.Windows.SDK.Contracts package reference. Example TMFs include: net5.0-windows10.0.19041.0 Windows 10 version 2004 net5.0-windows10.0.18362.0 Windows 10 version 1903 net5.0-windows10.0.17763.0 Windows 10 version 1809 References: Windows APIs Everywhere in .NET WinMD Windows metadata files (*.winmd) are machine-readable files that define Windows Runtime APIs. All public types in a .winmd file must be WinRT types. They use the same physical file format as CLR assemblies. Resources: Windows Metadata (WinMD) files Win32 WinRT The Windows Runtime is a framework introduced with Windows 8 to provide access to system resources. WinRT is separate from, although it is used by, .NET. Under the surface, WinRT is implemented as COM components. UWP XAML controls were included in WinRT until recently when they were moved to the WinUI NuGet package. Resources: Windows APIs Everywhere in .NET","title":"Windows programming"},{"location":"Coding/Windows/#windows-programming","text":"","title":"Windows programming"},{"location":"Coding/Windows/#glossary","text":"","title":"\ud83d\udcd8 Glossary"},{"location":"Coding/Windows/#ccx","text":"C++/CX is a legacy language projection that uses nonstandard C++ (see C++/WinRT ). Compile a C++ to a DLL ( src ) Invoke MSVC , creating: Library.dll Library.lib Library.exp Library.obj Command-line cl /w4 /ld Library.cpp Library.def Library.cpp #include \"Library.h\" #include <stdio.h> void Cluck () { printf ( \"C-style cluck! \\n \" ); } Library.h #pragma once void Cluck (); Library.def A definitions file is broken into sections, all of which are optional. EXPORTS Cluck Now the libary can be used in an application Command-line cl /W4 Application.cpp /link Library.lib Application.cpp #include \"Library.h\" int main () { Cluck (); }","title":"C++/CX"},{"location":"Coding/Windows/#cwinrt","text":"","title":"C++/WinRT "},{"location":"Coding/Windows/#cwin32","text":"Resources: Windows APIs Everywhere in .NET","title":"C#/Win32 "},{"location":"Coding/Windows/#cwinrt_1","text":".NET Core 3 and .NET Framework (\"NETFX\") applications can continue to use the Microsoft.Windows.SDK.Contracts NuGet package. C#/WinRT is the WinRT language projection for C#, created after .NET5 removed WinRT projection support for C# out of the .NET compiler. This represents a decoupling of the Windows-specific APIs from .NET It is used to create C# runtime components hosted in non-.NET languages by first building interop assemblies from Windows Metadata files using cswinrt.exe . Resources: How to call WinRT APIs from .NET5 applications","title":"C#/WinRT  "},{"location":"Coding/Windows/#com","text":"Component Object Model was developed in the late 1980s by Microsoft. The Component Object Model is a binary standard interface specification for objects. It originated in 1993 as a renaming of OLE (Object Linking and Embedding) 2.0, used by Microsoft Office at the time to link data between applications. COM objects support a collection of interfaces, most importantly IUnknown which all COM objects must implement. IUnknown includes QueryInteface() which returns pointers to the other interfaces also implemented by the COM object. It also includes AddRef() and Release() , which manage the object's lifetime. COM uses HRESULT s, 32-bit longs, to indicate success or failure. Bit 31 indicates success (0) or failure (1). With .NET interop , a failed HRESULT turns into a thrown COMException. Common HRESULT s include S_OK , S_FALSE , E_FAIL , and many other failure codes also beginning with E_ . COM objects are created by calling CoCreateInstance() . This function, which is stored in ole32.dll , searches the Registry for the given class ID, then loads the apropriate COM Server DLL file. This DLL file then creates a class factory which then creates the COM instance, which is passed back to the client as an interface pointer. ( src ) COM classes must be registered in the Registry, at minimum by mapping the class ID (a GUID) to a DLL, in the HKEY_CLASSES_ROOT hive. This is done using the regsvr32.exe utility. ( src )","title":"COM"},{"location":"Coding/Windows/#core-application","text":"Core application refers to the lifecycle of a UWP application, through which Windows offers app-specific services relating to power management, security, etc and abstracts the app itself. It offers a level of control over graphical applications comparable to that available for apps as services. ( src )","title":"Core application"},{"location":"Coding/Windows/#directx","text":"DirectX is a gaming API that was created by Eric Engstrom (d. 2020), Alex St. John, and Craig Eisler in 1994 to support game development on Windows 95.","title":"DirectX"},{"location":"Coding/Windows/#interop-assembly","text":"Interop assemblies allow .NET applications to call native code. They can be distributed along with applications that reference them. Language projections like C#/WinRT produce interop assemblies composed in C# which can then be compiled into a projection assembly .","title":"Interop assembly"},{"location":"Coding/Windows/#language-projection","text":"A language projection (or simply \"projection\") is a native adapter that enables programming APIs in a way that is idiomatic to a given language. Framework C# C++ WinRT C#/WinRT C++/WinRT Win32 C#/Win32 ? Resources: C#/WinRT (MSDocs) Windows APIs Everywhere in .NET","title":"Language projection"},{"location":"Coding/Windows/#tfm","text":"Target Framework Monikers (TFM) are used in NuGet packages and project files to refer to the flavor of .NET targeted by an application. Only for .NET5 , Microsoft introduced new monikers that indicate the targeted OS after a hyphen, e.g. net5.0-windows , etc. These monikers will pull in the projection assemblies that are needed to access those APIs and replace earlier use of the Microsoft.Windows.SDK.Contracts package reference. Example TMFs include: net5.0-windows10.0.19041.0 Windows 10 version 2004 net5.0-windows10.0.18362.0 Windows 10 version 1903 net5.0-windows10.0.17763.0 Windows 10 version 1809 References: Windows APIs Everywhere in .NET","title":"TFM"},{"location":"Coding/Windows/#winmd","text":"Windows metadata files (*.winmd) are machine-readable files that define Windows Runtime APIs. All public types in a .winmd file must be WinRT types. They use the same physical file format as CLR assemblies. Resources: Windows Metadata (WinMD) files","title":"WinMD"},{"location":"Coding/Windows/#win32","text":"","title":"Win32"},{"location":"Coding/Windows/#winrt","text":"The Windows Runtime is a framework introduced with Windows 8 to provide access to system resources. WinRT is separate from, although it is used by, .NET. Under the surface, WinRT is implemented as COM components. UWP XAML controls were included in WinRT until recently when they were moved to the WinUI NuGet package. Resources: Windows APIs Everywhere in .NET","title":"WinRT"},{"location":"Coding/Wired%20Brain%20Coffee/","text":"\u2615 Wired Brain Coffee Basic layout MainPage.xaml <Window x:Class= \"WiredBrainCoffee.UWP.MainWindow\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" > <Grid> <Grid.RowDefinitions> <RowDefinition Height= \"Auto\" /> <RowDefinition/> </Grid.RowDefinitions> <Grid.ColumnDefinitions> <ColumnDefinition Width= \"350\" /> <ColumnDefinition Width= \"*\" /> </Grid.ColumnDefinitions> <Border Grid.ColumnSpan= \"2\" Background= \"#f05a28\" > <StackPanel Orientation= \"Horizontal\" HorizontalAlignment= \"Center\" > <Image Height= \"90\" Margin= \"5\" Source= \"/Images/logo.png\" /> <TextBlock Text= \"Employee Manager\" FontSize= \"40\" VerticalAlignment= \"Center\" /> </StackPanel> </Border> <!-- Sidebar --> <Grid Grid.Row= \"1\" > <Grid.RowDefinitions> <RowDefinition Height= \"Auto\" /> <RowDefinition /> </Grid.RowDefinitions> <Button Content= \"Refresh\" Margin= \"10\" /> <ListView Grid.Row= \"1\" /> </Grid> <!--MainArea--> <Grid Grid.Row= \"1\" Grid.Column= \"1\" > <Grid.RowDefinitions> <RowDefinition Height= \"Auto\" /> <RowDefinition Height= \"Auto\" /> <RowDefinition Height= \"Auto\" /> <RowDefinition Height= \"Auto\" /> <RowDefinition Height= \"*\" /> </Grid.RowDefinitions> <TextBox Header= \"Firstname\" Margin= \"10\" /> <DatePicker Grid.Row= \"1\" Header= \"Entry date\" Margin= \"10\" /> <ComboBox Grid.Row= \"2\" Header= \"Job role\" Margin= \"10\" HorizontalAlignment= \"Stretch\" /> <CheckBox Grid.Row= \"3\" Content= \"Is coffee drinker?\" Margin= \"10\" /> <Button Grid.Row= \"4\" Content= \"Save\" Margin= \"10 10 10 30\" VerticalAlignment= \"Bottom\" HorizontalAlignment= \"Left\" /> </Grid> </Grid> </Window> Custom control Controls/HeaderControl.xaml <UserControl x:Class= \"WiredBrainCoffee.UWP.Controls.HeaderControl\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP.Controls\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" d:DesignHeight= \"300\" d:DesignWidth= \"400\" > <Border Background= \"#F05A28\" > <Grid> <Grid.ColumnDefinitions> <ColumnDefinition/> <ColumnDefinition Width= \"Auto\" /> </Grid.ColumnDefinitions> <StackPanel Orientation= \"Horizontal\" HorizontalAlignment= \"Center\" > <Image Source= \"/Images/WiredBrainLogo.png\" Height= \"90\" /> <TextBlock Text= \"Wired Brain Coffee\" FontSize= \"40\" VerticalAlignment= \"Center\" /> </StackPanel> <Button HorizontalAlignment= \"Right\" Grid.Column= \"1\" Margin= \"10\" > <SymbolIcon Symbol= \"AlignRight\" /> </Button> </Grid> </Border> </UserControl> MainPage.xaml <Page x:Class= \"WiredBrainCoffee.UWP.MainPage\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" xmlns:controls= \"using:WiredBrainCoffee.UWP.Controls\" mc:Ignorable= \"d\" Background= \"{ThemeResource ApplicationPageBackgroundThemeBrush}\" > <Grid> <Grid.RowDefinitions> <RowDefinition Height= \"Auto\" /> <RowDefinition/> </Grid.RowDefinitions> <Grid.ColumnDefinitions> <ColumnDefinition Width= \"350\" /> <ColumnDefinition/> </Grid.ColumnDefinitions> <!-- Header --> <controls:HeaderControl Grid.ColumnSpan= \"2\" /> <!-- Sidebar --> <Grid Grid.Row= \"1\" > <Grid.RowDefinitions> <RowDefinition Height= \"Auto\" /> <RowDefinition/> </Grid.RowDefinitions> <StackPanel Orientation= \"Horizontal\" > <Button Margin= \"10\" > <SymbolIcon Symbol= \"AddFriend\" /> </Button> <Button Margin= \"10\" > <SymbolIcon Symbol= \"Delete\" /> </Button> </StackPanel> <ListView Grid.Row= \"1\" > <ListViewItem> Aristotle </ListViewItem> <ListViewItem> Euclid </ListViewItem> <ListViewItem> Plato </ListViewItem> <ListViewItem> Socrates </ListViewItem> </ListView> </Grid> <StackPanel Grid.Row= \"1\" Grid.Column= \"1\" > <TextBox Header= \"First name\" Margin= \"10\" /> <TextBox Header= \"Last name\" Margin= \"10\" /> <CheckBox Content= \"Drinks coffee\" Margin= \"10\" /> </StackPanel> </Grid> </Page> Sidebar Setting an x:Name attribute on an element allows it to be manipulated in C#. ( src ) MainPage.xaml.cs private void btn_MoveSideBar_Click ( object sender , RoutedEventArgs e ) { int column = Grid . GetColumn ( customerListGrid ); int newcolumn ; if ( column == 0 ) { newcolumn = 2 ; btn_MoveSideBar_Symbol . Symbol = Symbol . AlignLeft ; } else { newcolumn = 0 ; btn_MoveSideBar_Symbol . Symbol = Symbol . AlignRight ; } Grid . SetColumn ( customerListGrid , newcolumn ); } Data provider A data provider class accomodates the need for mock data while also loosely coupling the data with the source. ( src ) DataProviders/CustomerDataProvider.cs using Newtonsoft.Json ; using System ; using System.Collections.Generic ; using System.Threading.Tasks ; using System.IO ; using Windows.Storage ; using Windows.Storage.Streams ; using WiredBrainCoffee.UWP.Models ; namespace WiredBrainCoffee.UWP.DataProviders { class CustomerDataProvider { private static readonly string _customersFileName = \"customers.json\" ; private static readonly StorageFolder _localFolder = ApplicationData . Current . LocalFolder ; public async Task < IEnumerable < Customer >> LoadCustomersAsync () { var storageFile = await _localFolder . TryGetItemAsync ( _customersFileName ) as StorageFile ; List < Customer > customerList = null ; if ( storageFile == null ) { customerList = new List < Customer > { new Customer { FirstName = \"Clark\" , LastName = \"Kent\" , IsCoffeeDrinker = true }, new Customer { FirstName = \"Bruce\" , LastName = \"Wayne\" , IsCoffeeDrinker = false }, new Customer { FirstName = \"Diana\" , LastName = \"Prince\" , IsCoffeeDrinker = true } }; } else { using ( var stream = await storageFile . OpenAsync ( FileAccessMode . Read )) { using ( var dataReader = new DataReader ( stream )) { await dataReader . LoadAsync (( uint ) stream . Size ); var json = dataReader . ReadString (( uint ) stream . Size ); customerList = JsonConvert . DeserializeObject < List < Customer >>( json ); } } } return customerList ; } public async Task SaveCustomersAsync ( IEnumerable < Customer > customers ) { var storageFile = await _localFolder . CreateFileAsync ( _customersFileName , CreationCollisionOption . ReplaceExisting ); using ( var stream = await storageFile . OpenAsync ( FileAccessMode . ReadWrite )) { using ( var dataWriter = new DataWriter ( stream )) { var json = JsonConvert . SerializeObject ( customers , Formatting . Indented ); dataWriter . WriteString ( json ); await dataWriter . StoreAsync (); } } } } } Models/Customer.cs namespace WiredBrainCoffee.UWP.Models { public class Customer { public string FirstName { get ; set ; } public string LastName { get ; set ; } public bool IsCoffeeDrinker { get ; set ; } } } Event hooks are used to populate the ListView with data from the data provider. MainPage.xaml.cs public MainPage () { this . InitializeComponent (); this . Loaded += MainPage_LoadedAsync ; App . Current . Suspending += App_SuspendingAsync ; _customerDataProvider = new CustomerDataProvider (); } private async void App_SuspendingAsync ( object sender , Windows . ApplicationModel . SuspendingEventArgs e ) { var deferral = e . SuspendingOperation . GetDeferral (); await _customerDataProvider . SaveCustomersAsync ( customerListView . Items . OfType < Customer >()); deferral . Complete (); } private async void MainPage_LoadedAsync ( object sender , RoutedEventArgs e ) { customerListView . Items . Clear (); var customers = await _customerDataProvider . LoadCustomersAsync (); foreach ( var customer in customers ) { customerListView . Items . Add ( customer ); } } Data binding using events Synchronize the customer detail textboxes to the selected item in the ListView. A rough form of data binding is possible with event handling . ( src ) First implement an event handler when the ListView.SelectionChanged event is fired. MainPage.xaml <ListView Grid.Row= \"1\" x:Name= \"customerListView\" DisplayMemberPath= \"FirstName\" SelectionChanged= \"customerListView_SelectionChanged\" /> MainPage.xaml.cs private void customerListView_SelectionChanged ( object sender , SelectionChangedEventArgs e ) { var customer = customerListView . SelectedItem as Customer ; txtFirstName . Text = customer ?. FirstName ?? \"\" ; txtLastName . Text = customer ?. LastName ?? \"\" ; chkDrinksCoffee . IsChecked = customer ?. IsCoffeeDrinker ; } Implement event handlers on the controls in the main area ( TextBox.TextChanged and CheckBox.Checked and CheckBox.Unchedked events) when changes are made. MainPage.xaml <StackPanel Grid.Row= \"1\" Grid.Column= \"1\" > <TextBox x:Name= \"txtFirstName\" Header= \"First name\" Margin= \"10\" TextChanged= \"UpdateCustomer\" /> <TextBox x:Name= \"txtLastName\" Header= \"Last name\" Margin= \"10\" TextChanged= \"UpdateCustomer\" /> <CheckBox x:Name= \"chkDrinksCoffee\" Content= \"Caffeine fiend\" Margin= \"10\" Checked= \"UpdateCustomer\" Unchecked= \"UpdateCustomer\" /> </StackPanel> MainPage.xaml.cs private void UpdateCustomer ( object sender , RoutedEventArgs e ) { var customer = customerListView . SelectedItem as Customer ; if ( customer != null ) { customer . FirstName = txtFirstName . Text ; customer . LastName = txtLastName . Text ; customer . IsCoffeeDrinker = chkDrinksCoffee . IsChecked . GetValueOrDefault (); } } Update ListView ListView still won't update as a result of changes. In order to implement this, you have to raise the PropertyChanged event. We implement the INotifyPropertyChanged interface and make it the base class of Customer. Also, we implement a helper method to fire the event handler whenever a property is changed. This helper is invoked every time a property is set. The CallerMemberName attribute passes the name of the calling property as a string, and allows us to avoid placing typeof(FirstName) , etc with every invocation. ( src ) Models/Customer.cs using System.ComponentModel ; using System.Runtime.CompilerServices ; namespace WiredBrainCoffee.UWP.Models { public class Observable : INotifyPropertyChanged { public event PropertyChangedEventHandler PropertyChanged ; protected virtual void OnPropertyChanged ([ CallerMemberName ] string propertyName = null ) { PropertyChanged ?. Invoke ( this , new PropertyChangedEventArgs ( propertyName )); } } public class Customer : Observable { private string firstName ; private string lastName ; private bool isCoffeeDrinker ; public string FirstName { get => firstName ; set { firstName = value ; OnPropertyChanged (); } } public string LastName { get => lastName ; set { lastName = value ; OnPropertyChanged (); } } public bool IsCoffeeDrinker { get => isCoffeeDrinker ; set { isCoffeeDrinker = value ; OnPropertyChanged (); } } } } Add/remove customers Implement event handlers for the Add and Delete buttons. ( src ) MainPage.xaml.cs private void DeleteCustomer_Click ( object sender , RoutedEventArgs e ) { var customer = customerListView . SelectedItem ; if ( customer != null ) { customerListView . Items . Remove ( customer ); } } private void AddCustomer_Click ( object sender , RoutedEventArgs e ) { var customer = new Customer { FirstName = \"New\" }; customerListView . Items . Add ( customer ); customerListView . SelectedItem = customer ; } MainPage.xaml <Button x:Name= \"AddCustomer\" Margin= \"10\" Click= \"AddCustomer_Click\" > <SymbolIcon Symbol= \"AddFriend\" /> </Button> <Button x:Name= \"DeleteCustomer\" Margin= \"10\" Click= \"DeleteCustomer_Click\" > <SymbolIcon Symbol= \"Delete\" /> </Button> Custom control We abstract controls in the main area of the app into a new CustomerDetailControl. As before, we cut the UI elements into a new XAML file and reference the new control in MainPage. However, now, customerListView is inaccessible. The lynchpin is forming a property on customerDetailControl that is populated with the customer object by the SelectionChanged event handler MainPage.xaml.cs private void customerListView_SelectionChanged ( object sender , SelectionChangedEventArgs e ) { var customer = customerListView . SelectedItem as Customer ; customerDetailControl . Customer = customer ; } Controls/CustomerDetailControl.xaml <UserControl x:Class= \"WiredBrainCoffee.UWP.Controls.CustomerDetailControl\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP.Controls\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" d:DesignHeight= \"300\" d:DesignWidth= \"400\" > <StackPanel> <TextBox x:Name= \"txtFirstName\" Header= \"First name\" Margin= \"10\" TextChanged= \"UpdateCustomer\" /> <TextBox x:Name= \"txtLastName\" Header= \"Last name\" Margin= \"10\" TextChanged= \"UpdateCustomer\" /> <CheckBox x:Name= \"chkDrinksCoffee\" Content= \"Caffeine fiend\" Margin= \"10\" Checked= \"UpdateCustomer\" Unchecked= \"UpdateCustomer\" /> </StackPanel> </UserControl> Controls/CustomerDetailControl.xaml.cs using Windows.UI.Xaml ; using Windows.UI.Xaml.Controls ; using WiredBrainCoffee.UWP.Models ; namespace WiredBrainCoffee.UWP.Controls { public sealed partial class CustomerDetailControl : UserControl { public CustomerDetailControl () { this . InitializeComponent (); } private Customer _customer ; public Customer Customer { get { return _customer ; } set { _customer = value ; txtFirstName . Text = _customer ?. FirstName ?? \"\" ; txtLastName . Text = _customer ?. LastName ?? \"\" ; chkDrinksCoffee . IsChecked = _customer ?. IsCoffeeDrinker ; } } private void UpdateCustomer ( object sender , RoutedEventArgs e ) { if ( Customer != null ) { Customer . FirstName = txtFirstName . Text ; Customer . LastName = txtLastName . Text ; Customer . IsCoffeeDrinker = chkDrinksCoffee . IsChecked . GetValueOrDefault (); } } } } Assign mock content We combine two different namespace mappings (one for the Customer model and another for the CustomerDetailControl) to prepopulate the CustomerDetailControl with a customer defined in XAML. Because CustomerDetailControl exposes a public Customer property, this data can be assigned to the Customer property using property-element syntax. ( src ) MainPage.xaml <controls:CustomerDetailControl x:Name= \"customerDetailControl\" Grid.Row= \"1\" Grid.Column= \"1\" > <controls:CustomerDetailControl.Customer> <model:Customer FirstName= \"Clark\" LastName= \"Kent\" IsCoffeeDrinker= \"True\" /> </controls:CustomerDetailControl.Customer> </controls:CustomerDetailControl> In order to be able to assign the customer as direct content without specifying the property explicitly, the custom control class has to be decorated with the ContentProperty attribute. This is because by default any direct child is assigned to the Content property, which does not exist for this custom control. Using the ContentProperty allows us to specify a property to which to assign direct children. Controls/CustomerDetailControl.xaml.cs [ContentProperty(Name = nameof(Customer))] public sealed partial class CustomerDetailControl : UserControl { /* ... */ } MainPage.xaml <controls:CustomerDetailControl x:Name= \"customerDetailControl\" Grid.Row= \"1\" Grid.Column= \"1\" > <model:Customer FirstName= \"Clark\" LastName= \"Kent\" IsCoffeeDrinker= \"True\" /> </controls:CustomerDetailControl> XAML Type conversion Passing the customer as an attribute requires custom logic to parse the string. The target model is then decorated with the CreateFromString attribute. This is only for custom classes: primitive types and enumerations can be parsed by the XAML processor automatically. Models/CustomerConverter.cs namespace WiredBrainCoffee.UWP.Models { public static class CustomerConverter { public static Customer ParseStringAsCustomer ( string s ) { string [] values = s . Split ( ';' ); return new Customer { FirstName = values [ 0 ], LastName = values [ 1 ], IsCoffeeDrinker = bool . Parse ( values [ 2 ]) }; } } } Models/Customer.cs [CreateFromString(MethodName =\"WiredBrainCoffee.UWP.Models.CustomerConverter.ParseCustomerFromString\")] public class Customer : Observable { /* ... */ } StaticResource You can use the StaticResource Markup Extension to define the equivalent of XAML variables to store elements for attribution using attribute syntax. Every UI element has a property named Resources to which you can assign elements. Unlike the Items property of a ListView, however, this property is a Dictionary type, which means you must specify a key for these values (i.e. specify x:Key ). Because the XAML processor looks for resources as it crawls up the element tree, these resources can be organized at any level of the application, even in the App.xaml where it will become available to other files: ( src ) MainPage.xaml <Page.Resources> <model:Customer x:Key= \"Shazam\" FirstName= \"William\" LastName= \"William Batson\" IsCoffeeDrinker= \"false\" /> </Page.Resources> <!-- ... --> <controls:CustomerDetailControl Customer= \"{StaticResource Shazam}\" /> However, mocking data in XAML is an anti-pattern; Resource dictionaries are typically used for colors and predefined strings. Resource dictionaries are consolidated into their own files: ( src ) Resources/Brushes.xaml <ResourceDictionary xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" > <SolidColorBrush x:Key= \"customerListBackgroundBrush\" Color= \"#EEEEEE\" /> </ResourceDictionary> Resources/Strings.xaml <ResourceDictionary xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" > <x:String x:Key= \"applicationTitle\" > Wired Brain Coffee </x:String> </ResourceDictionary> These can then be referenced from App.xaml and are available for assignment in any appropriate attribute App.xaml <Application x:Class= \"WiredBrainCoffee.UWP.App\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP\" > <Application.Resources> <ResourceDictionary> <ResourceDictionary.MergedDictionaries> <ResourceDictionary Source= \"Resources/Brushes.xaml\" /> <ResourceDictionary Source= \"Resources/Strings.xaml\" /> </ResourceDictionary.MergedDictionaries> </ResourceDictionary> </Application.Resources> </Application> Controls/HeaderControl.xaml <UserControl x:Class= \"WiredBrainCoffee.UWP.Controls.HeaderControl\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP.Controls\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" d:DesignHeight= \"300\" d:DesignWidth= \"400\" > <Border Background= \"#F05A28\" > <Grid> <Grid.ColumnDefinitions> <ColumnDefinition/> <ColumnDefinition Width= \"Auto\" /> </Grid.ColumnDefinitions> <StackPanel Orientation= \"Horizontal\" HorizontalAlignment= \"Center\" > <Image Source= \"/Images/WiredBrainLogo.png\" Height= \"90\" /> <TextBlock Text= \"{StaticResource applicationTitle}\" FontSize= \"40\" VerticalAlignment= \"Center\" /> </StackPanel> <Button x:Name= \"ButtonMove\" HorizontalAlignment= \"Right\" Grid.Column= \"1\" Margin= \"10\" Click= \"ButtonMove_Click\" > <SymbolIcon x:Name= \"ButtonMove_Symbol\" Symbol= \"AlignRight\" /> </Button> </Grid> </Border> </UserControl> ThemeResource The ThemeResource Markup Extension makes UWP-specific theme resource dictionaries available. These same resources are available using StaticResource, but with ThemeResource they will be updated if the user changes his Windows theme from light to dark. ( src ) Resources/Brushes.xaml <ResourceDictionary xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" > <ResourceDictionary.ThemeDictionaries> <ResourceDictionary x:Key= \"Dark\" > <SolidColorBrush x:Key= \"customerListBackgroundBrush\" Color= \"#222222\" /> </ResourceDictionary> <ResourceDictionary x:Key= \"Light\" > <SolidColorBrush x:Key= \"customListBackgroundBrush\" Color= \"#EEEEEE\" /> </ResourceDictionary> </ResourceDictionary.ThemeDictionaries> </ResourceDictionary> Theme selection A specific theme can be specified at any element by specifying a RequestedTheme attribute. However, this property cannot be changed at runtime. App.xaml <Application x:Class= \"WiredBrainCoffee.UWP.App\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP\" RequestedTheme= \"Dark\" > <Application.Resources> <ResourceDictionary> <ResourceDictionary.MergedDictionaries> <ResourceDictionary Source= \"Resources/Brushes.xaml\" /> <ResourceDictionary Source= \"Resources/Strings.xaml\" /> </ResourceDictionary.MergedDictionaries> </ResourceDictionary> </Application.Resources> </Application> A button to manually change theme involves simply assigning an ElementTheme enum value to the MainPage's RequestedTheme property. However, because on startup this has an ApplicationTheme enum value that is not evailable in ElementTheme , the MainPage's constructor must be changed to set the correct theme enum. Without this change, the first click after the application's startup will not change the theme at all, only set the correct ElementTheme . ( src ) MainPage.xaml <!-- Header --> <controls:HeaderControl Grid.ColumnSpan= \"3\" /> <Button Grid.ColumnSpan= \"3\" Click= \"ChangeTheme\" Margin= \"10\" VerticalAlignment= \"Top\" HorizontalAlignment= \"Right\" > <SymbolIcon Symbol= \"Placeholder\" /> </Button> MainPage.xaml.cs public MainPage () { this . InitializeComponent (); this . Loaded += MainPage_LoadedAsync ; App . Current . Suspending += App_SuspendingAsync ; _customerDataProvider = new CustomerDataProvider (); RequestedTheme = App . Current . RequestedTheme == ApplicationTheme . Dark ? ElementTheme . Dark : ElementTheme . Light ; } private void ChangeTheme ( object sender , RoutedEventArgs e ) { this . RequestedTheme = RequestedTheme == ElementTheme . Dark ? ElementTheme . Light : ElementTheme . Dark ; } Color theme The Fluent XAML Theme Editor on the Microsoft Store can generate ThemeResource dictionaries Data binding Use the Binding markup extension to establish a binding on the CustomerDetailControl to the Customer property of customerListView Here, the Customer property is the target property , and the SelectedItem property of customerListView is the source property . So this data binding makes the information in the customerDetailControl (target) dependent on which item is selected (source). MainPage.xaml.cs <controls:CustomerDetailControl x:Name= \"customerDetailControl\" Grid.Row= \"1\" Grid.Column= \"1\" Customer= \"{Binding ElementName=customerListView,Path=SelectedItem,Mode=OneWay}\" > However, the target of a data binding needs to be a Dependency Property . The purpose of dependency properties is to provide a way to compute the value of a property based on the value of other inputs. The Visual Studio snippet for a dependency property is propdp . A dependency property includes a static readonly field of type DependencyProperty and a normal property that works as a frontend for that field by wrapping GetValue and SetValue . We implement the logic to update the controls with the selected customer as a callback function passed as the second argument of the PropertyMetadata object in the dependency property definition. This callback must be a static void function, and as such it has no access to the instantiated objects we have already named with x:Name . However, these objects are retrievable from the DependencyObject and DependencyPropertyChangedEventArgs parameters that are passed to the callback. ( src ) Controls/CustomerDetailControl.xaml.cs public Customer Customer { get { return ( Customer ) GetValue ( CustomerProperty ); } set { SetValue ( CustomerProperty , value ); } } // Using a DependencyProperty as the backing store for Customer. This enables animation, styling, binding, etc... public static readonly DependencyProperty CustomerProperty = DependencyProperty . Register ( \"Customer\" , typeof ( Customer ), typeof ( CustomerDetailControl ), new PropertyMetadata ( null , CustomerChangedCallback )); private static void CustomerChangedCallback ( DependencyObject d , DependencyPropertyChangedEventArgs e ) { if ( d is CustomerDetailControl customerDetailControl ) { var customer = e . NewValue as Customer ; customerDetailControl . txtFirstName . Text = customer ?. FirstName ?? \"\" ; customerDetailControl . txtLastName . Text = customer ?. LastName ?? \"\" ; customerDetailControl . chkDrinksCoffee . IsChecked = customer ?. IsCoffeeDrinker ; } } This bound the customerDetailControl to the item selected in customerListView. Now we implement the data bindings on each control of customerDetailControl. We give the root UserControl an x:Name so that we can refer to it in the binding markup extensions of the children as the value of ElementName . We can also remove the x:Name s of the individual controls, as well any trace of the event handlers! ( src ) Controls/CustomerDetailControl.xaml <UserControl x:Class= \"WiredBrainCoffee.UWP.Controls.CustomerDetailControl\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP.Controls\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" d:DesignHeight= \"300\" d:DesignWidth= \"400\" x:Name= \"root\" > <StackPanel> <TextBox Header= \"First name\" Margin= \"10\" Text= \"{Binding ElementName=root,Path=Customer.FirstName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" /> <TextBox Header= \"Last name\" Margin= \"10\" Text= \"{Binding ElementName=root,Path=Customer.LastName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" /> <CheckBox Content= \"Caffeine fiend\" Margin= \"10\" IsChecked= \"{Binding ElementName=root,Path=Customer.FirstName,Mode=TwoWay}\" /> </StackPanel> </UserControl> ViewModel In the work above, we used the binding markup extension to bind one element to another, using that other element as a data source <TextBlock Text= \"{Binding ElementName=root,...}\" > We can use the MVVM pattern to assign the object to be bound to customerDetailControl as a data context . Every UI element has a DataContext property that can be set, and if it is set to an object then it can be placed there as a default data source that the XAML processor will find as it walks up the element tree. ( src ) This will allow us to simplify the markup, removing the x:Name from the root and the ElementName from the data bindings of the children. First we create the ViewModel, which incorporates some of the logic from the former App_SuspendingAsync and MainPage_LoadedAsync event handler methods. The ViewModel can dispose of the references to customerDetailControl and customerListView and replace them with its own Customers property. ViewModel/MainViewModel.cs using System.Collections.ObjectModel ; using System.Threading.Tasks ; using WiredBrainCoffee.UWP.DataProviders ; using WiredBrainCoffee.UWP.Models ; namespace WiredBrainCoffee.UWP.ViewModel { public class MainViewModel { public ObservableCollection < Customer > Customers { get ; } public MainViewModel () { _customerDataProvider = new CustomerDataProvider (); Customers = new ObservableCollection < Customer >(); } private CustomerDataProvider _customerDataProvider ; public async Task LoadAsync () { Customers . Clear (); var customers = await _customerDataProvider . LoadCustomersAsync (); foreach ( var customer in customers ) { Customers . Add ( customer ); } } public async Task SaveAsync () { await _customerDataProvider . SaveCustomersAsync ( Customers ); } } } To further decouple the ViewModel from the data provider, in order to facilitate testing, we extract an interface from CustomerDataProvider. DataProviders/ICustomerDataProvider.cs using System.Collections.Generic ; using System.Threading.Tasks ; using WiredBrainCoffee.UWP.Models ; namespace WiredBrainCoffee.UWP.DataProviders { public interface ICustomerDataProvider { Task < IEnumerable < Customer >> LoadCustomersAsync (); Task SaveCustomersAsync ( IEnumerable < Customer > customers ); } } Now we implement an ICustomerDataProvider parameter to the ViewModel constructor, and remember to pass in a new data provider as an argument implementing that interface. The private field _customerDataProvider can be removed. ViewModel/MainViewModel.cs private ICustomerDataProvider _customerDataProvider ; public MainViewModel ( ICustomerDataProvider customerDataProvider ) { _customerDataProvider = customerDataProvider ; Customers = new ObservableCollection < Customer >(); } MainPage.xaml.cs this . ViewModel = new MainViewModel ( new CustomerDataProvider ()); DataContext = ViewModel ; Finally, since we have a data context on MainPage, we can set it as a source for the ListView MainPage.xaml <ListView ItemsSource= \"{Binding Customers,Mode=OneWay}\" Grid.Row= \"1\" x:Name= \"customerListView\" DisplayMemberPath= \"FirstName\" > Binding the selected customer At this moment, customerDetailControl is still tied to customerListView's SelectedItem property directly, and not through the ViewModel. To change this, we implement a SelectedCustomer property on the ViewModel that will be bound to both. We reuse the Observable base class that implement the INotifyPropertyChanged interface. This allows us to use the OnPropertyChanged() method in the setter of the new SelectedItem property. We replace the element binding of customerListView with a binding to the SelectedCustomer property in the data context. ViewModel/MainViewModel.cs public class MainViewModel : Observable { private Customer selectedCustomer ; public Customer SelectedCustomer { get { return selectedCustomer ; } set { selectedCustomer = value ; OnPropertyChanged (); } } // ... } MainPage.xaml <ListView ItemsSource= \"{Binding Customers,Mode=OneWay}\" Grid.Row= \"1\" x:Name= \"customerListView\" DisplayMemberPath= \"FirstName\" SelectedItem= \"{Binding SelectedCustomer,Mode=TwoWay}\" > </ListView> DataTemplate At this moment, customerListView is being populated by a single property of each Customer - their first name. If we want to compose more complex information, we can assign DataTemplate to the ListView's ItemTemplate property. This will create the enclosed controls for each element in the ListView. Remember to remove the DisplayMemberPath attribute! <ListView ItemsSource= \"{Binding Customers,Mode=OneWay}\" Grid.Row= \"1\" x:Name= \"customerListView\" SelectedItem= \"{Binding SelectedCustomer,Mode=TwoWay}\" > <ListView.ItemTemplate> <DataTemplate> <StackPanel Orientation= \"Horizontal\" > <TextBlock Text= \"{Binding FirstName}\" /> <TextBlock Text= \"{Binding LastName}\" Margin= \"5 0 0 0\" FontWeight= \"Bold\" /> </StackPanel> </DataTemplate> </ListView.ItemTemplate> </ListView> x:Bind There are two data binding types available in XAML ( src ) Binding markup extension resolves the binding path at runtime. x:Bind resolves the binding path at compile-time , generating C# code and offering better performance and compile-time errors. You can also step into the compiled code, providing a better debugging experience. x:Bind should generally be preferred, however it is available only in UWP. Binding markup extension can have several different data sources, depending on defined attributes. ElementName Source RelativeSource If none of these are defined, then the binding markup extension resolves to the DataContext property. x:Bind, in contrast, binds only to the parent Page or UserControl element. So any property of MainPage will be accessible, and any property of that object will also be accessible using dot notation. Most bindings are easily translated between the two types if the ViewModel has already been implemented as a property of MainPage: Binding markup extension public MainPage () { this . InitializeComponent ; this . ViewModel = new MainViewModel (); DataContext = ViewModel ; } <ListView ItemsSource= \"{Binding Customers,Mode=OneWay}\" > <!-- ...--> </ListView> x:Bind public MainPage () { this . InitializeComponent ; this . ViewModel = new MainViewModel (); // DataContext = ViewModel; } <ListView ItemsSource= \"{x:Bind ViewModel.Customers,Mode=OneWay}\" > <!-- ...--> </ListView> Notably, the default binding mode of the Binding markup extension is OneWay x:Bind is OneTime , although this can be changed by setting x:DefaultBindMode on the root element. Set explicitly <Page> <Listview ItemsSource= \"{x:Bind ViewModelCustomers,Mode=OneWay}\" /> </Page> Changing default binding mode <Page x:DefaultBindMode= \"OneWay\" > <Listview ItemsSource= \"{x:Bind ViewModelCustomers}\" /> </Page> customDetailControl, which previously used the binding markup extension but set the root element as an explicitly named source property, is notably simplified after replacing with x:Bind . We can now directly access the user control's property. Before <UserControl x:Class= \"WiredBrainCoffee.UWP.Controls.CustomerDetailControl\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP.Controls\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" d:DesignHeight= \"300\" d:DesignWidth= \"400\" x:Name= \"root\" > <StackPanel> <TextBox Header= \"First name\" Margin= \"10\" Text= \"{Binding ElementName=root,Path=Customer.FirstName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" /> <TextBox Header= \"Last name\" Margin= \"10\" Text= \"{Binding ElementName=root,Path=Customer.LastName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" /> <CheckBox Content= \"Caffeine fiend\" Margin= \"10\" IsChecked= \"{Binding ElementName=root,Path=Customer.FirstName,Mode=TwoWay}\" /> </StackPanel> </UserControl> After <UserControl x:Class= \"WiredBrainCoffee.UWP.Controls.CustomerDetailControl\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP.Controls\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" d:DesignHeight= \"300\" d:DesignWidth= \"400\" > <StackPanel> <TextBox Header= \"First name\" Margin= \"10\" Text= \"{x:Bind Customer.FirstName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" /> <TextBox Header= \"Last name\" Margin= \"10\" Text= \"{x:Bind Customer.LastName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" /> <CheckBox Content= \"Caffeine fiend\" Margin= \"10\" IsChecked= \"{x:Bind Customer.IsCoffeeDrinker,Mode=TwoWay}\" /> </StackPanel> </UserControl> x:Bind can also be implemented in the ListView's ItemTemplate, so long as the x:DataType attribute is set on DataTemplate. We must also remember to set the Mode binding property, since x:Bind's default is OneTime! ( src ) <ListView Grid.Row= \"1\" ItemsSource= \"{x:Bind ViewModel.Customers}\" SelectedItem= \"{x:Bind ViewModel.SelectedCustomer,Mode=TwoWay}\" > <ListView.ItemTemplate> <DataTemplate x:DataType= \"model:Customer\" > <StackPanel Orientation= \"Horizontal\" > <TextBlock Text= \"{x:Bind FirstName}\" /> <TextBlock Text= \"{x:Bind LastName}\" Margin= \"5 0 0 0\" FontWeight= \"Bold\" /> </StackPanel> </DataTemplate> </ListView.ItemTemplate> </ListView> x:Bind can also hide or reveal controls depending on boolean value. A new boolean property is formed on the ViewModel, and we wire the OnPropertyChanged event handler to it. We also bind this value to the Visibility attribute of customerDetailControl. This will hide the customerDetailControl on application startup before the user selects a customer. ( src ) public Customer SelectedCustomer { get { return selectedCustomer ; } set { if ( selectedCustomer != value ) { selectedCustomer = value ; OnPropertyChanged (); OnPropertyChanged ( nameof ( IsCustomerSelected )); } } } public bool IsCustomerSelected => SelectedCustomer != null ; <controls:CustomerDetailControl x:Name= \"customerDetailControl\" Grid.Row= \"1\" Grid.Column= \"1\" Customer= \"{x:Bind ViewModel.SelectedCustomer,Mode=TwoWay}\" Visibility= \"{x:Bind ViewModel.IsCustomerSelected}\" /> We can also implement a third TextBlock in the ListView's ItemTemplate to show a string depending on the value of the CheckBox. <ListView Grid.Row= \"1\" ItemsSource= \"{x:Bind ViewModel.Customers}\" SelectedItem= \"{x:Bind ViewModel.SelectedCustomer,Mode=TwoWay}\" > <ListView.ItemTemplate> <DataTemplate x:DataType= \"model:Customer\" > <StackPanel Orientation= \"Horizontal\" > <TextBlock Text= \"Dev\" Opacity= \"0.5\" Visibility= \"{x:Bind IsCoffeeDrinker}\" Margin= \"0 0 5 0\" /> <TextBlock Text= \"{x:Bind FirstName}\" /> <TextBlock Text= \"{x:Bind LastName}\" Margin= \"5 0 0 0\" FontWeight= \"Bold\" /> </StackPanel> </DataTemplate> </ListView.ItemTemplate> </ListView> Toggling visibility In order to fully implement the logic of the MVVM pattern, we should move functionality that deals with the logic of the app as a whole to the ViewModel. That would include the Add and Delete buttons. ( src ) Before <CommandBar> <AppBarButton x:Name= \"AddCustomer\" Click= \"AddCustomer_Click\" Label= \"Add\" > <SymbolIcon Symbol= \"Add\" /> </AppBarButton> <AppBarButton x:Name= \"DeleteCustomer\" Click= \"DeleteCustomer_Click\" Label= \"Delete\" > <SymbolIcon Symbol= \"Delete\" /> </AppBarButton> <AppBarButton x:Name= \"btn_MoveSideBar\" Click= \"btn_MoveSideBar_Click\" Label= \"Move sidebar\" > <SymbolIcon x:Name= \"btn_MoveSideBar_Symbol\" Symbol= \"AlignRight\" /> </AppBarButton> </CommandBar> After <CommandBar> <AppBarButton x:Name= \"AddCustomer\" Click= \"{x:Bind ViewModel.AddCustomer_Click}\" Label= \"Add\" > <SymbolIcon Symbol= \"Add\" /> </AppBarButton> <AppBarButton x:Name= \"DeleteCustomer\" Click= \"{x:Bind ViewModel.DeleteCustomer_Click}\" Label= \"Delete\" > <SymbolIcon Symbol= \"Delete\" /> </AppBarButton> <AppBarButton x:Name= \"btn_MoveSideBar\" Click= \"btn_MoveSideBar_Click\" Label= \"Move sidebar\" > <SymbolIcon x:Name= \"btn_MoveSideBar_Symbol\" Symbol= \"AlignRight\" /> </AppBarButton> </CommandBar> Styling You can define a style that has to be used more than once by declaring a Style element on a UserControl's Resources property. ( src ) <UserControl> <UserControl.Resources> <Style x:Key= \"myTextBoxStyle\" TargetType= \"TextBox\" > <Style.Setters> <Setter Property= \"Margin\" Value= \"10\" /> <Setter Property= \"CornerRadius\" Value= \"10\" /> </Style.Setters> </Style> </UserControl.Resources> </UserControl> This Style can then be used as a StaticResource, setting the value of the Style attribute.","title":"\u2615 Wired Brain Coffee"},{"location":"Coding/Wired%20Brain%20Coffee/#wired-brain-coffee","text":"","title":"\u2615 Wired Brain Coffee"},{"location":"Coding/Wired%20Brain%20Coffee/#basic-layout","text":"MainPage.xaml <Window x:Class= \"WiredBrainCoffee.UWP.MainWindow\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" > <Grid> <Grid.RowDefinitions> <RowDefinition Height= \"Auto\" /> <RowDefinition/> </Grid.RowDefinitions> <Grid.ColumnDefinitions> <ColumnDefinition Width= \"350\" /> <ColumnDefinition Width= \"*\" /> </Grid.ColumnDefinitions> <Border Grid.ColumnSpan= \"2\" Background= \"#f05a28\" > <StackPanel Orientation= \"Horizontal\" HorizontalAlignment= \"Center\" > <Image Height= \"90\" Margin= \"5\" Source= \"/Images/logo.png\" /> <TextBlock Text= \"Employee Manager\" FontSize= \"40\" VerticalAlignment= \"Center\" /> </StackPanel> </Border> <!-- Sidebar --> <Grid Grid.Row= \"1\" > <Grid.RowDefinitions> <RowDefinition Height= \"Auto\" /> <RowDefinition /> </Grid.RowDefinitions> <Button Content= \"Refresh\" Margin= \"10\" /> <ListView Grid.Row= \"1\" /> </Grid> <!--MainArea--> <Grid Grid.Row= \"1\" Grid.Column= \"1\" > <Grid.RowDefinitions> <RowDefinition Height= \"Auto\" /> <RowDefinition Height= \"Auto\" /> <RowDefinition Height= \"Auto\" /> <RowDefinition Height= \"Auto\" /> <RowDefinition Height= \"*\" /> </Grid.RowDefinitions> <TextBox Header= \"Firstname\" Margin= \"10\" /> <DatePicker Grid.Row= \"1\" Header= \"Entry date\" Margin= \"10\" /> <ComboBox Grid.Row= \"2\" Header= \"Job role\" Margin= \"10\" HorizontalAlignment= \"Stretch\" /> <CheckBox Grid.Row= \"3\" Content= \"Is coffee drinker?\" Margin= \"10\" /> <Button Grid.Row= \"4\" Content= \"Save\" Margin= \"10 10 10 30\" VerticalAlignment= \"Bottom\" HorizontalAlignment= \"Left\" /> </Grid> </Grid> </Window>","title":"Basic layout"},{"location":"Coding/Wired%20Brain%20Coffee/#custom-control","text":"Controls/HeaderControl.xaml <UserControl x:Class= \"WiredBrainCoffee.UWP.Controls.HeaderControl\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP.Controls\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" d:DesignHeight= \"300\" d:DesignWidth= \"400\" > <Border Background= \"#F05A28\" > <Grid> <Grid.ColumnDefinitions> <ColumnDefinition/> <ColumnDefinition Width= \"Auto\" /> </Grid.ColumnDefinitions> <StackPanel Orientation= \"Horizontal\" HorizontalAlignment= \"Center\" > <Image Source= \"/Images/WiredBrainLogo.png\" Height= \"90\" /> <TextBlock Text= \"Wired Brain Coffee\" FontSize= \"40\" VerticalAlignment= \"Center\" /> </StackPanel> <Button HorizontalAlignment= \"Right\" Grid.Column= \"1\" Margin= \"10\" > <SymbolIcon Symbol= \"AlignRight\" /> </Button> </Grid> </Border> </UserControl> MainPage.xaml <Page x:Class= \"WiredBrainCoffee.UWP.MainPage\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" xmlns:controls= \"using:WiredBrainCoffee.UWP.Controls\" mc:Ignorable= \"d\" Background= \"{ThemeResource ApplicationPageBackgroundThemeBrush}\" > <Grid> <Grid.RowDefinitions> <RowDefinition Height= \"Auto\" /> <RowDefinition/> </Grid.RowDefinitions> <Grid.ColumnDefinitions> <ColumnDefinition Width= \"350\" /> <ColumnDefinition/> </Grid.ColumnDefinitions> <!-- Header --> <controls:HeaderControl Grid.ColumnSpan= \"2\" /> <!-- Sidebar --> <Grid Grid.Row= \"1\" > <Grid.RowDefinitions> <RowDefinition Height= \"Auto\" /> <RowDefinition/> </Grid.RowDefinitions> <StackPanel Orientation= \"Horizontal\" > <Button Margin= \"10\" > <SymbolIcon Symbol= \"AddFriend\" /> </Button> <Button Margin= \"10\" > <SymbolIcon Symbol= \"Delete\" /> </Button> </StackPanel> <ListView Grid.Row= \"1\" > <ListViewItem> Aristotle </ListViewItem> <ListViewItem> Euclid </ListViewItem> <ListViewItem> Plato </ListViewItem> <ListViewItem> Socrates </ListViewItem> </ListView> </Grid> <StackPanel Grid.Row= \"1\" Grid.Column= \"1\" > <TextBox Header= \"First name\" Margin= \"10\" /> <TextBox Header= \"Last name\" Margin= \"10\" /> <CheckBox Content= \"Drinks coffee\" Margin= \"10\" /> </StackPanel> </Grid> </Page>","title":"Custom control"},{"location":"Coding/Wired%20Brain%20Coffee/#sidebar","text":"Setting an x:Name attribute on an element allows it to be manipulated in C#. ( src ) MainPage.xaml.cs private void btn_MoveSideBar_Click ( object sender , RoutedEventArgs e ) { int column = Grid . GetColumn ( customerListGrid ); int newcolumn ; if ( column == 0 ) { newcolumn = 2 ; btn_MoveSideBar_Symbol . Symbol = Symbol . AlignLeft ; } else { newcolumn = 0 ; btn_MoveSideBar_Symbol . Symbol = Symbol . AlignRight ; } Grid . SetColumn ( customerListGrid , newcolumn ); }","title":"Sidebar"},{"location":"Coding/Wired%20Brain%20Coffee/#data-provider","text":"A data provider class accomodates the need for mock data while also loosely coupling the data with the source. ( src ) DataProviders/CustomerDataProvider.cs using Newtonsoft.Json ; using System ; using System.Collections.Generic ; using System.Threading.Tasks ; using System.IO ; using Windows.Storage ; using Windows.Storage.Streams ; using WiredBrainCoffee.UWP.Models ; namespace WiredBrainCoffee.UWP.DataProviders { class CustomerDataProvider { private static readonly string _customersFileName = \"customers.json\" ; private static readonly StorageFolder _localFolder = ApplicationData . Current . LocalFolder ; public async Task < IEnumerable < Customer >> LoadCustomersAsync () { var storageFile = await _localFolder . TryGetItemAsync ( _customersFileName ) as StorageFile ; List < Customer > customerList = null ; if ( storageFile == null ) { customerList = new List < Customer > { new Customer { FirstName = \"Clark\" , LastName = \"Kent\" , IsCoffeeDrinker = true }, new Customer { FirstName = \"Bruce\" , LastName = \"Wayne\" , IsCoffeeDrinker = false }, new Customer { FirstName = \"Diana\" , LastName = \"Prince\" , IsCoffeeDrinker = true } }; } else { using ( var stream = await storageFile . OpenAsync ( FileAccessMode . Read )) { using ( var dataReader = new DataReader ( stream )) { await dataReader . LoadAsync (( uint ) stream . Size ); var json = dataReader . ReadString (( uint ) stream . Size ); customerList = JsonConvert . DeserializeObject < List < Customer >>( json ); } } } return customerList ; } public async Task SaveCustomersAsync ( IEnumerable < Customer > customers ) { var storageFile = await _localFolder . CreateFileAsync ( _customersFileName , CreationCollisionOption . ReplaceExisting ); using ( var stream = await storageFile . OpenAsync ( FileAccessMode . ReadWrite )) { using ( var dataWriter = new DataWriter ( stream )) { var json = JsonConvert . SerializeObject ( customers , Formatting . Indented ); dataWriter . WriteString ( json ); await dataWriter . StoreAsync (); } } } } } Models/Customer.cs namespace WiredBrainCoffee.UWP.Models { public class Customer { public string FirstName { get ; set ; } public string LastName { get ; set ; } public bool IsCoffeeDrinker { get ; set ; } } } Event hooks are used to populate the ListView with data from the data provider. MainPage.xaml.cs public MainPage () { this . InitializeComponent (); this . Loaded += MainPage_LoadedAsync ; App . Current . Suspending += App_SuspendingAsync ; _customerDataProvider = new CustomerDataProvider (); } private async void App_SuspendingAsync ( object sender , Windows . ApplicationModel . SuspendingEventArgs e ) { var deferral = e . SuspendingOperation . GetDeferral (); await _customerDataProvider . SaveCustomersAsync ( customerListView . Items . OfType < Customer >()); deferral . Complete (); } private async void MainPage_LoadedAsync ( object sender , RoutedEventArgs e ) { customerListView . Items . Clear (); var customers = await _customerDataProvider . LoadCustomersAsync (); foreach ( var customer in customers ) { customerListView . Items . Add ( customer ); } }","title":"Data provider"},{"location":"Coding/Wired%20Brain%20Coffee/#data-binding-using-events","text":"Synchronize the customer detail textboxes to the selected item in the ListView. A rough form of data binding is possible with event handling . ( src ) First implement an event handler when the ListView.SelectionChanged event is fired. MainPage.xaml <ListView Grid.Row= \"1\" x:Name= \"customerListView\" DisplayMemberPath= \"FirstName\" SelectionChanged= \"customerListView_SelectionChanged\" /> MainPage.xaml.cs private void customerListView_SelectionChanged ( object sender , SelectionChangedEventArgs e ) { var customer = customerListView . SelectedItem as Customer ; txtFirstName . Text = customer ?. FirstName ?? \"\" ; txtLastName . Text = customer ?. LastName ?? \"\" ; chkDrinksCoffee . IsChecked = customer ?. IsCoffeeDrinker ; } Implement event handlers on the controls in the main area ( TextBox.TextChanged and CheckBox.Checked and CheckBox.Unchedked events) when changes are made. MainPage.xaml <StackPanel Grid.Row= \"1\" Grid.Column= \"1\" > <TextBox x:Name= \"txtFirstName\" Header= \"First name\" Margin= \"10\" TextChanged= \"UpdateCustomer\" /> <TextBox x:Name= \"txtLastName\" Header= \"Last name\" Margin= \"10\" TextChanged= \"UpdateCustomer\" /> <CheckBox x:Name= \"chkDrinksCoffee\" Content= \"Caffeine fiend\" Margin= \"10\" Checked= \"UpdateCustomer\" Unchecked= \"UpdateCustomer\" /> </StackPanel> MainPage.xaml.cs private void UpdateCustomer ( object sender , RoutedEventArgs e ) { var customer = customerListView . SelectedItem as Customer ; if ( customer != null ) { customer . FirstName = txtFirstName . Text ; customer . LastName = txtLastName . Text ; customer . IsCoffeeDrinker = chkDrinksCoffee . IsChecked . GetValueOrDefault (); } }","title":"Data binding using events"},{"location":"Coding/Wired%20Brain%20Coffee/#update-listview","text":"ListView still won't update as a result of changes. In order to implement this, you have to raise the PropertyChanged event. We implement the INotifyPropertyChanged interface and make it the base class of Customer. Also, we implement a helper method to fire the event handler whenever a property is changed. This helper is invoked every time a property is set. The CallerMemberName attribute passes the name of the calling property as a string, and allows us to avoid placing typeof(FirstName) , etc with every invocation. ( src ) Models/Customer.cs using System.ComponentModel ; using System.Runtime.CompilerServices ; namespace WiredBrainCoffee.UWP.Models { public class Observable : INotifyPropertyChanged { public event PropertyChangedEventHandler PropertyChanged ; protected virtual void OnPropertyChanged ([ CallerMemberName ] string propertyName = null ) { PropertyChanged ?. Invoke ( this , new PropertyChangedEventArgs ( propertyName )); } } public class Customer : Observable { private string firstName ; private string lastName ; private bool isCoffeeDrinker ; public string FirstName { get => firstName ; set { firstName = value ; OnPropertyChanged (); } } public string LastName { get => lastName ; set { lastName = value ; OnPropertyChanged (); } } public bool IsCoffeeDrinker { get => isCoffeeDrinker ; set { isCoffeeDrinker = value ; OnPropertyChanged (); } } } }","title":"Update ListView"},{"location":"Coding/Wired%20Brain%20Coffee/#addremove-customers","text":"Implement event handlers for the Add and Delete buttons. ( src ) MainPage.xaml.cs private void DeleteCustomer_Click ( object sender , RoutedEventArgs e ) { var customer = customerListView . SelectedItem ; if ( customer != null ) { customerListView . Items . Remove ( customer ); } } private void AddCustomer_Click ( object sender , RoutedEventArgs e ) { var customer = new Customer { FirstName = \"New\" }; customerListView . Items . Add ( customer ); customerListView . SelectedItem = customer ; } MainPage.xaml <Button x:Name= \"AddCustomer\" Margin= \"10\" Click= \"AddCustomer_Click\" > <SymbolIcon Symbol= \"AddFriend\" /> </Button> <Button x:Name= \"DeleteCustomer\" Margin= \"10\" Click= \"DeleteCustomer_Click\" > <SymbolIcon Symbol= \"Delete\" /> </Button>","title":"Add/remove customers"},{"location":"Coding/Wired%20Brain%20Coffee/#custom-control_1","text":"We abstract controls in the main area of the app into a new CustomerDetailControl. As before, we cut the UI elements into a new XAML file and reference the new control in MainPage. However, now, customerListView is inaccessible. The lynchpin is forming a property on customerDetailControl that is populated with the customer object by the SelectionChanged event handler MainPage.xaml.cs private void customerListView_SelectionChanged ( object sender , SelectionChangedEventArgs e ) { var customer = customerListView . SelectedItem as Customer ; customerDetailControl . Customer = customer ; } Controls/CustomerDetailControl.xaml <UserControl x:Class= \"WiredBrainCoffee.UWP.Controls.CustomerDetailControl\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP.Controls\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" d:DesignHeight= \"300\" d:DesignWidth= \"400\" > <StackPanel> <TextBox x:Name= \"txtFirstName\" Header= \"First name\" Margin= \"10\" TextChanged= \"UpdateCustomer\" /> <TextBox x:Name= \"txtLastName\" Header= \"Last name\" Margin= \"10\" TextChanged= \"UpdateCustomer\" /> <CheckBox x:Name= \"chkDrinksCoffee\" Content= \"Caffeine fiend\" Margin= \"10\" Checked= \"UpdateCustomer\" Unchecked= \"UpdateCustomer\" /> </StackPanel> </UserControl> Controls/CustomerDetailControl.xaml.cs using Windows.UI.Xaml ; using Windows.UI.Xaml.Controls ; using WiredBrainCoffee.UWP.Models ; namespace WiredBrainCoffee.UWP.Controls { public sealed partial class CustomerDetailControl : UserControl { public CustomerDetailControl () { this . InitializeComponent (); } private Customer _customer ; public Customer Customer { get { return _customer ; } set { _customer = value ; txtFirstName . Text = _customer ?. FirstName ?? \"\" ; txtLastName . Text = _customer ?. LastName ?? \"\" ; chkDrinksCoffee . IsChecked = _customer ?. IsCoffeeDrinker ; } } private void UpdateCustomer ( object sender , RoutedEventArgs e ) { if ( Customer != null ) { Customer . FirstName = txtFirstName . Text ; Customer . LastName = txtLastName . Text ; Customer . IsCoffeeDrinker = chkDrinksCoffee . IsChecked . GetValueOrDefault (); } } } }","title":"Custom control"},{"location":"Coding/Wired%20Brain%20Coffee/#assign-mock-content","text":"We combine two different namespace mappings (one for the Customer model and another for the CustomerDetailControl) to prepopulate the CustomerDetailControl with a customer defined in XAML. Because CustomerDetailControl exposes a public Customer property, this data can be assigned to the Customer property using property-element syntax. ( src ) MainPage.xaml <controls:CustomerDetailControl x:Name= \"customerDetailControl\" Grid.Row= \"1\" Grid.Column= \"1\" > <controls:CustomerDetailControl.Customer> <model:Customer FirstName= \"Clark\" LastName= \"Kent\" IsCoffeeDrinker= \"True\" /> </controls:CustomerDetailControl.Customer> </controls:CustomerDetailControl> In order to be able to assign the customer as direct content without specifying the property explicitly, the custom control class has to be decorated with the ContentProperty attribute. This is because by default any direct child is assigned to the Content property, which does not exist for this custom control. Using the ContentProperty allows us to specify a property to which to assign direct children. Controls/CustomerDetailControl.xaml.cs [ContentProperty(Name = nameof(Customer))] public sealed partial class CustomerDetailControl : UserControl { /* ... */ } MainPage.xaml <controls:CustomerDetailControl x:Name= \"customerDetailControl\" Grid.Row= \"1\" Grid.Column= \"1\" > <model:Customer FirstName= \"Clark\" LastName= \"Kent\" IsCoffeeDrinker= \"True\" /> </controls:CustomerDetailControl>","title":"Assign mock content"},{"location":"Coding/Wired%20Brain%20Coffee/#xaml-type-conversion","text":"Passing the customer as an attribute requires custom logic to parse the string. The target model is then decorated with the CreateFromString attribute. This is only for custom classes: primitive types and enumerations can be parsed by the XAML processor automatically. Models/CustomerConverter.cs namespace WiredBrainCoffee.UWP.Models { public static class CustomerConverter { public static Customer ParseStringAsCustomer ( string s ) { string [] values = s . Split ( ';' ); return new Customer { FirstName = values [ 0 ], LastName = values [ 1 ], IsCoffeeDrinker = bool . Parse ( values [ 2 ]) }; } } } Models/Customer.cs [CreateFromString(MethodName =\"WiredBrainCoffee.UWP.Models.CustomerConverter.ParseCustomerFromString\")] public class Customer : Observable { /* ... */ }","title":"XAML Type conversion"},{"location":"Coding/Wired%20Brain%20Coffee/#staticresource","text":"You can use the StaticResource Markup Extension to define the equivalent of XAML variables to store elements for attribution using attribute syntax. Every UI element has a property named Resources to which you can assign elements. Unlike the Items property of a ListView, however, this property is a Dictionary type, which means you must specify a key for these values (i.e. specify x:Key ). Because the XAML processor looks for resources as it crawls up the element tree, these resources can be organized at any level of the application, even in the App.xaml where it will become available to other files: ( src ) MainPage.xaml <Page.Resources> <model:Customer x:Key= \"Shazam\" FirstName= \"William\" LastName= \"William Batson\" IsCoffeeDrinker= \"false\" /> </Page.Resources> <!-- ... --> <controls:CustomerDetailControl Customer= \"{StaticResource Shazam}\" /> However, mocking data in XAML is an anti-pattern; Resource dictionaries are typically used for colors and predefined strings. Resource dictionaries are consolidated into their own files: ( src ) Resources/Brushes.xaml <ResourceDictionary xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" > <SolidColorBrush x:Key= \"customerListBackgroundBrush\" Color= \"#EEEEEE\" /> </ResourceDictionary> Resources/Strings.xaml <ResourceDictionary xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" > <x:String x:Key= \"applicationTitle\" > Wired Brain Coffee </x:String> </ResourceDictionary> These can then be referenced from App.xaml and are available for assignment in any appropriate attribute App.xaml <Application x:Class= \"WiredBrainCoffee.UWP.App\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP\" > <Application.Resources> <ResourceDictionary> <ResourceDictionary.MergedDictionaries> <ResourceDictionary Source= \"Resources/Brushes.xaml\" /> <ResourceDictionary Source= \"Resources/Strings.xaml\" /> </ResourceDictionary.MergedDictionaries> </ResourceDictionary> </Application.Resources> </Application> Controls/HeaderControl.xaml <UserControl x:Class= \"WiredBrainCoffee.UWP.Controls.HeaderControl\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP.Controls\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" d:DesignHeight= \"300\" d:DesignWidth= \"400\" > <Border Background= \"#F05A28\" > <Grid> <Grid.ColumnDefinitions> <ColumnDefinition/> <ColumnDefinition Width= \"Auto\" /> </Grid.ColumnDefinitions> <StackPanel Orientation= \"Horizontal\" HorizontalAlignment= \"Center\" > <Image Source= \"/Images/WiredBrainLogo.png\" Height= \"90\" /> <TextBlock Text= \"{StaticResource applicationTitle}\" FontSize= \"40\" VerticalAlignment= \"Center\" /> </StackPanel> <Button x:Name= \"ButtonMove\" HorizontalAlignment= \"Right\" Grid.Column= \"1\" Margin= \"10\" Click= \"ButtonMove_Click\" > <SymbolIcon x:Name= \"ButtonMove_Symbol\" Symbol= \"AlignRight\" /> </Button> </Grid> </Border> </UserControl>","title":"StaticResource"},{"location":"Coding/Wired%20Brain%20Coffee/#themeresource","text":"The ThemeResource Markup Extension makes UWP-specific theme resource dictionaries available. These same resources are available using StaticResource, but with ThemeResource they will be updated if the user changes his Windows theme from light to dark. ( src ) Resources/Brushes.xaml <ResourceDictionary xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" > <ResourceDictionary.ThemeDictionaries> <ResourceDictionary x:Key= \"Dark\" > <SolidColorBrush x:Key= \"customerListBackgroundBrush\" Color= \"#222222\" /> </ResourceDictionary> <ResourceDictionary x:Key= \"Light\" > <SolidColorBrush x:Key= \"customListBackgroundBrush\" Color= \"#EEEEEE\" /> </ResourceDictionary> </ResourceDictionary.ThemeDictionaries> </ResourceDictionary>","title":"ThemeResource"},{"location":"Coding/Wired%20Brain%20Coffee/#theme-selection","text":"A specific theme can be specified at any element by specifying a RequestedTheme attribute. However, this property cannot be changed at runtime. App.xaml <Application x:Class= \"WiredBrainCoffee.UWP.App\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP\" RequestedTheme= \"Dark\" > <Application.Resources> <ResourceDictionary> <ResourceDictionary.MergedDictionaries> <ResourceDictionary Source= \"Resources/Brushes.xaml\" /> <ResourceDictionary Source= \"Resources/Strings.xaml\" /> </ResourceDictionary.MergedDictionaries> </ResourceDictionary> </Application.Resources> </Application> A button to manually change theme involves simply assigning an ElementTheme enum value to the MainPage's RequestedTheme property. However, because on startup this has an ApplicationTheme enum value that is not evailable in ElementTheme , the MainPage's constructor must be changed to set the correct theme enum. Without this change, the first click after the application's startup will not change the theme at all, only set the correct ElementTheme . ( src ) MainPage.xaml <!-- Header --> <controls:HeaderControl Grid.ColumnSpan= \"3\" /> <Button Grid.ColumnSpan= \"3\" Click= \"ChangeTheme\" Margin= \"10\" VerticalAlignment= \"Top\" HorizontalAlignment= \"Right\" > <SymbolIcon Symbol= \"Placeholder\" /> </Button> MainPage.xaml.cs public MainPage () { this . InitializeComponent (); this . Loaded += MainPage_LoadedAsync ; App . Current . Suspending += App_SuspendingAsync ; _customerDataProvider = new CustomerDataProvider (); RequestedTheme = App . Current . RequestedTheme == ApplicationTheme . Dark ? ElementTheme . Dark : ElementTheme . Light ; } private void ChangeTheme ( object sender , RoutedEventArgs e ) { this . RequestedTheme = RequestedTheme == ElementTheme . Dark ? ElementTheme . Light : ElementTheme . Dark ; }","title":"Theme selection"},{"location":"Coding/Wired%20Brain%20Coffee/#color-theme","text":"The Fluent XAML Theme Editor on the Microsoft Store can generate ThemeResource dictionaries","title":"Color theme"},{"location":"Coding/Wired%20Brain%20Coffee/#data-binding","text":"Use the Binding markup extension to establish a binding on the CustomerDetailControl to the Customer property of customerListView Here, the Customer property is the target property , and the SelectedItem property of customerListView is the source property . So this data binding makes the information in the customerDetailControl (target) dependent on which item is selected (source). MainPage.xaml.cs <controls:CustomerDetailControl x:Name= \"customerDetailControl\" Grid.Row= \"1\" Grid.Column= \"1\" Customer= \"{Binding ElementName=customerListView,Path=SelectedItem,Mode=OneWay}\" > However, the target of a data binding needs to be a Dependency Property . The purpose of dependency properties is to provide a way to compute the value of a property based on the value of other inputs. The Visual Studio snippet for a dependency property is propdp . A dependency property includes a static readonly field of type DependencyProperty and a normal property that works as a frontend for that field by wrapping GetValue and SetValue . We implement the logic to update the controls with the selected customer as a callback function passed as the second argument of the PropertyMetadata object in the dependency property definition. This callback must be a static void function, and as such it has no access to the instantiated objects we have already named with x:Name . However, these objects are retrievable from the DependencyObject and DependencyPropertyChangedEventArgs parameters that are passed to the callback. ( src ) Controls/CustomerDetailControl.xaml.cs public Customer Customer { get { return ( Customer ) GetValue ( CustomerProperty ); } set { SetValue ( CustomerProperty , value ); } } // Using a DependencyProperty as the backing store for Customer. This enables animation, styling, binding, etc... public static readonly DependencyProperty CustomerProperty = DependencyProperty . Register ( \"Customer\" , typeof ( Customer ), typeof ( CustomerDetailControl ), new PropertyMetadata ( null , CustomerChangedCallback )); private static void CustomerChangedCallback ( DependencyObject d , DependencyPropertyChangedEventArgs e ) { if ( d is CustomerDetailControl customerDetailControl ) { var customer = e . NewValue as Customer ; customerDetailControl . txtFirstName . Text = customer ?. FirstName ?? \"\" ; customerDetailControl . txtLastName . Text = customer ?. LastName ?? \"\" ; customerDetailControl . chkDrinksCoffee . IsChecked = customer ?. IsCoffeeDrinker ; } } This bound the customerDetailControl to the item selected in customerListView. Now we implement the data bindings on each control of customerDetailControl. We give the root UserControl an x:Name so that we can refer to it in the binding markup extensions of the children as the value of ElementName . We can also remove the x:Name s of the individual controls, as well any trace of the event handlers! ( src ) Controls/CustomerDetailControl.xaml <UserControl x:Class= \"WiredBrainCoffee.UWP.Controls.CustomerDetailControl\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP.Controls\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" d:DesignHeight= \"300\" d:DesignWidth= \"400\" x:Name= \"root\" > <StackPanel> <TextBox Header= \"First name\" Margin= \"10\" Text= \"{Binding ElementName=root,Path=Customer.FirstName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" /> <TextBox Header= \"Last name\" Margin= \"10\" Text= \"{Binding ElementName=root,Path=Customer.LastName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" /> <CheckBox Content= \"Caffeine fiend\" Margin= \"10\" IsChecked= \"{Binding ElementName=root,Path=Customer.FirstName,Mode=TwoWay}\" /> </StackPanel> </UserControl>","title":"Data binding"},{"location":"Coding/Wired%20Brain%20Coffee/#viewmodel","text":"In the work above, we used the binding markup extension to bind one element to another, using that other element as a data source <TextBlock Text= \"{Binding ElementName=root,...}\" > We can use the MVVM pattern to assign the object to be bound to customerDetailControl as a data context . Every UI element has a DataContext property that can be set, and if it is set to an object then it can be placed there as a default data source that the XAML processor will find as it walks up the element tree. ( src ) This will allow us to simplify the markup, removing the x:Name from the root and the ElementName from the data bindings of the children. First we create the ViewModel, which incorporates some of the logic from the former App_SuspendingAsync and MainPage_LoadedAsync event handler methods. The ViewModel can dispose of the references to customerDetailControl and customerListView and replace them with its own Customers property. ViewModel/MainViewModel.cs using System.Collections.ObjectModel ; using System.Threading.Tasks ; using WiredBrainCoffee.UWP.DataProviders ; using WiredBrainCoffee.UWP.Models ; namespace WiredBrainCoffee.UWP.ViewModel { public class MainViewModel { public ObservableCollection < Customer > Customers { get ; } public MainViewModel () { _customerDataProvider = new CustomerDataProvider (); Customers = new ObservableCollection < Customer >(); } private CustomerDataProvider _customerDataProvider ; public async Task LoadAsync () { Customers . Clear (); var customers = await _customerDataProvider . LoadCustomersAsync (); foreach ( var customer in customers ) { Customers . Add ( customer ); } } public async Task SaveAsync () { await _customerDataProvider . SaveCustomersAsync ( Customers ); } } } To further decouple the ViewModel from the data provider, in order to facilitate testing, we extract an interface from CustomerDataProvider. DataProviders/ICustomerDataProvider.cs using System.Collections.Generic ; using System.Threading.Tasks ; using WiredBrainCoffee.UWP.Models ; namespace WiredBrainCoffee.UWP.DataProviders { public interface ICustomerDataProvider { Task < IEnumerable < Customer >> LoadCustomersAsync (); Task SaveCustomersAsync ( IEnumerable < Customer > customers ); } } Now we implement an ICustomerDataProvider parameter to the ViewModel constructor, and remember to pass in a new data provider as an argument implementing that interface. The private field _customerDataProvider can be removed. ViewModel/MainViewModel.cs private ICustomerDataProvider _customerDataProvider ; public MainViewModel ( ICustomerDataProvider customerDataProvider ) { _customerDataProvider = customerDataProvider ; Customers = new ObservableCollection < Customer >(); } MainPage.xaml.cs this . ViewModel = new MainViewModel ( new CustomerDataProvider ()); DataContext = ViewModel ; Finally, since we have a data context on MainPage, we can set it as a source for the ListView MainPage.xaml <ListView ItemsSource= \"{Binding Customers,Mode=OneWay}\" Grid.Row= \"1\" x:Name= \"customerListView\" DisplayMemberPath= \"FirstName\" >","title":"ViewModel"},{"location":"Coding/Wired%20Brain%20Coffee/#binding-the-selected-customer","text":"At this moment, customerDetailControl is still tied to customerListView's SelectedItem property directly, and not through the ViewModel. To change this, we implement a SelectedCustomer property on the ViewModel that will be bound to both. We reuse the Observable base class that implement the INotifyPropertyChanged interface. This allows us to use the OnPropertyChanged() method in the setter of the new SelectedItem property. We replace the element binding of customerListView with a binding to the SelectedCustomer property in the data context. ViewModel/MainViewModel.cs public class MainViewModel : Observable { private Customer selectedCustomer ; public Customer SelectedCustomer { get { return selectedCustomer ; } set { selectedCustomer = value ; OnPropertyChanged (); } } // ... } MainPage.xaml <ListView ItemsSource= \"{Binding Customers,Mode=OneWay}\" Grid.Row= \"1\" x:Name= \"customerListView\" DisplayMemberPath= \"FirstName\" SelectedItem= \"{Binding SelectedCustomer,Mode=TwoWay}\" > </ListView>","title":"Binding the selected customer"},{"location":"Coding/Wired%20Brain%20Coffee/#datatemplate","text":"At this moment, customerListView is being populated by a single property of each Customer - their first name. If we want to compose more complex information, we can assign DataTemplate to the ListView's ItemTemplate property. This will create the enclosed controls for each element in the ListView. Remember to remove the DisplayMemberPath attribute! <ListView ItemsSource= \"{Binding Customers,Mode=OneWay}\" Grid.Row= \"1\" x:Name= \"customerListView\" SelectedItem= \"{Binding SelectedCustomer,Mode=TwoWay}\" > <ListView.ItemTemplate> <DataTemplate> <StackPanel Orientation= \"Horizontal\" > <TextBlock Text= \"{Binding FirstName}\" /> <TextBlock Text= \"{Binding LastName}\" Margin= \"5 0 0 0\" FontWeight= \"Bold\" /> </StackPanel> </DataTemplate> </ListView.ItemTemplate> </ListView>","title":"DataTemplate"},{"location":"Coding/Wired%20Brain%20Coffee/#xbind","text":"There are two data binding types available in XAML ( src ) Binding markup extension resolves the binding path at runtime. x:Bind resolves the binding path at compile-time , generating C# code and offering better performance and compile-time errors. You can also step into the compiled code, providing a better debugging experience. x:Bind should generally be preferred, however it is available only in UWP. Binding markup extension can have several different data sources, depending on defined attributes. ElementName Source RelativeSource If none of these are defined, then the binding markup extension resolves to the DataContext property. x:Bind, in contrast, binds only to the parent Page or UserControl element. So any property of MainPage will be accessible, and any property of that object will also be accessible using dot notation. Most bindings are easily translated between the two types if the ViewModel has already been implemented as a property of MainPage: Binding markup extension public MainPage () { this . InitializeComponent ; this . ViewModel = new MainViewModel (); DataContext = ViewModel ; } <ListView ItemsSource= \"{Binding Customers,Mode=OneWay}\" > <!-- ...--> </ListView> x:Bind public MainPage () { this . InitializeComponent ; this . ViewModel = new MainViewModel (); // DataContext = ViewModel; } <ListView ItemsSource= \"{x:Bind ViewModel.Customers,Mode=OneWay}\" > <!-- ...--> </ListView> Notably, the default binding mode of the Binding markup extension is OneWay x:Bind is OneTime , although this can be changed by setting x:DefaultBindMode on the root element. Set explicitly <Page> <Listview ItemsSource= \"{x:Bind ViewModelCustomers,Mode=OneWay}\" /> </Page> Changing default binding mode <Page x:DefaultBindMode= \"OneWay\" > <Listview ItemsSource= \"{x:Bind ViewModelCustomers}\" /> </Page> customDetailControl, which previously used the binding markup extension but set the root element as an explicitly named source property, is notably simplified after replacing with x:Bind . We can now directly access the user control's property. Before <UserControl x:Class= \"WiredBrainCoffee.UWP.Controls.CustomerDetailControl\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP.Controls\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" d:DesignHeight= \"300\" d:DesignWidth= \"400\" x:Name= \"root\" > <StackPanel> <TextBox Header= \"First name\" Margin= \"10\" Text= \"{Binding ElementName=root,Path=Customer.FirstName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" /> <TextBox Header= \"Last name\" Margin= \"10\" Text= \"{Binding ElementName=root,Path=Customer.LastName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" /> <CheckBox Content= \"Caffeine fiend\" Margin= \"10\" IsChecked= \"{Binding ElementName=root,Path=Customer.FirstName,Mode=TwoWay}\" /> </StackPanel> </UserControl> After <UserControl x:Class= \"WiredBrainCoffee.UWP.Controls.CustomerDetailControl\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP.Controls\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" d:DesignHeight= \"300\" d:DesignWidth= \"400\" > <StackPanel> <TextBox Header= \"First name\" Margin= \"10\" Text= \"{x:Bind Customer.FirstName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" /> <TextBox Header= \"Last name\" Margin= \"10\" Text= \"{x:Bind Customer.LastName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" /> <CheckBox Content= \"Caffeine fiend\" Margin= \"10\" IsChecked= \"{x:Bind Customer.IsCoffeeDrinker,Mode=TwoWay}\" /> </StackPanel> </UserControl> x:Bind can also be implemented in the ListView's ItemTemplate, so long as the x:DataType attribute is set on DataTemplate. We must also remember to set the Mode binding property, since x:Bind's default is OneTime! ( src ) <ListView Grid.Row= \"1\" ItemsSource= \"{x:Bind ViewModel.Customers}\" SelectedItem= \"{x:Bind ViewModel.SelectedCustomer,Mode=TwoWay}\" > <ListView.ItemTemplate> <DataTemplate x:DataType= \"model:Customer\" > <StackPanel Orientation= \"Horizontal\" > <TextBlock Text= \"{x:Bind FirstName}\" /> <TextBlock Text= \"{x:Bind LastName}\" Margin= \"5 0 0 0\" FontWeight= \"Bold\" /> </StackPanel> </DataTemplate> </ListView.ItemTemplate> </ListView> x:Bind can also hide or reveal controls depending on boolean value. A new boolean property is formed on the ViewModel, and we wire the OnPropertyChanged event handler to it. We also bind this value to the Visibility attribute of customerDetailControl. This will hide the customerDetailControl on application startup before the user selects a customer. ( src ) public Customer SelectedCustomer { get { return selectedCustomer ; } set { if ( selectedCustomer != value ) { selectedCustomer = value ; OnPropertyChanged (); OnPropertyChanged ( nameof ( IsCustomerSelected )); } } } public bool IsCustomerSelected => SelectedCustomer != null ; <controls:CustomerDetailControl x:Name= \"customerDetailControl\" Grid.Row= \"1\" Grid.Column= \"1\" Customer= \"{x:Bind ViewModel.SelectedCustomer,Mode=TwoWay}\" Visibility= \"{x:Bind ViewModel.IsCustomerSelected}\" /> We can also implement a third TextBlock in the ListView's ItemTemplate to show a string depending on the value of the CheckBox. <ListView Grid.Row= \"1\" ItemsSource= \"{x:Bind ViewModel.Customers}\" SelectedItem= \"{x:Bind ViewModel.SelectedCustomer,Mode=TwoWay}\" > <ListView.ItemTemplate> <DataTemplate x:DataType= \"model:Customer\" > <StackPanel Orientation= \"Horizontal\" > <TextBlock Text= \"Dev\" Opacity= \"0.5\" Visibility= \"{x:Bind IsCoffeeDrinker}\" Margin= \"0 0 5 0\" /> <TextBlock Text= \"{x:Bind FirstName}\" /> <TextBlock Text= \"{x:Bind LastName}\" Margin= \"5 0 0 0\" FontWeight= \"Bold\" /> </StackPanel> </DataTemplate> </ListView.ItemTemplate> </ListView>","title":"x:Bind"},{"location":"Coding/Wired%20Brain%20Coffee/#toggling-visibility","text":"In order to fully implement the logic of the MVVM pattern, we should move functionality that deals with the logic of the app as a whole to the ViewModel. That would include the Add and Delete buttons. ( src ) Before <CommandBar> <AppBarButton x:Name= \"AddCustomer\" Click= \"AddCustomer_Click\" Label= \"Add\" > <SymbolIcon Symbol= \"Add\" /> </AppBarButton> <AppBarButton x:Name= \"DeleteCustomer\" Click= \"DeleteCustomer_Click\" Label= \"Delete\" > <SymbolIcon Symbol= \"Delete\" /> </AppBarButton> <AppBarButton x:Name= \"btn_MoveSideBar\" Click= \"btn_MoveSideBar_Click\" Label= \"Move sidebar\" > <SymbolIcon x:Name= \"btn_MoveSideBar_Symbol\" Symbol= \"AlignRight\" /> </AppBarButton> </CommandBar> After <CommandBar> <AppBarButton x:Name= \"AddCustomer\" Click= \"{x:Bind ViewModel.AddCustomer_Click}\" Label= \"Add\" > <SymbolIcon Symbol= \"Add\" /> </AppBarButton> <AppBarButton x:Name= \"DeleteCustomer\" Click= \"{x:Bind ViewModel.DeleteCustomer_Click}\" Label= \"Delete\" > <SymbolIcon Symbol= \"Delete\" /> </AppBarButton> <AppBarButton x:Name= \"btn_MoveSideBar\" Click= \"btn_MoveSideBar_Click\" Label= \"Move sidebar\" > <SymbolIcon x:Name= \"btn_MoveSideBar_Symbol\" Symbol= \"AlignRight\" /> </AppBarButton> </CommandBar>","title":"Toggling visibility"},{"location":"Coding/Wired%20Brain%20Coffee/#styling","text":"You can define a style that has to be used more than once by declaring a Style element on a UserControl's Resources property. ( src ) <UserControl> <UserControl.Resources> <Style x:Key= \"myTextBoxStyle\" TargetType= \"TextBox\" > <Style.Setters> <Setter Property= \"Margin\" Value= \"10\" /> <Setter Property= \"CornerRadius\" Value= \"10\" /> </Style.Setters> </Style> </UserControl.Resources> </UserControl> This Style can then be used as a StaticResource, setting the value of the Style attribute.","title":"Styling"},{"location":"Coding/XAML/","text":"\ud83e\udde0 XAML TODO: Watch WinRT course Download Windows Community Toolkit Sample App Win2D Example Gallery Extend Superheroes app with DataGrid SplitView ListDetailsView NavigationView Markdown browser using MarkdownTextBlock Complete interactive image gallery tutorial UWP FPS using DirectX Dependency properties Research interoperability with DirectX (apparently possible if you wrap DirectX calls in a separate Windows Runtime metadata file ) Syntax Every XAML element maps to a C# class , and every XAML attribute maps to a class property or an event . There are several syntaxes available for use that correspond to various methods of declaring objects: Object-element syntax has the type's name within angle brackets, similar to HTML. When the object contains other objects, it is called a container . If the object does not contain other objects, it can be declared with a self-closing tag: Attribute syntax has the property value set by declaring an attribute. In property-element syntax , signified by a period in the element name, where the portion of the element following the dot representing the identifier of the property. Content-property syntax is similar to the property element syntax in that a property's value is set by a child. However, in this case the XAML processor interpolates the correct property element, typically Content. Some controls can accept more than one child element. In the background, this is actually using the content-property syntax to assign to the Children property. Object-element <Canvas> <Rectangle /> </Canvas> Attribute <Rectangle Name= \"rectangle1\" Width= \"100\" Height= \"100\" Fill= \"Blue\" /> Property-element <Rectangle Name= \"rectangle1\" Width= \"100\" Height= \"100\" > <Rectangle.Fill> <SolidColorBrush Color= \"Blue\" /> </Rectangle.Fill> </Rectangle> Content-property <Button> Add customer </Button> Multiple children <StackPanel> <TextBlock> Hello </TextBlock> <TextBlock> World </TextBlock> </StackPanel> Namespaces A XAML file usually declares a default XAML namespace in its root element. This default namespace defines elements that can be declared without qualifying them by a prefix (e.g. x: ). XAML namespaces apply only to the specific element on which they are declared and its children, which explains why they are typically placed on the root element. ( src ) Most XAML files have two xmlns declarations xmlns maps a default XAML namespace xmlns:x maps a separate XAML namespace for XAML-defined language elements to the x: prefix: xmlns:mc indicates and supports a markup compatibility mode for reading XAML. xmlns:mc=\"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable=\"d\" Directives x:Name uniquely identifies object elements for access to the object from code-behind x:Key sets a unique key for each resource in a ResourceDictionary x:Class specifies the CLR namespace and class name for the class that provides code-behind for a XAML page. The x:Class directive configures XAML markup compilation to join partial classes between markup and code-behind. In this example it can be seen how it refers to the MainPage class within the Project namespace. ( src ) x:Bind is a form of data-binding Language elements are commonly used features necessary for Windows Runtime apps. For example, to join any code-behind to a XAML file through a partial class, you must name that class as the x:Class attribute in the root element of the XAML file. XAML <Page x:Class= \"Project.MainPage\" /> Code-behind namespace Project { public sealed partial class MainPage : Page { public MainPage () { this . InitializeComponent (); } } } Attached property Attached properties can be modified or queried in C# code as well, as long as the XAML element has an x:Name defined, which gives that particular element an identifier. For the column property of Grid , the static methods GetColumn and SetColumn are available. Here a button press changes the column value of an enclosing Grid and toggles between two different Symbol values. ( src ) private void ButtonMove_Click ( object sender , RoutedEventArgs e ) { int column = Grid . GetColumn ( customerListGrid ); int newcolumn = column == 0 ? 2 : 0 ; Grid . SetColumn ( customerListGrid , newcolumn ); moveSymbolIcon . Symbol = newcolumn == 0 ? Symbol . Forward : Symbol . Back ; } UWP Universal Windows Platform (UWP) refers to both a UI framework incorporating the Fluent Design System as well as an app model . When the UWP XAML framework was released in 2012, UWP was touted as a means to develop for many different device platforms, including mobile and tablet. Until recently, the UWP XAML framework was only available for applications using the UWP app model for apps destined for the Microsoft Store. However, UWP development has floundered over the past half decade as Microsoft has been unable to produce sufficient interest in its mobile and tablet devices or the Microsoft Store. Microsoft itself has ceased development of UWP apps for Office or for Xbox, for which it has turned rather to Electron. WinUI Windows UI Library (WinUI) 3 is a native UI framework that represents a rebranding of the UWP UI framework , which had previously only been available for applications using the UWP app model , and a move to make it available for both app models. Still in development, it promises to deliver a unified framework and all the styles and controls previously distributed in WinUI 2, which in turn is a NuGet packaging containing the UWP XAML controls and styles. Unfortunately, because the UWP framework had been available only for the UWP app model , it did not experience wide adoption among developers who prefered the flexibility of the older Win32 \"app model\" (or rather, the lack of one). App models App model is a term Microsoft began using after the release of UWP in 2012 to refer to the hosting model, or the rigidly defined parameters for how an application is installed, stores state, manages versions, and integrates with the operating system and other apps, that generally define an application lifecycle. Specifically, the term was used in the context of describing the security sandbox and other restrictions of the UWP app model for Microsoft Store applications. Previous to UWP, there had been no definition of an \"app model\" per se and developers of a Win32 application were free to determine these parameters individually. This caused wide disparity in implementation among applications, resulting in registry bloat and poorly managed uninstallations. The UWP App Model was specifically introduced to answer these concerns which had plagued generations of Windows. ( src ) The failure of UWP as an app model resulted in Project Reunion , an effort to reunify the bifurcated Windows development landscape. Event handling Event hooks can be used to subscribe to events. For example, the MainPage class exposes a Loaded event that can be used to fill a prototype app with dogfood data, rather than putting hardcoding it in the XAML markup. Notably, these event handlers must return only void types. public sealed partial class MainPage : Page { public MainPage () { this . InitializeComponent (); this . Loaded += MainPage_Loaded ; } private void MainPage_Loaded ( object sender , RoutedEventArgs e ) { throw new NotImplementedException (); } } Some XAML controls have attributes that map to events: Element Property Button Click CheckBox Checked , Unchecked ListView SelectionChanged TextBox TextChanged Defining an event handler in XAML is equivalent to doing so in C#. XAML <Button Content= \"Add customer\" Click= \"Button_ClickHandler\" > C# var btn = new Button { Content = \"Add customer\" this . Click = // No clue if this is right... }; SelectionChanged Markup <ListView x:Name= \"customerListView\" SelectionChanged= \"CustomerListView_SelectionChanged\" /> Code-behind private void CustomerListView_SelectionChanged ( object sender , SelectionChangedEventArgs e ) { var customer = customerListView . SelectedItem as Customer ; txtFirstName . Text = customer ?. FirstName ?? \"\" ; txtLastName . Text = customer ?. LastName ?? \"\" ; chkIsDeveloper . IsChecked = customer ?. IsDeveloper ; } TextChanged Markup <StackPanel> <TextBox x:Name= \"txtFirstName\" Header= \"Firstname\" TextChanged= \"TextBox_TextChanged\" Margin= \"10\" /> <TextBox x:Name= \"txtLastName\" Header= \"Lastname\" TextChanged= \"TextBox_TextChanged\" Margin= \"10\" /> <CheckBox x:Name= \"chkIsDeveloper\" Content= \"IsDeveloper\" Margin= \"10\" Checked= \"CheckBox_IsCheckedChanged\" Unchecked= \"CheckBox_IsCheckedChanged\" /> </StackPanel> Code-behind private void TextBox_TextChanged ( object sender , TextChangedEventArgs e ) { UpdateCustomer (); } private void CheckBox_IsCheckedChanged ( object sender , RoutedEventArgs e ) { UpdateCustomer (); } private void UpdateCustomer () { var customer = customerListView . SelectedItem as Customer ; if ( customer != null ) { customer . FirstName = txtFirstName . Text ; customer . LastName = txtLastName . Text ; customer . IsDeveloper = chkIsDeveloper . IsChecked .{{ c4 :: GetValueOrDefault }}(); } } Handlers for Loaded and Unloaded are automatically attached to any Page that uses the NavigationHelper class. Loaded Unloaded Custom controls You can take a group of controls and create a custom control using what is called a namespace mapping , where a C# class is made available in the XAML markup. More specifically, a custom control defined in XAML along with its accompanying code-behind file must use UserControl as its base class. In this sense, a custom control is really a special case of a namespace mapping, which can also be used to populate an application with mock data during development. Markup <UserControl x:Class= \"Project.Controls.CustomControl\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP.Controls\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" > <TextBlock Text= \"Hello, world!\" /> </UserControl> Code-behind namespace Project.Controls { public sealed partial class CustomControl : UserControl { public CustomControl () { this . InitializeComponent (); } } } Mock data The most basic method of adding mock data is by hardcoding data in the XAML markup . Somewhat more sophisticated is the option of hardcoding data in the code-behind . The x:Bind directive can be used to bind an IEnumerable data source to either the Items or ItemsSource attributes. An ObservableCollection is preferred in WinUI programming because it raises events when properties are changed, but Lists and Arrays also work. If the collection is made of objects, the DisplayMemberPath allows the specification of a particular property on those objects to be displayed. Notably, classes specifically need to be used, and not structs, for the members of these collections. With namespace mapping , classes within the C# namespace can be used in XAML markup. ( src ) Most robust of all is creating a Data Provider class which will fall back to mock data when the data source is not available. This will allow any number of other data sources to be plugged in, such as databases or REST services. ( src ) Hardcoding in XAML <ListView> <ListViewItem> Aristotle </ListViewItem> <ListViewItem> Euclid </ListViewItem> <ListViewItem> Plato </ListViewItem> <ListViewItem> Socrates </ListViewItem> </ListView> Hardcoding in C# Markup <Page> <ListView ItemsSource= \"{x:Bind Starships}\" SelectedItem= \"{x:Bind Starships[0]}\" DisplayMemberPath= \"Display\" /> </Page> Code-behind namespace Starships.Models { class Starship { public string Name ; public string Registry ; public int Crew ; public string Display { get { return Name + Registry ; } } } } Namespace mapping Markup <Page xmlns:model= \"using:Project.Models\" > <ListView DisplayMemberPath= \"Display\" > <model:Starship Name= \"USS Enterprise\" Registry= \"NCC-1701\" Crew= \"140\" /> </ListView> </Page> Code-behind namespace Starships.Models { class Starship { public string Name ; public string Registry ; public int Crew ; public string Display { get { return Name + Registry ; } } } } Data provider Themes Windows 10 themes ( \"Light\" , \"Dark\" , and \"HighContrast\" can be specified as a property of the Application element: Light <Application x:Class= \"Scratchpad.App\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:Scratchpad\" RequestedTheme= \"Light\" > <Application.Resources> <ResourceDictionary> <ResourceDictionary.MergedDictionaries> <XamlControlsResources xmlns= \"using:Microsoft.UI.Xaml.Controls\" /> <!-- Other merged dictionaries here --> </ResourceDictionary.MergedDictionaries> <!-- Other app resources here --> </ResourceDictionary> </Application.Resources> </Application> Dark <Application x:Class= \"Scratchpad.App\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:Scratchpad\" RequestedTheme= \"Dark\" > <Application.Resources> <ResourceDictionary> <ResourceDictionary.MergedDictionaries> <XamlControlsResources xmlns= \"using:Microsoft.UI.Xaml.Controls\" /> <!-- Other merged dictionaries here --> </ResourceDictionary.MergedDictionaries> <!-- Other app resources here --> </ResourceDictionary> </Application.Resources> </Application> Theme colors Layout The layout panels in XAML serve a similar function to the geometry manager methods in tkinter. There are various layout panels available. Grid RelativePanel StackPanel VariableSizeWrapGrid Data binding There are two data binding types available in XAML ( src ) Binding markup extension resolves the binding path at runtime. It can accept data sources from the binding properties ElementName , Source , and RelativeSource . If none of these are defined, then the binding markup extension resolves to the DataContext property. x:Bind resolves the binding path at compile-time , generating C# code and offering better performance and compile-time errors. You can also step into the compiled code, providing a better debugging experience. x:Bind should generally be preferred, however it is available only in UWP. x:Bind , in contrast, binds only to the parent Page or UserControl element. So any property of MainPage will be accessible, and any property of that object will also be accessible using dot notation. Most bindings are easily translated between the two types: Binding markup extension public MainPage () { this . InitializeComponent ; this . ViewModel = new MainViewModel (); DataContext = ViewModel ; } <ListView ItemsSource= \"{Binding Customers,Mode=OneWay}\" > <!-- ...--> </ListView> x:Bind public MainPage () { this . InitializeComponent ; this . ViewModel = new MainViewModel (); // DataContext = ViewModel; } <ListView ItemsSource= \"{x:Bind ViewModel.Customers,Mode=OneWay}\" > <!-- ...--> </ListView> Notably, the default binding mode of the binding markup extension is OneWay **, whereas that of x:Bind is ** OneTime , although this can be changed by setting x:DefaultBindMode on the root element. Changing default bind mode <Page x:DefaultBindMode= \"OneWay\" > <Listview ItemsSource= \"{x:Bind ViewModelCustomers}\" /> </Page> Set explicitly <Page> <Listview ItemsSource= \"{x:Bind ViewModelCustomers,Mode=OneWay}\" /> </Page> Type conversion By default, XAML attribute values are strings. In order for the XAML processor to interpret them as objects, they must be converted. Type converters can convert string representations of attribute values to property elements. For example, a type converter is what is used with the XAML declaration HorizontalAlignment=\"Left\" , which is mapped to a specific enumeration within the Windows.UI.XAML namespace. ref In UWP, the XAML processor integrates the conversion logic to convert the Margin declaration to a Thickness object. But in WPF , TypeConverters are used. Markup <Button Margin= \"10,20,10,30\" Content= \"Click me\" /> Code-behind var btn = new Button { Margin = new Thickness(10, 20, 10, 30); }; Markup Extensions Markup extensions are placed between curly braces { and } and create a reference to a ResourceDictionary . They can be used to unify styling across an application. The most common markup extensions are: StaticResource refers to resources defined in resource dictionaries ThemeResource for Windows native themeing colors Binding for data binding expressions, which require the bound property to be a dependency property Here, the background of the grid is bound to a color from the Windows-native theming and the TextBlock's Foreground property is bound to a color defined in a resource dictionary defined on the same page. Multiple properties can be set at the same time by setting a Style property element. <Page> <Page.Resources> <Style TargetType= \"Button\" x:Key= \"MyButtonStyle\" > <Setter Property= \"Background\" Value= \"Blue\" /> <Setter Property= \"FontFamily\" Value= \"Arial Black\" /> <Setter Property= \"FontSize\" Value= \"36\" /> </Style> </Page.Resources> <Button Content= \"Hello world\" Style= \"{StaticResource MyButtonStyle}\" /> </Page> <Page> <Page.Resources> <SolidColorBrush x:Key= \"MyBrush\" Color= \"Brown\" /> </Page.Resources> <Grid Background= \"{ThemeResource ApplicationPageBackgroundThemeBrush}\" > <TextBlock Text= \"Hello World\" Foreground= \"{StaticResource MyBrush}\" /> </Grid> </Page> src Dependency properties Only dependency properties can be targets for data binding in UWP and WPF . The propdp snippet in Visual Studio can be used to create one. The DependencyObject class, which is a base class of all UI elements in UWP and WPF, exposes the GetValue and SetValue methods, which are used to ... Multi-instance A multi-instance application is one that can run in several instances, which is necessary to allow users to open new windows. A collection of templates is available as a Visual Studio extension . These templates modify the appxmanifest file by setting the SupportsMultipleInstances attribute to true: <Package ... xmlns:desktop4= \"http://schemas.microsoft.com/appx/manifest/desktop/windows10/4\" xmlns:iot2= \"http://schemas.microsoft.com/appx/manifest/iot/windows10/2\" IgnorableNamespaces= \"uap mp desktop4 iot2\" > ... <Applications> <Application Id= \"App\" ... desktop4:SupportsMultipleInstances= \"true\" iot2:SupportsMultipleInstances= \"true\" > ... </Application> </Applications> ... </Package> Resources: Create a multi-instance UWP app Patterns Action on focus UI elements expose the GotFocus event hook for when a user clicks or tabs into a control. Example handler selecting all text in a TextBox: ( src ) Markup <Page> <TextBox GotFocus= \"TextBox_GotFocus\" /> </Page> Code-behind void TextBox_GotFocus ( object sender , RoutedEventArgs e ) { TextBox textBox = sender as TextBox ; textBox . SelectAll (); } Master/Details The master/details pattern has a master pane (usually a ListView ) and a details pane for content. MVVM In the Model, View, ViewModel (MVVM) pattern, which implicitly relies on OOP principles, the Model represents the data model for the objects being manipulated, and the ViewModel is the model for the View , that is, the state of the application as represented in a class. In WinUI, the project that contains the Views (that is, the XAML files) must first add references to the projects where the Models and ViewModel are contained. These will allow the code-behind file of the MainWindow to reference those classes. The class representing the ViewModel is instantiated and assigned to an attribute. That class's methods can then be called by using the x:Bind attribute syntax on, for instance, a ListView element. Markup <Window> <ListView ItemsSource= \"{x:Bind ViewModel.Employees, Mode=OneWay}\" SelectedItem= \"{x:Bind ViewModel.SelectedEmployee, Mode=OneWay}\" DisplayMemberPath= \"FirstName\" /> </Window> Code-behind using EmployeeManager.DataAccess ; using EmployeeManager.ViewModel ; using Microsoft.UI.Xaml ; namespace EmployeeManager.WinUI { public sealed partial class MainWindow : Window { public MainWindow () { ViewModel = new MainViewModel ( new EmployeeDataProvider ()); this . InitializeComponent (); } public MainViewModel ViewModel { get ; } } } Navigation App layout typically begins with the choice of navigation model , which defines how users navigate between pages in the app. There are two common navigation models: left nav and top nav Examples List and details <Page x:Class= \"Superheroes.MainPage\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:Superheroes\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" Background= \"{ThemeResource ApplicationPageBackgroundThemeBrush}\" > <StackPanel> <CommandBar> <AppBarButton x:Name= \"AddHero\" Click= \"AddHero_Click\" Label= \"Add hero\" > <SymbolIcon Symbol= \"Add\" /> </AppBarButton> <AppBarButton x:Name= \"DelHero\" Click= \"DelHero_Click\" Label= \"Remove hero\" > <SymbolIcon Symbol= \"Delete\" /> </AppBarButton> </CommandBar> <TextBox Header= \"First name\" Text= \"{Binding ElementName=heroesListView,Path=SelectedItem.FirstName,Mode=TwoWay}\" /> <TextBox Header= \"Last name\" Text= \"{Binding ElementName=heroesListView,Path=SelectedItem.LastName,Mode=TwoWay}\" /> <TextBox Header= \"Alias\" Text= \"{Binding ElementName=heroesListView,Path=SelectedItem.Superhero,Mode=TwoWay}\" /> <ListView x:Name= \"heroesListView\" ItemsSource= \"{Binding Heroes,Mode=TwoWay}\" SelectedItem= \"{Binding SelectedHero,Mode=TwoWay}\" > <ListView.ItemTemplate> <DataTemplate> <StackPanel Orientation= \"Horizontal\" > <TextBlock Text= \"{Binding Superhero}\" FontWeight= \"Bold\" /> <TextBlock Text= \"{Binding FirstName}\" Margin= \"5 0 0 0\" /> <TextBlock Text= \"{Binding LastName}\" Margin= \"5 0 0 0\" /> </StackPanel> </DataTemplate> </ListView.ItemTemplate> </ListView> </StackPanel> </Page> In order to link two controls, some form of data binding is necessary. Here, TextBox elements are individually bound to the string properties of the selected ListView element with the Binding markup extension using the ElementName binding property. The selected item and the path to the string property are combined using dot notation and assigned to the Path binding property. Custom control The textboxes can also be consolidated into a custom control using UserControl . In this case, the custom control must expose a target property that will be bound to the ListView's SelectedItem property. Individual TextBoxes bound to ListView <TextBox Header= \"First name\" Text= \"{Binding ElementName=heroesListView,Path=SelectedItem.FirstName,Mode=TwoWay}\" /> <TextBox Header= \"Last name\" Text= \"{Binding ElementName=heroesListView,Path=SelectedItem.LastName,Mode=TwoWay}\" /> <TextBox Header= \"Alias\" Text= \"{Binding ElementName=heroesListView,Path=SelectedItem.Superhero,Mode=TwoWay}\" /> <!-- <local:MyTextBoxes Hero=\"{Binding ElementName=heroesListView,Path=SelectedItem,Mode=TwoWay}\"/> --> UserControl <!--<TextBox Header=\"First name\" Text=\"{Binding ElementName=heroesListView,Path=SelectedItem.FirstName,Mode=TwoWay}\"/> <TextBox Header=\"Last name\" Text=\"{Binding ElementName=heroesListView,Path=SelectedItem.LastName,Mode=TwoWay}\"/> <TextBox Header=\"Alias\" Text=\"{Binding ElementName=heroesListView,Path=SelectedItem.Superhero,Mode=TwoWay}\"/>--> <local:MyTextBoxes Hero= \"{Binding ElementName=heroesListView,Path=SelectedItem,Mode=TwoWay}\" /> In order to accept the data binding, the target property must be implemented as a dependency property . This dependency property works through a static callback function which sets the individual textbox values to the bound target property. Naively, the callback function can be made to change the individual TextBox elements, provided they have x:Name defined. But a better technique is using the Binding markup extension to bind the individual TextBoxes to the root node using the ElementName binding property. This requires the root node to have x:Name defined. Both MyTextBoxes's binding to heroesListView and the individual TextBox bindings to the Hero property of root need to be TwoWay in order for changes made in the TextBox to take effect. Notably, the ListView does not reflect any changes made yet. Callback <TextBox x:Name= \"FirstNameTextbox\" Header= \"First name\" Margin= \"10\" /> <TextBox x:Name= \"LastNameTextbox\" Header= \"Last name\" Margin= \"10\" /> <TextBox x:Name= \"SuperheroTextbox\" Header= \"Alias\" Margin= \"10\" /> private static void HeroChangedCallback ( DependencyObject d , DependencyPropertyChangedEventArgs e ) { if ( d is MyTextBoxes myTextBoxes ) { var hero = e . NewValue as Hero ; myTextBoxes . FirstNameTextbox . Text = hero . FirstName ; myTextBoxes . LastNameTextbox . Text = hero . LastName ; myTextBoxes . SuperheroTextbox . Text = hero . Superhero ; } } Binding <TextBox x:Name= \"FirstNameTextbox\" Header= \"First name\" Margin= \"10\" Text= \"{Binding ElementName=root,Path=Hero.FirstName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" /> <TextBox x:Name= \"LastNameTextbox\" Header= \"Last name\" Margin= \"10\" Text= \"{Binding ElementName=root,Path=Hero.LastName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" /> <TextBox x:Name= \"SuperheroTextbox\" Header= \"Alias\" Margin= \"10\" Text= \"{Binding ElementName=root,Path=Hero.Superhero,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" /> private static void HeroChangedCallback ( DependencyObject d , DependencyPropertyChangedEventArgs e ) { //if (d is MyTextBoxes myTextBoxes) //{ // var hero = e.NewValue as Hero; // myTextBoxes.FirstNameTextbox.Text = hero.FirstName; // myTextBoxes.LastNameTextbox.Text = hero.LastName; // myTextBoxes.SuperheroTextbox.Text = hero.Superhero; //} } ListView In order for the ListView to update, the underlying model must raise the PropertyChanged event. I really don't understand this very well, so here is the Observable class that implements the INotifyPropertyChanged interface. The model must inherit from this base class and the Set accessor of any property should fire OnPropertyChanged() . public class Observable : INotifyPropertyChanged { public event PropertyChangedEventHandler PropertyChanged ; protected void OnPropertyChanged ([ CallerMemberName ] string propertyName = null ) { PropertyChanged ?. Invoke ( this , new PropertyChangedEventArgs ( propertyName )); } } ViewModel x:Bind Binding markup extensions are trivially changed to compiled data bindings by replacing Binding with x:Bind . Note Note that ListView will enter an infinite loop if the SelectedHero property does not incorporate additional logic. public Hero SelectedHero { get { return _selectedHero ; } set { if ( _selectedHero != value ) { _selectedHero = value ; OnPropertyChanged (); OnPropertyChanged ( nameof ( IsHeroSelected )); } } } Binding markup extension <ListView x:Name= \"heroesListView\" ItemsSource= \"{Binding Heroes,Mode=OneWay}\" SelectedItem= \"{Binding SelectedHero,Mode=TwoWay}\" Margin= \"0\" > <TextBox x:Name= \"FirstNameTextbox\" Header= \"First name\" Margin= \"10\" Text= \"{Binding ElementName=root,Path=Hero.FirstName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" GotFocus= \"Textbox_GotFocus\" /> <TextBox x:Name= \"LastNameTextbox\" Header= \"Last name\" Margin= \"10\" Text= \"{Binding ElementName=root,Path=Hero.LastName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" GotFocus= \"Textbox_GotFocus\" /> <TextBox x:Name= \"SuperheroTextbox\" Header= \"Alias\" Margin= \"10\" Text= \"{Binding ElementName=root,Path=Hero.Superhero,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" GotFocus= \"Textbox_GotFocus\" /> Compiled data binding <ListView x:Name= \"heroesListView\" ItemsSource= \"{x:Bind ViewModel.Heroes,Mode=OneWay}\" SelectedItem= \"{x:Bind ViewModel.SelectedHero,Mode=TwoWay}\" Margin= \"0\" > <TextBox x:Name= \"FirstNameTextbox\" Header= \"First name\" Margin= \"10\" Text= \"{x:Bind Hero.FirstName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" GotFocus= \"Textbox_GotFocus\" /> <TextBox x:Name= \"LastNameTextbox\" Header= \"Last name\" Margin= \"10\" Text= \"{x:Bind Hero.LastName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" GotFocus= \"Textbox_GotFocus\" /> <TextBox x:Name= \"SuperheroTextbox\" Header= \"Alias\" Margin= \"10\" Text= \"{x:Bind Hero.Superhero,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" GotFocus= \"Textbox_GotFocus\" /> x:Bind can also be implemented in the ListView's ItemTemplate, so long as the x:DataType attribute is set on DataTemplate. We must also remember to set the Mode binding property, since x:Bind's default is OneTime! Binding markup extension <ListView.ItemTemplate> <DataTemplate> <StackPanel Orientation= \"Horizontal\" > <TextBlock Text= \"{Binding Superhero}\" FontWeight= \"Bold\" /> <TextBlock Text= \"{Binding FirstName}\" Margin= \"5 0 0 0\" /> <TextBlock Text= \"{Binding LastName}\" Margin= \"5 0 0 0\" /> </StackPanel> </DataTemplate> </ListView.ItemTemplate> Compiled data binding <ListView.ItemTemplate> <DataTemplate x:DataType= \"local:Hero\" > <StackPanel Orientation= \"Horizontal\" > <TextBlock Text= \"{x:Bind Superhero}\" FontWeight= \"Bold\" /> <TextBlock Text= \"{x:Bind FirstName}\" Margin= \"5 0 0 0\" /> <TextBlock Text= \"{x:Bind LastName}\" Margin= \"5 0 0 0\" /> </StackPanel> </DataTemplate> </ListView.ItemTemplate> x:Bind can also hide or reveal controls depending on boolean value. A new boolean field is formed on the ViewModel. public class ViewModel { public bool IsHeroSelected => SelectedHero != null ; } Before <CommandBar> <AppBarButton x:Name= \"AddHero\" Click= \"AddHero_Click\" Label= \"Add hero\" > <SymbolIcon Symbol= \"Add\" /> </AppBarButton> <AppBarButton x:Name= \"DelHero\" Click= \"DelHero_Click\" Label= \"Remove hero\" > <SymbolIcon Symbol= \"Delete\" /> </AppBarButton> </CommandBar> After <CommandBar> <AppBarButton x:Name= \"AddHero\" Click= \"AddHero_Click\" Label= \"Add hero\" > <SymbolIcon Symbol= \"Add\" /> </AppBarButton> <AppBarButton x:Name= \"DelHero\" Click= \"DelHero_Click\" Label= \"Remove hero\" IsEnabled= \"{x:Bind ViewModel.IsHeroSelected,Mode=OneWay}\" > <SymbolIcon Symbol= \"Delete\" /> </AppBarButton> </CommandBar> History Windows has a long history of introducing UI frameworks to facilitate the creation of GUI applications: MFC (1992) was based on Native C++ WinForms (2002) was based on .NET Framework WPF (2006) was also based on .NET Framework UWP XAML (2012) was based on C++ and .NET WinUI 2 is a NuGet package containing controls and styles for UWP Apps, intended to decouple UWP applications from the latest version of Windows WinUI 3 (2020) is meant to provide a UX framework for both Win32 and UWP applications XAML is a declarative markup language used to create UIs for .NET Core apps. The logic of the app is separated in code-behind files that are joined to the markup through partial class definitions. In Visual Studio this is emphasized by the fact that the code-behind file is literally presented as a child node of the XAML document. The root element (of which there must be only one in order to be a valid XAML file) contains attributes that define the XAML namespaces for the program that will be parsing the XAML file, or a namescope . \ud83d\udcd8 Glossary ComboBox Important attributes: Items or ItemsSource specify the collection (preferably IObservableCollection to support event handling on value changes) to be used to populate the control. SelectedItem defines the element that appears selected by the control by default. If not defined, no element will be selected. DisplayMemberPath defines the name of the property to be used to display each individual choice. <ComboBox ItemsSource= \"{x:Bind Items}\" SelectedItem= \"{x:Bind Items[0]}\" DisplayMemberPath= \"Display\" /> CommandBar CommandBar is a lightweight control that can organize a bar of buttons. MainPage.xaml <CommandBar> <AppBarButton x:Name= \"AddCustomer\" Click= \"AddCustomer_Click\" Label= \"Add\" > <SymbolIcon Symbol= \"AddFriend\" /> </AppBarButton> <AppBarButton x:Name= \"DeleteCustomer\" Click= \"DeleteCustomer_Click\" Label= \"Delete\" > <SymbolIcon Symbol= \"Delete\" /> </AppBarButton> <AppBarButton x:Name= \"btn_MoveSideBar\" Click= \"btn_MoveSideBar_Click\" Label= \"Move sidebar\" > <SymbolIcon x:Name= \"btn_MoveSideBar_Symbol\" Symbol= \"AlignRight\" /> </AppBarButton> </CommandBar> DataGrid Dialog boxes In XAML, the MessageDialog object can be used to create a modal dialog box (i.e. one that does not allow interaction with the main window until the dialog box has been cleared). The MessageDialog object can take a string argument containing the text of the dialog box. It is actually displayed by calling the object's ShowAsync() method. Because this is an asynchronous call, it must be await ed, which requires the System namespace. ( src ) C# using Microsoft.UI.Xaml ; using Microsoft.UI.Xaml.Controls ; using Windows.UI.Popups ; using System ; namespace WiredBrainCoffee.UWP { public sealed partial class MainPage : Page { public MainPage () { this . InitializeComponent (); } private async void AddCustomer ( object sender , RoutedEventArgs e ) { var messageDialog = new MessageDialog ( \"Customer added!\" ); await messageDialog . ShowAsync (); } } } Grid The Grid ** layout panel is analogous to the ** grid geometry manager in tkinter. However, in XAML you are forced to explicitly declare RowDefinition ** and ** ColumnDefinition elements. Whereas in tkinter, the widget being placed declares its own grid position. If the grid is sparse, the empty rows and columns appear to be ignored. Grid star-sizing works similar to flex-grow and flex-shrink CSS style statements used with Flexbox. Rows <Grid> <Grid.RowDefinitions> <RowDefinition/> <RowDefinition/> <RowDefinition/> <RowDefinition/> <RowDefinition/> <RowDefinition/> <RowDefinition/> <RowDefinition/> </Grid.RowDefinitions> <Rectangle Fill= \"LightGray\" /> <Rectangle Fill= \"LightSteelBlue\" Grid.Row= \"1\" /> <Rectangle Fill= \"LightBlue\" Grid.Row= \"2\" /> <Rectangle Fill= \"LightCyan\" Grid.Row= \"3\" /> <Rectangle Fill= \"LightSeaGreen\" Grid.Row= \"4\" /> <Rectangle Fill= \"LightGreen\" Grid.Row= \"5\" /> <Rectangle Fill= \"LightGoldenrodYellow\" Grid.Row= \"6\" /> <Rectangle Fill= \"LightSalmon\" Grid.Row= \"7\" /> <Rectangle Fill= \"LightCoral\" Grid.Row= \"8\" /> </Grid> Columns <Grid> <Grid.ColumnDefinitions> <ColumnDefinition/> <ColumnDefinition/> <ColumnDefinition/> <ColumnDefinition/> <ColumnDefinition/> <ColumnDefinition/> <ColumnDefinition/> <ColumnDefinition/> </Grid.ColumnDefinitions> <Rectangle Fill= \"LightGray\" /> <Rectangle Fill= \"LightSteelBlue\" Grid.Column= \"1\" /> <Rectangle Fill= \"LightBlue\" Grid.Column= \"2\" /> <Rectangle Fill= \"LightCyan\" Grid.Column= \"3\" /> <Rectangle Fill= \"LightSeaGreen\" Grid.Column= \"4\" /> <Rectangle Fill= \"LightGreen\" Grid.Column= \"5\" /> <Rectangle Fill= \"LightGoldenrodYellow\" Grid.Column= \"6\" /> <Rectangle Fill= \"LightSalmon\" Grid.Column= \"7\" /> <Rectangle Fill= \"LightCoral\" Grid.Column= \"8\" /> </Grid> ListDetailsView ListDetailsView is a custom control available from the Windows Community Toolkit (Nuget package Microsoft.Toolkit.UWP ) that implements the Master/Details pattern . Illustration Basic structure xmlns:toolkit=\"using:Microsoft.Toolkit.Uwp.UI.Controls : namespace toolkit:ListDetailsView toolkit:ListDetailsView. ItemTemplate property-element for how items are rendered in sidebar toolkit:ListDetailsView. DetailsTemplate property-element for how items are rendered in the main pane toolkit:ListDetailsView. NoSelectionContentTemplate property-element for how the main pane is rendered with no item selected toolkit:ListDetailsView. AppCommandBar XAML <Page <!-- ... -- > xmlns:toolkit=\"using:Microsoft.Toolkit.Uwp.UI.Controls\"> <toolkit:ListDetailsView> <toolkit:ListDetailsView.ItemTemplate> </toolkit:ListDetailsView.ItemTemplate> <toolkit:ListDetailsView.DetailsTemplate> </toolkit:ListDetailsView.DetailsTemplate> <toolkit:ListDetailsView.NoSelectionContentTemplate> </toolkit:ListDetailsView.NoSelectionContentTemplate> <toolkit:ListDetailsView.AppCommandBar> </toolkit:ListDetailsView.AppCommandBar> </toolkit:ListDetailsView> </Page> ListView <ListView ItemsSource= \"{x:Bind Items}\" SelectedItem= \"{x:Bind Items[0]}\" DisplayMemberPath= \"Display\" /> Important attributes: Items or ItemsSource specify the collection (preferably IObservableCollection to support event handling on value changes) to be used to populate the control. SelectedItem defines the element that appears selected by the control by default. If not defined, no element will be selected. DisplayMemberPath defines the name of the property to be used to display each individual choice. MainWindow.xaml <Window x:Class= \"Scratchpad1.MainWindow\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:Scratchpad1\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" > <StackPanel Orientation= \"Horizontal\" HorizontalAlignment= \"Center\" VerticalAlignment= \"Center\" > <ComboBox ItemsSource= \"{x:Bind Items}\" SelectedItem= \"{x:Bind Items[0]}\" DisplayMemberPath= \"Name\" /> </StackPanel> </Window> MainWindow.xaml.cs using System.Collections.Generic ; using Microsoft.UI.Xaml ; namespace Scratchpad1 { public class Starship { public string Name { get ; set ; } public string Registry { get ; set ; } public int Crew { get ; set ; } public Starship ( string name , string registry , int crew ) { Name = name ; Registry = registry ; Crew = crew ; } } public sealed partial class MainWindow : Window { //List<string> Items = new List<string>(); Starship [] Items = new Starship [ 3 ]; public MainWindow () { this . InitializeComponent (); Items [ 0 ]= new Starship ( \"USS Enterprise\" , \"NCC-1701\" , 204 ); Items [ 1 ]= new Starship ( \"USS Constitution\" , \"NCC-1700\" , 203 ); Items [ 2 ]= new Starship ( \"USS Defiant\" , \"NCC-1764\" , 202 ); } } } NavigationView NavigationView provides top-level navigation by implementing a collapsible navigation bar (\"hamburger menu\") that is reactive to window size. It organizes two areas - Pane and Content - into two layout options that differ in the placement of the Pane. The Header content is placed above the Content in both layouts. Left Top It has various display modes that can be specified by setting PaneDisplayMode . By default, PaneDisplayMode is set to Auto , which enables adaptive (i.e. reactive) behavior, switching between Left, LeftCompact, and LeftMinimal display modes depending on window size. Left LeftCompact LeftMinimal Top The Pane is the central feature of NavigationView, and it can contain many items organized into several sections. MenuItems is the main section containing NavigationView items at the beginning of the control. FooterMenuItems is similar but they are added to the end of the control. PaneTitle can accept text, which will appear next to the menu button. PaneHeader is similar but can accept non-text content. PaneFooter can also accept any content. NavigationView items can be of various types: NavigationViewItemHeader can visually organize other navigation items NavigationViewItem exposes Content and Icon properties. NavigationViewItemSeparator AutoSuggestBox Settings button visible by default but can be hidden by setting IsSettingsVisible Left Top MenuItems <Page> <NavigationView> <NavigationView.MenuItems/> </NavigationView> </Page> Navigation using NavigationView is not automatically implemented but relies on event handling. NavigationView raises an ItemInvoked event when selected, and if the selection has changed then SelectionChanged is also raised. NavigationView also feature a Back button, which can be disabled or removed by setting IsBackButtonVisible or IsBackEnabled to false. If enabled, this button raises the BackRequested event. Typical implementations pair NavigationView with a Frame nested within ScrollViewer to support navigating to different views while supporting the back button (see below). XAML <Page> <Grid> <NavigationView> <ScrollViewer> <Frame x:Name= \"ContentFrame\" /> </ScrollViewer> </NavigationView> </Grid> </Page> Code-behind private void Navigation_ItemInvoked ( NavigationView sender , NavigationViewItemInvokedArgs args ) { ContentFrame . Navigate ( typeof ( Page1 )); } Page The Page element in UWP is equivalent to Window in WPF. Page elements can only accept a single Content sub-element, necessitating the use of a layout panel like Grid , StackPanel , etc. RelativePanel RelativePanel allows children to declare attributes (e.g. RelativePanel.RightOf ) to specify position relative to the x:Name of other children. This is useful in building responsive layouts. Supports several attached properties that allow elements to be aligned with siblings or with the panel itself. Panel alignment relations like AlignLeftWithPanel , AlignTopWithPanel , AlignRightWithPanel , AlignBottomWithPanel , align controls to the border of the RelativePanel containing them. Sibling alignment relationships like AlignLeftWith , AlignTopWith , AlignVerticalCenterWith etc. specify the name of a sibling control to provide alignment. Sibling positional relations like LeftOf , Above , RightOf , and Below also specify a sibling control. ResourceDictionary Resource dictionaries Here, Buttons will now be able to be styled using a markup extension <Button Style= \"{StaticResource SubmitButton}\" Content= \"Submit\" /> App.xaml <Application> <Application.Resources> <ResourceDictionary Source= \"ResourceDictionaries/ButtonDictionary.xaml\" /> </Application.Resources> </Application> /ResourceDictionaries/ButtonDictionary.xaml <ResourceDictionary> <Style TargetType= \"Button\" x:Key= \"SubmitButton\" > <Setter Property= \"Background\" Value= \"Green\" /> <Setter Property= \"Padding\" Value= \"5\" /> </Style> </ResourceDictionary> Managing a consistent style will typically necessitate using multiple resource dictionaries. But some elements can only contain a single ResourceDictionary element. The solution is to place a ResourceDictionary.MergedDictionaries property element within the outermost ResourceDictionary . Multiple ResourceDictionary objects can be placed as children of it. <Page> <Page.Resources> <ResourceDictionary> <ResourceDictionary.MergedDictionaries> <ResourceDictionary Source= \"Dictionary1.xaml\" /> <ResourceDictionary Source= \"Dictionary2.xaml\" /> </ResourceDictionary.MergedDictionaries> </ResourceDictionary> </Page.Resources> </Page> Sources ResourceDictionary YouTube SplitView SplitView can be used to implement hamburger-style navigation. SplitView has two attributes into which controls can be placed, Pane and Content . Pane is not displayed by default. However, by setting the SplitView instance's IsPaneOpen attribute to True it can be displayed. The DisplayMode attribute controls how the Pane interacts with Content with opened: Overlay : Pane covers up Content Inline : Pane pushes Content to the right. CompactInline : Where Pane will fit Pane elements closely, if CompactPaneLength is not specified CompactOverley : Pane's dimensions can be specified using CompactPaneLength and OpenPaneLength StackPanel The StackPanel ** layout panel in XAML is similar in function to the ** pack() geometry manager, although its default behavior appears to horizontally center elements and stack them vertically. Notably, StackPanel does not support scroll bars. ( src ) TabView Textbox XAML <Window x:Class= \"EmployeeManager.WinUI.MainWindow\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:EmployeeManager.WinUI\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" > <TextBox Header= \"First name\" /> </Window> tkinter import tkinter as tk from tkinter.ttk import Entry from tkinter.ttk import LabelFrame win = tk . Tk () frame = LabelFrame ( win , text = \"First name\" ) frame . pack () Entry ( frame ) . pack () tk . mainloop () VariableSizedWrapGrid VariableSizedWrapGrid can be used to define a field of tiles similar to an HTML flex container ( display: flex; with flex-wrap: wrap; ). The Orientation property is similar to a flex container's flex-direction , in that the direction of alignment can be specified. <Page> <VariableSizedWrapGrid Orientation= \"Horizontal\" ItemWidth= \"100\" ItemHeight= \"100\" > <Rectangle Fill= \"LightGray\" /> <Rectangle Fill= \"LightSteelBlue\" /> <Rectangle Fill= \"LightBlue\" /> <Rectangle Fill= \"LightCyan\" /> <Rectangle Fill= \"LightSeaGreen\" /> <Rectangle Fill= \"LightGreen\" /> <Rectangle Fill= \"LightGoldenrodYellow\" /> <Rectangle Fill= \"LightSalmon\" /> <Rectangle Fill= \"LightCoral\" /> <Rectangle Fill= \"Gray\" /> <Rectangle Fill= \"SteelBlue\" /> <Rectangle Fill= \"CadetBlue\" /> <Rectangle Fill= \"Cyan\" /> <Rectangle Fill= \"SeaGreen\" /> <Rectangle Fill= \"Green\" /> <Rectangle Fill= \"Goldenrod\" /> <Rectangle Fill= \"Salmon\" /> <Rectangle Fill= \"Coral\" /> </VariableSizedWrapGrid> </Page> Notably, the horizontal or vertical alignment of XAML controls is defined on each control, whereas in HTML alignment is specified at the level of the enclosing container. XAML <TextBlock Content= \"Hello, world!\" HorizontalAlignment= \"Left\" VerticalAlignment= \"Top\" /> HTML and CSS .container p Hello, world! . container { text-align : right top ; }","title":"\ud83e\udde0 XAML"},{"location":"Coding/XAML/#xaml","text":"TODO: Watch WinRT course Download Windows Community Toolkit Sample App Win2D Example Gallery Extend Superheroes app with DataGrid SplitView ListDetailsView NavigationView Markdown browser using MarkdownTextBlock Complete interactive image gallery tutorial UWP FPS using DirectX Dependency properties Research interoperability with DirectX (apparently possible if you wrap DirectX calls in a separate Windows Runtime metadata file )","title":"\ud83e\udde0 XAML"},{"location":"Coding/XAML/#syntax","text":"Every XAML element maps to a C# class , and every XAML attribute maps to a class property or an event . There are several syntaxes available for use that correspond to various methods of declaring objects: Object-element syntax has the type's name within angle brackets, similar to HTML. When the object contains other objects, it is called a container . If the object does not contain other objects, it can be declared with a self-closing tag: Attribute syntax has the property value set by declaring an attribute. In property-element syntax , signified by a period in the element name, where the portion of the element following the dot representing the identifier of the property. Content-property syntax is similar to the property element syntax in that a property's value is set by a child. However, in this case the XAML processor interpolates the correct property element, typically Content. Some controls can accept more than one child element. In the background, this is actually using the content-property syntax to assign to the Children property. Object-element <Canvas> <Rectangle /> </Canvas> Attribute <Rectangle Name= \"rectangle1\" Width= \"100\" Height= \"100\" Fill= \"Blue\" /> Property-element <Rectangle Name= \"rectangle1\" Width= \"100\" Height= \"100\" > <Rectangle.Fill> <SolidColorBrush Color= \"Blue\" /> </Rectangle.Fill> </Rectangle> Content-property <Button> Add customer </Button> Multiple children <StackPanel> <TextBlock> Hello </TextBlock> <TextBlock> World </TextBlock> </StackPanel>","title":"Syntax"},{"location":"Coding/XAML/#namespaces","text":"A XAML file usually declares a default XAML namespace in its root element. This default namespace defines elements that can be declared without qualifying them by a prefix (e.g. x: ). XAML namespaces apply only to the specific element on which they are declared and its children, which explains why they are typically placed on the root element. ( src ) Most XAML files have two xmlns declarations xmlns maps a default XAML namespace xmlns:x maps a separate XAML namespace for XAML-defined language elements to the x: prefix: xmlns:mc indicates and supports a markup compatibility mode for reading XAML. xmlns:mc=\"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable=\"d\"","title":"Namespaces"},{"location":"Coding/XAML/#directives","text":"x:Name uniquely identifies object elements for access to the object from code-behind x:Key sets a unique key for each resource in a ResourceDictionary x:Class specifies the CLR namespace and class name for the class that provides code-behind for a XAML page. The x:Class directive configures XAML markup compilation to join partial classes between markup and code-behind. In this example it can be seen how it refers to the MainPage class within the Project namespace. ( src ) x:Bind is a form of data-binding Language elements are commonly used features necessary for Windows Runtime apps. For example, to join any code-behind to a XAML file through a partial class, you must name that class as the x:Class attribute in the root element of the XAML file. XAML <Page x:Class= \"Project.MainPage\" /> Code-behind namespace Project { public sealed partial class MainPage : Page { public MainPage () { this . InitializeComponent (); } } }","title":"Directives"},{"location":"Coding/XAML/#attached-property","text":"Attached properties can be modified or queried in C# code as well, as long as the XAML element has an x:Name defined, which gives that particular element an identifier. For the column property of Grid , the static methods GetColumn and SetColumn are available. Here a button press changes the column value of an enclosing Grid and toggles between two different Symbol values. ( src ) private void ButtonMove_Click ( object sender , RoutedEventArgs e ) { int column = Grid . GetColumn ( customerListGrid ); int newcolumn = column == 0 ? 2 : 0 ; Grid . SetColumn ( customerListGrid , newcolumn ); moveSymbolIcon . Symbol = newcolumn == 0 ? Symbol . Forward : Symbol . Back ; }","title":"Attached property"},{"location":"Coding/XAML/#uwp","text":"Universal Windows Platform (UWP) refers to both a UI framework incorporating the Fluent Design System as well as an app model . When the UWP XAML framework was released in 2012, UWP was touted as a means to develop for many different device platforms, including mobile and tablet. Until recently, the UWP XAML framework was only available for applications using the UWP app model for apps destined for the Microsoft Store. However, UWP development has floundered over the past half decade as Microsoft has been unable to produce sufficient interest in its mobile and tablet devices or the Microsoft Store. Microsoft itself has ceased development of UWP apps for Office or for Xbox, for which it has turned rather to Electron.","title":"UWP"},{"location":"Coding/XAML/#winui","text":"Windows UI Library (WinUI) 3 is a native UI framework that represents a rebranding of the UWP UI framework , which had previously only been available for applications using the UWP app model , and a move to make it available for both app models. Still in development, it promises to deliver a unified framework and all the styles and controls previously distributed in WinUI 2, which in turn is a NuGet packaging containing the UWP XAML controls and styles. Unfortunately, because the UWP framework had been available only for the UWP app model , it did not experience wide adoption among developers who prefered the flexibility of the older Win32 \"app model\" (or rather, the lack of one).","title":"WinUI"},{"location":"Coding/XAML/#app-models","text":"App model is a term Microsoft began using after the release of UWP in 2012 to refer to the hosting model, or the rigidly defined parameters for how an application is installed, stores state, manages versions, and integrates with the operating system and other apps, that generally define an application lifecycle. Specifically, the term was used in the context of describing the security sandbox and other restrictions of the UWP app model for Microsoft Store applications. Previous to UWP, there had been no definition of an \"app model\" per se and developers of a Win32 application were free to determine these parameters individually. This caused wide disparity in implementation among applications, resulting in registry bloat and poorly managed uninstallations. The UWP App Model was specifically introduced to answer these concerns which had plagued generations of Windows. ( src ) The failure of UWP as an app model resulted in Project Reunion , an effort to reunify the bifurcated Windows development landscape.","title":"App models"},{"location":"Coding/XAML/#event-handling","text":"Event hooks can be used to subscribe to events. For example, the MainPage class exposes a Loaded event that can be used to fill a prototype app with dogfood data, rather than putting hardcoding it in the XAML markup. Notably, these event handlers must return only void types. public sealed partial class MainPage : Page { public MainPage () { this . InitializeComponent (); this . Loaded += MainPage_Loaded ; } private void MainPage_Loaded ( object sender , RoutedEventArgs e ) { throw new NotImplementedException (); } } Some XAML controls have attributes that map to events: Element Property Button Click CheckBox Checked , Unchecked ListView SelectionChanged TextBox TextChanged Defining an event handler in XAML is equivalent to doing so in C#. XAML <Button Content= \"Add customer\" Click= \"Button_ClickHandler\" > C# var btn = new Button { Content = \"Add customer\" this . Click = // No clue if this is right... }; SelectionChanged Markup <ListView x:Name= \"customerListView\" SelectionChanged= \"CustomerListView_SelectionChanged\" /> Code-behind private void CustomerListView_SelectionChanged ( object sender , SelectionChangedEventArgs e ) { var customer = customerListView . SelectedItem as Customer ; txtFirstName . Text = customer ?. FirstName ?? \"\" ; txtLastName . Text = customer ?. LastName ?? \"\" ; chkIsDeveloper . IsChecked = customer ?. IsDeveloper ; } TextChanged Markup <StackPanel> <TextBox x:Name= \"txtFirstName\" Header= \"Firstname\" TextChanged= \"TextBox_TextChanged\" Margin= \"10\" /> <TextBox x:Name= \"txtLastName\" Header= \"Lastname\" TextChanged= \"TextBox_TextChanged\" Margin= \"10\" /> <CheckBox x:Name= \"chkIsDeveloper\" Content= \"IsDeveloper\" Margin= \"10\" Checked= \"CheckBox_IsCheckedChanged\" Unchecked= \"CheckBox_IsCheckedChanged\" /> </StackPanel> Code-behind private void TextBox_TextChanged ( object sender , TextChangedEventArgs e ) { UpdateCustomer (); } private void CheckBox_IsCheckedChanged ( object sender , RoutedEventArgs e ) { UpdateCustomer (); } private void UpdateCustomer () { var customer = customerListView . SelectedItem as Customer ; if ( customer != null ) { customer . FirstName = txtFirstName . Text ; customer . LastName = txtLastName . Text ; customer . IsDeveloper = chkIsDeveloper . IsChecked .{{ c4 :: GetValueOrDefault }}(); } } Handlers for Loaded and Unloaded are automatically attached to any Page that uses the NavigationHelper class. Loaded Unloaded","title":"Event handling"},{"location":"Coding/XAML/#custom-controls","text":"You can take a group of controls and create a custom control using what is called a namespace mapping , where a C# class is made available in the XAML markup. More specifically, a custom control defined in XAML along with its accompanying code-behind file must use UserControl as its base class. In this sense, a custom control is really a special case of a namespace mapping, which can also be used to populate an application with mock data during development. Markup <UserControl x:Class= \"Project.Controls.CustomControl\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP.Controls\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" > <TextBlock Text= \"Hello, world!\" /> </UserControl> Code-behind namespace Project.Controls { public sealed partial class CustomControl : UserControl { public CustomControl () { this . InitializeComponent (); } } }","title":"Custom controls"},{"location":"Coding/XAML/#mock-data","text":"The most basic method of adding mock data is by hardcoding data in the XAML markup . Somewhat more sophisticated is the option of hardcoding data in the code-behind . The x:Bind directive can be used to bind an IEnumerable data source to either the Items or ItemsSource attributes. An ObservableCollection is preferred in WinUI programming because it raises events when properties are changed, but Lists and Arrays also work. If the collection is made of objects, the DisplayMemberPath allows the specification of a particular property on those objects to be displayed. Notably, classes specifically need to be used, and not structs, for the members of these collections. With namespace mapping , classes within the C# namespace can be used in XAML markup. ( src ) Most robust of all is creating a Data Provider class which will fall back to mock data when the data source is not available. This will allow any number of other data sources to be plugged in, such as databases or REST services. ( src ) Hardcoding in XAML <ListView> <ListViewItem> Aristotle </ListViewItem> <ListViewItem> Euclid </ListViewItem> <ListViewItem> Plato </ListViewItem> <ListViewItem> Socrates </ListViewItem> </ListView> Hardcoding in C# Markup <Page> <ListView ItemsSource= \"{x:Bind Starships}\" SelectedItem= \"{x:Bind Starships[0]}\" DisplayMemberPath= \"Display\" /> </Page> Code-behind namespace Starships.Models { class Starship { public string Name ; public string Registry ; public int Crew ; public string Display { get { return Name + Registry ; } } } } Namespace mapping Markup <Page xmlns:model= \"using:Project.Models\" > <ListView DisplayMemberPath= \"Display\" > <model:Starship Name= \"USS Enterprise\" Registry= \"NCC-1701\" Crew= \"140\" /> </ListView> </Page> Code-behind namespace Starships.Models { class Starship { public string Name ; public string Registry ; public int Crew ; public string Display { get { return Name + Registry ; } } } } Data provider","title":"Mock data"},{"location":"Coding/XAML/#themes","text":"Windows 10 themes ( \"Light\" , \"Dark\" , and \"HighContrast\" can be specified as a property of the Application element: Light <Application x:Class= \"Scratchpad.App\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:Scratchpad\" RequestedTheme= \"Light\" > <Application.Resources> <ResourceDictionary> <ResourceDictionary.MergedDictionaries> <XamlControlsResources xmlns= \"using:Microsoft.UI.Xaml.Controls\" /> <!-- Other merged dictionaries here --> </ResourceDictionary.MergedDictionaries> <!-- Other app resources here --> </ResourceDictionary> </Application.Resources> </Application> Dark <Application x:Class= \"Scratchpad.App\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:Scratchpad\" RequestedTheme= \"Dark\" > <Application.Resources> <ResourceDictionary> <ResourceDictionary.MergedDictionaries> <XamlControlsResources xmlns= \"using:Microsoft.UI.Xaml.Controls\" /> <!-- Other merged dictionaries here --> </ResourceDictionary.MergedDictionaries> <!-- Other app resources here --> </ResourceDictionary> </Application.Resources> </Application> Theme colors","title":"Themes"},{"location":"Coding/XAML/#layout","text":"The layout panels in XAML serve a similar function to the geometry manager methods in tkinter. There are various layout panels available. Grid RelativePanel StackPanel VariableSizeWrapGrid","title":"Layout"},{"location":"Coding/XAML/#data-binding","text":"There are two data binding types available in XAML ( src ) Binding markup extension resolves the binding path at runtime. It can accept data sources from the binding properties ElementName , Source , and RelativeSource . If none of these are defined, then the binding markup extension resolves to the DataContext property. x:Bind resolves the binding path at compile-time , generating C# code and offering better performance and compile-time errors. You can also step into the compiled code, providing a better debugging experience. x:Bind should generally be preferred, however it is available only in UWP. x:Bind , in contrast, binds only to the parent Page or UserControl element. So any property of MainPage will be accessible, and any property of that object will also be accessible using dot notation. Most bindings are easily translated between the two types: Binding markup extension public MainPage () { this . InitializeComponent ; this . ViewModel = new MainViewModel (); DataContext = ViewModel ; } <ListView ItemsSource= \"{Binding Customers,Mode=OneWay}\" > <!-- ...--> </ListView> x:Bind public MainPage () { this . InitializeComponent ; this . ViewModel = new MainViewModel (); // DataContext = ViewModel; } <ListView ItemsSource= \"{x:Bind ViewModel.Customers,Mode=OneWay}\" > <!-- ...--> </ListView> Notably, the default binding mode of the binding markup extension is OneWay **, whereas that of x:Bind is ** OneTime , although this can be changed by setting x:DefaultBindMode on the root element. Changing default bind mode <Page x:DefaultBindMode= \"OneWay\" > <Listview ItemsSource= \"{x:Bind ViewModelCustomers}\" /> </Page> Set explicitly <Page> <Listview ItemsSource= \"{x:Bind ViewModelCustomers,Mode=OneWay}\" /> </Page>","title":"Data binding"},{"location":"Coding/XAML/#type-conversion","text":"By default, XAML attribute values are strings. In order for the XAML processor to interpret them as objects, they must be converted. Type converters can convert string representations of attribute values to property elements. For example, a type converter is what is used with the XAML declaration HorizontalAlignment=\"Left\" , which is mapped to a specific enumeration within the Windows.UI.XAML namespace. ref In UWP, the XAML processor integrates the conversion logic to convert the Margin declaration to a Thickness object. But in WPF , TypeConverters are used. Markup <Button Margin= \"10,20,10,30\" Content= \"Click me\" /> Code-behind var btn = new Button { Margin = new Thickness(10, 20, 10, 30); };","title":"Type conversion"},{"location":"Coding/XAML/#markup-extensions","text":"Markup extensions are placed between curly braces { and } and create a reference to a ResourceDictionary . They can be used to unify styling across an application. The most common markup extensions are: StaticResource refers to resources defined in resource dictionaries ThemeResource for Windows native themeing colors Binding for data binding expressions, which require the bound property to be a dependency property Here, the background of the grid is bound to a color from the Windows-native theming and the TextBlock's Foreground property is bound to a color defined in a resource dictionary defined on the same page. Multiple properties can be set at the same time by setting a Style property element. <Page> <Page.Resources> <Style TargetType= \"Button\" x:Key= \"MyButtonStyle\" > <Setter Property= \"Background\" Value= \"Blue\" /> <Setter Property= \"FontFamily\" Value= \"Arial Black\" /> <Setter Property= \"FontSize\" Value= \"36\" /> </Style> </Page.Resources> <Button Content= \"Hello world\" Style= \"{StaticResource MyButtonStyle}\" /> </Page> <Page> <Page.Resources> <SolidColorBrush x:Key= \"MyBrush\" Color= \"Brown\" /> </Page.Resources> <Grid Background= \"{ThemeResource ApplicationPageBackgroundThemeBrush}\" > <TextBlock Text= \"Hello World\" Foreground= \"{StaticResource MyBrush}\" /> </Grid> </Page> src","title":"Markup Extensions"},{"location":"Coding/XAML/#dependency-properties","text":"Only dependency properties can be targets for data binding in UWP and WPF . The propdp snippet in Visual Studio can be used to create one. The DependencyObject class, which is a base class of all UI elements in UWP and WPF, exposes the GetValue and SetValue methods, which are used to ...","title":"Dependency properties"},{"location":"Coding/XAML/#multi-instance","text":"A multi-instance application is one that can run in several instances, which is necessary to allow users to open new windows. A collection of templates is available as a Visual Studio extension . These templates modify the appxmanifest file by setting the SupportsMultipleInstances attribute to true: <Package ... xmlns:desktop4= \"http://schemas.microsoft.com/appx/manifest/desktop/windows10/4\" xmlns:iot2= \"http://schemas.microsoft.com/appx/manifest/iot/windows10/2\" IgnorableNamespaces= \"uap mp desktop4 iot2\" > ... <Applications> <Application Id= \"App\" ... desktop4:SupportsMultipleInstances= \"true\" iot2:SupportsMultipleInstances= \"true\" > ... </Application> </Applications> ... </Package> Resources: Create a multi-instance UWP app","title":"Multi-instance"},{"location":"Coding/XAML/#patterns","text":"","title":"Patterns"},{"location":"Coding/XAML/#action-on-focus","text":"UI elements expose the GotFocus event hook for when a user clicks or tabs into a control. Example handler selecting all text in a TextBox: ( src ) Markup <Page> <TextBox GotFocus= \"TextBox_GotFocus\" /> </Page> Code-behind void TextBox_GotFocus ( object sender , RoutedEventArgs e ) { TextBox textBox = sender as TextBox ; textBox . SelectAll (); }","title":"Action on focus"},{"location":"Coding/XAML/#masterdetails","text":"The master/details pattern has a master pane (usually a ListView ) and a details pane for content.","title":"Master/Details"},{"location":"Coding/XAML/#mvvm","text":"In the Model, View, ViewModel (MVVM) pattern, which implicitly relies on OOP principles, the Model represents the data model for the objects being manipulated, and the ViewModel is the model for the View , that is, the state of the application as represented in a class. In WinUI, the project that contains the Views (that is, the XAML files) must first add references to the projects where the Models and ViewModel are contained. These will allow the code-behind file of the MainWindow to reference those classes. The class representing the ViewModel is instantiated and assigned to an attribute. That class's methods can then be called by using the x:Bind attribute syntax on, for instance, a ListView element. Markup <Window> <ListView ItemsSource= \"{x:Bind ViewModel.Employees, Mode=OneWay}\" SelectedItem= \"{x:Bind ViewModel.SelectedEmployee, Mode=OneWay}\" DisplayMemberPath= \"FirstName\" /> </Window> Code-behind using EmployeeManager.DataAccess ; using EmployeeManager.ViewModel ; using Microsoft.UI.Xaml ; namespace EmployeeManager.WinUI { public sealed partial class MainWindow : Window { public MainWindow () { ViewModel = new MainViewModel ( new EmployeeDataProvider ()); this . InitializeComponent (); } public MainViewModel ViewModel { get ; } } }","title":"MVVM"},{"location":"Coding/XAML/#navigation","text":"App layout typically begins with the choice of navigation model , which defines how users navigate between pages in the app. There are two common navigation models: left nav and top nav","title":"Navigation"},{"location":"Coding/XAML/#examples","text":"","title":"Examples"},{"location":"Coding/XAML/#list-and-details","text":"<Page x:Class= \"Superheroes.MainPage\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:Superheroes\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" Background= \"{ThemeResource ApplicationPageBackgroundThemeBrush}\" > <StackPanel> <CommandBar> <AppBarButton x:Name= \"AddHero\" Click= \"AddHero_Click\" Label= \"Add hero\" > <SymbolIcon Symbol= \"Add\" /> </AppBarButton> <AppBarButton x:Name= \"DelHero\" Click= \"DelHero_Click\" Label= \"Remove hero\" > <SymbolIcon Symbol= \"Delete\" /> </AppBarButton> </CommandBar> <TextBox Header= \"First name\" Text= \"{Binding ElementName=heroesListView,Path=SelectedItem.FirstName,Mode=TwoWay}\" /> <TextBox Header= \"Last name\" Text= \"{Binding ElementName=heroesListView,Path=SelectedItem.LastName,Mode=TwoWay}\" /> <TextBox Header= \"Alias\" Text= \"{Binding ElementName=heroesListView,Path=SelectedItem.Superhero,Mode=TwoWay}\" /> <ListView x:Name= \"heroesListView\" ItemsSource= \"{Binding Heroes,Mode=TwoWay}\" SelectedItem= \"{Binding SelectedHero,Mode=TwoWay}\" > <ListView.ItemTemplate> <DataTemplate> <StackPanel Orientation= \"Horizontal\" > <TextBlock Text= \"{Binding Superhero}\" FontWeight= \"Bold\" /> <TextBlock Text= \"{Binding FirstName}\" Margin= \"5 0 0 0\" /> <TextBlock Text= \"{Binding LastName}\" Margin= \"5 0 0 0\" /> </StackPanel> </DataTemplate> </ListView.ItemTemplate> </ListView> </StackPanel> </Page> In order to link two controls, some form of data binding is necessary. Here, TextBox elements are individually bound to the string properties of the selected ListView element with the Binding markup extension using the ElementName binding property. The selected item and the path to the string property are combined using dot notation and assigned to the Path binding property.","title":"List and details"},{"location":"Coding/XAML/#custom-control","text":"The textboxes can also be consolidated into a custom control using UserControl . In this case, the custom control must expose a target property that will be bound to the ListView's SelectedItem property. Individual TextBoxes bound to ListView <TextBox Header= \"First name\" Text= \"{Binding ElementName=heroesListView,Path=SelectedItem.FirstName,Mode=TwoWay}\" /> <TextBox Header= \"Last name\" Text= \"{Binding ElementName=heroesListView,Path=SelectedItem.LastName,Mode=TwoWay}\" /> <TextBox Header= \"Alias\" Text= \"{Binding ElementName=heroesListView,Path=SelectedItem.Superhero,Mode=TwoWay}\" /> <!-- <local:MyTextBoxes Hero=\"{Binding ElementName=heroesListView,Path=SelectedItem,Mode=TwoWay}\"/> --> UserControl <!--<TextBox Header=\"First name\" Text=\"{Binding ElementName=heroesListView,Path=SelectedItem.FirstName,Mode=TwoWay}\"/> <TextBox Header=\"Last name\" Text=\"{Binding ElementName=heroesListView,Path=SelectedItem.LastName,Mode=TwoWay}\"/> <TextBox Header=\"Alias\" Text=\"{Binding ElementName=heroesListView,Path=SelectedItem.Superhero,Mode=TwoWay}\"/>--> <local:MyTextBoxes Hero= \"{Binding ElementName=heroesListView,Path=SelectedItem,Mode=TwoWay}\" /> In order to accept the data binding, the target property must be implemented as a dependency property . This dependency property works through a static callback function which sets the individual textbox values to the bound target property. Naively, the callback function can be made to change the individual TextBox elements, provided they have x:Name defined. But a better technique is using the Binding markup extension to bind the individual TextBoxes to the root node using the ElementName binding property. This requires the root node to have x:Name defined. Both MyTextBoxes's binding to heroesListView and the individual TextBox bindings to the Hero property of root need to be TwoWay in order for changes made in the TextBox to take effect. Notably, the ListView does not reflect any changes made yet. Callback <TextBox x:Name= \"FirstNameTextbox\" Header= \"First name\" Margin= \"10\" /> <TextBox x:Name= \"LastNameTextbox\" Header= \"Last name\" Margin= \"10\" /> <TextBox x:Name= \"SuperheroTextbox\" Header= \"Alias\" Margin= \"10\" /> private static void HeroChangedCallback ( DependencyObject d , DependencyPropertyChangedEventArgs e ) { if ( d is MyTextBoxes myTextBoxes ) { var hero = e . NewValue as Hero ; myTextBoxes . FirstNameTextbox . Text = hero . FirstName ; myTextBoxes . LastNameTextbox . Text = hero . LastName ; myTextBoxes . SuperheroTextbox . Text = hero . Superhero ; } } Binding <TextBox x:Name= \"FirstNameTextbox\" Header= \"First name\" Margin= \"10\" Text= \"{Binding ElementName=root,Path=Hero.FirstName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" /> <TextBox x:Name= \"LastNameTextbox\" Header= \"Last name\" Margin= \"10\" Text= \"{Binding ElementName=root,Path=Hero.LastName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" /> <TextBox x:Name= \"SuperheroTextbox\" Header= \"Alias\" Margin= \"10\" Text= \"{Binding ElementName=root,Path=Hero.Superhero,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" /> private static void HeroChangedCallback ( DependencyObject d , DependencyPropertyChangedEventArgs e ) { //if (d is MyTextBoxes myTextBoxes) //{ // var hero = e.NewValue as Hero; // myTextBoxes.FirstNameTextbox.Text = hero.FirstName; // myTextBoxes.LastNameTextbox.Text = hero.LastName; // myTextBoxes.SuperheroTextbox.Text = hero.Superhero; //} }","title":"Custom control"},{"location":"Coding/XAML/#listview","text":"In order for the ListView to update, the underlying model must raise the PropertyChanged event. I really don't understand this very well, so here is the Observable class that implements the INotifyPropertyChanged interface. The model must inherit from this base class and the Set accessor of any property should fire OnPropertyChanged() . public class Observable : INotifyPropertyChanged { public event PropertyChangedEventHandler PropertyChanged ; protected void OnPropertyChanged ([ CallerMemberName ] string propertyName = null ) { PropertyChanged ?. Invoke ( this , new PropertyChangedEventArgs ( propertyName )); } }","title":"ListView"},{"location":"Coding/XAML/#viewmodel","text":"","title":"ViewModel"},{"location":"Coding/XAML/#xbind","text":"Binding markup extensions are trivially changed to compiled data bindings by replacing Binding with x:Bind . Note Note that ListView will enter an infinite loop if the SelectedHero property does not incorporate additional logic. public Hero SelectedHero { get { return _selectedHero ; } set { if ( _selectedHero != value ) { _selectedHero = value ; OnPropertyChanged (); OnPropertyChanged ( nameof ( IsHeroSelected )); } } } Binding markup extension <ListView x:Name= \"heroesListView\" ItemsSource= \"{Binding Heroes,Mode=OneWay}\" SelectedItem= \"{Binding SelectedHero,Mode=TwoWay}\" Margin= \"0\" > <TextBox x:Name= \"FirstNameTextbox\" Header= \"First name\" Margin= \"10\" Text= \"{Binding ElementName=root,Path=Hero.FirstName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" GotFocus= \"Textbox_GotFocus\" /> <TextBox x:Name= \"LastNameTextbox\" Header= \"Last name\" Margin= \"10\" Text= \"{Binding ElementName=root,Path=Hero.LastName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" GotFocus= \"Textbox_GotFocus\" /> <TextBox x:Name= \"SuperheroTextbox\" Header= \"Alias\" Margin= \"10\" Text= \"{Binding ElementName=root,Path=Hero.Superhero,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" GotFocus= \"Textbox_GotFocus\" /> Compiled data binding <ListView x:Name= \"heroesListView\" ItemsSource= \"{x:Bind ViewModel.Heroes,Mode=OneWay}\" SelectedItem= \"{x:Bind ViewModel.SelectedHero,Mode=TwoWay}\" Margin= \"0\" > <TextBox x:Name= \"FirstNameTextbox\" Header= \"First name\" Margin= \"10\" Text= \"{x:Bind Hero.FirstName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" GotFocus= \"Textbox_GotFocus\" /> <TextBox x:Name= \"LastNameTextbox\" Header= \"Last name\" Margin= \"10\" Text= \"{x:Bind Hero.LastName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" GotFocus= \"Textbox_GotFocus\" /> <TextBox x:Name= \"SuperheroTextbox\" Header= \"Alias\" Margin= \"10\" Text= \"{x:Bind Hero.Superhero,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" GotFocus= \"Textbox_GotFocus\" /> x:Bind can also be implemented in the ListView's ItemTemplate, so long as the x:DataType attribute is set on DataTemplate. We must also remember to set the Mode binding property, since x:Bind's default is OneTime! Binding markup extension <ListView.ItemTemplate> <DataTemplate> <StackPanel Orientation= \"Horizontal\" > <TextBlock Text= \"{Binding Superhero}\" FontWeight= \"Bold\" /> <TextBlock Text= \"{Binding FirstName}\" Margin= \"5 0 0 0\" /> <TextBlock Text= \"{Binding LastName}\" Margin= \"5 0 0 0\" /> </StackPanel> </DataTemplate> </ListView.ItemTemplate> Compiled data binding <ListView.ItemTemplate> <DataTemplate x:DataType= \"local:Hero\" > <StackPanel Orientation= \"Horizontal\" > <TextBlock Text= \"{x:Bind Superhero}\" FontWeight= \"Bold\" /> <TextBlock Text= \"{x:Bind FirstName}\" Margin= \"5 0 0 0\" /> <TextBlock Text= \"{x:Bind LastName}\" Margin= \"5 0 0 0\" /> </StackPanel> </DataTemplate> </ListView.ItemTemplate> x:Bind can also hide or reveal controls depending on boolean value. A new boolean field is formed on the ViewModel. public class ViewModel { public bool IsHeroSelected => SelectedHero != null ; } Before <CommandBar> <AppBarButton x:Name= \"AddHero\" Click= \"AddHero_Click\" Label= \"Add hero\" > <SymbolIcon Symbol= \"Add\" /> </AppBarButton> <AppBarButton x:Name= \"DelHero\" Click= \"DelHero_Click\" Label= \"Remove hero\" > <SymbolIcon Symbol= \"Delete\" /> </AppBarButton> </CommandBar> After <CommandBar> <AppBarButton x:Name= \"AddHero\" Click= \"AddHero_Click\" Label= \"Add hero\" > <SymbolIcon Symbol= \"Add\" /> </AppBarButton> <AppBarButton x:Name= \"DelHero\" Click= \"DelHero_Click\" Label= \"Remove hero\" IsEnabled= \"{x:Bind ViewModel.IsHeroSelected,Mode=OneWay}\" > <SymbolIcon Symbol= \"Delete\" /> </AppBarButton> </CommandBar>","title":"x:Bind"},{"location":"Coding/XAML/#history","text":"Windows has a long history of introducing UI frameworks to facilitate the creation of GUI applications: MFC (1992) was based on Native C++ WinForms (2002) was based on .NET Framework WPF (2006) was also based on .NET Framework UWP XAML (2012) was based on C++ and .NET WinUI 2 is a NuGet package containing controls and styles for UWP Apps, intended to decouple UWP applications from the latest version of Windows WinUI 3 (2020) is meant to provide a UX framework for both Win32 and UWP applications XAML is a declarative markup language used to create UIs for .NET Core apps. The logic of the app is separated in code-behind files that are joined to the markup through partial class definitions. In Visual Studio this is emphasized by the fact that the code-behind file is literally presented as a child node of the XAML document. The root element (of which there must be only one in order to be a valid XAML file) contains attributes that define the XAML namespaces for the program that will be parsing the XAML file, or a namescope .","title":"History"},{"location":"Coding/XAML/#glossary","text":"","title":"\ud83d\udcd8 Glossary"},{"location":"Coding/XAML/#combobox","text":"Important attributes: Items or ItemsSource specify the collection (preferably IObservableCollection to support event handling on value changes) to be used to populate the control. SelectedItem defines the element that appears selected by the control by default. If not defined, no element will be selected. DisplayMemberPath defines the name of the property to be used to display each individual choice. <ComboBox ItemsSource= \"{x:Bind Items}\" SelectedItem= \"{x:Bind Items[0]}\" DisplayMemberPath= \"Display\" />","title":"ComboBox"},{"location":"Coding/XAML/#commandbar","text":"CommandBar is a lightweight control that can organize a bar of buttons. MainPage.xaml <CommandBar> <AppBarButton x:Name= \"AddCustomer\" Click= \"AddCustomer_Click\" Label= \"Add\" > <SymbolIcon Symbol= \"AddFriend\" /> </AppBarButton> <AppBarButton x:Name= \"DeleteCustomer\" Click= \"DeleteCustomer_Click\" Label= \"Delete\" > <SymbolIcon Symbol= \"Delete\" /> </AppBarButton> <AppBarButton x:Name= \"btn_MoveSideBar\" Click= \"btn_MoveSideBar_Click\" Label= \"Move sidebar\" > <SymbolIcon x:Name= \"btn_MoveSideBar_Symbol\" Symbol= \"AlignRight\" /> </AppBarButton> </CommandBar>","title":"CommandBar"},{"location":"Coding/XAML/#datagrid","text":"","title":"DataGrid "},{"location":"Coding/XAML/#dialog-boxes","text":"In XAML, the MessageDialog object can be used to create a modal dialog box (i.e. one that does not allow interaction with the main window until the dialog box has been cleared). The MessageDialog object can take a string argument containing the text of the dialog box. It is actually displayed by calling the object's ShowAsync() method. Because this is an asynchronous call, it must be await ed, which requires the System namespace. ( src ) C# using Microsoft.UI.Xaml ; using Microsoft.UI.Xaml.Controls ; using Windows.UI.Popups ; using System ; namespace WiredBrainCoffee.UWP { public sealed partial class MainPage : Page { public MainPage () { this . InitializeComponent (); } private async void AddCustomer ( object sender , RoutedEventArgs e ) { var messageDialog = new MessageDialog ( \"Customer added!\" ); await messageDialog . ShowAsync (); } } }","title":"Dialog boxes"},{"location":"Coding/XAML/#grid","text":"The Grid ** layout panel is analogous to the ** grid geometry manager in tkinter. However, in XAML you are forced to explicitly declare RowDefinition ** and ** ColumnDefinition elements. Whereas in tkinter, the widget being placed declares its own grid position. If the grid is sparse, the empty rows and columns appear to be ignored. Grid star-sizing works similar to flex-grow and flex-shrink CSS style statements used with Flexbox. Rows <Grid> <Grid.RowDefinitions> <RowDefinition/> <RowDefinition/> <RowDefinition/> <RowDefinition/> <RowDefinition/> <RowDefinition/> <RowDefinition/> <RowDefinition/> </Grid.RowDefinitions> <Rectangle Fill= \"LightGray\" /> <Rectangle Fill= \"LightSteelBlue\" Grid.Row= \"1\" /> <Rectangle Fill= \"LightBlue\" Grid.Row= \"2\" /> <Rectangle Fill= \"LightCyan\" Grid.Row= \"3\" /> <Rectangle Fill= \"LightSeaGreen\" Grid.Row= \"4\" /> <Rectangle Fill= \"LightGreen\" Grid.Row= \"5\" /> <Rectangle Fill= \"LightGoldenrodYellow\" Grid.Row= \"6\" /> <Rectangle Fill= \"LightSalmon\" Grid.Row= \"7\" /> <Rectangle Fill= \"LightCoral\" Grid.Row= \"8\" /> </Grid> Columns <Grid> <Grid.ColumnDefinitions> <ColumnDefinition/> <ColumnDefinition/> <ColumnDefinition/> <ColumnDefinition/> <ColumnDefinition/> <ColumnDefinition/> <ColumnDefinition/> <ColumnDefinition/> </Grid.ColumnDefinitions> <Rectangle Fill= \"LightGray\" /> <Rectangle Fill= \"LightSteelBlue\" Grid.Column= \"1\" /> <Rectangle Fill= \"LightBlue\" Grid.Column= \"2\" /> <Rectangle Fill= \"LightCyan\" Grid.Column= \"3\" /> <Rectangle Fill= \"LightSeaGreen\" Grid.Column= \"4\" /> <Rectangle Fill= \"LightGreen\" Grid.Column= \"5\" /> <Rectangle Fill= \"LightGoldenrodYellow\" Grid.Column= \"6\" /> <Rectangle Fill= \"LightSalmon\" Grid.Column= \"7\" /> <Rectangle Fill= \"LightCoral\" Grid.Column= \"8\" /> </Grid>","title":"Grid"},{"location":"Coding/XAML/#listdetailsview","text":"ListDetailsView is a custom control available from the Windows Community Toolkit (Nuget package Microsoft.Toolkit.UWP ) that implements the Master/Details pattern . Illustration Basic structure xmlns:toolkit=\"using:Microsoft.Toolkit.Uwp.UI.Controls : namespace toolkit:ListDetailsView toolkit:ListDetailsView. ItemTemplate property-element for how items are rendered in sidebar toolkit:ListDetailsView. DetailsTemplate property-element for how items are rendered in the main pane toolkit:ListDetailsView. NoSelectionContentTemplate property-element for how the main pane is rendered with no item selected toolkit:ListDetailsView. AppCommandBar XAML <Page <!-- ... -- > xmlns:toolkit=\"using:Microsoft.Toolkit.Uwp.UI.Controls\"> <toolkit:ListDetailsView> <toolkit:ListDetailsView.ItemTemplate> </toolkit:ListDetailsView.ItemTemplate> <toolkit:ListDetailsView.DetailsTemplate> </toolkit:ListDetailsView.DetailsTemplate> <toolkit:ListDetailsView.NoSelectionContentTemplate> </toolkit:ListDetailsView.NoSelectionContentTemplate> <toolkit:ListDetailsView.AppCommandBar> </toolkit:ListDetailsView.AppCommandBar> </toolkit:ListDetailsView> </Page>","title":"ListDetailsView "},{"location":"Coding/XAML/#listview_1","text":"<ListView ItemsSource= \"{x:Bind Items}\" SelectedItem= \"{x:Bind Items[0]}\" DisplayMemberPath= \"Display\" /> Important attributes: Items or ItemsSource specify the collection (preferably IObservableCollection to support event handling on value changes) to be used to populate the control. SelectedItem defines the element that appears selected by the control by default. If not defined, no element will be selected. DisplayMemberPath defines the name of the property to be used to display each individual choice. MainWindow.xaml <Window x:Class= \"Scratchpad1.MainWindow\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:Scratchpad1\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" > <StackPanel Orientation= \"Horizontal\" HorizontalAlignment= \"Center\" VerticalAlignment= \"Center\" > <ComboBox ItemsSource= \"{x:Bind Items}\" SelectedItem= \"{x:Bind Items[0]}\" DisplayMemberPath= \"Name\" /> </StackPanel> </Window> MainWindow.xaml.cs using System.Collections.Generic ; using Microsoft.UI.Xaml ; namespace Scratchpad1 { public class Starship { public string Name { get ; set ; } public string Registry { get ; set ; } public int Crew { get ; set ; } public Starship ( string name , string registry , int crew ) { Name = name ; Registry = registry ; Crew = crew ; } } public sealed partial class MainWindow : Window { //List<string> Items = new List<string>(); Starship [] Items = new Starship [ 3 ]; public MainWindow () { this . InitializeComponent (); Items [ 0 ]= new Starship ( \"USS Enterprise\" , \"NCC-1701\" , 204 ); Items [ 1 ]= new Starship ( \"USS Constitution\" , \"NCC-1700\" , 203 ); Items [ 2 ]= new Starship ( \"USS Defiant\" , \"NCC-1764\" , 202 ); } } }","title":"ListView"},{"location":"Coding/XAML/#navigationview","text":"NavigationView provides top-level navigation by implementing a collapsible navigation bar (\"hamburger menu\") that is reactive to window size. It organizes two areas - Pane and Content - into two layout options that differ in the placement of the Pane. The Header content is placed above the Content in both layouts. Left Top It has various display modes that can be specified by setting PaneDisplayMode . By default, PaneDisplayMode is set to Auto , which enables adaptive (i.e. reactive) behavior, switching between Left, LeftCompact, and LeftMinimal display modes depending on window size. Left LeftCompact LeftMinimal Top The Pane is the central feature of NavigationView, and it can contain many items organized into several sections. MenuItems is the main section containing NavigationView items at the beginning of the control. FooterMenuItems is similar but they are added to the end of the control. PaneTitle can accept text, which will appear next to the menu button. PaneHeader is similar but can accept non-text content. PaneFooter can also accept any content. NavigationView items can be of various types: NavigationViewItemHeader can visually organize other navigation items NavigationViewItem exposes Content and Icon properties. NavigationViewItemSeparator AutoSuggestBox Settings button visible by default but can be hidden by setting IsSettingsVisible Left Top MenuItems <Page> <NavigationView> <NavigationView.MenuItems/> </NavigationView> </Page> Navigation using NavigationView is not automatically implemented but relies on event handling. NavigationView raises an ItemInvoked event when selected, and if the selection has changed then SelectionChanged is also raised. NavigationView also feature a Back button, which can be disabled or removed by setting IsBackButtonVisible or IsBackEnabled to false. If enabled, this button raises the BackRequested event. Typical implementations pair NavigationView with a Frame nested within ScrollViewer to support navigating to different views while supporting the back button (see below). XAML <Page> <Grid> <NavigationView> <ScrollViewer> <Frame x:Name= \"ContentFrame\" /> </ScrollViewer> </NavigationView> </Grid> </Page> Code-behind private void Navigation_ItemInvoked ( NavigationView sender , NavigationViewItemInvokedArgs args ) { ContentFrame . Navigate ( typeof ( Page1 )); }","title":"NavigationView  "},{"location":"Coding/XAML/#page","text":"The Page element in UWP is equivalent to Window in WPF. Page elements can only accept a single Content sub-element, necessitating the use of a layout panel like Grid , StackPanel , etc.","title":"Page"},{"location":"Coding/XAML/#relativepanel","text":"RelativePanel allows children to declare attributes (e.g. RelativePanel.RightOf ) to specify position relative to the x:Name of other children. This is useful in building responsive layouts. Supports several attached properties that allow elements to be aligned with siblings or with the panel itself. Panel alignment relations like AlignLeftWithPanel , AlignTopWithPanel , AlignRightWithPanel , AlignBottomWithPanel , align controls to the border of the RelativePanel containing them. Sibling alignment relationships like AlignLeftWith , AlignTopWith , AlignVerticalCenterWith etc. specify the name of a sibling control to provide alignment. Sibling positional relations like LeftOf , Above , RightOf , and Below also specify a sibling control.","title":"RelativePanel"},{"location":"Coding/XAML/#resourcedictionary","text":"Resource dictionaries Here, Buttons will now be able to be styled using a markup extension <Button Style= \"{StaticResource SubmitButton}\" Content= \"Submit\" /> App.xaml <Application> <Application.Resources> <ResourceDictionary Source= \"ResourceDictionaries/ButtonDictionary.xaml\" /> </Application.Resources> </Application> /ResourceDictionaries/ButtonDictionary.xaml <ResourceDictionary> <Style TargetType= \"Button\" x:Key= \"SubmitButton\" > <Setter Property= \"Background\" Value= \"Green\" /> <Setter Property= \"Padding\" Value= \"5\" /> </Style> </ResourceDictionary> Managing a consistent style will typically necessitate using multiple resource dictionaries. But some elements can only contain a single ResourceDictionary element. The solution is to place a ResourceDictionary.MergedDictionaries property element within the outermost ResourceDictionary . Multiple ResourceDictionary objects can be placed as children of it. <Page> <Page.Resources> <ResourceDictionary> <ResourceDictionary.MergedDictionaries> <ResourceDictionary Source= \"Dictionary1.xaml\" /> <ResourceDictionary Source= \"Dictionary2.xaml\" /> </ResourceDictionary.MergedDictionaries> </ResourceDictionary> </Page.Resources> </Page> Sources ResourceDictionary YouTube","title":"ResourceDictionary"},{"location":"Coding/XAML/#splitview","text":"SplitView can be used to implement hamburger-style navigation. SplitView has two attributes into which controls can be placed, Pane and Content . Pane is not displayed by default. However, by setting the SplitView instance's IsPaneOpen attribute to True it can be displayed. The DisplayMode attribute controls how the Pane interacts with Content with opened: Overlay : Pane covers up Content Inline : Pane pushes Content to the right. CompactInline : Where Pane will fit Pane elements closely, if CompactPaneLength is not specified CompactOverley : Pane's dimensions can be specified using CompactPaneLength and OpenPaneLength","title":"SplitView"},{"location":"Coding/XAML/#stackpanel","text":"The StackPanel ** layout panel in XAML is similar in function to the ** pack() geometry manager, although its default behavior appears to horizontally center elements and stack them vertically. Notably, StackPanel does not support scroll bars. ( src )","title":"StackPanel"},{"location":"Coding/XAML/#tabview","text":"","title":"TabView"},{"location":"Coding/XAML/#textbox","text":"XAML <Window x:Class= \"EmployeeManager.WinUI.MainWindow\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:EmployeeManager.WinUI\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" > <TextBox Header= \"First name\" /> </Window> tkinter import tkinter as tk from tkinter.ttk import Entry from tkinter.ttk import LabelFrame win = tk . Tk () frame = LabelFrame ( win , text = \"First name\" ) frame . pack () Entry ( frame ) . pack () tk . mainloop ()","title":"Textbox"},{"location":"Coding/XAML/#variablesizedwrapgrid","text":"VariableSizedWrapGrid can be used to define a field of tiles similar to an HTML flex container ( display: flex; with flex-wrap: wrap; ). The Orientation property is similar to a flex container's flex-direction , in that the direction of alignment can be specified. <Page> <VariableSizedWrapGrid Orientation= \"Horizontal\" ItemWidth= \"100\" ItemHeight= \"100\" > <Rectangle Fill= \"LightGray\" /> <Rectangle Fill= \"LightSteelBlue\" /> <Rectangle Fill= \"LightBlue\" /> <Rectangle Fill= \"LightCyan\" /> <Rectangle Fill= \"LightSeaGreen\" /> <Rectangle Fill= \"LightGreen\" /> <Rectangle Fill= \"LightGoldenrodYellow\" /> <Rectangle Fill= \"LightSalmon\" /> <Rectangle Fill= \"LightCoral\" /> <Rectangle Fill= \"Gray\" /> <Rectangle Fill= \"SteelBlue\" /> <Rectangle Fill= \"CadetBlue\" /> <Rectangle Fill= \"Cyan\" /> <Rectangle Fill= \"SeaGreen\" /> <Rectangle Fill= \"Green\" /> <Rectangle Fill= \"Goldenrod\" /> <Rectangle Fill= \"Salmon\" /> <Rectangle Fill= \"Coral\" /> </VariableSizedWrapGrid> </Page> Notably, the horizontal or vertical alignment of XAML controls is defined on each control, whereas in HTML alignment is specified at the level of the enclosing container. XAML <TextBlock Content= \"Hello, world!\" HorizontalAlignment= \"Left\" VerticalAlignment= \"Top\" /> HTML and CSS .container p Hello, world! . container { text-align : right top ; }","title":"VariableSizedWrapGrid"},{"location":"Linux/GRUB/","text":"GRUB rescue prompt When GRUB2 is unable to find the GRUB folder or its contents are missing or corrupted, it displays the prompt grub rescue> This means it failed to load the normal module. howtoforge.com From GRUB rescue prompt: set prefix=(hd0,1)/boot/grub set root=(hd0,1) insmod normal normal After booting the system, GRUB should be updated and reinstalled: Update GRUB config file update-grub Reinstall GRUB grub-install /dev/sdx grub-install Install GRUB as bootloader grub-install --target = i386-pc /dev/sdb Install boot images within a directory other than /boot grub-install --boot-directory grub-mkconfig Generate GRUB configuration grub-mkconfig -o /boot/grub/grub.cfg grub2-mkconfig Send output of grub2-mkconfig to the correct location for booting grub2-mkconfig --output = /boot/grub2/grub.cfg grub2-editenv Disable the Nouveau display driver while installing the proprietary Nvidia display driver on Fedora linuxconfig.org grub2-editenv - set \" $( grub2-editenv - list | grep kernelopts ) nouveau.modeset=0\" update-grub Update GRUB config file update-grub","title":"GRUB"},{"location":"Linux/GRUB/#grub-rescue-prompt","text":"When GRUB2 is unable to find the GRUB folder or its contents are missing or corrupted, it displays the prompt grub rescue> This means it failed to load the normal module. howtoforge.com From GRUB rescue prompt: set prefix=(hd0,1)/boot/grub set root=(hd0,1) insmod normal normal After booting the system, GRUB should be updated and reinstalled: Update GRUB config file update-grub Reinstall GRUB grub-install /dev/sdx","title":"GRUB rescue prompt"},{"location":"Linux/GRUB/#grub-install","text":"Install GRUB as bootloader grub-install --target = i386-pc /dev/sdb Install boot images within a directory other than /boot grub-install --boot-directory","title":"grub-install"},{"location":"Linux/GRUB/#grub-mkconfig","text":"Generate GRUB configuration grub-mkconfig -o /boot/grub/grub.cfg","title":"grub-mkconfig"},{"location":"Linux/GRUB/#grub2-mkconfig","text":"Send output of grub2-mkconfig to the correct location for booting grub2-mkconfig --output = /boot/grub2/grub.cfg","title":"grub2-mkconfig"},{"location":"Linux/GRUB/#grub2-editenv","text":"Disable the Nouveau display driver while installing the proprietary Nvidia display driver on Fedora linuxconfig.org grub2-editenv - set \" $( grub2-editenv - list | grep kernelopts ) nouveau.modeset=0\"","title":"grub2-editenv"},{"location":"Linux/GRUB/#update-grub","text":"Update GRUB config file update-grub","title":"update-grub"},{"location":"Linux/SSH/","text":"SSH curl ftp rsync ssh ssh-add ssh-agent ssh-keygen ssh-keyscan stty sshfs ssh-copy-id In Windows, this command is not available, so a workaround is to simply pipe the public key over SSH itself. type $env :USERPROFILE \\. ssh \\i d_rsa.pub | ssh { IP-ADDRESS-OR-FQDN } \"cat >> .ssh/authorized_keys\" ssh-keygen Before starting the SSH daemon, hostkeys must be generated: sudo ssh-keygen -A /etc/ssh/sshd_config Disable cleartext passwords PasswordAuthentication no","title":"SSH"},{"location":"Linux/SSH/#ssh","text":"curl ftp rsync ssh ssh-add ssh-agent ssh-keygen ssh-keyscan stty sshfs","title":"SSH"},{"location":"Linux/SSH/#ssh-copy-id","text":"In Windows, this command is not available, so a workaround is to simply pipe the public key over SSH itself. type $env :USERPROFILE \\. ssh \\i d_rsa.pub | ssh { IP-ADDRESS-OR-FQDN } \"cat >> .ssh/authorized_keys\"","title":"ssh-copy-id"},{"location":"Linux/SSH/#ssh-keygen","text":"Before starting the SSH daemon, hostkeys must be generated: sudo ssh-keygen -A","title":"ssh-keygen"},{"location":"Linux/SSH/#etcsshsshd_config","text":"Disable cleartext passwords PasswordAuthentication no","title":"/etc/ssh/sshd_config"},{"location":"Linux/X/","text":"X Test X11 with the config file automatically generated after Xorg -configure X -config $HOME /xorg.conf.new","title":"X"},{"location":"Linux/X/#x","text":"Test X11 with the config file automatically generated after Xorg -configure X -config $HOME /xorg.conf.new","title":"X"},{"location":"Linux/Xorg/","text":"Xorg Enable automatic configuration of X11 server Xorg -configure","title":"Xorg"},{"location":"Linux/Xorg/#xorg","text":"Enable automatic configuration of X11 server Xorg -configure","title":"Xorg"},{"location":"Linux/add-apt-repository/","text":"add-apt-repository Add Docker repos ( src ) curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $( lsb_release -cs ) stable\" add-apt-repository \"deb http://security.ubuntu.com/ubuntu trusty-security main universe\" [Convert][LPM 231 Regolith Linux] Ubuntu to Regolith Linux add-apt-repository -y ppa:kgilmer/regolith-stable apt updatelib apt install regolith-desktop","title":"add-apt-repository"},{"location":"Linux/add-apt-repository/#add-apt-repository","text":"Add Docker repos ( src ) curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $( lsb_release -cs ) stable\" add-apt-repository \"deb http://security.ubuntu.com/ubuntu trusty-security main universe\" [Convert][LPM 231 Regolith Linux] Ubuntu to Regolith Linux add-apt-repository -y ppa:kgilmer/regolith-stable apt updatelib apt install regolith-desktop","title":"add-apt-repository"},{"location":"Linux/amixer/","text":"amixer Command-line mixer for ALSA sound card driver [Schatz][Schatz] Display simplified list of controls amixer scontrols Display full list of controls amixer controls Show information about a mixer device amixer info","title":"amixer"},{"location":"Linux/amixer/#amixer","text":"Command-line mixer for ALSA sound card driver [Schatz][Schatz] Display simplified list of controls amixer scontrols Display full list of controls amixer controls Show information about a mixer device amixer info","title":"amixer"},{"location":"Linux/apt-key/","text":"apt-key Add a public GPG key to keyring # Google Cloud SDK curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - # https://medium.com/faun/docker-running-seamlessly-in-windows-subsystem-linux-6ef8412377aa curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - Install key from Mono apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 3FA7E0328081BFF6A14DA29AA6A19B38D3D831EF Add key specified by apt in error message sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 68980A0EA10B4DE8","title":"apt-key"},{"location":"Linux/apt-key/#apt-key","text":"Add a public GPG key to keyring # Google Cloud SDK curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - # https://medium.com/faun/docker-running-seamlessly-in-windows-subsystem-linux-6ef8412377aa curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - Install key from Mono apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 3FA7E0328081BFF6A14DA29AA6A19B38D3D831EF Add key specified by apt in error message sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 68980A0EA10B4DE8","title":"apt-key"},{"location":"Linux/apt/","text":"apt /etc/apt/sources.list Entries are made of three parts, delimited by whitespace: ( src ) Source type: deb for binary packages or deb-src for source packages Base URL of the source: beginning with http:// , ftp:// , file:// , or even cdrom: Name of the chosen distribution followed by sections that differentiate packages by license. Kali, for example, contains main , non-free , and contrib . deb http://us-central1.gce.archive.ubuntu.com/ubuntu/ bionic main restricted deb http://us-central1.gce.archive.ubuntu.com/ubuntu/ bionic universe deb http://us-central1.gce.archive.ubuntu.com/ubuntu/ bionic-updates main restricted deb http://us-central1.gce.archive.ubuntu.com/ubuntu/ bionic-updates universe MongoDB repo deb [ arch = amd64 ] https://repo.mongodb.org/apt/ubuntu bionic/mongodb-org/4.2 multiverse gcloud deb [ signed-by = /usr/share/keyrings/cloud.google.gpg ] http://packages.cloud.google.com/apt cloud-sdk main mailx ```sh deb http://security.ubuntu.com/ubuntu trusty-security main universe ```","title":"apt"},{"location":"Linux/apt/#apt","text":"","title":"apt"},{"location":"Linux/apt/#etcaptsourceslist","text":"Entries are made of three parts, delimited by whitespace: ( src ) Source type: deb for binary packages or deb-src for source packages Base URL of the source: beginning with http:// , ftp:// , file:// , or even cdrom: Name of the chosen distribution followed by sections that differentiate packages by license. Kali, for example, contains main , non-free , and contrib . deb http://us-central1.gce.archive.ubuntu.com/ubuntu/ bionic main restricted deb http://us-central1.gce.archive.ubuntu.com/ubuntu/ bionic universe deb http://us-central1.gce.archive.ubuntu.com/ubuntu/ bionic-updates main restricted deb http://us-central1.gce.archive.ubuntu.com/ubuntu/ bionic-updates universe MongoDB repo deb [ arch = amd64 ] https://repo.mongodb.org/apt/ubuntu bionic/mongodb-org/4.2 multiverse gcloud deb [ signed-by = /usr/share/keyrings/cloud.google.gpg ] http://packages.cloud.google.com/apt cloud-sdk main mailx ```sh deb http://security.ubuntu.com/ubuntu trusty-security main universe ```","title":"/etc/apt/sources.list"},{"location":"Linux/at/","text":"at Execute a command at a given time Pipe echo \"Hello, world!\" | at time Shell at time > echo \"Hello, world!\"","title":"at"},{"location":"Linux/at/#at","text":"Execute a command at a given time Pipe echo \"Hello, world!\" | at time Shell at time > echo \"Hello, world!\"","title":"at"},{"location":"Linux/ausearch/","text":"ausearch Display audit logs from $STARTDATE to $ENDDATE ausearch --start $STARTDATE --end $ENDDATE Search audit logs for today for logins of UID 500 ausearch --start today --loginuid 500","title":"ausearch"},{"location":"Linux/ausearch/#ausearch","text":"Display audit logs from $STARTDATE to $ENDDATE ausearch --start $STARTDATE --end $ENDDATE Search audit logs for today for logins of UID 500 ausearch --start today --loginuid 500","title":"ausearch"},{"location":"Linux/awk/","text":"awk f v F Variables ARGC ARGV FILENAME FNR FS NF NR OFS ORS RS Functions gsub() sub() index() length() match() printf() split() substr() tolower() toupper() \"The basic function of awk is to search files for lines that contain certain patterns.\" (GEAP: 17 Pattern-scanning utility and processing language, one of the two primary commands which accept regular expressions in Unix systems. awk programs can be defined inline or in a program-file (PGL: 528) - inline: awk options 'program' input-files - program-file , also \"source-file\" (GEAP:18): awk options -f program-file input-files awk programs can be run without defining input-files , in which case awk will accept input from STDIN awk programs are the equivalent of sed \"instructions\", and similarly combine patterns and actions (PGL: 530, GEAP: 17) Patterns can be: - regular expressions or fixed strings - line numbers using builtin variable NR - predefined patterns BEGIN or END , whose actions are executed before and after processing any lines of the data file, respectively change \":\" to newlines in PATH variable; equivalent to echo $PATH \\| tr \":\" \"\\n\" echo $PATH | awk 'BEGIN {RS=\":\"} {print}' print the first field of all files in the current directory, taking semicolon ; as the field separator, outputting filename, line number, and first field of matches, with colon : between the filename and line number awk 'BEGIN {FS=\";\"} /enable/ {print FILENAME \":\" FNR,$1}' * search for string MA in all files, outputting filename, line, and line number for matches awk ' /MA/ { OFS = \" \" print FILENAME OFS FNR OFS $0 } * change field separator ( FS ) to a colon ( : ) and run awkscr awk -F: -f awkscr /etc/passwd flag also works for awk awk -f script files ` ` -f print the first field of each line in the input file awk '{ print $1 }' list equivalent to grep MA * ( {print} is implied) awk '/MA/' * | awk '/MA/ {print}' * -F flag is followed by field separator awk -F, '/MA/ { print $1 }' list pipe output of free to awk to get free memory and total memory free -h | awk ' /^Mem | / { print $3 \"/\" $2 } pipe output of sensors to awk to get CPU temperature sensors | awk ' /^temp1/ { print $2 } replace initial \"fake.\" with \"real;\" in file fake_isbn awk 'sub(^fake.,\"real;\")' fake_isbn print all lines awk '1 { print }' file remove file header awk 'NR>1' file remove file header awk ' NR>1 { print } file print lines in a range awk 'NR>1 && NR < 4' file remove whitespace-only lines awk 'NF' file remove all blank lines awk '1' RS = '' file extract fields awk '{ print $1, $3}' FS = , OFS = , file perform column-wise calculations awk '{ SUM=SUM+$1 } END { print SUM }' FS = , OFS = , file count the number of nonempty lines awk '/./ { COUNT+=1 } END { print COUNT }' file count the number of nonempty lines awk 'NF { COUNT+=1 } END { print COUNT }' file count the number of nonempty lines awk '+$1 { COUNT+=1 } END { print COUNT }' file Arrays awk '+$1 { CREDITS[$3]+=$1 } END { for (NAME in CREDITS) print NAME, CREDITS[NAME] }' FS = , file Identify duplicate lines awk 'a[$0]++' file Remove duplicate lines awk '!a[$0]++' file Remove multiple spaces awk '$1=$1' file Join lines awk '{ print $3 }' FS = , ORS = ' ' file ; echo awk '+$1 { SUM+=$1; NUM+=1 } END { printf(\"AVG=%f\",SUM/NUM); }' FS = , file ` | format awk '+$1 { SUM+=$1; NUM+=1 } END { printf(\"AVG=%6.1f\",SUM/NUM); }' FS = , file Convert to uppercase awk '$3 { print toupper($0); }' file Change part of a string awk '{ $3 = toupper(substr($3,1,1)) substr($3,2) } $3' FS = , OFS = , file Split the second field (\"EXPDATE\") by spaces, storing the result into the array DATE; then print credits ($1) and username ($3) as well as the month (DATE[2]) and year (DATE[3]) awk '+$1 { split($2, DATE, \" \"); print $1,$3, DATE[2], DATE[3] }' FS = , OFS = , file awk '+$1 { split($4, GRP, \":\"); print $3, GRP[1], GRP[2] }' FS = , file awk '+$1 { split($4, GRP, /:+/); print $3, GRP[1], GRP[2] }' FS = , file Search and replace with comma awk '+$1 { gsub(/ +/, \"-\", $2); print }' FS = , file Adding date awk 'BEGIN { printf(\"UPDATED: \"); system(\"date\") } /^UPDATED:/ { next } 1' file Modify a field externally awk '+$1 { CMD | getline $5; close(CMD); print }' CMD = \"uuid -v4\" FS = , OFS = , file Invoke dynamically generated command awk '+$1 { cmd = sprintf(FMT, $2); cmd | getline $2; close(cmd); print }' FMT = 'date -I -d \"%s\"' FS = , file Join data awk '+$1 { CMD | getline $5; print }' CMD = 'od -vAn -w4 -t x /dev/urandom' FS = , file Add up all first records to {sum}, then print that number out at the end awk '{sum += $1} END {print sum}' file","title":"awk"},{"location":"Linux/awk/#awk","text":"f v F","title":"awk"},{"location":"Linux/awk/#variables","text":"ARGC ARGV FILENAME FNR FS NF NR OFS ORS RS","title":"Variables"},{"location":"Linux/awk/#functions","text":"gsub() sub() index() length() match() printf() split() substr() tolower() toupper() \"The basic function of awk is to search files for lines that contain certain patterns.\" (GEAP: 17 Pattern-scanning utility and processing language, one of the two primary commands which accept regular expressions in Unix systems. awk programs can be defined inline or in a program-file (PGL: 528) - inline: awk options 'program' input-files - program-file , also \"source-file\" (GEAP:18): awk options -f program-file input-files awk programs can be run without defining input-files , in which case awk will accept input from STDIN awk programs are the equivalent of sed \"instructions\", and similarly combine patterns and actions (PGL: 530, GEAP: 17) Patterns can be: - regular expressions or fixed strings - line numbers using builtin variable NR - predefined patterns BEGIN or END , whose actions are executed before and after processing any lines of the data file, respectively change \":\" to newlines in PATH variable; equivalent to echo $PATH \\| tr \":\" \"\\n\" echo $PATH | awk 'BEGIN {RS=\":\"} {print}' print the first field of all files in the current directory, taking semicolon ; as the field separator, outputting filename, line number, and first field of matches, with colon : between the filename and line number awk 'BEGIN {FS=\";\"} /enable/ {print FILENAME \":\" FNR,$1}' * search for string MA in all files, outputting filename, line, and line number for matches awk ' /MA/ { OFS = \" \" print FILENAME OFS FNR OFS $0 } * change field separator ( FS ) to a colon ( : ) and run awkscr awk -F: -f awkscr /etc/passwd flag also works for awk awk -f script files ` ` -f print the first field of each line in the input file awk '{ print $1 }' list equivalent to grep MA * ( {print} is implied) awk '/MA/' * | awk '/MA/ {print}' * -F flag is followed by field separator awk -F, '/MA/ { print $1 }' list pipe output of free to awk to get free memory and total memory free -h | awk ' /^Mem | / { print $3 \"/\" $2 } pipe output of sensors to awk to get CPU temperature sensors | awk ' /^temp1/ { print $2 } replace initial \"fake.\" with \"real;\" in file fake_isbn awk 'sub(^fake.,\"real;\")' fake_isbn print all lines awk '1 { print }' file remove file header awk 'NR>1' file remove file header awk ' NR>1 { print } file print lines in a range awk 'NR>1 && NR < 4' file remove whitespace-only lines awk 'NF' file remove all blank lines awk '1' RS = '' file extract fields awk '{ print $1, $3}' FS = , OFS = , file perform column-wise calculations awk '{ SUM=SUM+$1 } END { print SUM }' FS = , OFS = , file count the number of nonempty lines awk '/./ { COUNT+=1 } END { print COUNT }' file count the number of nonempty lines awk 'NF { COUNT+=1 } END { print COUNT }' file count the number of nonempty lines awk '+$1 { COUNT+=1 } END { print COUNT }' file Arrays awk '+$1 { CREDITS[$3]+=$1 } END { for (NAME in CREDITS) print NAME, CREDITS[NAME] }' FS = , file Identify duplicate lines awk 'a[$0]++' file Remove duplicate lines awk '!a[$0]++' file Remove multiple spaces awk '$1=$1' file Join lines awk '{ print $3 }' FS = , ORS = ' ' file ; echo awk '+$1 { SUM+=$1; NUM+=1 } END { printf(\"AVG=%f\",SUM/NUM); }' FS = , file ` | format awk '+$1 { SUM+=$1; NUM+=1 } END { printf(\"AVG=%6.1f\",SUM/NUM); }' FS = , file Convert to uppercase awk '$3 { print toupper($0); }' file Change part of a string awk '{ $3 = toupper(substr($3,1,1)) substr($3,2) } $3' FS = , OFS = , file Split the second field (\"EXPDATE\") by spaces, storing the result into the array DATE; then print credits ($1) and username ($3) as well as the month (DATE[2]) and year (DATE[3]) awk '+$1 { split($2, DATE, \" \"); print $1,$3, DATE[2], DATE[3] }' FS = , OFS = , file awk '+$1 { split($4, GRP, \":\"); print $3, GRP[1], GRP[2] }' FS = , file awk '+$1 { split($4, GRP, /:+/); print $3, GRP[1], GRP[2] }' FS = , file Search and replace with comma awk '+$1 { gsub(/ +/, \"-\", $2); print }' FS = , file Adding date awk 'BEGIN { printf(\"UPDATED: \"); system(\"date\") } /^UPDATED:/ { next } 1' file Modify a field externally awk '+$1 { CMD | getline $5; close(CMD); print }' CMD = \"uuid -v4\" FS = , OFS = , file Invoke dynamically generated command awk '+$1 { cmd = sprintf(FMT, $2); cmd | getline $2; close(cmd); print }' FMT = 'date -I -d \"%s\"' FS = , file Join data awk '+$1 { CMD | getline $5; print }' CMD = 'od -vAn -w4 -t x /dev/urandom' FS = , file Add up all first records to {sum}, then print that number out at the end awk '{sum += $1} END {print sum}' file","title":"Functions"},{"location":"Linux/bash/","text":"bash getopt and argp_parse reorder the elements of argv by default, but this behavior can be suppressed by setting the _POSIX_OPTION_ORDER environment variable ANSI/VT100 terminals and emulators can display colors and formatted texts by using escape sequences ( escape character followed by format code , terminated by \"m\").\\ Three escape characters: - \\e - \\033 - \\x1B Event designator Effect !^ first argument from previous command !$ last argument from previous command ^$STRING repeat last command, deleting the first instance of $STRING ^$STRING^$SUBSTITUTE repeat last command, substituting the first instance of $STRING with $SUBSTITUTE !!:n {n}th argument from previous command (0-indexed) !#:n {n}th word of current command (0-indexed) Syntax Effect $((...)) arithmetic expansion [...] alias for test $(...) command substitution ${...} variable substitution ${var:start:size} variable slicing filename{,.new} brace expansion makes multiple arguments from a single one Content of all loops is bracketed by do and done : for i in ... do ...; done; and for ((i=0; i<N; i++)); do cmd; done; - for i in {01..07}; do cmd; done; - for i in {000..100}; do cmd; done; - for i in 1 2 3 4 5; do cmd; done; Number bases Description r#n general format for interpreting number as having base (radix) 2#1111001110011010 binary 0x32 hex numbers 032 octal 32#@_ base-32: a range of ASCII characters can be used to define numbers with bases up to 64: 10 digits, 26 lowercase characters, 26 uppercase characters, '@', and '_' Command sequence syntax Effect ; run commands synchnonously & run commands asynchronously && logical AND; run next command synchronously only if first command succeeds || logical OR; run next command synchronously only if first command fails Extracting audio from mp4 files in a directory # Using variable substitution to replace on file extension with another for f in *.mp4 ; do ffmpeg -i $f ${ f /.mp4/.wav } Less gracefully: for f in *.mp4 do # Find length of filename, removing 4 for the filename extension l = $( expr length \" $f \" - 4 ) # Slice string to remove the file extension, then concatenate extensions again # using brace expansion to form two quoted strings from the original filename echo \\\" ${ f :: $l } { .mp4 \\\" ,.wav \\\" } | xargs ffmpeg -i done Validating arguments $# # From Sobell p. 548 if [ $# ! = 2 ] then echo \"...\" exit 1 fi -z $1 #From https://youtu.be/ksAfmJfdub0 [ -z \" $1 \" ] && echo \"...\" && exit 1 ! -z $2 # From https://coderwall.com/p/kq9ghg/yakuake-scripting if [ ! -z \" $2 \" ] ; then ... ; fi while ... break Placed in a while loop, if user responds with anything except \"y\" (the read command will read only the first letter) the loop will terminate [Cannon][CLKF] read -p \"Backup another server? (y/n)\" -n 1 [ \" $BACKUP_AGAIN \" = \"y\" ] || break","title":"bash"},{"location":"Linux/bash/#bash","text":"getopt and argp_parse reorder the elements of argv by default, but this behavior can be suppressed by setting the _POSIX_OPTION_ORDER environment variable ANSI/VT100 terminals and emulators can display colors and formatted texts by using escape sequences ( escape character followed by format code , terminated by \"m\").\\ Three escape characters: - \\e - \\033 - \\x1B Event designator Effect !^ first argument from previous command !$ last argument from previous command ^$STRING repeat last command, deleting the first instance of $STRING ^$STRING^$SUBSTITUTE repeat last command, substituting the first instance of $STRING with $SUBSTITUTE !!:n {n}th argument from previous command (0-indexed) !#:n {n}th word of current command (0-indexed) Syntax Effect $((...)) arithmetic expansion [...] alias for test $(...) command substitution ${...} variable substitution ${var:start:size} variable slicing filename{,.new} brace expansion makes multiple arguments from a single one Content of all loops is bracketed by do and done : for i in ... do ...; done; and for ((i=0; i<N; i++)); do cmd; done; - for i in {01..07}; do cmd; done; - for i in {000..100}; do cmd; done; - for i in 1 2 3 4 5; do cmd; done; Number bases Description r#n general format for interpreting number as having base (radix) 2#1111001110011010 binary 0x32 hex numbers 032 octal 32#@_ base-32: a range of ASCII characters can be used to define numbers with bases up to 64: 10 digits, 26 lowercase characters, 26 uppercase characters, '@', and '_' Command sequence syntax Effect ; run commands synchnonously & run commands asynchronously && logical AND; run next command synchronously only if first command succeeds || logical OR; run next command synchronously only if first command fails Extracting audio from mp4 files in a directory # Using variable substitution to replace on file extension with another for f in *.mp4 ; do ffmpeg -i $f ${ f /.mp4/.wav } Less gracefully: for f in *.mp4 do # Find length of filename, removing 4 for the filename extension l = $( expr length \" $f \" - 4 ) # Slice string to remove the file extension, then concatenate extensions again # using brace expansion to form two quoted strings from the original filename echo \\\" ${ f :: $l } { .mp4 \\\" ,.wav \\\" } | xargs ffmpeg -i done Validating arguments $# # From Sobell p. 548 if [ $# ! = 2 ] then echo \"...\" exit 1 fi -z $1 #From https://youtu.be/ksAfmJfdub0 [ -z \" $1 \" ] && echo \"...\" && exit 1 ! -z $2 # From https://coderwall.com/p/kq9ghg/yakuake-scripting if [ ! -z \" $2 \" ] ; then ... ; fi while ... break Placed in a while loop, if user responds with anything except \"y\" (the read command will read only the first letter) the loop will terminate [Cannon][CLKF] read -p \"Backup another server? (y/n)\" -n 1 [ \" $BACKUP_AGAIN \" = \"y\" ] || break","title":"bash"},{"location":"Linux/bluetoothctl/","text":"bluetoothctl [ref][http://www.linux-magazine.com/Issues/2017/197/Command-Line-bluetoothctl#article_i1] [ref][https://computingforgeeks.com/connect-to-bluetooth-device-from-linux-terminal/] device \u2022 list \u2022 pair \u2022 pairable on scan on select show","title":"bluetoothctl"},{"location":"Linux/bluetoothctl/#bluetoothctl","text":"[ref][http://www.linux-magazine.com/Issues/2017/197/Command-Line-bluetoothctl#article_i1] [ref][https://computingforgeeks.com/connect-to-bluetooth-device-from-linux-terminal/] device \u2022 list \u2022 pair \u2022 pairable on scan on select show","title":"bluetoothctl"},{"location":"Linux/chage/","text":"chage d l m E I M W","title":"chage"},{"location":"Linux/chage/#chage","text":"d l m E I M W","title":"chage"},{"location":"Linux/chgrp/","text":"chgrp Change ownership of $FILE to $USER and $GROUP chgrp $USER : $GROUP $FILE","title":"chgrp"},{"location":"Linux/chgrp/#chgrp","text":"Change ownership of $FILE to $USER and $GROUP chgrp $USER : $GROUP $FILE","title":"chgrp"},{"location":"Linux/chkconfig/","text":"chkconfig Turn services on or off for runlevels Without arguments, chkconfig defaults to runlevels 3 or 5: chkconfig Display all services and runlevels chkconfig --list Turn {daemon} on for runlevels 3 and 5 chkconfig --level 35 daemon on Turn {daemon} off chkconfig daemon off chkconfig NetworkManager off Turn {daemon} service on chkconfig daemon on","title":"chkconfig"},{"location":"Linux/chkconfig/#chkconfig","text":"Turn services on or off for runlevels Without arguments, chkconfig defaults to runlevels 3 or 5: chkconfig Display all services and runlevels chkconfig --list Turn {daemon} on for runlevels 3 and 5 chkconfig --level 35 daemon on Turn {daemon} off chkconfig daemon off chkconfig NetworkManager off Turn {daemon} service on chkconfig daemon on","title":"chkconfig"},{"location":"Linux/chmod/","text":"chmod Set sticky bit on $FILE chmod +t $FILE Clear sticky bit on $FILE chmod -t file Clear SGID bit on $FILE chmod g-s file Set SGID bit on $FILE chmod g+s file Clear SUID bit on $FILE chmod u-s file Set SUID bit on $FILE chmod u+s file Set setuid permission on $FILE chmod +s file","title":"chmod"},{"location":"Linux/chmod/#chmod","text":"Set sticky bit on $FILE chmod +t $FILE Clear sticky bit on $FILE chmod -t file Clear SGID bit on $FILE chmod g-s file Set SGID bit on $FILE chmod g+s file Clear SUID bit on $FILE chmod u-s file Set SUID bit on $FILE chmod u+s file Set setuid permission on $FILE chmod +s file","title":"chmod"},{"location":"Linux/chown/","text":"chown c f h v H L P R Change a file or directory's ownership. To change the user and group owner of a file to {user} and {group}, chown 's syntax is of the format user:group ]. chown susan:delta file # Assign {file} to user `susan` and group `delta` chown alan file # Assign {file} to user `alan` chown alan: file # Assign {file} to user and group `alan` chown :gamma file # Assign {file} to the group `gamma` Recursively grant {user} ownership to {path} chown -R user path Assign {path} to susan and group delta , recursively and with verbose output chown --verbose --recursive susan:delta path chown -vR susan:delta path chown -vR --reference = . path # Use a `reference` file to match the configuration of a particular file chown -cfR --preserve-root alan # `preserve-root` prevents changes to files in the root directory, but has no effect when not used with `recursive`","title":"chown"},{"location":"Linux/chown/#chown","text":"c f h v H L P R Change a file or directory's ownership. To change the user and group owner of a file to {user} and {group}, chown 's syntax is of the format user:group ]. chown susan:delta file # Assign {file} to user `susan` and group `delta` chown alan file # Assign {file} to user `alan` chown alan: file # Assign {file} to user and group `alan` chown :gamma file # Assign {file} to the group `gamma` Recursively grant {user} ownership to {path} chown -R user path Assign {path} to susan and group delta , recursively and with verbose output chown --verbose --recursive susan:delta path chown -vR susan:delta path chown -vR --reference = . path # Use a `reference` file to match the configuration of a particular file chown -cfR --preserve-root alan # `preserve-root` prevents changes to files in the root directory, but has no effect when not used with `recursive`","title":"chown"},{"location":"Linux/chpass/","text":"chpass Change default shell to Fish chpass -s /usr/local/bin/fish","title":"chpass"},{"location":"Linux/chpass/#chpass","text":"Change default shell to Fish chpass -s /usr/local/bin/fish","title":"chpass"},{"location":"Linux/chrony/","text":"Synchronize system time using NTP (cf. [ timedatectl ][timedatectl] Stop the systemd-timesyncd service sudo systemctl stop systemd-timesyncd.service Install chrony if it is not already present and enable and start the service sudo systemctl enable chronyd && sudo systemctl start chronyd","title":"Chrony"},{"location":"Linux/chsh/","text":"Change the user's default shell to Bash chsh-s /bin/bash Change the user's default shell to Fish chsh-s /usr/local/bin/fish","title":"Chsh"},{"location":"Linux/cron/","text":"crontab Directive Effect @hourly equivalent to 0 * * * * @midnight @daily equivalent to 0 0 * * * @weekly equivalent to 0 0 * * 0 @monthly equivalent to 0 0 1 * * @annually @yearly equivalent to 0 0 1 1 * @reboot run at startup Run /root/backup.sh at 0300 everyday 0 3 * * * /root/backup.sh Run /path/to/script.sh at 16:30 on the 2nd of every month 30 16 2 * * /path/to/script.sh Run /scripts/phpscript.php at 22:00 every weekday 0 22 * * 1 -5 /scripts/phpscript.php Run /path/to/perlscript.pl at 00:23, 02:23, and 04:23 everyday 23 0 -23/2 * * * /path/to/perlscript.pl Run linuxcommand at 04:05 every Sunday 5 4 * * sun /path/to/linuxcommand","title":"Cron"},{"location":"Linux/cron/#crontab","text":"Directive Effect @hourly equivalent to 0 * * * * @midnight @daily equivalent to 0 0 * * * @weekly equivalent to 0 0 * * 0 @monthly equivalent to 0 0 1 * * @annually @yearly equivalent to 0 0 1 1 * @reboot run at startup Run /root/backup.sh at 0300 everyday 0 3 * * * /root/backup.sh Run /path/to/script.sh at 16:30 on the 2nd of every month 30 16 2 * * /path/to/script.sh Run /scripts/phpscript.php at 22:00 every weekday 0 22 * * 1 -5 /scripts/phpscript.php Run /path/to/perlscript.pl at 00:23, 02:23, and 04:23 everyday 23 0 -23/2 * * * /path/to/perlscript.pl Run linuxcommand at 04:05 every Sunday 5 4 * * sun /path/to/linuxcommand","title":"crontab"},{"location":"Linux/cryptsetup/","text":"Incorporate full-disk encryption on /dev/sdb1, asking for passphrase twice cryptsetup --verify-passphrase luksFormat /dev/sdb1 Assign virtual name \"storage1\" to encrypted disk /dev/sdb1 cryptsetup luksOpen /dev/sdb1 storage1","title":"Cryptsetup"},{"location":"Linux/curl/","text":"# d f h m o q s u v x C K L M O S T U V \\ connect-timeout create-dirs disable-epsv ftp-pasv limit-rate max-filesize url Use the dict network protocol to retrieve the definition of a word. ref curl dict://dict.org/d:<word> Sending a POST method to a FastAPI app ( src ) curl -X POST \"http://127.0.0.1:8000/purchase/item/\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\\\"name\\\":\\\"sample item\\\",\\\"info\\\":\\\"This is info for the item\\\",\\\"price\\\":40,\\\"qty\\\":2}\"","title":"Curl"},{"location":"Linux/dar/","text":"Create a differential (or incremental) backup of {file}, using full.bak as reference dar -R /path/to/file -c diff1.bak -A full.bak Create a full backup of {file} dar -R /path/to/file -c full.bak Restore full.bak dar -x full.bak","title":"Dar"},{"location":"Linux/date/","text":"d f r s u I R Metacharacters a b c d e g h j k l m n p r s t u w x y A B D F G H I M N R S T U V W X Y Z Set only the year using \"next year\" or \"last year\" date -s \"next year\" date -s \"last year\" Set only the day date -s \"next day\" date -s \"monday\" Display the date fifty days into the future [devconnected.com][https://devconnected.com/user-administration-complete-guide-on-linux/#Setting_an_account_expiration_date_easily] date -d '+50days' +%F","title":"Date"},{"location":"Linux/dd/","text":"Implement a simple CPU benchmark by writing 1 GB of zeroes and piping it to md5sum dd if = /dev/zero bs = 1M count = 1024 | md5sum","title":"Dd"},{"location":"Linux/dig/","text":"DNS lookup tool that returns the text of the actual response from the DNS server, useful when troubleshooting a DNS issue (cf. nslookup ) Nameserver dig example.com NS Mail server dig example.com MX Perform a reverse DNS lookup on an IP address dig -x 8 .8.8.8 Specify an alternate DNS server to query dig @8.8.8.8 example.com Find authoritative nameservers for the zone and display SOA records dig +nsearch example.com Lookup the IP associated with a domain name dig +short example.com Lookup the mail server IP associated with a domain name dig +short example.com MX example.com MX Perform iterative queries and display the entire trace path to resolve a domain name dig +trace example.com Get all types of records for a given domain name dig example.com ANY Display Start of Authority information for a domain dig example.com soa","title":"Dig"},{"location":"Linux/dnf/","text":"View all dnf commands dnf history View all packages installed by user dnf history userinstalled Display information about a package group [ref][https://docs.fedoraproject.org/en-US/quick-docs/getting-started-with-virtualization/] dnf groupinfo virtualization Install a package group [ref][https://docs.fedoraproject.org/en-US/quick-docs/getting-started-with-virtualization/] dnf install @virtualization Install a package group, including optional packages [ref][https://docs.fedoraproject.org/en-US/quick-docs/getting-started-with-virtualization/] dnf group install --with-optional virtualization /etc/yum.conf Exclude packages from updates permanently [main] exclude = kernel* php*","title":"Dnf"},{"location":"Linux/dnf/#etcyumconf","text":"Exclude packages from updates permanently [main] exclude = kernel* php*","title":"/etc/yum.conf"},{"location":"Linux/dpkg-reconfigure/","text":"Produce a curses-based interface where you can select a display manager dpkg-reconfigure gdm Change the time zone on a Debian based system using package-based tools dpkg-reconfigure tzdata","title":"Dpkg reconfigure"},{"location":"Linux/elvish/","text":"elvish Conditional logic ~> if $true { echo good } else { echo bad } Iterating over an array ~> for x [lorem ipsum] { echo $x.pdf } Raising and catching an error ~> try { fail 'bad error' } except e { echo error $e } else { echo ok }","title":"elvish"},{"location":"Linux/elvish/#elvish","text":"Conditional logic ~> if $true { echo good } else { echo bad } Iterating over an array ~> for x [lorem ipsum] { echo $x.pdf } Raising and catching an error ~> try { fail 'bad error' } except e { echo error $e } else { echo ok }","title":"elvish"},{"location":"Linux/exif/","text":"View image metadata. Unlike alternatives like file and ImageMagick's identify , exif produces columnar output [ref][31] exif image.png","title":"Exif"},{"location":"Linux/fallocate/","text":"Create a file size of 1 gigabyte fallocate -l 1G $FILENAME # gibibyte fallocate -l 1GB $FILENAME # gigabyte","title":"Fallocate"},{"location":"Linux/file/","text":"View image metadata ostechnix.com file image.png # => file type, dimensions, color depth","title":"File"},{"location":"Linux/find/","text":"Search for files in a directory hierarchy Find all files in {$PATH} that are owned by {user} find $PATH -user username Find recently modified files/folders There are 3 timestamps associated with files in Linux 2daygeek.com - atime \"access time\": last time file was accessed by a command or application - mtime \"modify time\": last time file's contents were modified - ctime \"change time\": last time file's attribute was modified Numerical arguments can be specified in 3 ways: - +n greater than {n} days ago - -n less than {n} days ago - n exactly {n} days ago find $PATH -type f -mtime +120 -ls # Find only files that were modified more than 120 days ago find $PATH -type f -mtime -15 -ls # Modified less than 15 days ago find $PATH -type f -mtime 10 -ls # Modified exactly 10 days ago # Find files modified over the past day find $PATH -type f -newermt \"1 day ago\" -ls find $PATH -type f -newermt \"-24 hours\" -ls find $PATH -type f -newermt \"yesterday\" -ls find $PATH -type f -ctime -1 -ls # Find files created today","title":"Find"},{"location":"Linux/firewall-cmd/","text":"add-port add-service get-active-zones get-default-zone get-services list-services new-zone permanent reload remove-service state Successor to iptables in Red Hat, and like its predecessor a frontend to the netfilter protocols. Places network traffic into zones. Commands have to be written twice: once to affect running config and again to have the change saved Configuration file Description /etc/sysconfig/network-scripts/ifcfg-ens33 interface settings /usr/lib/firewalld/services .xml files that define services (\"ZONE=public\") Add a new zone, and write the change to disk firewall-cmd --new-zone=testlab --permanent","title":"Firewall cmd"},{"location":"Linux/fold/","text":"Display text of $FILE , wrapping long lines fold $FILE","title":"Fold"},{"location":"Linux/free/","text":"b c h k m s t List memory statistics in kilobytes. Without any options, free returns a table listing general statistics in kilobytes: free Command-line memory dashboard watch free -h Display amount of free and used memory in the system [ref][L5PMT-memory] free -m","title":"Free"},{"location":"Linux/gdb/","text":"gdb allows you to interactively debug an application from the terminal with keyboard commands: s step into n step over (execute the currently visualized line of code) p print value of a variable (i.e. p x will display the current value of variable x)","title":"Gdb"},{"location":"Linux/gem/","text":"Install a Ruby $PACKAGE gem install $PACKAGE Display currently installed Ruby packages gem list Remove $PACKAGE gem uninstall package Update $PACKAGE gem update package","title":"Gem"},{"location":"Linux/getent/","text":"Get entries from the passwd file ] getent passwd sonny timmy sonny:x:1001:1002:Sonny:/home/sonny:/bin/bash timmy:x:1002:1003::/home/timmy:/bin/bash getent group sonny timmy sonny:x:1002: timmy:x:1003:","title":"Getent"},{"location":"Linux/git/","text":"git add ? branch ? checkout ? cherry-pick ? clean ? config ? log ? ls-files ? mv ? push ? rebase ? remote ? reset ? revert ? rm ? stash ? git clean d f i n x X git add Add file, located in $HOME to the git repo at $PATH git --git-dir = $PATH .git --work-tree = $HOME add file Update index to include all files in the working tree, including removals git add -A git add --no-ignore-removal Stage all modifications in work-tree, including deletions git add -u git branch See a list of branches. A \"*\" indicates that branch is checked out. git branch Display the last commit for each branch git branch -v Display branches that have not been merged git branch --no-merged git checkout Discard unstaged uncommitted changes to file git checkout -- file Switch to branch git checkout branch git cherry-pick Apply a single, specific commit from another branch git cherry-pick commit git config Provides a frontend to the INI formatted config files typically found within .git/config (or ~/.gitconfig when using --global ) Set up alias \"br\" for branch git config --global alias.br branch Equivalent to: [alias] br = branch Store authentication details in a cache git config --global credential.helper cache Equivalent to [credential] helper = cache git log Show commits between January 1 and January 5, 2016 git log --after = \"2016-01-01\" --before = \"2016-01-05\" See commits that are on {branch} but not on {master} git log master..branch git ls-files Show tracked files git ls-files Show tracked files, each line is terminated by a null byte git ls-files -z Show tracked files that have been deleted git ls-files --deleted git mv Move or rename a tracked file git mv file git push Transfer data from local branch {master} to remote {origin} git push -u origin master git rebase To add changes to a commit that is not the most recent, a rebase is necessary. First stash the changes to be added, then initiate a rebase and mark the commit to be edited with edit or e . Leave the other commits alone, save, and drop back to the stash. Pop the stash ( git stash pop ), which will apply the changes stored in the most recent stash. Now you can stage the changes and commit: git commit --amend --no-edit Finally, continue the rebase, rewriting the rest of the commits against the new one. git rebase --continue Combine branches by replaying the changes made on one branch to another git rebase git remote Manage repositories whose branches you track git remote Add remote repo git remote add $REPO $URL Display URL of remote repo git remote get-url $REPO Set url for existing repo git remote set-url $URL $REPO git reset Undo unstaged changes since last commit git reset --hard Reset master to state before last commit git reset --hard HEAD~ git revert Remove (committed) changes in {commit} git revert commit git rm Remove tracked file from repo git rm file git stash Stash changes to work-tree git stash View stashes in stash stack git stash list Apply changes in most recent stash git stash apply Apply changes in stash $STASH git stash apply stash@ $STASH Rebasing Rebase changes committed to branch onto git checkout $BRANCH git rebase $MASTER This will rewind $BRANCH to the commit shared by the two branches, then applying all changes made subsequently to $MASTER. git checkout <master> git merge <branch> Now the history will appear as though all changes were made in series, when they were actually made in parallel on separate branches. Move the last commit to a new branch git branch test # create a new branch with current HEAD git reset --hard HEAD~ # reset master to before last commit git checkout test # continue on new branch Line endings Git will automatically append CRLF endings on Windows. This setting can be displayed with the following command: git config core.autocrlf In order to disable this, adjust the setting git config core.autocrlf false Squashing Sometimes many commits are made to resolve a single issue. These should be \"squashed\". To squash the last 4 commits: git rebase -i HEAD~4 This will open a text editor where you will have to select what to do with each of the 4 commits. Most recent commits are at the bottom, and at least the top (oldest) commit has to remain \"pick\" in order to squash the others. The repo will have to be force-pushed once these changes have been made. git push --force To add changes to the most recent commit, stage changes as normal (including removals), but when committing use the --amend option. This will present an editor, allowing you to edit the commit message, if necessary. [ 6 ] git add README.md git commit --amend To split up $COMMIT git rebase -i \" $COMMIT \" ^ # Start a rebase at the commit you want to split Mark the commit to be split with edit . Now reset state to the previous commit git reset HEAD^ The files are presented unstaged, and can be added to new commits as needed. Finally, finish the rebase git rebase --continue tig Provides a curses-based browser that allows you to navigate the commits in the current branch. It is essentially a wrapper around git log , and therefore accepts the same arguments that can be passed to it. Config file Description $HOME/.tigrc Browse the commit history for a single {file} tig file Browse the commit history for a single {file}, filtering to a specific date range tig --after = \"2017-01-01\" --before = \"2018-05-16\" -- README.md Find who made a change toa file and why tig blame file Browse stash tig stash Browse refs tig refs Navigate the output of git grep tig grep -i foo lib/Bar Pipe a list of commit IDs to tig git rev-list --author = olaf HEAD | tig show --stdin","title":"git"},{"location":"Linux/git/#git","text":"add ? branch ? checkout ? cherry-pick ? clean ? config ? log ? ls-files ? mv ? push ? rebase ? remote ? reset ? revert ? rm ? stash ?","title":"git"},{"location":"Linux/git/#git-clean","text":"d f i n x X","title":"git clean"},{"location":"Linux/git/#git-add","text":"Add file, located in $HOME to the git repo at $PATH git --git-dir = $PATH .git --work-tree = $HOME add file Update index to include all files in the working tree, including removals git add -A git add --no-ignore-removal Stage all modifications in work-tree, including deletions git add -u","title":"git add"},{"location":"Linux/git/#git-branch","text":"See a list of branches. A \"*\" indicates that branch is checked out. git branch Display the last commit for each branch git branch -v Display branches that have not been merged git branch --no-merged","title":"git branch"},{"location":"Linux/git/#git-checkout","text":"Discard unstaged uncommitted changes to file git checkout -- file Switch to branch git checkout branch","title":"git checkout"},{"location":"Linux/git/#git-cherry-pick","text":"Apply a single, specific commit from another branch git cherry-pick commit","title":"git cherry-pick"},{"location":"Linux/git/#git-config","text":"Provides a frontend to the INI formatted config files typically found within .git/config (or ~/.gitconfig when using --global ) Set up alias \"br\" for branch git config --global alias.br branch Equivalent to: [alias] br = branch Store authentication details in a cache git config --global credential.helper cache Equivalent to [credential] helper = cache","title":"git config"},{"location":"Linux/git/#git-log","text":"Show commits between January 1 and January 5, 2016 git log --after = \"2016-01-01\" --before = \"2016-01-05\" See commits that are on {branch} but not on {master} git log master..branch","title":"git log"},{"location":"Linux/git/#git-ls-files","text":"Show tracked files git ls-files Show tracked files, each line is terminated by a null byte git ls-files -z Show tracked files that have been deleted git ls-files --deleted","title":"git ls-files"},{"location":"Linux/git/#git-mv","text":"Move or rename a tracked file git mv file","title":"git mv"},{"location":"Linux/git/#git-push","text":"Transfer data from local branch {master} to remote {origin} git push -u origin master","title":"git push"},{"location":"Linux/git/#git-rebase","text":"To add changes to a commit that is not the most recent, a rebase is necessary. First stash the changes to be added, then initiate a rebase and mark the commit to be edited with edit or e . Leave the other commits alone, save, and drop back to the stash. Pop the stash ( git stash pop ), which will apply the changes stored in the most recent stash. Now you can stage the changes and commit: git commit --amend --no-edit Finally, continue the rebase, rewriting the rest of the commits against the new one. git rebase --continue Combine branches by replaying the changes made on one branch to another git rebase","title":"git rebase"},{"location":"Linux/git/#git-remote","text":"Manage repositories whose branches you track git remote Add remote repo git remote add $REPO $URL Display URL of remote repo git remote get-url $REPO Set url for existing repo git remote set-url $URL $REPO","title":"git remote"},{"location":"Linux/git/#git-reset","text":"Undo unstaged changes since last commit git reset --hard Reset master to state before last commit git reset --hard HEAD~","title":"git reset"},{"location":"Linux/git/#git-revert","text":"Remove (committed) changes in {commit} git revert commit","title":"git revert"},{"location":"Linux/git/#git-rm","text":"Remove tracked file from repo git rm file","title":"git rm"},{"location":"Linux/git/#git-stash","text":"Stash changes to work-tree git stash View stashes in stash stack git stash list Apply changes in most recent stash git stash apply Apply changes in stash $STASH git stash apply stash@ $STASH","title":"git stash"},{"location":"Linux/git/#rebasing","text":"Rebase changes committed to branch onto git checkout $BRANCH git rebase $MASTER This will rewind $BRANCH to the commit shared by the two branches, then applying all changes made subsequently to $MASTER. git checkout <master> git merge <branch> Now the history will appear as though all changes were made in series, when they were actually made in parallel on separate branches. Move the last commit to a new branch git branch test # create a new branch with current HEAD git reset --hard HEAD~ # reset master to before last commit git checkout test # continue on new branch Line endings Git will automatically append CRLF endings on Windows. This setting can be displayed with the following command: git config core.autocrlf In order to disable this, adjust the setting git config core.autocrlf false","title":"Rebasing"},{"location":"Linux/git/#squashing","text":"Sometimes many commits are made to resolve a single issue. These should be \"squashed\". To squash the last 4 commits: git rebase -i HEAD~4 This will open a text editor where you will have to select what to do with each of the 4 commits. Most recent commits are at the bottom, and at least the top (oldest) commit has to remain \"pick\" in order to squash the others. The repo will have to be force-pushed once these changes have been made. git push --force To add changes to the most recent commit, stage changes as normal (including removals), but when committing use the --amend option. This will present an editor, allowing you to edit the commit message, if necessary. [ 6 ] git add README.md git commit --amend To split up $COMMIT git rebase -i \" $COMMIT \" ^ # Start a rebase at the commit you want to split Mark the commit to be split with edit . Now reset state to the previous commit git reset HEAD^ The files are presented unstaged, and can be added to new commits as needed. Finally, finish the rebase git rebase --continue","title":"Squashing"},{"location":"Linux/git/#tig","text":"Provides a curses-based browser that allows you to navigate the commits in the current branch. It is essentially a wrapper around git log , and therefore accepts the same arguments that can be passed to it. Config file Description $HOME/.tigrc Browse the commit history for a single {file} tig file Browse the commit history for a single {file}, filtering to a specific date range tig --after = \"2017-01-01\" --before = \"2018-05-16\" -- README.md Find who made a change toa file and why tig blame file Browse stash tig stash Browse refs tig refs Navigate the output of git grep tig grep -i foo lib/Bar Pipe a list of commit IDs to tig git rev-list --author = olaf HEAD | tig show --stdin","title":"tig"},{"location":"Linux/gpasswd/","text":"Add $USER to $GROUP gpasswd -a $USER $GROUP Add $USER as admin of $GROUP gpasswd -A $USER $GROUP Remove $USER from $GROUP gpasswd -d $USER $GROUP","title":"Gpasswd"},{"location":"Linux/gpg/","text":"Decrypt file gpg file.txt Export GPG public key gpg --export --output ~/jdoe.pub Import another person's public key gpg --import jdoe.pub List available GPG keys gpg --list-key Encrypt a file gpg --encrypt -r jdoe@dplaptop.lab.itpro.tv ./file.txt Sign {file} without encrypting it (produces file.asc) gpg --clearsign file Generate a key gpg --generate-key gpg --gen-key Import another person's public key gpg --import ~/jdoe.pub Send keys to {keyserver} gpg --send-keys keyIDs --keyserver keyserver","title":"Gpg"},{"location":"Linux/grep/","text":"c f h i l n r v w A B C E F H head c n Print first 8 characters of $FILE head -c8 $FILE","title":"Grep"},{"location":"Linux/grep/#head","text":"c n Print first 8 characters of $FILE head -c8 $FILE","title":"head"},{"location":"Linux/groupmod/","text":"g n","title":"Groupmod"},{"location":"Linux/gsettings/","text":"Change function of Caps Lock key [ref][https://superuser.com/questions/1196241/how-to-remap-caps-lock-on-wayland] gsettings set org.gnome.desktop.input-sources xkb-options \"['caps:ctrl_modifier']\" Change mouse cursor size to $SIZE , which can be one of the values 24 (default), 32, 48, 64, or 96. [ref][https://vitux.com/how-to-change-cursor-size-on-ubuntu-desktop/] gsettings set org.gnome.desktop.interface $SIZE","title":"Gsettings"},{"location":"Linux/gzip/","text":"c d r v Zip a single file in-place; each file is replaced by one with the extension .gz or .z , maintaining ownership modes, access and modification times gzip -# gzip --fast gzip --best Compress {symlink} gzip -f symlink gzip --force symlink Page through .gz files","title":"Gzip"},{"location":"Linux/hostnamectl/","text":"Permanently change hostname to $HOSTNAME hostnamectl set-hostname $HOSTNAME","title":"Hostnamectl"},{"location":"Linux/hwclock/","text":"Connection to an NTP server is necessary for a variety of services. Linux systems have two clocks: 1. hardware clock/real-time clock (RTC) 2. system clock Manually synchronize hardware clock to system clock (generally only required if no NTP server is available ) hwclock --hctosys","title":"Hwclock"},{"location":"Linux/i3/","text":"i3 configuration file is in a format similar to that of other whitespace-delimited config files like .bashrc. Comments ( # ) must be placed on their own line and may not be placed after a statement.","title":"I3"},{"location":"Linux/ifconfig/","text":"ifconfig Enable an interface ifconfig ifconfig eth0 up ifup ifup eth0 Disable an interface ifconfig ifconfig eth0 down ifdown ifdown eth0 Apply a static IP address to interface {eth0} and turn it on (\"up\") ifconfig eth0 up 10.1.230.245 netmask 255.255.255.0 Display details of all interfaces, even those that are disabled (\"RX\" and \"TX\" stand for received and transmitted). ifconfig -a Configure eth0 with an additional IPv6 address ifconfig eth0 inet6 add fdd6:551:b09e::/128 ifup Bring online all interfaces marked as auto within the networking configuration ifup -a","title":"ifconfig"},{"location":"Linux/ifconfig/#ifconfig","text":"Enable an interface ifconfig ifconfig eth0 up ifup ifup eth0 Disable an interface ifconfig ifconfig eth0 down ifdown ifdown eth0 Apply a static IP address to interface {eth0} and turn it on (\"up\") ifconfig eth0 up 10.1.230.245 netmask 255.255.255.0 Display details of all interfaces, even those that are disabled (\"RX\" and \"TX\" stand for received and transmitted). ifconfig -a Configure eth0 with an additional IPv6 address ifconfig eth0 inet6 add fdd6:551:b09e::/128","title":"ifconfig"},{"location":"Linux/ifconfig/#ifup","text":"Bring online all interfaces marked as auto within the networking configuration ifup -a","title":"ifup"},{"location":"Linux/imagemagick/","text":"imagemagick identify mogrify identify Option Effect -format {string} display formatted image characteristics; {string} is formatted string using %[key] escape sequences %w current width in pixels %h current height in pixels mogrify Option Effect -write $FILENAME save to filename -resize $Xx$Y resize image to $X pixels by $Y pixels -crop $SIZE$OFFSET -gravity $TYPE set current gravity suggestion for various other options possible values NorthWest , North , NorthEast , West , Center , East , SouthWest , South , or SouthEast Argument patterns bla {geometry} {size}{offset} {size} {scale}% height and weight are both scaled by a specified percentage {width}x{height} maximum values of height and width {width}x{height}^ minimum values of height and width {offset} also affected by -gravity setting +0+0 top-left corner gravity Resize images magick mogrify -resize 1920x1200 -write mars-bg.jpg pia22511.jpg magick identify -format \"%w x %h\" pia22511.jpg Save the output of a command as an image ( convert is from the ImageMagick software suite) [ref][CLKF] cmd | convert label:@- image.png View image metadata ( identify is from the ImageMagick software suite) ostechnix.com identify image.png # => dimensions, color depth, color profile identify -verbose image.png","title":"imagemagick"},{"location":"Linux/imagemagick/#imagemagick","text":"identify mogrify","title":"imagemagick"},{"location":"Linux/imagemagick/#identify","text":"Option Effect -format {string} display formatted image characteristics; {string} is formatted string using %[key] escape sequences %w current width in pixels %h current height in pixels","title":"identify"},{"location":"Linux/imagemagick/#mogrify","text":"Option Effect -write $FILENAME save to filename -resize $Xx$Y resize image to $X pixels by $Y pixels -crop $SIZE$OFFSET -gravity $TYPE set current gravity suggestion for various other options possible values NorthWest , North , NorthEast , West , Center , East , SouthWest , South , or SouthEast Argument patterns bla {geometry} {size}{offset} {size} {scale}% height and weight are both scaled by a specified percentage {width}x{height} maximum values of height and width {width}x{height}^ minimum values of height and width {offset} also affected by -gravity setting +0+0 top-left corner gravity Resize images magick mogrify -resize 1920x1200 -write mars-bg.jpg pia22511.jpg magick identify -format \"%w x %h\" pia22511.jpg Save the output of a command as an image ( convert is from the ImageMagick software suite) [ref][CLKF] cmd | convert label:@- image.png View image metadata ( identify is from the ImageMagick software suite) ostechnix.com identify image.png # => dimensions, color depth, color profile identify -verbose image.png","title":"mogrify"},{"location":"Linux/init/","text":"Access different runlevels init Switch to runlevel {n} init n init 6 # reboot","title":"Init"},{"location":"Linux/initctl/","text":"Reload configuration files (on Upstart-controlled system) initctl reload","title":"Initctl"},{"location":"Linux/install/","text":"Copy files while maintaining various metadata, including timestamp, owner, etc. [ 9 ] Copy a file while preserving timestamp. The copy will have the install default of 755 , but the original's mtime is maintained: install --preserve-timestamp example/foo . Copy a file, setting permissions, owner, and group install --preserve-timestamp --owner = jdoe --group = sudoers --mode = 753","title":"Install"},{"location":"Linux/ip/","text":"Newer alternative to the old ifconfig ip addr Show L2 status (links) ip link Listen for netlink messages ip monitor Display routing information ip route Change the default gateway to 192.168.1.1 on eth0 ip route change default via 192 .168.1.1 dev eth0 Turn on interface wlp2s0 sudo ip link set wlp2s0 up","title":"Ip"},{"location":"Linux/iptables/","text":"Once a popular firewall but no longer installed on modern distros. Like firewalld , it is a frontend for the kernel-level netfilters service. Display rules as written on disk iptables --list-rules Accept SSH traffic from a particular IP iptables -A INPUT -p ssh -s 10 .0.222.222 -j ACCEPT Accept incoming TCP traffic to port 80 iptables -A INPUT -p tcp --dport 80 -j ACCEPT Reload configuration file iptables -F Show statistics for configuration lines iptables -vnL --lines Display rules as written on disk iptables --list-rules Set an iptable rule to accept SSH traffic from a particular IP iptables -A INPUT -p ssh -s 10 .0.222.222 -j ACCEPT Set an iptable rule to accept incoming TCP traffic to port 80 iptables -A INPUT -p tcp --dport 80 -j ACCEPT Reload configuration file iptables -F Show statistics for configuration lines iptables -vnL --lines","title":"Iptables"},{"location":"Linux/irssi/","text":"irssi allows you to connect directly from the command-line irssi --connect = irc.chat.twitch.tv --password = $TOKEN irssi -c irc.chat.twitch.tv -w $TOKEN","title":"Irssi"},{"location":"Linux/iscsiadm/","text":"Discover iSCSI targets iscsiadm discovery","title":"Iscsiadm"},{"location":"Linux/iw/","text":"Show or manipulate wireless devices and their configuration; followed by dev , phy , or reg depending on the device. Check the name of network device ( src ) iw dev Check the connecdtion status of the Wi-Fi device wlp2s0 iw wlp2s0 link","title":"Iw"},{"location":"Linux/journalctl/","text":"Show current disk usage of all journal files journalctl --disk-usage Continuously update the display as new log entries are created journalctl -f Display output in reverse (newest entries first) journalctl -r","title":"Journalctl"},{"location":"Linux/kquitapp/","text":"Allows you to quit a dbus enabled application. Two options: Specify service to be stopped kquitapp --service Specify path to dbus interface kquitapp --path","title":"Kquitapp"},{"location":"Linux/kstart/","text":"Restarting KDE Plasma 4 ref killall plasma-desktop kstart plasma-desktop Restarting KDE Plasma 5 ref killall plasmashell kstart plasmashell kquitapp5 plasmashell kstart plasmashell","title":"Kstart"},{"location":"Linux/ldd/","text":"Display dependencies of $PROGRAM ldd $PROGRAM","title":"Ldd"},{"location":"Linux/lnewaliases/","text":"Refresh the mail system after a change to /etc/aliases. Must be run after making a change to email aliases on a server running postfix","title":"Lnewaliases"},{"location":"Linux/localectl/","text":"Change locale to French localectl set-locale LANG = fr_FR.utf8","title":"Localectl"},{"location":"Linux/lowriter/","text":"lowriter is a command-line utility installed with LibreOffice Writer. ( src ) Convert a single file to PDF lowriter --convert-to pdf filename.doc Convert a batch of files using globbing lowriter --convert-to pdf *.docx","title":"Lowriter"},{"location":"Linux/lsb_release/","text":"lsb_release Display version of Ubuntu lsb_release -sc /etc/lsb-release DISTRIB_ID = Ubuntu DISTRIB_RELEASE = 14.04 DISTRIB_CODENAME = trusty DISTRIB_DESCRIPTION = \"Ubuntu 14.04.6 LTS\"","title":"lsb_release"},{"location":"Linux/lsb_release/#lsb_release","text":"Display version of Ubuntu lsb_release -sc","title":"lsb_release"},{"location":"Linux/lsb_release/#etclsb-release","text":"DISTRIB_ID = Ubuntu DISTRIB_RELEASE = 14.04 DISTRIB_CODENAME = trusty DISTRIB_DESCRIPTION = \"Ubuntu 14.04.6 LTS\"","title":"/etc/lsb-release"},{"location":"Linux/lsmod/","text":"Display currently loaded modules. Output in three columns: Module name Module size (bytes) Processes, filesystems, or other modules using the module","title":"Lsmod"},{"location":"Linux/lsof/","text":"Show open network connections sudo lsof -Pni","title":"Lsof"},{"location":"Linux/lvcreate/","text":"Create a 20 gigabyte logical volume named \"Marketing\" from volume group {vg1} lvcreate -L 20G vg1 -n Marketing Create logical volume named lv1 of size 500G from volume group vg1 lvcreate -L 500G vg1 -n lv1","title":"Lvcreate"},{"location":"Linux/lvresize/","text":"Resize existent logical volume {Marketing} in volume group {vg1} to have an additional 10 gigabytes of space lvresize -L +10G /dev/vg1/Marketing","title":"Lvresize"},{"location":"Linux/mail/","text":"mail Mail User Agent (MUA) which accepts interactive input using the & prompt Check email of $USER mail -u $USER Send email to $USER mail $USER Send email from the command-line Send email interactively mail $ADDRESS Send message via pipe echo 'message' | mail -s 'subject' recipient@domain.com Send an email attachment from the command-line mail -a /path/to/attachment Send message via pipe echo 'message' | mail -s 'subject' -a /path/to/attachment","title":"Mail"},{"location":"Linux/mail/#mail","text":"Mail User Agent (MUA) which accepts interactive input using the & prompt Check email of $USER mail -u $USER Send email to $USER mail $USER Send email from the command-line Send email interactively mail $ADDRESS Send message via pipe echo 'message' | mail -s 'subject' recipient@domain.com Send an email attachment from the command-line mail -a /path/to/attachment Send message via pipe echo 'message' | mail -s 'subject' -a /path/to/attachment","title":"mail"},{"location":"Linux/mailmerge/","text":"mailmerge Mailmerge is a command-line Python program that provides a powerful way to send many customized emails by using Jinja2 templating. It is available from Fedora's repositories through dnf and is also available from PyPI. [ ref ] Configuration file Description $HOME/mailmerge_server.conf SMTP host configuration details $HOME/mailmerge_database.csv custom data for each email, including email addresses of recipients. The email address to be used as test recipient (i.e. the user) is the first entry by convention. $HOME/mailmerge_template.txt email's text with placeholder fields that will be replaced using data from mailmerge_database.csv Review message to be sent to first recipient This will display the message addressed to the first address specified in the recipient database in the terminal mailmerge Review every message to be sent mailmerge --no-limit Send test message to first recipient mailmerge --no-dry-run Send all emails mailmerge --no-dry-run --no-limit","title":"Mailmerge"},{"location":"Linux/mailmerge/#mailmerge","text":"Mailmerge is a command-line Python program that provides a powerful way to send many customized emails by using Jinja2 templating. It is available from Fedora's repositories through dnf and is also available from PyPI. [ ref ] Configuration file Description $HOME/mailmerge_server.conf SMTP host configuration details $HOME/mailmerge_database.csv custom data for each email, including email addresses of recipients. The email address to be used as test recipient (i.e. the user) is the first entry by convention. $HOME/mailmerge_template.txt email's text with placeholder fields that will be replaced using data from mailmerge_database.csv Review message to be sent to first recipient This will display the message addressed to the first address specified in the recipient database in the terminal mailmerge Review every message to be sent mailmerge --no-limit Send test message to first recipient mailmerge --no-dry-run Send all emails mailmerge --no-dry-run --no-limit","title":"mailmerge"},{"location":"Linux/mailq/","text":"Display the current mail queue on a Postfix server","title":"Mailq"},{"location":"Linux/mailstats/","text":"Gather and display statistics about mail processed on a server running sendmail","title":"Mailstats"},{"location":"Linux/make/","text":"b d e f h i j k l m n o p q r s t v w B C I S W A common formula when installing software from source is the following sequence of commands ./configure make make install Given there are 3 example files (main.cpp, message.cpp, and message.h) in a directory, it produces an executable file named a.out g++ main.cpp message.cpp Install an alternate version of a program like Python cd /opt wget https://www.python.org/ftp/python/3.8.0/Python-3.8.0.tgz tar xzf Python-3.8.0.tgz cd Python-3.8.0 ./configure --enable-optimizations make altinstall Makefiles are sensitive to whitespace, so indentation is significant. The format follows the pattern: {target} : { dependencies } { action } where {target} is the filename produced by the operation {action} , each of which are shell commands. In this example , hw.o is produced from hw.cpp first because it is a dependency of the executable hw . Notably, Makefiles appear to require hard tabs. CC = g++ all : hw hw : hw . o ${ CC } -o hw hw.o hw.o : hw . cpp ${ CC } -c hw.cpp clean : rm *.o To remove the .o files: make clean Targets - tinyconfig smallest possible kernel configuration - allnoconfig answer no to every question when creating a config file A configure script is responsible for preparing the software build, ensuring dependencies are available, such as a C compiler for C programs. make is invoked after the configure script has done its job. The configure script converts a Makefile.in template into a Makefile . They are not built by hand but packaged by yet another program in the autotools suite, such as autoconf , automake , and others. A configure.ac file written in m4sh (a combination of m4 macros and shell script) is prepared. The first m4 macro called i AC_INIT , which initializes autoconf: AC_INIT([helloworld], [0.1], [george@thoughtbot.com]) The AM_INIT_AUTOMAKE macro is also called because we're using automake : AM_INIT_AUTOMAKE","title":"Make"},{"location":"Linux/mkdir/","text":"Quickly create multiple directories using brace expansion mkdir -p ~/my-app/ { bin,lib,log } Create new directory {dirname} along with all of the parents in its pathname, if they do not exist mkdir -p dirname mkdir --parents dirname","title":"Mkdir"},{"location":"Linux/mkfs/","text":"Create an ext4 filesystem on {partition} mkfs -t ext4 partition Specify {filesystemtype} to be created mkfs -T filesystemtype","title":"Mkfs"},{"location":"Linux/mongod/","text":"Run MongoDB service in the background on port 80 mongod --dbpath $HOME /db --port 80 --fork --logpath /var/tmp/mongodb","title":"Mongod"},{"location":"Linux/mpstat/","text":"mpstat -P all","title":"Mpstat"},{"location":"Linux/nc/","text":"The netcat utility allows testing of a host's ports, similar to ping , but more versatile because ping only uses the portless ICMP protocol. GNU and OpenBSD versions available Option Effect -l listening mode Connect to host on port 80 nc example.com 80 Scan ports # Scan a single port nc -v -w 2 z 192.168.56.1 22 # Scan multiple ports nc -v -w 2 z 192.168.56.1 22 80 # Scan a range of ports nc -v -w 2 z 192.168.56.1 22-25 Transfer files between servers This example uses the pv utility to monitor progress. # Run `nc` in listening mode (`-l` option) on port 3000 tar -zcf - debian-10.0.0-amd64-xfce-CD-1.iso | pv | nc -l -p 3000 -q 5 # On the receiving client, to obtain the file: nc 192.168.1.4 3000 | pv | tar -zxf - Create a command-line chat server # Create chat server listening on port 5000 nc -l -vv -p 5000 # Launch a chat session on the other system nc 192.168.56.1 5000 Find a service running on port Obtain port banners ( -n disables DNS lookup) nc -v -n 192.168.56.110 80 Create stream sockets Create and listen on a UNIX-domain stream socket nc -lU /var/tmp/mysocket & ss -lpn | grep \"/var/tmp/\" Create a backdoor Netcat needs to listen on a chosen port (here 3001): -d disables reading from stdin; -e specifies the command to run on the target system nc -L -p 3001 -d -e cmd.exe Connect to {port} at {host} nc host port Netcat command that retrieves a webpage nc host port get","title":"Nc"},{"location":"Linux/netstat/","text":"a c g i l n o p r s t u v M Show network traffic netstat -an Refresh every five seconds netstat -c5 Show the current default route without performing DNS lookups on the IP addresses involved netstat -rn Count number of TCP connections netstat -a | grep tcp - | wc -l Active sessions netstat -tp All sessions netstat -atp Routing table with name resolution netstat -rn Get the list of IPs and ports that are connected via https on your webserver every second watch -n 1 'netstat -an | grep \":443\"' Get the total number of connections on port 80 every second watch -n 1 'netstat -an | grep \":80\" | wc -l' netstat -tulpn","title":"Netstat"},{"location":"Linux/nginx/","text":"nginx nginx The default config for nginx may be at various paths ( /etc/nginx/nginx.conf ), depending on installation method, or a config can be explicitly specified with --conf-path / -c . Nginx can be interrogated for its default config with -t Nginx config files resemble C# code files. Simple directives like listen *:80; are made of a name, multiple optional parameters, and a closing semicolon. Parameters themselves can pass a value after an equal sign, i.e. backlog=511 . Context directives or simply contexts like events , http , and server wrap a group of other directives in a pair of braces and can be nested. Most simple directives can only be declared in specific contexts. There is also an implied main context which wraps all the contents of the file, and putting a simple directive into the main context means making it a top-level statemtn. Examples A very simple representative config that creates an HTTP server listening on port 80 of every network interface, with no HTTP Host specified, from the specified root path: Default events { } http { server { } } Expanded with explicit values user nobody nogroup ; worker_processes 1 ; events { worker_connections 512 ; } http { server { listen *:80 ; server_name \"\" ; root /usr/share/nginx/html ; } } http { server { listen 8080; root /www; location /images { root /; } } } events { } nginx -s stop nginx -s start nginx restart Reverse proxy Each Nginx virtual server should be described by a file in the /etc/nginx/sites-available directory. These are linked to by symlinks placed in /etc/nginx/sites-enabled . Configuring a reverse proxy involves associating routes to proxied servers in these virtual server configs. NGINX.org server { listen 80; location / { proxy_pass \"http://127.0.0.1:8000\"; } } The configuration to serve static files placed in the local directory /path/to/staticfiles from the URL /static is: location /static/ { root /path/to/staticfiles/ } Load balancer A load balancer is similar to a reverse proxy, with the following differences. Load balancers perform reverse proxy across many backends, rather than a single one Load balancers operate at either Layer 7 or Layer 4, whereas a reverse proxy operates only at Level 7 Load balancers are expected to handle much higher scale. Load balancers themselves tend to be load balanced by DNS servers, which can serve multiple A records to clients which are supposed to choose one of the IP addresses at random. Some DNS providers like AWS Route 53 randomize the order of these records per query. http { upstream backend { server 192 .0.2.10 ; server 192 .0.2.11 ; } server { listen 80 ; location / { proxy_pass http://backend ; } } } \ud83d\udcd8 Glossary default server block First server block defined in the config events Context that governs connection processing configuration. http Context that allows configuration of HTTP servers and typically serves as a container for the server context. Directives that are intended to apply to multiple server contexts are typically placed within the http context. listen Simple directive in the server context that configures the network interfaces and ports to listen on for requests. server { listen *:80 ; listen *:81 ; } location Context directive used when there is not a 1:1 mapping between path of the HTTP request and the filesystem. Prefix location blocks , the most basic implementation, allow various root directories to be specified depending on the request path. Various modifiers can be used to modify how the request path is matched. When resolving paths, the most specific match is used. Location blocks in order of precedence: Exact match modifier ( = ) Order matters with regex blocks because the first match will be used. Non-regex prefix ( ^~ ) overrides any regex match Case-sensitive regex ( ~ ) Case-insensitive regex ( ~* ) Prefix (no modifier) location /images/ { root /var/www/images ; } location = /images/business_cat.gif { # ... } location ~ \\.(gif|jpg)$ { # ... } location ~ * \\.(GIF|JPG) $ { # ... } location ^~ /foobar/images { root /var/www/foobar ; } server (context directive) http-context context directive. server (simple directive) upstream -context directive that specifies a backend node. Additional parameters can specify load-balancing behavior. - **weight** controls weighting of backend nodes (default value is 1) - **max_fails** controls the number of times that the server can be marked as unhealthy before it is removed from the pool - **fail_timeout** controls the time a server is removed from the pool when it is marked unhealthy, and for how long `max_fails` is good for server_name Server context simple directive that enables virtual hosting. Values of this directive are matched to the HTTP GET request header's Host . If no matches are found, nginx uses the default server block . upstream http-context simple directive that can, using the server simple directive, specify a pool of backend server. upstream backend { server 192.0.2.10 : 443 weight=3 ; server app1.example.com ; server unix:/u/apps/my_app/current/tmp/unicorn.sock ; } Each server directive can additionally user Main context simple directive that sets the Unix user and group that nginx worker processes will run as; by default nobody and the group is nogroup . worker process The non-root nginx process that serves incoming HTTP requests. Each worker process is single-threaded and runs a non-blocking event loop to process requests efficiently. worker_connections Main context simple directive that sets the maximum number of simultaneous connections that can be opened by each worker process . worker_processes Main context simple directive that sets the number of worker processes to serve HTTP requests (1 by default).","title":"nginx"},{"location":"Linux/nginx/#nginx","text":"nginx The default config for nginx may be at various paths ( /etc/nginx/nginx.conf ), depending on installation method, or a config can be explicitly specified with --conf-path / -c . Nginx can be interrogated for its default config with -t Nginx config files resemble C# code files. Simple directives like listen *:80; are made of a name, multiple optional parameters, and a closing semicolon. Parameters themselves can pass a value after an equal sign, i.e. backlog=511 . Context directives or simply contexts like events , http , and server wrap a group of other directives in a pair of braces and can be nested. Most simple directives can only be declared in specific contexts. There is also an implied main context which wraps all the contents of the file, and putting a simple directive into the main context means making it a top-level statemtn.","title":"nginx"},{"location":"Linux/nginx/#examples","text":"A very simple representative config that creates an HTTP server listening on port 80 of every network interface, with no HTTP Host specified, from the specified root path: Default events { } http { server { } } Expanded with explicit values user nobody nogroup ; worker_processes 1 ; events { worker_connections 512 ; } http { server { listen *:80 ; server_name \"\" ; root /usr/share/nginx/html ; } } http { server { listen 8080; root /www; location /images { root /; } } } events { } nginx -s stop nginx -s start nginx restart","title":"Examples"},{"location":"Linux/nginx/#reverse-proxy","text":"Each Nginx virtual server should be described by a file in the /etc/nginx/sites-available directory. These are linked to by symlinks placed in /etc/nginx/sites-enabled . Configuring a reverse proxy involves associating routes to proxied servers in these virtual server configs. NGINX.org server { listen 80; location / { proxy_pass \"http://127.0.0.1:8000\"; } } The configuration to serve static files placed in the local directory /path/to/staticfiles from the URL /static is: location /static/ { root /path/to/staticfiles/ }","title":"Reverse proxy"},{"location":"Linux/nginx/#load-balancer","text":"A load balancer is similar to a reverse proxy, with the following differences. Load balancers perform reverse proxy across many backends, rather than a single one Load balancers operate at either Layer 7 or Layer 4, whereas a reverse proxy operates only at Level 7 Load balancers are expected to handle much higher scale. Load balancers themselves tend to be load balanced by DNS servers, which can serve multiple A records to clients which are supposed to choose one of the IP addresses at random. Some DNS providers like AWS Route 53 randomize the order of these records per query. http { upstream backend { server 192 .0.2.10 ; server 192 .0.2.11 ; } server { listen 80 ; location / { proxy_pass http://backend ; } } }","title":"Load balancer"},{"location":"Linux/nginx/#glossary","text":"default server block First server block defined in the config events Context that governs connection processing configuration. http Context that allows configuration of HTTP servers and typically serves as a container for the server context. Directives that are intended to apply to multiple server contexts are typically placed within the http context. listen Simple directive in the server context that configures the network interfaces and ports to listen on for requests. server { listen *:80 ; listen *:81 ; } location Context directive used when there is not a 1:1 mapping between path of the HTTP request and the filesystem. Prefix location blocks , the most basic implementation, allow various root directories to be specified depending on the request path. Various modifiers can be used to modify how the request path is matched. When resolving paths, the most specific match is used. Location blocks in order of precedence: Exact match modifier ( = ) Order matters with regex blocks because the first match will be used. Non-regex prefix ( ^~ ) overrides any regex match Case-sensitive regex ( ~ ) Case-insensitive regex ( ~* ) Prefix (no modifier) location /images/ { root /var/www/images ; } location = /images/business_cat.gif { # ... } location ~ \\.(gif|jpg)$ { # ... } location ~ * \\.(GIF|JPG) $ { # ... } location ^~ /foobar/images { root /var/www/foobar ; } server (context directive) http-context context directive. server (simple directive) upstream -context directive that specifies a backend node. Additional parameters can specify load-balancing behavior. - **weight** controls weighting of backend nodes (default value is 1) - **max_fails** controls the number of times that the server can be marked as unhealthy before it is removed from the pool - **fail_timeout** controls the time a server is removed from the pool when it is marked unhealthy, and for how long `max_fails` is good for server_name Server context simple directive that enables virtual hosting. Values of this directive are matched to the HTTP GET request header's Host . If no matches are found, nginx uses the default server block . upstream http-context simple directive that can, using the server simple directive, specify a pool of backend server. upstream backend { server 192.0.2.10 : 443 weight=3 ; server app1.example.com ; server unix:/u/apps/my_app/current/tmp/unicorn.sock ; } Each server directive can additionally user Main context simple directive that sets the Unix user and group that nginx worker processes will run as; by default nobody and the group is nogroup . worker process The non-root nginx process that serves incoming HTTP requests. Each worker process is single-threaded and runs a non-blocking event loop to process requests efficiently. worker_connections Main context simple directive that sets the maximum number of simultaneous connections that can be opened by each worker process . worker_processes Main context simple directive that sets the number of worker processes to serve HTTP requests (1 by default).","title":"\ud83d\udcd8 Glossary"},{"location":"Linux/nice/","text":"Priorities range from 0-19 in csh (10 is default); higher values run at lower priority nice View priorities of jobs ps -l Run cmd at a higher priority nice -5 cmd & Run $CMD at a nice value of (positive) 10 nice -10 $CMD nice -n 10 nice $CMD","title":"Nice"},{"location":"Linux/nl/","text":"b f h Number all lines, including blank lines nl -b a file nl --body-numbering = a file","title":"Nl"},{"location":"Linux/nmap/","text":"Scan hosts and ports on a network\\ Scan hosts from a text file nmap -iL hosts.txt Identify a host's operating system nmap -A localhost.example.com Determine whether a host has a firewall enabled nmap -sA localhost.example.com Scan a specified range of ports nmap -p 10 -300 localhost.example.com Perform a SYN TCP scan, stealthier than the TCP connect scan nmap -sT localhost.example.com Aggressive scan nmap -A 192 .168.1.0/24 Ping scan home network (not bothering with ports) nmap -sn 192 .168.1.0/24 Fast port scan using SYN packets nmap -sS -F 192 .168.1.0/24 Port scan using SYN (\"synchronize\") packet, first element of TCP handshake nmap -sS 192 .168.1.0/24 Port scan using normal TCP nmap -sT 192 .168.1.0/24 Port scan using UDP nmap -sU 192 .168.1.0/24 Xmas scan nmap -sX Scan a range of IPs [ref][Sec+ Lab] nmap 192 .168.27.0/24 > hosts.txt Identify operating system and scan ports using TCP SYN packets [ref][Sec+ Lab] nmap -O -sS 192 .168.27.0/24 > hosts.txt","title":"Nmap"},{"location":"Linux/nmcli/","text":"Control NetworkManager and report network status Display devices and statuses nmcli device status Display information on interfaces as well as status Including other network connections not managed by network manager (\"unmanaged\") or not connected (\"unavailable\") nmcli dev status Display what connections are enabled nmcli general status Display UUIDs associated with network connections nmcli connection show --active Display much more information on network devices nmcli device show Configure settings for network interface {ens01} via interactive shell nmcli connection edit ens01 List all connections NetworkManager has nmcli connection show Show settings for network interface {ens01} nmcli device show ens01 Show status for all devices nmcli device status Display devices and status nmcli device status Display currently configured hostname nmcli general hostname Set hostname to {hostname} nmcli general hostname hostname Show overall status of NetworkManager nmcli general status","title":"Nmcli"},{"location":"Linux/nslookup/","text":"Perform a DNS lookup in an interactive shell with cleaner output than dig . Enter a domain name and you get output in two sections. Retrieve IP address of {host} nslookup host Get IP address of a website nslookup url Get only nameservers nslookup -type=ns url Get only MX records nslookup -type=mx url Get Start of Authority (SOA) record nslookup -type=soa url Display all available records nslookup -type=any url Perform reverse DNS lookup on {ipaddress} nslookup ipaddress Specify port {portno} in the lookup nslookup -port=portno url","title":"Nslookup"},{"location":"Linux/ntpdate/","text":"Synchronize system clock to that of an online Network Time Protocol server ntpdate -upool.ntp.org","title":"Ntpdate"},{"location":"Linux/pacman/","text":"Option POSIX option Effect Q --query list all installed packages -R remove {pkg}, but leave dependencies -Qe list programs explicitly installed by user or program command -Qeq list only program names explicitly installed -Qm list programs only installed from AUR -Qn list programs only installed from main repositories -Qdt list dependencies no longer needed (orphans) -Ql --query --list list all files owned by a package -S --sync install -Sy synchronize package database -Su update programs -Syu sync package database ( Sy ) and upgrade all programs ( u ) (equivalent to apt-get update && apt-get upgrade ) -Syy force double-check of repositories -Syyuw downloads programs but doesn't install them, for the option of manual installation -Rs remove {pkg} as well as its dependencies -Rns remove {pkg}, dependencies, as well as config files List installed packages Short pacman -Q POSIX pacman --query List all orphaned dependencies (no longer needed) pacman -Qdt pacman --query --deps --unrequired List only explicitly installed packages and versions pacman -Qe pacman --query --explicit List explicitly installed packages, limiting output to program names pacman -Qeq pacman --query --explicit --quiet List all packages installed from the AUR pacman -Qm pacman --query --foreign List all packages installed from main repos pacman -Qn pacman --query --native Find which package owns {file} pacman -Qo file pacman --query --owns file List all install packages, filtering output to packages that are out-of-date on the local system pacman -Qu pacman --query --upgrades Remove {package} pacman -R package pacman --remove package Remove {package}, dependencies, and config files pacman -Rns package pacman --remove --recursive --nosave Remove {package} as well as its dependencies pacman -Rs pacman --remove --recursive Install {pkg} from the AUR pacman -S package pacman --sync package Remove all packages from the cache as well as unused sync databases pacman -Scc pacman --sync --clean --clean Display information about {package} pacman -Si package pacman --sync --info package Search for {pkg} in AUR repos pacman -Ss package pacman --sync --search package Search for packages matching {searchexpression} pacman -Ss pattern pacman --sync --search pattern Update package database pacman -Sy pacman --sync --refresh Update all packages from AUR and official repos pacman -Syu pacman --sync --refresh --sysupgrade Force refresh of all package databases, even if they appear to be up-to-date pacman -Syy pacman --sync --refresh --refresh Download program updates but don't install them pacman -Syyuw pacman --sync --refresh --refresh --sysupgrade --downloadonly Get number of total installed packages pacman -Q | wc -l","title":"Pacman"},{"location":"Linux/partx/","text":"partx is a utility that provides information on drive partitions to the Linux kernel. Display partition table of a drive partx --show /dev/sda Show details of only one partition of a drive partx --show /dev/sda1 Specify a range of partitions on a drive partx -o START, END --nr 10 /dev/sda Add all partitions on a disk to the system partx -a /dev/sda Display length in sectors and human-readable size of a partition partx -o SECTORS,SIZE /dev/sda1 /dev/sda Remove the last partition partx -d --nr -1:-1 /dev/sda Disable headers partx -o START -g --nr 5 /dev/sda","title":"Partx"},{"location":"Linux/passwd/","text":"d e i l m u w M","title":"Passwd"},{"location":"Linux/paste/","text":"Merge lines of files Make a .csv file from two lists paste -d ',' file1 file2 Transpose rows paste -s file1 file2","title":"Paste"},{"location":"Linux/pip/","text":"Display installed packages pip list Display information about {package} pip show package","title":"Pip"},{"location":"Linux/polybar/","text":"Polybar configuration is in dosini format, with comments preceded by ; and values set with = . [bar/bar] width = 100% height = 40 background = #c00 foreground = #000 font-0 = Noto Sans Mono:style=Regular:size=18;0 modules-center = i3 [module/i3] type = internal/i3 format = <label-state> ; vim:ft=dosini Although whitespace appears to be ignored in most places, any whitespace after the closing bracket on the line reading \" [module/i3] \",for example, will cause that module not to be loaded. Each module has to have one of a few accepted type values which will determine what other values it can accept and the module's behavior in the finished bar. Option Effect -c $CONFIGFILE specify The established demos appear to rely on old versions of Font Awesome, the following font names should be used after installing. The following changes fixed the error where \"-1\" would appear for every workspace: label-focused = %name% label-unfocused = %name% label-visible = %name% label-urgent = %name% Syntax | Description ; Comment | comments marked by semicolon [bar/BARNAME] | begin definition of BARNAME [module/mpd] | disabling this module by commenting it out removed a lot of error polybar -vvv : \"+i3\" will appear in the output if this module is installed pin-workspaces = true | will show monitor-specific workspaces format | appears to interpolate or concatenate various values enclosed in <> angle brackets ANY extraneous whitespace is likely to cause issues. Here, label needs to match name of the i3 workspace aws-icon-n = label;icon` Appears to display the raw name of the workspace as configured in the i3 config file, and is common to many of the polybar configs format = <label-state> <label-mode>","title":"Polybar"},{"location":"Linux/postfix/","text":"postfix Designed to replace Sendmail. multiple processes with no particular parent/child relationship Receives mail by two methods: Local mail (sendmail) Internet mail (SMTP) Before mail is queued for delivery, it goes through a cleanup daemon, which can be configured to do header and body inspection using regex Qmgr is the heart of postfix mail delivery; it maintains an active queue, which attempts delivery. It delivers mail using three methods: Local inboxes Internet (SMTP) Piped to programs /etc/postfix/main.cf Set e-mail domain name [Eckert][Eckert] mydomain = sample.com Set local access to domain name [Eckert][Eckert] myorigin = $mydomain Configure postfix to listen for email on all interfaces [Eckert][Eckert] inet_interfaces = all Configure destination domain for email [Eckert][Eckert] mydestination = $myhostname, localhost.$mydomain, localhost, $mydomain Trust email from computers on the local network [Eckert][Eckert] mynetworks_style = class","title":"postfix"},{"location":"Linux/postfix/#postfix","text":"Designed to replace Sendmail. multiple processes with no particular parent/child relationship Receives mail by two methods: Local mail (sendmail) Internet mail (SMTP) Before mail is queued for delivery, it goes through a cleanup daemon, which can be configured to do header and body inspection using regex Qmgr is the heart of postfix mail delivery; it maintains an active queue, which attempts delivery. It delivers mail using three methods: Local inboxes Internet (SMTP) Piped to programs","title":"postfix"},{"location":"Linux/postfix/#etcpostfixmaincf","text":"Set e-mail domain name [Eckert][Eckert] mydomain = sample.com Set local access to domain name [Eckert][Eckert] myorigin = $mydomain Configure postfix to listen for email on all interfaces [Eckert][Eckert] inet_interfaces = all Configure destination domain for email [Eckert][Eckert] mydestination = $myhostname, localhost.$mydomain, localhost, $mydomain Trust email from computers on the local network [Eckert][Eckert] mynetworks_style = class","title":"/etc/postfix/main.cf"},{"location":"Linux/postqueue/","text":"Cause mail queue to be processed on a postfix server postqueue -f","title":"Postqueue"},{"location":"Linux/postsuper/","text":"Delete all of the messages from the queue on a postfix server postsuper -d","title":"Postsuper"},{"location":"Linux/ps/","text":"a e f l u w x C U Display processes in a tree-like display illustrating parent-child relationships ps -f ps --forest Show system processes ps ax ps -e Display full listing of processes ps u ps -f Display user processes ps xG ps -a Display SELinux contexts for processes ps auxZ","title":"Ps"},{"location":"Linux/rename/","text":"rename uses regular expressions [Network World][https://www.networkworld.com/article/3433865/how-to-rename-a-group-of-files-on-linux.html#tk.rss_linux] Option POSIX option Effect -n --nono dry-run: describe the changes the command would make, without actually doing them Rename multiple files # Renaming file.old to file.new rename 's/old/new/' this.old # Use globbing to rename all matching files rename 's/old/new/' *.old rename 's/report/review/' * # Change all uppercase letters to lowercase rename 'y/A-Z/a-z/' *","title":"Rename"},{"location":"Linux/restorecon/","text":"Restore security context default in the policy restorecon -Rv website","title":"Restorecon"},{"location":"Linux/rfkill/","text":"block list unblock Tool for enabling and disabling wireless devices Unblock Bluetooth, if it is blocked [ref][https://computingforgeeks.com/connect-to-bluetooth-device-from-linux-terminal/] rfkill unblock bluetooth","title":"Rfkill"},{"location":"Linux/rpm/","text":"a c d e f h i l p q s v I K R U V force nodeps nofiles nomd5 nopgp provides test Query repos for information on {package} rpm -qi package rpm --query --info package Upgrade or install {package}, with progress bars rpm -Uvh package rpm --upgrade --verbose --hash package Display version of Fedora rpm -E %fedora","title":"Rpm"},{"location":"Linux/rsync/","text":"a b e g l o p r t v z Copy $FILE locally 2daygeek.com rsync -zvr $FILE $PATH Copy $FILE to $PATH on remote $HOST rsync $FILE $HOST : $PATH Copy $FILE from $HOST to local $PATH rsync $HOST : $FILE $PATH Copy $DIR recursively 2daygeek.com rsync -zvr $DIR $PATH rsync -avz $DIR $PATH Copy to remote systems over SSH 2daygeek.com rsync -zvre ssh $DIR $HOST : $REMOTEPATH rsync -avze ssh $DIR $HOST : $REMOTEPATH Synchronize only specific file type 2daygeek.com rsync -zvre ssh --include '*.php' --exclude '*' $PATH","title":"Rsync"},{"location":"Linux/runlevel/","text":"Show runlevel for system runlevel","title":"Runlevel"},{"location":"Linux/samba/","text":"Samba /etc/samba/smb.conf Configure Samba ref [samba-share] comment = Samba on Ubuntu path = /samba read only = no browsable = yes Configure anonymous unsecured file sharing on a shared directory [ 41 ] [global] workgroup = WORKGROUP netbios name = rhel security = user ... [Anonymous] comment = Anonymous File Server Share path = /srv/samba/anonymous browsable = yes writable = yes guest ok = yes read only = no force user = nobody","title":"Samba"},{"location":"Linux/samba/#samba","text":"","title":"Samba"},{"location":"Linux/samba/#etcsambasmbconf","text":"Configure Samba ref [samba-share] comment = Samba on Ubuntu path = /samba read only = no browsable = yes Configure anonymous unsecured file sharing on a shared directory [ 41 ] [global] workgroup = WORKGROUP netbios name = rhel security = user ... [Anonymous] comment = Anonymous File Server Share path = /srv/samba/anonymous browsable = yes writable = yes guest ok = yes read only = no force user = nobody","title":"/etc/samba/smb.conf"},{"location":"Linux/sed/","text":"e f i n Run sed commands in $SCRIPT on $FILE sed -f $SCRIPT $FILE Suppress automatic printing of pattern space sed -n sed --quiet sed --silent sed (\"Stream-oriented editor\") is typically used for applying repetitive edits across all lines of multiple files. In particular it is, alongside awk one of the two primary commands which accept regular expressions in Unix systems. Script syntax Effect # comments begin with octothorpe #n If first line of script begins with these two characters, it is equivalent to using the -n flag Invocation syntax has two forms: # Inline sed options 'instruction' $FILE # \"Command file\" sed options -f $SCRIPT $FILE sed instructions are made of addresses and procedures . Sources do not use consistent terminology to describe the two components of most sed commands: :--- | :--- sed 'pattern {procedure}' file | SA: 14] sed 'address {action}' file | YUG: 449 sed 'address {instruction}' file | PGL: 56 Zero, one, or two addresses can precede a procedure. - In the absence of an address, the procedure is executed over every line of input - With one address, the procedure will be executed over every line of input that matches - With two addresses, the procedure will be executed over groups of lines whereby: - The first address selects the first line in the first group - The second address selects the next subsequent line that it matches, which becomes the last line in the first group - If no match for the second address is found, it point to the end of the file - After the match, the selection process for the next group begins by searching for a match to the first address Addressing can be done in one of two ways: 1. Line addressing , specifying line numbers separated by a comma (e.g. 3,7p ); $ represents the last line of input 2. Context addressing , using a regular expression enclosed by forward slashes (e.g. /From:/p ) Procedure Description !c negation operator can be used with any procedure a append text to the addressed lines d cause sed not to display the addressed lines (\"delete\"); can emulate grep -v , which selects lines which do not match the specified pattern i prepend text to the addressed lines n write out the currently selected line if appropriate, read the next input line, and start processing the new line with the next instruction x where {x} is a number, specifying occurrence (e.g. 2 would replace only the second occurrence of each pattern per line) g replace all occurrences p print original content (e.g. sed -n 's/test/another test/p' myfile ) w outputfile write results to {outputfile} (e.g. sed 's/test/another test/w output' myfile ) s/pattern/replacement/flags replace regex {pattern} with {replacement} (\"substitute\") g replace all instances of the search pattern with the replacement, rather than the first instance (global) & known as the repeated pattern , represents the represents the entire source string; the only special character used in the replacement string - all other characters are treated literally Edit the file in-place, but save a backup copy of the original with {suffix} appended to - the filename -i = suffix Display first 3 lines YUG: 450 sed '3q' emp.lst Display first 5 lines, similar to head -5 emp.lst PGL : 569 sed '5q' new Pipe output of ps to sed , displaying top 10 memory-intensive processes ps axch -o cmd,%mem --sort = -%mem | sed 11q Pipe output of ps to sed , displaying top 10 CPU-intensive processes ps axch -o cmd:15,%cpu --sort = -%cpu | sed 11q Display first two lines of file Without -n , each line will be printed twice sed -n '1,2p' emp.lst Prepending ! to the procedure reverses the sense of the command (YUG: 450) sed -n '3,$!p' emp.lst Display a range of lines sed -n '9,11p' emp.lst Use the -e flag to precede multiple instructions sed -n -e '1,2p' -e '7,9p' -e '$p' emp.lst Delete lines Delete second line alone sed '2d' myfile Delete a range of lines: from the 2nd through the 3rd sed '2,3d' myfile Delete a range of lines, from the first occurrence of 'second' to the line with the first occurrence of 'fourth' sed '/second/,/fourth/d' myfile Print all of a file except for specific lines Suppress any line with 'test' in it sed '/test/d' myfile Suppress from the 3rd line to EOF sed '3,$d' myfile Replace text Replace the first instance of the | character with : and display the first two lines [YUG:455] sed ' s/ | /:/ emp.lst | head -2 Replace all instances of the | character with : , displaying the first two lines [YUG:455] sed 's/|/:/g' emp.lst | head -2 Substitute HTML tags: sed 's/<I>/<EM>/g' These commands will replace \"director\" with \"executive director\" sed 's/director/executive director/' emp.lst sed 's/director/executive &/' emp.lst sed '/director/s//executive &/' emp.lst [YUG: 456-457] Searching for text\\ Equivalent to grep MA * sed -n '/MA/p' * Stringing sed statements together with pipe Take lines beginning with \"fake\" and remove all instances of \"fake.\", piping them... remove all parentheses with content and count lines of output (results) sed -n '/^fake/s/fake\\.//p' * | sed -nr 's/\\(.*\\)//p' | wc -l Take lines of all files in CWD beginning with \"fake\" and remove all instances of string \"fake.\" Then remove all parentheses with any content within them and print only the top 10 lines sed -ne '/^fake/p' * | sed -n 's/fake\\.//p' | sed -nr 's/\\(.*\\)//p' | sed 11q Count the number of pipes replaced by piping output to cmp , which will use the -l option to output byte numbers of differing values, then counting the lines of output (YUG:456) sed 's/|/:/g' emp.lst | cmp -l - emp.lst | wc -l","title":"Sed"},{"location":"Linux/semanage/","text":"Amend policy to add a file context semanage fcontext -a -t httpd_sys_content_t website Add a port context semanage port -a -t http_port_t -p tcp 8080 Display all ports with attached types semanage port -l","title":"Semanage"},{"location":"Linux/sendmail/","text":"Mail daemon once the de facto standard for accepting and redirecting mail on Linux distributions, long ago fallen into disuse. It was infamous for its difficulty to set up, with roots in ARPANET itself. src","title":"Sendmail"},{"location":"Linux/seq/","text":"Sequence from 1 to 15 seq -f \"%03g\" 15 Sequence from 5 to 99, separated by a space instead of a newline seq -s \" \" 5 99 Sequence every third number from 5 to 20 seq 5 320 Sequence from 1 to 8 seq 8","title":"Seq"},{"location":"Linux/service/","text":"Restart network service service network restart Check status of {daemon} service daemon status Stop {daemon} service daemon stop service mongodb stop","title":"Service"},{"location":"Linux/sestatus/","text":"Display status of SELinux sestatus","title":"Sestatus"},{"location":"Linux/setenforce/","text":"Change SELinux mode setenforce enforcing setenforce 1 setenforce permissive setenforce 0 setenforce disabled","title":"Setenforce"},{"location":"Linux/setfacl/","text":"b k m s x M X Grant user {lisa} right to read {file} setfacl -m u:lisa:r file Remove named group {staff} from {file}'s ACL setfacl -x g:staff file Modify file access control list for {file} to revoke write access from all groups and all named users setfacl -m m::rx file Grant read access to o ther users setfacl -m o::rwx file4.txt Add user {zach} to list of users of file4.txt setfacl -m u:zach:rw file4.txt","title":"Setfacl"},{"location":"Linux/sfdisk/","text":"Script-based partition table editor, similar to fdisk and gdisk , which can be run interactively. It does not interface with GPT format, neither is it designed for large partitions. [ref][11] List partitions on all devices Display size of {partition} or {device} This command produces the size of {partition} (i.e. /dev/sda1 ) or even {device} ( /dev/sda ) in blocks sfdisk -s partition sfdisk -s device Apply consistency checks to {partition} or {device} sfdisk -V partition sfdisk --verify device Create a partition sfdisk device Save sectors changed This command will allow recovery using the following command sfdisk /dev/hdd -O hdd-partition-sectors.save Recovery Man page indicates this flag is no longer supported, and recommends use of dd instead. sfdisk /dev/hdd -I hdd-partition-sectors.save","title":"Sfdisk"},{"location":"Linux/shred/","text":"Write random data to an unmounted disk for {n} passes shred --iterations = n","title":"Shred"},{"location":"Linux/shuf/","text":"Print random selection of integers from {x} to {y} (inclusive) without replacement shuf -i x-y Print random selection of integers from {x} to {y} (inclusive), with replacement shuf -i x-y -r Shuffle items separated by newline in file cards.txt , displaying only one shuf -n 1 items.txt","title":"Shuf"},{"location":"Linux/shutdown/","text":"Shut down at 8 pm shutdown 20 :00","title":"Shutdown"},{"location":"Linux/sort/","text":"f h k n r t u M Sort by space-delimited columns. Processes consuming the most memory will be at the bottom ps aux | sort -nk 4 Processes consuming the most CPU will be at the bottom ps aux | sort -nk 3","title":"Sort"},{"location":"Linux/sosreport/","text":"SOS is an open-source data collection tool that can be used to collect system configuration details and diagnostic information from a Unix-like operating system. It is installed by default on Ubuntu Server. ( src ) Collect system configuration details (without arguments, the report will be generated and stored in $TMPDIR ) sosreport Specify alternative temporary directory sosreport --tmp-dir /opt Specify alternative compression ( xz by default) sosreport --compression-type gzip Generate report for only specific plugins sosreport -o apache --batch","title":"Sosreport"},{"location":"Linux/speaker-test/","text":"Test loudspeakers with a 2-speaker setup speaker-test -c 2","title":"Speaker test"},{"location":"Linux/ss/","text":"a l t u x Options are of two kinds: 1. Connection type (listening or established) 2. Protocol type Display port numbers instead of protocol names ss -n ss --numeric Do name lookups and display all information ss -an Display all active TCP sessions ss -atp Display active TCP sessions ss -tp Display routing table (cf. ip route ) ss --route Display programs with open ports ss --program Show all running servers \"Tuna please\" ss -tunapl Do name lookups and display all information ss -an ss --all --numeric Display all sessions, filtering to just TCP that are actively listening ss -atp ss --all --tcp --processes Display active TCP connections ss -tp ss --tcp --processes","title":"Ss"},{"location":"Linux/ssmtp/","text":"Send {msg} to {recipient} from {user} at {host} using password {pw} ssmtp -au recipient -ap pw user@host < msg","title":"Ssmtp"},{"location":"Linux/stty/","text":"Return number of rows and columns of the terminal stty size","title":"Stty"},{"location":"Linux/su/","text":"Obtain the normal login environment su - Execute a single command with a non-interactive session su -c cmd","title":"Su"},{"location":"Linux/sudo/","text":"sudo /etc/sudoers Allow sudo access to user linuxize only for command /bin/mkdir linuxize ALL=/bin/mkdir Allow user linuxize to run sudo commands without authenticating himself linuxize ALL=(ALL) NOPASSWD: ALL Change timeout to 10 minutes Defaults timestamp_timeout=10 Change timeout to 10 minutes only for user linuxize Defaults:linuxize timestamp_timeout=10","title":"sudo"},{"location":"Linux/sudo/#sudo","text":"","title":"sudo"},{"location":"Linux/sudo/#etcsudoers","text":"Allow sudo access to user linuxize only for command /bin/mkdir linuxize ALL=/bin/mkdir Allow user linuxize to run sudo commands without authenticating himself linuxize ALL=(ALL) NOPASSWD: ALL Change timeout to 10 minutes Defaults timestamp_timeout=10 Change timeout to 10 minutes only for user linuxize Defaults:linuxize timestamp_timeout=10","title":"/etc/sudoers"},{"location":"Linux/swapon/","text":"Instruct system to begin using $PARTITION as a swap file swapon $PARTITION","title":"Swapon"},{"location":"Linux/sysbench/","text":"Benchmark CPU by calculating prime numbers YouTube sysbench --test = cpu --cpu-max-prime = 20000 run File I/O benchmarking YouTube sysbench --test = fileio --file-total-size = 10G --file-test-mode = rndrw --init-rng = on --max-time = 300 --max-requests = 0 run","title":"Sysbench"},{"location":"Linux/sysconfig/","text":"sysconfig /etc/sysconfig/desktop Specify desktop environment and display manager on Red Hat. DESKTOP = \"KDE\" DISPLAYMANAGER = \"KDE\" DESKTOP = \"XFCE\" DISPLAYMANAGER = \"XDM\" DESKTOP = \"Gnome\" DISPLAYMANAGER = \"GDM\" /etc/sysconfig/network-scripts/ Directory containing file configurations for each network device you may have or want to add on your system ref DEVICE = eth0 IPADDR = 208.164.186.1 NETMASK = 255.255.255.0 NETWORK = 208.164.186.0 BROADCAST = 208.164.186.255 ONBOOT = yes BOOTPROTO = none USERCTL = no","title":"sysconfig"},{"location":"Linux/sysconfig/#sysconfig","text":"","title":"sysconfig"},{"location":"Linux/sysconfig/#etcsysconfigdesktop","text":"Specify desktop environment and display manager on Red Hat. DESKTOP = \"KDE\" DISPLAYMANAGER = \"KDE\" DESKTOP = \"XFCE\" DISPLAYMANAGER = \"XDM\" DESKTOP = \"Gnome\" DISPLAYMANAGER = \"GDM\"","title":"/etc/sysconfig/desktop"},{"location":"Linux/sysconfig/#etcsysconfignetwork-scripts","text":"Directory containing file configurations for each network device you may have or want to add on your system ref DEVICE = eth0 IPADDR = 208.164.186.1 NETMASK = 255.255.255.0 NETWORK = 208.164.186.0 BROADCAST = 208.164.186.255 ONBOOT = yes BOOTPROTO = none USERCTL = no","title":"/etc/sysconfig/network-scripts/"},{"location":"Linux/sysctl/","text":"View and configure kernel parameters at runtime Display current hostname as known to the kernel sysctl -n kernel.hostname Disable IPv6 sysctl -w net.ipv6.conf.all.disable_ipv6 = 1 sysctl -w net.ipv6.conf.default.disable_ipv6 = 1 Enable IPv6 sysctl -w net.ipv6.conf.all.disable_ipv6 = 0 sysctl -w net.ipv6.conf.default.disable_ipv6 = 0 ( src )","title":"Sysctl"},{"location":"Linux/systemctl/","text":"daemon-reload disable enable get-default isolate kill list-unit-files list-units mask reboot restart set-default start status stop suspend Configure iptables to start on boot and start it immediately systemctl enable --now iptables Disable $SERVICE , ensuring it does not run on boot systemctl disable $SERVICE Change signal type sent to process to be killed systemctl kill -s Equivalent to chkconfig --list systemctl list-unit-files --type = service Prevent firewalld from being started inadvertently by another process systemctl mask firewalld Restart iptables.service systemctl restart iptables Configure system to boot to a GUI systemctl set-default graphical.target Start $SERVICE systemctl start $SERVICE Check status of $SERVICE systemctl status $SERVICE sudo systemctl is-active $SERVICE Terminate $SERVICE systemctl stop $SERVICE Stop $SERVICE systemctl stop $SERVICE Suspend the system systemctl suspend Change target to runlevel emergency systemctl isolate emergency.target Enable systemd service for newly-installed $DISPLAYMANAGER ; may require disabling previous display manager first systemctl enable $DISPLAYMANAGER .service -f Service files Creating a service file ( src ) [Unit] Description = Runs /usr/local/bin/mystartup.sh [Service] ExecStart = /usr/local/bin/mystartup.sh [Install] WantedBy = multi-user.target","title":"Systemctl"},{"location":"Linux/systemctl/#service-files","text":"Creating a service file ( src ) [Unit] Description = Runs /usr/local/bin/mystartup.sh [Service] ExecStart = /usr/local/bin/mystartup.sh [Install] WantedBy = multi-user.target","title":"Service files"},{"location":"Linux/systemd-delta/","text":"Show files that are overridden with systemd. Display differences among files when they are overridden systemd-delta --diff","title":"Systemd delta"},{"location":"Linux/tail/","text":"Output last lines beginning at 30th line from the start Short option tail -n = +30 POSIX tail --lines = +30","title":"Tail"},{"location":"Linux/tar/","text":"c d f j p r t u v x z A C J Create {archive} from contents of {path} tar -cf archive path tar --create --file archive path Create bzip2-compressed {archive} from contents of {path} tar -cfj archive path tar --create --file --bzip archive path Create gzip-compressed {archive} from contents of {path} tar -cfz archive path tar --create --file --gzip archive path Add {file} to {archive} tar -rf archive file tar --append --file archive file List the contents of {archive} tar -tf archive tar --list --file archive Extract contents of {tarfile} in the current directory tar -xf archive tar --extract --file archive Extract only {file} from {archive} tar -xf archive file tar--extract --file archive file Extract contents of gzip-compressed {archive} to {path} tar -xzf archive -C path","title":"Tar"},{"location":"Linux/tcpdump/","text":"Inspect actual IP packets All network data will be displayed to STDOUT tcpdump -i eth0 Set snapshot length of capture (default 65,535B) tcpdump -s","title":"Tcpdump"},{"location":"Linux/telinit/","text":"Refresh system after changes to /etc/inittab telinit Cause operation to not send any notice to logged-on users telinit--no-wall","title":"Telinit"},{"location":"Linux/tput/","text":"Return width of current terminal window tput cols Return height of current terminal window tput lines","title":"Tput"},{"location":"Linux/tr/","text":"c d s Change the case of a string ] tr [ :upper: ] [ :lower: ] Remove a character or set of characters from a string or line of output tr -d \"text\"","title":"Tr"},{"location":"Linux/tune2fs/","text":"Adjust various ... Run fsck on {/dev/sdb1} on every boot tune2fs -c 1 /dev/sdb1 Run fsck on {/dev/sda1} at intervals of 60 mounts or 6 months tune2fs -c 60 -i 6m /dev/sda1 Enable journaling on ext2 partition {/dev/sdc1} tune2fs -j /dev/sdc1 Assign label \"Sales\" to logical volume {/dev/vg1/Sales} tune2fs -L Sales /dev/vg1/Sales","title":"Tune2fs"},{"location":"Linux/ufw/","text":"Allow traffic associated with SSH, HTTP, and HTTP ufw allow ssh ufw allow http ufw allow htts","title":"Ufw"},{"location":"Linux/uname/","text":"a i m n o p r s v Check kernel version uname -srm","title":"Uname"},{"location":"Linux/unshare/","text":"unshare Run a program in a namespace unshared from its parent process. [ref][https://opensource.com/article/19/10/namespaces-and-containers-linux] sudo unshare --fork --pid --mount-proc zsh","title":"unshare"},{"location":"Linux/unshare/#unshare","text":"Run a program in a namespace unshared from its parent process. [ref][https://opensource.com/article/19/10/namespaces-and-containers-linux] sudo unshare --fork --pid --mount-proc zsh","title":"unshare"},{"location":"Linux/useradd/","text":"useradd Create a new user, setting their default shell to /bin/bash useradd -s /bin/bash Create a new user account {luke} belonging to default group {wheel}, creating a home directory useradd -m -g wheel luke Create a new user account with {comment} useradd -c \"comment\" Create a new user account, adding it to groups {grp1} and {grp2} useradd -G grp1 grp2 Create a new user account, specifying {UUID} useradd -u UUID Add $USER useradd $USER Add $USER , noting her full $NAME useradd $USER -c $NAME Add $USER , specifying home directory at $PATH useradd $USER -d $PATH Add $USER , specifying expiration $DATE (YYYY-MM-DD) useradd $USER -e $DATE Create new $USER leaving a $COMMENT field (conventionally noting the full name of the user) and creating a home directory useradd -c $COMMENT -m $USER Create a system user rather than a normal user useradd -r \ud83d\udee0\ufe0f Config /etc/default/useradd Default values for account creation. Properties: EXPIRE , GROUP , HOME , INACTIVE , SHELL , SKEL b c d e f g k m r s u D G","title":"useradd"},{"location":"Linux/useradd/#useradd","text":"Create a new user, setting their default shell to /bin/bash useradd -s /bin/bash Create a new user account {luke} belonging to default group {wheel}, creating a home directory useradd -m -g wheel luke Create a new user account with {comment} useradd -c \"comment\" Create a new user account, adding it to groups {grp1} and {grp2} useradd -G grp1 grp2 Create a new user account, specifying {UUID} useradd -u UUID Add $USER useradd $USER Add $USER , noting her full $NAME useradd $USER -c $NAME Add $USER , specifying home directory at $PATH useradd $USER -d $PATH Add $USER , specifying expiration $DATE (YYYY-MM-DD) useradd $USER -e $DATE Create new $USER leaving a $COMMENT field (conventionally noting the full name of the user) and creating a home directory useradd -c $COMMENT -m $USER Create a system user rather than a normal user useradd -r","title":"useradd"},{"location":"Linux/useradd/#config","text":"/etc/default/useradd Default values for account creation. Properties: EXPIRE , GROUP , HOME , INACTIVE , SHELL , SKEL b c d e f g k m r s u D G","title":"\ud83d\udee0&#xfe0f; Config"},{"location":"Linux/userdel/","text":"userdel Delete an existing user account Delete an existing user account as well as the user's home directory userdel -r user","title":"userdel"},{"location":"Linux/userdel/#userdel","text":"Delete an existing user account Delete an existing user account as well as the user's home directory userdel -r user","title":"userdel"},{"location":"Linux/usermod/","text":"usermod a c d g l s u G L U","title":"usermod"},{"location":"Linux/usermod/#usermod","text":"a c d g l s u G L U","title":"usermod"},{"location":"Linux/watch/","text":"watch Execute $CMD at periods of $N seconds, watching its output CLKF watch $CMD -n $N Check memory usage in megabytes ( -m ) every 5 seconds Enki watch -n 5 free -m","title":"watch"},{"location":"Linux/watch/#watch","text":"Execute $CMD at periods of $N seconds, watching its output CLKF watch $CMD -n $N Check memory usage in megabytes ( -m ) every 5 seconds Enki watch -n 5 free -m","title":"watch"},{"location":"Linux/weechat/","text":"weechat Resources: Quickstart guide Commands: /help /server add freenode chat.freenode.net /connect freenode /list /join #channel Switch between core and server buffers Ctrl x /server add twitch irc.chat.twitch.tv/6667 /connect twitch -password=oauth:...... The /list command is unavailable, but any Twitch streamer's chat can be joined by specifying the streamer's handle in lowercase . /join #gmhikaru","title":"weechat"},{"location":"Linux/weechat/#weechat","text":"Resources: Quickstart guide Commands: /help /server add freenode chat.freenode.net /connect freenode /list /join #channel Switch between core and server buffers Ctrl x /server add twitch irc.chat.twitch.tv/6667 /connect twitch -password=oauth:...... The /list command is unavailable, but any Twitch streamer's chat can be joined by specifying the streamer's handle in lowercase . /join #gmhikaru","title":"weechat"},{"location":"Linux/xhost/","text":"xhost Enable access control to X server xhost - Disable access control to X server, allowing clients from any host to connect (not unsafe if you use a firewall that allows only SSH) xhost + Add $HOST to list of authorized clients for X server xhost + $HOST Remove $HOST from list of authorized clients for X server xhost - $HOST Add $USER to ACL xhost si:localuser: $USER","title":"xhost"},{"location":"Linux/xhost/#xhost","text":"Enable access control to X server xhost - Disable access control to X server, allowing clients from any host to connect (not unsafe if you use a firewall that allows only SSH) xhost + Add $HOST to list of authorized clients for X server xhost + $HOST Remove $HOST from list of authorized clients for X server xhost - $HOST Add $USER to ACL xhost si:localuser: $USER","title":"xhost"},{"location":"Linux/xmodmap/","text":"xmodmap Replacing Caps Lock with Escape ! Swap caps lock and escape remove Lock = Caps_Lock keysym Escape = Caps_Lock keysym Caps_Lock = Escape add Lock = Caps_Lock","title":"xmodmap"},{"location":"Linux/xmodmap/#xmodmap","text":"Replacing Caps Lock with Escape ! Swap caps lock and escape remove Lock = Caps_Lock keysym Escape = Caps_Lock keysym Caps_Lock = Escape add Lock = Caps_Lock","title":"xmodmap"},{"location":"Linux/xrandr/","text":"xrandr Change resolution of DisplayPort1 to 1920x1080 xrandr --output DP1 --mode 1920x1080 Disable VGA1 output xrandr --output VGA1 --off Display current state of the system xrandr -q --query","title":"xrandr"},{"location":"Linux/xrandr/#xrandr","text":"Change resolution of DisplayPort1 to 1920x1080 xrandr --output DP1 --mode 1920x1080 Disable VGA1 output xrandr --output VGA1 --off Display current state of the system xrandr -q --query","title":"xrandr"},{"location":"Linux/xset/","text":"xset Dynamically add fonts [Haeder: 307][Haeder] xset fp+ /usr/local/fonts","title":"xset"},{"location":"Linux/xset/#xset","text":"Dynamically add fonts [Haeder: 307][Haeder] xset fp+ /usr/local/fonts","title":"xset"},{"location":"Miscellany/","text":"Processing cookbooks Cookbooks are collections of tasks with representative implementations (e.g. Azure commands and procedures for the AZ-103.) Number tasks for easy reference, indexing, and linking in markdown Catalog tasks and desciptions in a spreadsheet Copy catalog with task and description to markdown. This will serve as both an index of tasks as well as the skeleton for the content. Use multiple cursors to introduce #### heading syntax before the task identifier, followed by a carriage return before the one-line description of the task. This will ensure that the task is easily found by identifier. These should be collected in a single-cell table, producing a \"cloud\" of tasks.\" Fill markdown with syntax, producing a true reference of the source's syntax Map each form-based feature (e.g. commands) to tasks in a spreadsheet (Command | Task). Once organized by command, the resulting associations can form another table of content which associates form features to tasks. These should be placed in another single-cell cloud where each token is followed by links to the tasks in which it appears. The tokens should be organized, either by command group or roughly by domain. Index form-based features at the top of the markdown as a concordance. Bootloaders bootloader : software located in the first sector (Master Boot Record) of a HDD, which is read by the BIOS - implementing interruptions requires knowledge of Assembler - expertise in low-level programming in C - Java and C# produce intermediate code, which must be executed by a special virtual machine - mixed-code technique requires at least two compilers (one for Assembler and C, another as a linker to join the *.obj files to create a single executable file) Bots Discord Create the bot user on Discord and register it with a guild. Write code that uses Discord\u2019s APIs and implements your bot\u2019s behaviors. Create a Discord connection ^ A Client is an object that represents a connection to Discord, handling events, tracking state, and interacting with Discord APIs. # bot.py import os , discord from dotenv import load_dotenv # Install via `pip install -U python-dotenv` load_dotenv () token = os . getenv ( 'DISCORD_TOKEN' ) client = discord . Client () @client . event async def on_ready (): print ( f ' { client . user } has connected to Discord!' ) client . run ( token ) Store token in .env file .env should be placed in the same directory as bot.py # .env DISCORD_TOKEN={your-bot-token} Twitch Nightbot Mee6 Ruby bot programming Ruby library \"socket\" allows integration with Twitch's IRC API , which provides an oauth token which can be stored as password. Command write_to_system appears to be what is needed to concatenate IRC commands PASS #{@password , NICK #{@nickname} , USER #{@nickname} 0 * #{@nickname} , and JOIN #@{channel} \\ From the REPL, you instantiate an instance of the class after running the script, which will allow passing messages to the chat room by using an instance method ^ bot = TwitchBot . new bot . write_to_chat \"Hello world\" VMware Hypervisor, similar to Hyper-V, but provided at a cost, with a robust command-line interface via PowerShell. ^","title":"Index"},{"location":"Miscellany/#processing-cookbooks","text":"Cookbooks are collections of tasks with representative implementations (e.g. Azure commands and procedures for the AZ-103.) Number tasks for easy reference, indexing, and linking in markdown Catalog tasks and desciptions in a spreadsheet Copy catalog with task and description to markdown. This will serve as both an index of tasks as well as the skeleton for the content. Use multiple cursors to introduce #### heading syntax before the task identifier, followed by a carriage return before the one-line description of the task. This will ensure that the task is easily found by identifier. These should be collected in a single-cell table, producing a \"cloud\" of tasks.\" Fill markdown with syntax, producing a true reference of the source's syntax Map each form-based feature (e.g. commands) to tasks in a spreadsheet (Command | Task). Once organized by command, the resulting associations can form another table of content which associates form features to tasks. These should be placed in another single-cell cloud where each token is followed by links to the tasks in which it appears. The tokens should be organized, either by command group or roughly by domain. Index form-based features at the top of the markdown as a concordance.","title":"Processing cookbooks"},{"location":"Miscellany/#bootloaders","text":"bootloader : software located in the first sector (Master Boot Record) of a HDD, which is read by the BIOS - implementing interruptions requires knowledge of Assembler - expertise in low-level programming in C - Java and C# produce intermediate code, which must be executed by a special virtual machine - mixed-code technique requires at least two compilers (one for Assembler and C, another as a linker to join the *.obj files to create a single executable file)","title":"Bootloaders"},{"location":"Miscellany/#bots","text":"","title":"Bots"},{"location":"Miscellany/#discord","text":"Create the bot user on Discord and register it with a guild. Write code that uses Discord\u2019s APIs and implements your bot\u2019s behaviors. Create a Discord connection ^ A Client is an object that represents a connection to Discord, handling events, tracking state, and interacting with Discord APIs. # bot.py import os , discord from dotenv import load_dotenv # Install via `pip install -U python-dotenv` load_dotenv () token = os . getenv ( 'DISCORD_TOKEN' ) client = discord . Client () @client . event async def on_ready (): print ( f ' { client . user } has connected to Discord!' ) client . run ( token ) Store token in .env file .env should be placed in the same directory as bot.py # .env DISCORD_TOKEN={your-bot-token}","title":"Discord"},{"location":"Miscellany/#twitch","text":"Nightbot Mee6","title":"Twitch"},{"location":"Miscellany/#ruby-bot-programming","text":"Ruby library \"socket\" allows integration with Twitch's IRC API , which provides an oauth token which can be stored as password. Command write_to_system appears to be what is needed to concatenate IRC commands PASS #{@password , NICK #{@nickname} , USER #{@nickname} 0 * #{@nickname} , and JOIN #@{channel} \\ From the REPL, you instantiate an instance of the class after running the script, which will allow passing messages to the chat room by using an instance method ^ bot = TwitchBot . new bot . write_to_chat \"Hello world\"","title":"Ruby bot programming"},{"location":"Miscellany/#vmware","text":"Hypervisor, similar to Hyper-V, but provided at a cost, with a robust command-line interface via PowerShell. ^","title":"VMware"},{"location":"Miscellany/CI-CD/","text":"CI/CD Github Actions All that's needed is a yaml file in .github/workflows with the following keys: YouTube - name - on - jobs In order to use the Python projects need to have a .spec file GitHub actions can be either Docker or JavaScript name : Build and publish presentation with reveal-md on : push jobs : release : name : Build & Publish runs-on : ubuntu-latest steps : - uses : actions/checkout@v1 - name : Install dependencies and build presentation run : | sudo npm install -g reveal-md --unsafe-perm sudo reveal-md Presentation.md --static _site --highlight-... on src on : push : branches : - main jobs src jobs : release : if : github.actor == 'JackMcKew' && startsWith(github.event.head_commit.message, 'Update README') name : Build runs-on : ubuntu-latest steps : - uses : actions/checkout@v1 - name : Set up Python 3.7 uses : actions/setup-python@v1 with : python-version : 3.7 - name : Install dependencies & Convert README.ipynb run : | python -m pip install --upgrade pip pip install -r requirements.txt jupyter nbconvert --template \"pythoncodeblocks.tpl\" --ClearMetadataPreprocessor.enabled=True --ClearOutput.enabled=True --to markdown README.ipynb - name : Commit files run : | git config --local user.email \"action@github.com\" git config --local user.name \"GitHub Action\" git add README.md git commit -m \"Convert README.ipynb to README.md\" -a - name : Push changes if : success() uses : ad-m/github-push-action@master with : branch : main github_token : ${{ secrets.ACCESS_TOKEN }} Jenkins Freestyle jobs are simpler than Pipeline jobs, which require the Pipeline plugin. A Jenkinsfile should exist in the root directory of the project Jenkinsfile has 3 stages: ref - stage('Build') - stage('Test') - stage('Deliver')","title":"CI/CD"},{"location":"Miscellany/CI-CD/#cicd","text":"","title":"CI/CD"},{"location":"Miscellany/CI-CD/#github-actions","text":"All that's needed is a yaml file in .github/workflows with the following keys: YouTube - name - on - jobs In order to use the Python projects need to have a .spec file GitHub actions can be either Docker or JavaScript name : Build and publish presentation with reveal-md on : push jobs : release : name : Build & Publish runs-on : ubuntu-latest steps : - uses : actions/checkout@v1 - name : Install dependencies and build presentation run : | sudo npm install -g reveal-md --unsafe-perm sudo reveal-md Presentation.md --static _site --highlight-...","title":"Github Actions"},{"location":"Miscellany/CI-CD/#on","text":"src on : push : branches : - main","title":"on"},{"location":"Miscellany/CI-CD/#jobs","text":"src jobs : release : if : github.actor == 'JackMcKew' && startsWith(github.event.head_commit.message, 'Update README') name : Build runs-on : ubuntu-latest steps : - uses : actions/checkout@v1 - name : Set up Python 3.7 uses : actions/setup-python@v1 with : python-version : 3.7 - name : Install dependencies & Convert README.ipynb run : | python -m pip install --upgrade pip pip install -r requirements.txt jupyter nbconvert --template \"pythoncodeblocks.tpl\" --ClearMetadataPreprocessor.enabled=True --ClearOutput.enabled=True --to markdown README.ipynb - name : Commit files run : | git config --local user.email \"action@github.com\" git config --local user.name \"GitHub Action\" git add README.md git commit -m \"Convert README.ipynb to README.md\" -a - name : Push changes if : success() uses : ad-m/github-push-action@master with : branch : main github_token : ${{ secrets.ACCESS_TOKEN }}","title":"jobs"},{"location":"Miscellany/CI-CD/#jenkins","text":"Freestyle jobs are simpler than Pipeline jobs, which require the Pipeline plugin. A Jenkinsfile should exist in the root directory of the project Jenkinsfile has 3 stages: ref - stage('Build') - stage('Test') - stage('Deliver')","title":"Jenkins"},{"location":"Miscellany/bittorrent/","text":"BitTorrent Bram Cohen invented BitTorrent protocol in 2001 and wrote the first client in Python. It is a peer-to-peer file sharing protocol where those who share a file are called seeders and those who download are called peers . All seeders and peers related to a particular torrent comprise the swarm . The tracker server or tracker serves as a repository for information about peers associated with the same file. Files are downloaded in hashed pieces from multiple seeders to distribute the burden of seeding a file. ^ A Torrent Descriptor file is a hashmap file Torrent Descriptor property Description Announce URL of the tracker Info dictionary whose keys depend on whether one or more files are being shared, including: Files: list of dictionaries, only exists when multiple files are being shared, each dictionary has two keys and corresponds to a file Length: size of the file in bytes Path: list of strings corresponding to subdirectory names, the last of which is the actual filename length size of the file in bytes (when one file is being shared name suggested file or directory name Pieces length number of bytes per piece; must be a power of 2 and at least 16KiB Pieces list of SHA-1 160-bit hashes calculated on various parts of data { \"Announce\" : \"url of tracker\" , \"Info\" : { \"Files\" : [{ \"Length\" : 16 , \"path\" : \"/folder/to/path\" }, { \"length\" : 193 , \"path\" : \"/another/folder\" }] }, \"length\" : 192 , \"name\" : \" Ubuntu.iso\" , \"Pieces length\" : 262144 , \"Pieces\" : [ AAF 4 C 61 DDCC 5E8 A 2 DABEDE 0 F 3 B 482 CD 9 AEA 9434 D , CFEA 2496442 C 091 FDDD 1 BA 215 D 62 A 69E C 34E94 D 0 ] } BitTorrent clients Deluge ^ ^ command-line functionality open-source with expandable functionalities chosen as the best torrent client by Lifehacker qBittorrent open-source, ad-free alternative to uTorrent Tixati closed-source Transmission installed by default on Ubuntu (ca. 2017) Tribler Vuze has ads and is closed-source Frostwire : multiplatform, including Android WebTorrent Desktop","title":"BitTorrent"},{"location":"Miscellany/bittorrent/#bittorrent","text":"Bram Cohen invented BitTorrent protocol in 2001 and wrote the first client in Python. It is a peer-to-peer file sharing protocol where those who share a file are called seeders and those who download are called peers . All seeders and peers related to a particular torrent comprise the swarm . The tracker server or tracker serves as a repository for information about peers associated with the same file. Files are downloaded in hashed pieces from multiple seeders to distribute the burden of seeding a file. ^ A Torrent Descriptor file is a hashmap file Torrent Descriptor property Description Announce URL of the tracker Info dictionary whose keys depend on whether one or more files are being shared, including: Files: list of dictionaries, only exists when multiple files are being shared, each dictionary has two keys and corresponds to a file Length: size of the file in bytes Path: list of strings corresponding to subdirectory names, the last of which is the actual filename length size of the file in bytes (when one file is being shared name suggested file or directory name Pieces length number of bytes per piece; must be a power of 2 and at least 16KiB Pieces list of SHA-1 160-bit hashes calculated on various parts of data { \"Announce\" : \"url of tracker\" , \"Info\" : { \"Files\" : [{ \"Length\" : 16 , \"path\" : \"/folder/to/path\" }, { \"length\" : 193 , \"path\" : \"/another/folder\" }] }, \"length\" : 192 , \"name\" : \" Ubuntu.iso\" , \"Pieces length\" : 262144 , \"Pieces\" : [ AAF 4 C 61 DDCC 5E8 A 2 DABEDE 0 F 3 B 482 CD 9 AEA 9434 D , CFEA 2496442 C 091 FDDD 1 BA 215 D 62 A 69E C 34E94 D 0 ] }","title":"BitTorrent"},{"location":"Miscellany/bittorrent/#bittorrent-clients","text":"Deluge ^ ^ command-line functionality open-source with expandable functionalities chosen as the best torrent client by Lifehacker qBittorrent open-source, ad-free alternative to uTorrent Tixati closed-source Transmission installed by default on Ubuntu (ca. 2017) Tribler Vuze has ads and is closed-source Frostwire : multiplatform, including Android WebTorrent Desktop","title":"BitTorrent clients"},{"location":"Miscellany/cryptocurrency/","text":"\ud83e\ude99 Cryptocurrency The establishment of the ERC-20 protocol allows new tokens to launch on Ethereum's blockchain using Ethereum smart contracts. This resulted in an explosion of new cryptocurrencies in 2017. Since then, investors have switched focus to tokens that are exchanged on creditable exchanges. Initial Exchange Offerings (IEO) have become popular. ref Crypto tokens form one of the two categories of cryptocurrency and represent a tradable asset or utility that is found on a blockchain. Cryptocurrency is a standard currency used for the sole purpose of making or receivng payments on the blockchain. Crypto tokens represent an underlying asset, customer loyalty points for example. ref Blockchain Blockchain is a distributed digital ledger consisting of interlinked blocks, each of which stores information that cannot be retroactively tampered with or deleted. ref Blockchain is a database technology that uses hashes to ensure reliability and security of data stored across a network of computers, popularized by BitCoin. Records, containing information, are validated and then added to Blocks , or hashed containers, which are then concatenated in a chain by associating each block with the hash of both of its neighbors. ref There can only be a a maximum of 21 million BTC, and the reward for mining a new bitcoin halves every 210,000 blocks. This has already occurred twice in the 10-year history of Bitcoin until late 2019, and another blockhalving is expected to occur in May 2020. ref","title":"\ud83e\ude99 Cryptocurrency"},{"location":"Miscellany/cryptocurrency/#cryptocurrency","text":"The establishment of the ERC-20 protocol allows new tokens to launch on Ethereum's blockchain using Ethereum smart contracts. This resulted in an explosion of new cryptocurrencies in 2017. Since then, investors have switched focus to tokens that are exchanged on creditable exchanges. Initial Exchange Offerings (IEO) have become popular. ref Crypto tokens form one of the two categories of cryptocurrency and represent a tradable asset or utility that is found on a blockchain. Cryptocurrency is a standard currency used for the sole purpose of making or receivng payments on the blockchain. Crypto tokens represent an underlying asset, customer loyalty points for example. ref","title":"\ud83e\ude99 Cryptocurrency"},{"location":"Miscellany/cryptocurrency/#blockchain","text":"Blockchain is a distributed digital ledger consisting of interlinked blocks, each of which stores information that cannot be retroactively tampered with or deleted. ref Blockchain is a database technology that uses hashes to ensure reliability and security of data stored across a network of computers, popularized by BitCoin. Records, containing information, are validated and then added to Blocks , or hashed containers, which are then concatenated in a chain by associating each block with the hash of both of its neighbors. ref There can only be a a maximum of 21 million BTC, and the reward for mining a new bitcoin halves every 210,000 blocks. This has already occurred twice in the 10-year history of Bitcoin until late 2019, and another blockhalving is expected to occur in May 2020. ref","title":"Blockchain"},{"location":"Miscellany/learn/","text":"Learning Retrieval Retrieval-based learning refers to the coupling of two ideas: Retrieval processes , those involved in using available cues to actively reconstruct knowledge, are most important in learning Active retrieval is most important for producing learning. Retrieval-based learning characterizes knowledge as something that is reconstructed at the time of recall. Knowledge reconstruction is affected by the presence or absence of retrieval cues . This is contrasted with the traditional analogy of knowledge as static storage. Retrieval-based learning is an outgrowth of recent cognitive science research that shows that the act of measuring knowledge - by recall, answering questions, or solving novel problems - actually aids learning. The testing effect refers to the observation that the act of taking a test, by itself and without any feedback or further study, results in improved learning. ( src ) Retrieval effort is a key concept in retrieval-based learning, which refers to the observation that activities that are difficult and require effort can be good for learning. In other words, the effort involved in retrieval is the key to learning. Learning activites that engage retrieval processes include group discussions, reciprocal teaching, and questioning techniques. ( src ) Free recall refers to a specific task provided to subjects whereby the experimenter presents a list of items to be remembered and the subject is free to recall them in any order. Paying forward What's the best way to organize information for newcomers? That is to say, what is the best way to organize notes on the sources of information that I stumble across, rather than merely the information itself? The information itself is practical, but the context of what type of material assisted me in learning it should also be preserved, somehow. For example, for someone new to vim , Chris Toomey's talk on YouTube might be a very good tool for learners who prefer to see lectures by impassioned and articulate people. Someone who is more mechanically inclined might benefit more from pacvim , or other hands-on activities. Maybe a combination of both? Some segments of PluralSight videos on esoteric technical topics appear to cover basic material concisely and effectively, in a way that made me wish I had access to those segments when I was learning it before. Gathering this type of information could be useful, not to me, but to others. How do I organize my thoughts and observations on the value of sources without doubling my effort? Command-line syntax After several weeks of refining my note-taking technique with regard to syntax, I believe I have settled on an improved workflow. For all command-line syntax: 1. Note the command itself within \"Commands\" spreadsheet (separate from \"Terms\", which is for vocabulary) 2. If any options are encountered, document them in Options 3. If commands form command groups (like apt , docker , git , netsh , etc), those command groups need to be broken out separately (\"Group-style commands\") 4. If a command launches its own REPL ( bluetoothctl , diskpart , fdisk ...) those are broken out as well Finding magic numbers Before understanding the \"lay of the land\", or rather the best epistemology for a unit of information, you are first confronted with a list of information without context. This happened while studying for the Network+. On the topic of authentication, I learned a list of material, basically concepts associated with AAA. Authentication process of determining... Authorization identifying the resources... Accounting tracking methods used ... Authentication, Authorization, Accounting, and Auditing (AAAA) conceptual model... Remote Authentication Dial-In User Service (RADIUS) protocol that enables ... Terminal Access Control Access Control System (TACACS) security protocol designed ... Kerberos security system ... ticket security tokens issued to clients ... Local authentication subsystem (LASS) authenticates users ... But after further research, I found that once you understand the role of authentication, then really there are only three main systems that implement it (according to the material): Kerberos , TACACS , and RADIUS . Reducing a confusing mass of knowledge into a magic number (2, 3, 4, etc) helps in identifying interrelationships between concepts and entities Anki Made good progress incorporating task-based learning by simplifying procedures into command sequences with no parameters or (at most) one or two. Cloze notes with input comprise a low-level way of practicing the skill. The best strategy to pursue is to identify common patterns, and make the most common elements of those patterns into cloze cards. Basic templates: DSC Configuration DnsClient { Import-DscResource -ModuleName \"xNetworking\" Node ( \"ServerA\" , \"ServerB\" ) { xDnsServerAddress DnsServer { Address = 10 . 0 . 0 . 1 AddressFamily = \"Ipv4\" InterfaceAlias = \"Ethernet\" } } } C# namespace HelloWorld { class Program { static void Main ( string [] args ) { Console . WriteLine ( \"Hello world!\" ); } } } Two tasks with related forms # Get connection string of account constring =$( az storage account show-connection -string ) # Create file share with connection string az storage share create - -connection-string $constring Single task with different implementations (see tasks associated with cloud providers ) Creating an availability set Azure Powershell New-AzAvailabilitySet -PlatformUpdateDomainCount -PlatformFaultDomainCount -Sku \"{{c8::Aligned}}\" Azure CLI az vm availability-set create --platform-update-domain-count --platform-fault-domain-count Get - Set pattern in PowerShell Capture a managed VM image $vm = Get-AzVM $image = New-AzImageConfig -SourceVirtualMachineId $vm . Id New-AzImage -Image $image Create a subnet $vnet2 = Get-AzVirtualNetwork -Name VNet2 -ResourceGroupName ExamRefRG $vnet2 . Subnets += New-AzVirtualNetworkSubnetConfig -Name GatewaySubnet -AddressPrefix 10 . 2 . 1 . 0 / 27 $vnet2 = Set-AzVirtualNetwork -VirtualNetwork $vnet2 Create DNS record set $records = @() $records += New-AzDnsRecordConfig -IPv4Address \"1.2.3.4\" $records += New-AzDnsRecordConfig -IPv4Address \"5.6.7.8\" New-AzDnsRecordSet -Name \"@\" -RecordType A -ZoneName examref . com -ResourceGroupName ExamRefRG -Ttl 3600 -DnsRecords $records Change VM size $vm = Get-AzVM $vm . HardwareProfile . VMSize = Standard_DS3_v2 Update-AzVM -VM $vm Add a route to a routing table $rt = New-AzRouteTable Add-AzRouteConfig -RouteTable $rt Set-AzRouteTable -RouteTable $rt Structure Playing around with reference-style links and tooltips has me thinking that there really should be a more structured, flexible way of generating text reports from object-style hierarchical information. For example, whether a definition appears beside a word dictionary style or in a tooltip on hover is really an implementation detail. There should be an easy way of storing that data and specifying that presentation dynamically. What I have settled on is a multilayered note-taking strategy. Every lexeme is defined first in a slug or one-line description that establishes its epistemological context as well as its semantic significance. A stub further elaborates the lexeme, especially insofar as it encapsulates further lexemes or can be analyzed into components. These slugs and stubs can be presented in various ways. Most recently I have gotten into the habit of putting slugs into tooltips that appear when I hover over lexemes in my markdown notes. This is an especially elegant solution in tables, where I can provide a highly condensed and legible index of commands, each of which can be understood at a high level by hovering the mouse while still providing full details when clicked on. This is also an elegant solution in tables of contents, where I can use a tooltip to contain a synopsis of a chapter which still links to the full notes. It provides a way of rendering information of intermediary fidelity, between the mere title and fully developed notes.","title":"Learning"},{"location":"Miscellany/learn/#learning","text":"","title":"Learning"},{"location":"Miscellany/learn/#retrieval","text":"Retrieval-based learning refers to the coupling of two ideas: Retrieval processes , those involved in using available cues to actively reconstruct knowledge, are most important in learning Active retrieval is most important for producing learning. Retrieval-based learning characterizes knowledge as something that is reconstructed at the time of recall. Knowledge reconstruction is affected by the presence or absence of retrieval cues . This is contrasted with the traditional analogy of knowledge as static storage. Retrieval-based learning is an outgrowth of recent cognitive science research that shows that the act of measuring knowledge - by recall, answering questions, or solving novel problems - actually aids learning. The testing effect refers to the observation that the act of taking a test, by itself and without any feedback or further study, results in improved learning. ( src ) Retrieval effort is a key concept in retrieval-based learning, which refers to the observation that activities that are difficult and require effort can be good for learning. In other words, the effort involved in retrieval is the key to learning. Learning activites that engage retrieval processes include group discussions, reciprocal teaching, and questioning techniques. ( src ) Free recall refers to a specific task provided to subjects whereby the experimenter presents a list of items to be remembered and the subject is free to recall them in any order.","title":"Retrieval"},{"location":"Miscellany/learn/#paying-forward","text":"What's the best way to organize information for newcomers? That is to say, what is the best way to organize notes on the sources of information that I stumble across, rather than merely the information itself? The information itself is practical, but the context of what type of material assisted me in learning it should also be preserved, somehow. For example, for someone new to vim , Chris Toomey's talk on YouTube might be a very good tool for learners who prefer to see lectures by impassioned and articulate people. Someone who is more mechanically inclined might benefit more from pacvim , or other hands-on activities. Maybe a combination of both? Some segments of PluralSight videos on esoteric technical topics appear to cover basic material concisely and effectively, in a way that made me wish I had access to those segments when I was learning it before. Gathering this type of information could be useful, not to me, but to others. How do I organize my thoughts and observations on the value of sources without doubling my effort?","title":"Paying forward"},{"location":"Miscellany/learn/#command-line-syntax","text":"After several weeks of refining my note-taking technique with regard to syntax, I believe I have settled on an improved workflow. For all command-line syntax: 1. Note the command itself within \"Commands\" spreadsheet (separate from \"Terms\", which is for vocabulary) 2. If any options are encountered, document them in Options 3. If commands form command groups (like apt , docker , git , netsh , etc), those command groups need to be broken out separately (\"Group-style commands\") 4. If a command launches its own REPL ( bluetoothctl , diskpart , fdisk ...) those are broken out as well","title":"Command-line syntax"},{"location":"Miscellany/learn/#finding-magic-numbers","text":"Before understanding the \"lay of the land\", or rather the best epistemology for a unit of information, you are first confronted with a list of information without context. This happened while studying for the Network+. On the topic of authentication, I learned a list of material, basically concepts associated with AAA. Authentication process of determining... Authorization identifying the resources... Accounting tracking methods used ... Authentication, Authorization, Accounting, and Auditing (AAAA) conceptual model... Remote Authentication Dial-In User Service (RADIUS) protocol that enables ... Terminal Access Control Access Control System (TACACS) security protocol designed ... Kerberos security system ... ticket security tokens issued to clients ... Local authentication subsystem (LASS) authenticates users ... But after further research, I found that once you understand the role of authentication, then really there are only three main systems that implement it (according to the material): Kerberos , TACACS , and RADIUS . Reducing a confusing mass of knowledge into a magic number (2, 3, 4, etc) helps in identifying interrelationships between concepts and entities","title":"Finding magic numbers"},{"location":"Miscellany/learn/#anki","text":"Made good progress incorporating task-based learning by simplifying procedures into command sequences with no parameters or (at most) one or two. Cloze notes with input comprise a low-level way of practicing the skill. The best strategy to pursue is to identify common patterns, and make the most common elements of those patterns into cloze cards. Basic templates: DSC Configuration DnsClient { Import-DscResource -ModuleName \"xNetworking\" Node ( \"ServerA\" , \"ServerB\" ) { xDnsServerAddress DnsServer { Address = 10 . 0 . 0 . 1 AddressFamily = \"Ipv4\" InterfaceAlias = \"Ethernet\" } } } C# namespace HelloWorld { class Program { static void Main ( string [] args ) { Console . WriteLine ( \"Hello world!\" ); } } } Two tasks with related forms # Get connection string of account constring =$( az storage account show-connection -string ) # Create file share with connection string az storage share create - -connection-string $constring Single task with different implementations (see tasks associated with cloud providers ) Creating an availability set Azure Powershell New-AzAvailabilitySet -PlatformUpdateDomainCount -PlatformFaultDomainCount -Sku \"{{c8::Aligned}}\" Azure CLI az vm availability-set create --platform-update-domain-count --platform-fault-domain-count Get - Set pattern in PowerShell Capture a managed VM image $vm = Get-AzVM $image = New-AzImageConfig -SourceVirtualMachineId $vm . Id New-AzImage -Image $image Create a subnet $vnet2 = Get-AzVirtualNetwork -Name VNet2 -ResourceGroupName ExamRefRG $vnet2 . Subnets += New-AzVirtualNetworkSubnetConfig -Name GatewaySubnet -AddressPrefix 10 . 2 . 1 . 0 / 27 $vnet2 = Set-AzVirtualNetwork -VirtualNetwork $vnet2 Create DNS record set $records = @() $records += New-AzDnsRecordConfig -IPv4Address \"1.2.3.4\" $records += New-AzDnsRecordConfig -IPv4Address \"5.6.7.8\" New-AzDnsRecordSet -Name \"@\" -RecordType A -ZoneName examref . com -ResourceGroupName ExamRefRG -Ttl 3600 -DnsRecords $records Change VM size $vm = Get-AzVM $vm . HardwareProfile . VMSize = Standard_DS3_v2 Update-AzVM -VM $vm Add a route to a routing table $rt = New-AzRouteTable Add-AzRouteConfig -RouteTable $rt Set-AzRouteTable -RouteTable $rt","title":"Anki"},{"location":"Miscellany/learn/#structure","text":"Playing around with reference-style links and tooltips has me thinking that there really should be a more structured, flexible way of generating text reports from object-style hierarchical information. For example, whether a definition appears beside a word dictionary style or in a tooltip on hover is really an implementation detail. There should be an easy way of storing that data and specifying that presentation dynamically. What I have settled on is a multilayered note-taking strategy. Every lexeme is defined first in a slug or one-line description that establishes its epistemological context as well as its semantic significance. A stub further elaborates the lexeme, especially insofar as it encapsulates further lexemes or can be analyzed into components. These slugs and stubs can be presented in various ways. Most recently I have gotten into the habit of putting slugs into tooltips that appear when I hover over lexemes in my markdown notes. This is an especially elegant solution in tables, where I can provide a highly condensed and legible index of commands, each of which can be understood at a high level by hovering the mouse while still providing full details when clicked on. This is also an elegant solution in tables of contents, where I can use a tooltip to contain a synopsis of a chapter which still links to the full notes. It provides a way of rendering information of intermediary fidelity, between the mere title and fully developed notes.","title":"Structure"},{"location":"Text-editors/atom/","text":"Atom Key bindings Add key bindings to the keymap.cson file (File menu) - Soft wrap: editor:toggle-soft-wrap - Delete line: editor:delete-line","title":"Atom"},{"location":"Text-editors/atom/#atom","text":"","title":"Atom"},{"location":"Text-editors/atom/#key-bindings","text":"Add key bindings to the keymap.cson file (File menu) - Soft wrap: editor:toggle-soft-wrap - Delete line: editor:delete-line","title":"Key bindings"},{"location":"Text-editors/code/","text":"VS Code Code can be folded by placing markers Markdown <!-- #region --> ... <!-- #endregion --> C# #region ... #endregion Python #region ... #endregion Snippets \"Editor group\" refers to window panes. Default keyboard binding Command Effect Ctrl \\ workbench.action.splitEditor Split Editor Ctrl k Ctrl+UpArrow workbench.action.focusAboveGroup View: Focus Above Editor Group Ctrl k Ctrl+RightArrow workbench.action.focusRightGroup View: Focus Right Editor Group Ctrl k Ctrl+DownArrow workbench.action.focusBelowGroup View: Focus Below Editor Group Ctrl k Ctrl+LeftArrow workbench.action.focusLeftGroup View: Focus Left Editor Group Ctrl k UpArrow workbench.action.moveActiveEditorGroupUp View: Move Editor Group Up Ctrl k RightArrow workbench.action.moveActiveEditorGroupRight View: Move Editor Group Right Ctrl k DownArrow workbench.action.moveActiveEditorGroupDown View: Move Editor Group Down Ctrl k LeftArrow workbench.action.moveActiveEditorGroupLeft View: Move Editor Group Left Alt UpArrow editor.action.moveLinesUpAction Move line up Alt DownArrow editor.action.moveLinesDownAction Move line down Option Command -DownArrow Add a cursor down Option Command -UpArrow Add a cursor up Option Shift -Left Click Click and drag to add cursors Ctrl Shift +5 Terminal: Split terminal Ctrl h Replace Ctrl l Expand line selection Ctrl j workbench.action.togglePanel View: Toggle Panel Ctrl b View: Toggle Side Bar Visibility","title":"VS Code"},{"location":"Text-editors/code/#vs-code","text":"Code can be folded by placing markers Markdown <!-- #region --> ... <!-- #endregion --> C# #region ... #endregion Python #region ... #endregion Snippets \"Editor group\" refers to window panes. Default keyboard binding Command Effect Ctrl \\ workbench.action.splitEditor Split Editor Ctrl k Ctrl+UpArrow workbench.action.focusAboveGroup View: Focus Above Editor Group Ctrl k Ctrl+RightArrow workbench.action.focusRightGroup View: Focus Right Editor Group Ctrl k Ctrl+DownArrow workbench.action.focusBelowGroup View: Focus Below Editor Group Ctrl k Ctrl+LeftArrow workbench.action.focusLeftGroup View: Focus Left Editor Group Ctrl k UpArrow workbench.action.moveActiveEditorGroupUp View: Move Editor Group Up Ctrl k RightArrow workbench.action.moveActiveEditorGroupRight View: Move Editor Group Right Ctrl k DownArrow workbench.action.moveActiveEditorGroupDown View: Move Editor Group Down Ctrl k LeftArrow workbench.action.moveActiveEditorGroupLeft View: Move Editor Group Left Alt UpArrow editor.action.moveLinesUpAction Move line up Alt DownArrow editor.action.moveLinesDownAction Move line down Option Command -DownArrow Add a cursor down Option Command -UpArrow Add a cursor up Option Shift -Left Click Click and drag to add cursors Ctrl Shift +5 Terminal: Split terminal Ctrl h Replace Ctrl l Expand line selection Ctrl j workbench.action.togglePanel View: Toggle Panel Ctrl b View: Toggle Side Bar Visibility","title":"VS Code"},{"location":"Text-editors/vim/","text":"vim Unlike WYSIWYG editors which optimize input for writing text, vim optimizes for editing it. Vim offers a composable language for expressing these editing changes whose syntax can be composed into two elements, operations and text objects , which are analogous to verbs and nouns in language. YouTube The framework of understanding vim's syntax as a language appears to date back to an influential 2011 Stack Overflow post . Commands Use :normal to define a series of normal-mode commands ; Select all : normal ggVG Keybindings There are two kinds of keybindings in vim - Recursive using command words map , nmap , vmap , etc. In these keybindings, the mapping itself is interpreted. - Nonrecursive There are two types of keycodes ref - Terminal keycodes that appear similar to ^[[1;2A . These may or may not be identifiable with the keycodes which the Linux kernel maps to raw keybaord scancodes . ref - Vim keycodes which are identifiable as being in angle brackets: <Space> , <Return> , etc Leader The leader key is used to create more complicated keybindings using any arbitrary keypress, for example using , or <Space> . let mapleader = ' ' Settings conceallevel ? expandtab ? relativenumber ? splitright ? splitbelow ? termguicolors ? Key bindings Map <Alt-j> and <Alt-k> to move lines of text up or down vim.fandom.com nnoremap <A-j> :m .+1<CR>== nnoremap <A-k> :m .-2<CR>== inoremap <A-j> <Esc>:m .+1<CR>==gi inoremap <A-k> <Esc>:m .-2<CR>==gi vnoremap <A-j> :m '>+1<CR>gv=gv vnoremap <A-k> :m '<-2<CR>gv=gv Autocommands Autocommands expose an API that allows programming in response to editor events like BufNewFile , BufReadPost , BufWritePost , BufWinLeave , etc. Turning syntax highlighting on (assuming it is off by default) only for certain filetypes: ref augroup PatchDiffHighlight autocmd! autocmd BufEnter *.patch,*.rej,*.diff syntax enable augroup END filetype on augroup PatchDiffHighlight autocmd! autocmd FileType diff syntax enable augroup END Color Elements : Directory Identifier LineNr NonText Normal String Title VertSplit Comment Constant Cursor Folded Function Keyword Number PreProc SpecialKey Special Statement StatusLineNC StatusLine Todo Type Visual Change the color of an Element highlight ELEMENT ctermfg = COLOR ctermbg = COLOR guifg = #abc123 guibg = #abc123 Select alternative colorschemes : colo [rscheme] < tab > Display all available colorschemes : colo < C - d > Clear custom color commands : highlight clear : hi clear vim plugins Vim 8 supports native loading of plugins (put in .vim/pack/xx/start/ where xx is an arbitrary directory name Set file format to Unix/DOS ref : set fileformat = unix : set fileformat = dos Completion Context-aware completion Ctrl x Ctrl l Omni completion Ctrl x Ctrl o","title":"Vim"},{"location":"Text-editors/vim/#vim","text":"Unlike WYSIWYG editors which optimize input for writing text, vim optimizes for editing it. Vim offers a composable language for expressing these editing changes whose syntax can be composed into two elements, operations and text objects , which are analogous to verbs and nouns in language. YouTube The framework of understanding vim's syntax as a language appears to date back to an influential 2011 Stack Overflow post .","title":"vim"},{"location":"Text-editors/vim/#commands","text":"Use :normal to define a series of normal-mode commands ; Select all : normal ggVG","title":"Commands"},{"location":"Text-editors/vim/#keybindings","text":"There are two kinds of keybindings in vim - Recursive using command words map , nmap , vmap , etc. In these keybindings, the mapping itself is interpreted. - Nonrecursive There are two types of keycodes ref - Terminal keycodes that appear similar to ^[[1;2A . These may or may not be identifiable with the keycodes which the Linux kernel maps to raw keybaord scancodes . ref - Vim keycodes which are identifiable as being in angle brackets: <Space> , <Return> , etc","title":"Keybindings"},{"location":"Text-editors/vim/#leader","text":"The leader key is used to create more complicated keybindings using any arbitrary keypress, for example using , or <Space> . let mapleader = ' '","title":"Leader"},{"location":"Text-editors/vim/#settings","text":"conceallevel ? expandtab ? relativenumber ? splitright ? splitbelow ? termguicolors ?","title":"Settings"},{"location":"Text-editors/vim/#key-bindings","text":"Map <Alt-j> and <Alt-k> to move lines of text up or down vim.fandom.com nnoremap <A-j> :m .+1<CR>== nnoremap <A-k> :m .-2<CR>== inoremap <A-j> <Esc>:m .+1<CR>==gi inoremap <A-k> <Esc>:m .-2<CR>==gi vnoremap <A-j> :m '>+1<CR>gv=gv vnoremap <A-k> :m '<-2<CR>gv=gv","title":"Key bindings"},{"location":"Text-editors/vim/#autocommands","text":"Autocommands expose an API that allows programming in response to editor events like BufNewFile , BufReadPost , BufWritePost , BufWinLeave , etc. Turning syntax highlighting on (assuming it is off by default) only for certain filetypes: ref augroup PatchDiffHighlight autocmd! autocmd BufEnter *.patch,*.rej,*.diff syntax enable augroup END filetype on augroup PatchDiffHighlight autocmd! autocmd FileType diff syntax enable augroup END","title":"Autocommands"},{"location":"Text-editors/vim/#color","text":"Elements : Directory Identifier LineNr NonText Normal String Title VertSplit Comment Constant Cursor Folded Function Keyword Number PreProc SpecialKey Special Statement StatusLineNC StatusLine Todo Type Visual Change the color of an Element highlight ELEMENT ctermfg = COLOR ctermbg = COLOR guifg = #abc123 guibg = #abc123 Select alternative colorschemes : colo [rscheme] < tab > Display all available colorschemes : colo < C - d > Clear custom color commands : highlight clear : hi clear","title":"Color"},{"location":"Text-editors/vim/#vim-plugins","text":"Vim 8 supports native loading of plugins (put in .vim/pack/xx/start/ where xx is an arbitrary directory name Set file format to Unix/DOS ref : set fileformat = unix : set fileformat = dos","title":"vim plugins"},{"location":"Text-editors/vim/#completion","text":"Context-aware completion Ctrl x Ctrl l Omni completion Ctrl x Ctrl o","title":"Completion"},{"location":"Web/","text":"\ud83c\udf0e Web Emoji The codepoint U+FE0F (\"variation select 16\") is placed immediately after an emoji to indicate it should indeed be rendered as an emoji. This is used for some ambiguous codepoints that can be rendered either as emoji or as older-style glyphs, such as the heart, arrows, etc. \u2665&#xFE0F; Firefox about:config font.name-list.emoji Font used for emoji","title":"\ud83c\udf0e Web"},{"location":"Web/#web","text":"","title":"\ud83c\udf0e Web"},{"location":"Web/#emoji","text":"The codepoint U+FE0F (\"variation select 16\") is placed immediately after an emoji to indicate it should indeed be rendered as an emoji. This is used for some ambiguous codepoints that can be rendered either as emoji or as older-style glyphs, such as the heart, arrows, etc. \u2665&#xFE0F;","title":"Emoji"},{"location":"Web/#firefox","text":"","title":"Firefox"},{"location":"Web/#aboutconfig","text":"font.name-list.emoji Font used for emoji","title":"about:config"},{"location":"Web/CSS/","text":"CSS Browser compatibility User agent stylesheets are bundled with the browser and contain default CSS rules in the absence of an external stylesheet. Their styles may be removed using a reset stylesheet. Attribute selectors Selectors enclosed in square brackets \u201c[\u201d are called attribute selectors, and define selectors by the presence of an attribute - a[href] selects anchor tags with an href attribute - a[href=\"#\"] selects anchor tags with an href attribute defined as \u201c#\u201d - a[href^=\"http\"] all anchors with href attribute that starts with \u201chttp\u201d - a[href$=\".com\"] all anchors with href attribute that ends with \u201c.com\u201d. - a[href*=\"volusion\"] all anchors with href attribute that contains the word volusion. Miscellaneous properties overflow controls how content overflows its container. Three possible values: - visible by default - hidden from view - scroll producing a scroll bar within the container visibility allows the content of elements to be hidden. The space reserved for the element will still be there. - visible by default - hidden display an element to be completely removed from the rendered webpage, unlike Visibility. - none - unset will reset the value, for example in responsive web design box-sizing defines how width and height of elements are calculated - content-box by default, width and height properties includes only content - border-box includes content, padding, and border BEM naming convention In the BEM (\u201cblock, element, modifier\u201d) naming convention a double hyphen \"--\" separates an element from its modifier while a double underscore \u201c__\u201d separates an element from its subelement. For example: - .person - .person__hand - .person--female - .person--female__hand - .person__hand--left Pseudo classes Dynamic selectors :link unvisited link :visited visited link :hover mouse hover :active when a user activates an element (between click and release of mouse button) Structural selectors :first-child first child of the parent element :last-child last child :nth-child(an+b) where a represents every second, third element if a was 2 or 3 and b represents the first element to start the subset Media Queries @media only screen and (max-width: 480px) \\{ ... \\} - @media begins a media query - only screen instructs the CSS compiler to apply the rules only to display devices (other options are print and handheld ) - and (max-width: 480px) called a media feature , CSS compiler is instructed to apply the rules to screens of less than 480 pixels - CSS rules follow in between curly braces, which will be applied if the media query is satisfied @media only screen and (max-width: 480px) and (min-resolution: 300dpi) \\{ ...\\} - and can be used to chain multiple conditions together @media only screen and (min-width: 480px), (orientation: landscape) \\{ ... \\} - comma (,) separates criteria either of which may be satisfied for the rules to take effect Positioning Adjusting the position of HTML elements with the following 5 properties: 1. position which changes the default position of an element - static by default - relative which places the element relative to its default static position by means of the 4 offset properties: top , bottom , left , and right - absolute all other elements will ignore the element, and the element will be positioned to its closest parent element, and will scroll with the rest of the page - fixed ignores user scrolling and places the element at a fixed location on the page (useful for navbars, for example). max-width:100% must be specified 2. display with 3 values: - inline - block - inline-block Some elements, such as a , strong , em , and a are inline by default, and their height and width are defined by their content. And some elements, such as p and h1 are block by default. But these values can be manipulated in CSS. Inline-block combines features of both inline and block. They appear next to each other but their width and height can be manipulated, such as images. 3. z-index accepts integer values to control the depth of elements: a higher value will bring the element to the front 4. float which will move elements as far left or right as possible. Width must be specified . 5. clear specifies how elements behave when they bump into each other on the left , right , both , or none sides Flexbox A new tool developed for CSS3 that simplifies the layout and positioning of some elements. the display property of the container must be set to : flex or inline-flex There are then 10 properties that specify how its children can behave. 1. justify-content (parent) with 5 values: flex-start , flex-end , center , space-around , space-between 2. align-items (parent) vertical alignment, similar to vertical justification: - stretch by default, child elements will stretch from top to bottom of parent, unless height is defined. min-height is permissible - flex-start , flex-end , center , baseline 3. flex-grow (child) allows items to grow. This property takes an integer value: higher values allow the element to grow larger. max-width takes precedence 0 by default 4. flex-shrink (child) allows items to shrink, similar to flex-grow. min-width takes precedence - 1 by default 5. flex-basis (child) allows the parent to be sized 6. flex (child) allows flex-grow, flex-shrink, and flex-basis to be defined in a single line, in that order 7. flex-wrap (parent) similar to text-wrapping, causing child elements to overflow to another line. 3 values: nowrap by default, wrap , wrap-reverse 8. align-content (parent) affects the behavior of child elements while wrapping similar to align-items: flex-start , flex-end , center , space-between , space-around , stretch 9. flex-direction (parent) allows flex containers to define how children are populated and which directions are assigned to the major and cross axes: row , row-reverse , column , column-reverse 10. flex-flow (parent) allows both flex-wrap and flex-direction properties to be declared on a single line Animation Simple transitions - transition-property specifies the property which is to be animated - transition-duration specifies animation length, in seconds ( s ) or milliseconds ( ms ) - transition-delay Timing function Default value is ease - ease-in starts slow, accelerates quickly, stops abruptly - ease-out begins abruptly, slows down, and ends slowly - ease-in-out starts slow, gets fast, ends slow - linear constant speed Transition declaration combines multiple declarations for brevity transition: transition-property transition-duration transition-timing-function (optional: transition-delay) Accessibility Accessible Rich Internet Applications (ARIA) describe guidelines to make webpages accessible to a broad audience. - Use semantic elements: article , aside , details , figcaption , figure , footer , header , main , mark , nav , section , summary , time . - role=\"complementary\" attribute and value help a screen reader to understand that the element contains information supporting other text - role=\"presentation\" suppresses a screen reader\u2019s reading of the name of the element when unnecessary (for ordered lists, for example) - ARIA properties such as aria-label=\"...\" identify the significance of ambiguous elements - alt attributes of some elements is built in (i.e. img). It can be empty but should always be present. Limit of 150 characters. Bootstrap Most popular front-end framework. CSS library used by developers to quickly build responsive websites - .navbar applied to top-level nav element - themes: .navbar-default , .navbar-inverse (dark), .navbar-brand applied to anchor element Positioning: .navbar-fixed-top , .navbar-fixed-bottom , .navbar-left , .navbar-right Jumbotron: showcase area directly beneath header, drawing attention to important site content - .jumbotron , .jumbotron-fluid Grid system All grid content must be wrapped in a .container or .container-fluid div - .row direct children of the .container have 12 columns - .col-md-\\# direct children of .row , spans # columns; .col-md-4 4 columns, .col-md-6 6 columns, etc - .col-xs-\\# applied to img , spans # columns Responsive design .col-xs-x when viewport is less than 768px .col-sm-x 768-991px .col-md-x 992-1199px .col-lg-x greater than 1199px Buttons .btn Color schemes: .btn-primary , .btn-secondary , .btn-success , .btn-info , .btn-warning , .btn-danger , .btn-link Low-Quality Image Placeholders (LQIP) in React devtips (a-74Zy9EfMQ) Wrap the image in a div <div className=\"imageWrapper\"> Use padding-bottom to define height in terms of width padding-bottom: 150%; Create logic to replace the placeholder image with the high-quality image upon load const image = { original: './path/original.jpg', optimized: './path/optimized.jpg', svg: './path/svg.svg' } componentDidMount() { const img img.src = image.original img.onload = function() { // Query the DOM document .querySelector('imageWrapper') .style['background-image'] = `url('${img.src}')` } } Ensure proper scaling background-size: 100%;","title":"CSS"},{"location":"Web/CSS/#css","text":"","title":"CSS"},{"location":"Web/CSS/#browser-compatibility","text":"User agent stylesheets are bundled with the browser and contain default CSS rules in the absence of an external stylesheet. Their styles may be removed using a reset stylesheet.","title":"Browser compatibility"},{"location":"Web/CSS/#attribute-selectors","text":"Selectors enclosed in square brackets \u201c[\u201d are called attribute selectors, and define selectors by the presence of an attribute - a[href] selects anchor tags with an href attribute - a[href=\"#\"] selects anchor tags with an href attribute defined as \u201c#\u201d - a[href^=\"http\"] all anchors with href attribute that starts with \u201chttp\u201d - a[href$=\".com\"] all anchors with href attribute that ends with \u201c.com\u201d. - a[href*=\"volusion\"] all anchors with href attribute that contains the word volusion.","title":"Attribute selectors"},{"location":"Web/CSS/#miscellaneous-properties","text":"overflow controls how content overflows its container. Three possible values: - visible by default - hidden from view - scroll producing a scroll bar within the container visibility allows the content of elements to be hidden. The space reserved for the element will still be there. - visible by default - hidden display an element to be completely removed from the rendered webpage, unlike Visibility. - none - unset will reset the value, for example in responsive web design box-sizing defines how width and height of elements are calculated - content-box by default, width and height properties includes only content - border-box includes content, padding, and border","title":"Miscellaneous properties"},{"location":"Web/CSS/#bem-naming-convention","text":"In the BEM (\u201cblock, element, modifier\u201d) naming convention a double hyphen \"--\" separates an element from its modifier while a double underscore \u201c__\u201d separates an element from its subelement. For example: - .person - .person__hand - .person--female - .person--female__hand - .person__hand--left","title":"BEM naming convention"},{"location":"Web/CSS/#pseudo-classes","text":"","title":"Pseudo classes"},{"location":"Web/CSS/#dynamic-selectors","text":":link unvisited link :visited visited link :hover mouse hover :active when a user activates an element (between click and release of mouse button)","title":"Dynamic selectors"},{"location":"Web/CSS/#structural-selectors","text":":first-child first child of the parent element :last-child last child :nth-child(an+b) where a represents every second, third element if a was 2 or 3 and b represents the first element to start the subset","title":"Structural selectors"},{"location":"Web/CSS/#media-queries","text":"@media only screen and (max-width: 480px) \\{ ... \\} - @media begins a media query - only screen instructs the CSS compiler to apply the rules only to display devices (other options are print and handheld ) - and (max-width: 480px) called a media feature , CSS compiler is instructed to apply the rules to screens of less than 480 pixels - CSS rules follow in between curly braces, which will be applied if the media query is satisfied @media only screen and (max-width: 480px) and (min-resolution: 300dpi) \\{ ...\\} - and can be used to chain multiple conditions together @media only screen and (min-width: 480px), (orientation: landscape) \\{ ... \\} - comma (,) separates criteria either of which may be satisfied for the rules to take effect","title":"Media Queries"},{"location":"Web/CSS/#positioning","text":"Adjusting the position of HTML elements with the following 5 properties: 1. position which changes the default position of an element - static by default - relative which places the element relative to its default static position by means of the 4 offset properties: top , bottom , left , and right - absolute all other elements will ignore the element, and the element will be positioned to its closest parent element, and will scroll with the rest of the page - fixed ignores user scrolling and places the element at a fixed location on the page (useful for navbars, for example). max-width:100% must be specified 2. display with 3 values: - inline - block - inline-block Some elements, such as a , strong , em , and a are inline by default, and their height and width are defined by their content. And some elements, such as p and h1 are block by default. But these values can be manipulated in CSS. Inline-block combines features of both inline and block. They appear next to each other but their width and height can be manipulated, such as images. 3. z-index accepts integer values to control the depth of elements: a higher value will bring the element to the front 4. float which will move elements as far left or right as possible. Width must be specified . 5. clear specifies how elements behave when they bump into each other on the left , right , both , or none sides","title":"Positioning"},{"location":"Web/CSS/#flexbox","text":"A new tool developed for CSS3 that simplifies the layout and positioning of some elements. the display property of the container must be set to : flex or inline-flex There are then 10 properties that specify how its children can behave. 1. justify-content (parent) with 5 values: flex-start , flex-end , center , space-around , space-between 2. align-items (parent) vertical alignment, similar to vertical justification: - stretch by default, child elements will stretch from top to bottom of parent, unless height is defined. min-height is permissible - flex-start , flex-end , center , baseline 3. flex-grow (child) allows items to grow. This property takes an integer value: higher values allow the element to grow larger. max-width takes precedence 0 by default 4. flex-shrink (child) allows items to shrink, similar to flex-grow. min-width takes precedence - 1 by default 5. flex-basis (child) allows the parent to be sized 6. flex (child) allows flex-grow, flex-shrink, and flex-basis to be defined in a single line, in that order 7. flex-wrap (parent) similar to text-wrapping, causing child elements to overflow to another line. 3 values: nowrap by default, wrap , wrap-reverse 8. align-content (parent) affects the behavior of child elements while wrapping similar to align-items: flex-start , flex-end , center , space-between , space-around , stretch 9. flex-direction (parent) allows flex containers to define how children are populated and which directions are assigned to the major and cross axes: row , row-reverse , column , column-reverse 10. flex-flow (parent) allows both flex-wrap and flex-direction properties to be declared on a single line","title":"Flexbox"},{"location":"Web/CSS/#animation","text":"Simple transitions - transition-property specifies the property which is to be animated - transition-duration specifies animation length, in seconds ( s ) or milliseconds ( ms ) - transition-delay Timing function Default value is ease - ease-in starts slow, accelerates quickly, stops abruptly - ease-out begins abruptly, slows down, and ends slowly - ease-in-out starts slow, gets fast, ends slow - linear constant speed Transition declaration combines multiple declarations for brevity transition: transition-property transition-duration transition-timing-function (optional: transition-delay)","title":"Animation"},{"location":"Web/CSS/#accessibility","text":"Accessible Rich Internet Applications (ARIA) describe guidelines to make webpages accessible to a broad audience. - Use semantic elements: article , aside , details , figcaption , figure , footer , header , main , mark , nav , section , summary , time . - role=\"complementary\" attribute and value help a screen reader to understand that the element contains information supporting other text - role=\"presentation\" suppresses a screen reader\u2019s reading of the name of the element when unnecessary (for ordered lists, for example) - ARIA properties such as aria-label=\"...\" identify the significance of ambiguous elements - alt attributes of some elements is built in (i.e. img). It can be empty but should always be present. Limit of 150 characters.","title":"Accessibility"},{"location":"Web/CSS/#bootstrap","text":"Most popular front-end framework. CSS library used by developers to quickly build responsive websites - .navbar applied to top-level nav element - themes: .navbar-default , .navbar-inverse (dark), .navbar-brand applied to anchor element Positioning: .navbar-fixed-top , .navbar-fixed-bottom , .navbar-left , .navbar-right Jumbotron: showcase area directly beneath header, drawing attention to important site content - .jumbotron , .jumbotron-fluid","title":"Bootstrap"},{"location":"Web/CSS/#grid-system","text":"All grid content must be wrapped in a .container or .container-fluid div - .row direct children of the .container have 12 columns - .col-md-\\# direct children of .row , spans # columns; .col-md-4 4 columns, .col-md-6 6 columns, etc - .col-xs-\\# applied to img , spans # columns","title":"Grid system"},{"location":"Web/CSS/#responsive-design","text":".col-xs-x when viewport is less than 768px .col-sm-x 768-991px .col-md-x 992-1199px .col-lg-x greater than 1199px","title":"Responsive design"},{"location":"Web/CSS/#buttons","text":".btn Color schemes: .btn-primary , .btn-secondary , .btn-success , .btn-info , .btn-warning , .btn-danger , .btn-link","title":"Buttons"},{"location":"Web/CSS/#low-quality-image-placeholders-lqip-in-react","text":"devtips (a-74Zy9EfMQ) Wrap the image in a div <div className=\"imageWrapper\"> Use padding-bottom to define height in terms of width padding-bottom: 150%; Create logic to replace the placeholder image with the high-quality image upon load const image = { original: './path/original.jpg', optimized: './path/optimized.jpg', svg: './path/svg.svg' } componentDidMount() { const img img.src = image.original img.onload = function() { // Query the DOM document .querySelector('imageWrapper') .style['background-image'] = `url('${img.src}')` } } Ensure proper scaling background-size: 100%;","title":"Low-Quality Image Placeholders (LQIP) in React"},{"location":"Web/mkdocs/","text":"Supporting emoji ( src ) mkdocs.yml markdown_extensions: - pymdownx.emoji: # from https://squidfunk.github.io/mkdocs-material/reference/icons-emojis/#emoji emoji_index: !!python/name:materialx.emoji.twemoji emoji_generator: !!python/name:materialx.emoji.to_svg","title":"Mkdocs"},{"location":"Web/JavaScript/","text":"Axios Accessing RESTful web services and HTTP APIs in JavaScript. Inspired by the $http service in Angular, and development of Axios was an effort to provide a similar standalone service for use outside of Angular. axios .get(url) .then(response => console.log(response)) .catch(error => console.log(error)) Installation $ npm install axios cdn link: https://unpkg.com/axios/dist/axios.min.js Tutorial GET request for a given user ID: const axios = require ( 'axios' ) axios . get ( '/user?ID=12345' ) // Request user with given ID . then ( function ( res ) { console . log ( res ) // Handle success }) . catch ( function ( err ) { console . log ( err ) // Handle error } ) . then ( function () {}) // Always executed Alternatively axios . get ( 'user' , { params : { ID : 12345 } } ) . then ( function ( response ) { console . log ( response ) }) . catch ( function ( error ) { console . log ( error ) }) . then ( function () {}) Blade Templating engine used in Laravel Directives @section('component') Used to define a section of content @yield('variable') Used to display the contents of a given section @endsection @parent append content, rather than overwrite Gulp Toolkit for automating time-consuming tasks in development workflow - const gulp = require('gulp'); - gulp-uglify const uglify = require('gulp-uglify'); gulp . task ( 'scripts' , function () { gulp . src ( 'src/*.js' ) . pipe ( uglify ()) . pipe ( gulp . dest ( 'dist' )); } ); Laravel PHP framework for the development of web applications following the model-view-controller architectural pattern. Developed as a more advanced alternative to the CodeIgniter framework. Installation XAMPP, MAMP (Mac), or WAMP (Windows) to ensure the Apache server environment Composer dependency manager for PHP, similar to npm . It will want to know where the PHP.exe file is, which will be in the directory installed of the server environment installed above. VS Code, Git Bash, Git Initialization composer create-project laravel/laravel lsapp cd lsapp This project will be accessible at localhost/lsapp/ . This is a security issue and must be fixed: Edit C:/xampp/apache/conf/extra/httpd-vhosts.conf (if using XAMPP) Uncomment the opening and closing tags for the VirtualHost tag Uncomment the DocumentRoot line, changing the path to the public directory Uncomment the ServerName line, which can be named arbitrarily, e.g. lsapp.dev Add another VirtualHost pointing to the htdocs directory (within which lsapp was created) and set ServerName to localhost Open Notepad as an administrator Navigate to C:/Windows/System32/drivers/etc , view All files , and open hosts Add 127.0.0.1 localhost Add 127.0.0.1 lsapp.dev Stop and then start the Apache server Folder structure public/ frontend of the application app/ all models go in this folder, including User.php Http/Controllers/ contains all controllers resources/views/ all Laravel views use the Blade template engine routes routing middleware can be installed here config/database.php contains settings Syntax namespace keyword assigns an identifier Illuminate refers to Laravel core TypeScript Programming language developed and maintained by Microsoft, a strict syntactical superset of JavaScript which supports generic programming static typing using annotations, including number , boolean , string , and any . These declarations can be exported to a declarations file. function add ( left : number , right : number ) : number { return left + right ; } Promises A promise is the eventual result of an asynchronous operation. Stages: Wrapping then -ing Catching Chaining Promises can be in one of 4 states: fulfilled : action relating to the promise succeeded reject : action relating to the promise failed pending : has not fulfilled or rejected yet settled : has fulfilled or rejected Promises vs. events an event listener registered after an event has occurred will never fire an action set for resolution can fire after a promise has already resolved an event can fire many times, but promises can only settle once Promise is a try/catch wrapper around code that will finish at an unpredictable time A Promise takes a function as argument. That function takes two arguments: resolve and reject . They are both callbacks that will execute when the promise has succeeded or failed any argument passed to resolve or reject , will then be received by then or catch methods, respectively resolve leads to the next then in the chain, and reject leads to the next catch Glossary Babel JavaScript preprocessor","title":"Axios"},{"location":"Web/JavaScript/#axios","text":"Accessing RESTful web services and HTTP APIs in JavaScript. Inspired by the $http service in Angular, and development of Axios was an effort to provide a similar standalone service for use outside of Angular. axios .get(url) .then(response => console.log(response)) .catch(error => console.log(error))","title":"Axios"},{"location":"Web/JavaScript/#installation","text":"$ npm install axios cdn link: https://unpkg.com/axios/dist/axios.min.js","title":"Installation"},{"location":"Web/JavaScript/#tutorial","text":"GET request for a given user ID: const axios = require ( 'axios' ) axios . get ( '/user?ID=12345' ) // Request user with given ID . then ( function ( res ) { console . log ( res ) // Handle success }) . catch ( function ( err ) { console . log ( err ) // Handle error } ) . then ( function () {}) // Always executed Alternatively axios . get ( 'user' , { params : { ID : 12345 } } ) . then ( function ( response ) { console . log ( response ) }) . catch ( function ( error ) { console . log ( error ) }) . then ( function () {})","title":"Tutorial"},{"location":"Web/JavaScript/#blade","text":"Templating engine used in Laravel","title":"Blade"},{"location":"Web/JavaScript/#directives","text":"@section('component') Used to define a section of content @yield('variable') Used to display the contents of a given section @endsection @parent append content, rather than overwrite","title":"Directives"},{"location":"Web/JavaScript/#gulp","text":"Toolkit for automating time-consuming tasks in development workflow - const gulp = require('gulp'); - gulp-uglify const uglify = require('gulp-uglify'); gulp . task ( 'scripts' , function () { gulp . src ( 'src/*.js' ) . pipe ( uglify ()) . pipe ( gulp . dest ( 'dist' )); } );","title":"Gulp"},{"location":"Web/JavaScript/#laravel","text":"PHP framework for the development of web applications following the model-view-controller architectural pattern. Developed as a more advanced alternative to the CodeIgniter framework. Installation XAMPP, MAMP (Mac), or WAMP (Windows) to ensure the Apache server environment Composer dependency manager for PHP, similar to npm . It will want to know where the PHP.exe file is, which will be in the directory installed of the server environment installed above. VS Code, Git Bash, Git Initialization composer create-project laravel/laravel lsapp cd lsapp This project will be accessible at localhost/lsapp/ . This is a security issue and must be fixed: Edit C:/xampp/apache/conf/extra/httpd-vhosts.conf (if using XAMPP) Uncomment the opening and closing tags for the VirtualHost tag Uncomment the DocumentRoot line, changing the path to the public directory Uncomment the ServerName line, which can be named arbitrarily, e.g. lsapp.dev Add another VirtualHost pointing to the htdocs directory (within which lsapp was created) and set ServerName to localhost Open Notepad as an administrator Navigate to C:/Windows/System32/drivers/etc , view All files , and open hosts Add 127.0.0.1 localhost Add 127.0.0.1 lsapp.dev Stop and then start the Apache server","title":"Laravel"},{"location":"Web/JavaScript/#folder-structure","text":"public/ frontend of the application app/ all models go in this folder, including User.php Http/Controllers/ contains all controllers resources/views/ all Laravel views use the Blade template engine routes routing middleware can be installed here config/database.php contains settings","title":"Folder structure"},{"location":"Web/JavaScript/#syntax","text":"namespace keyword assigns an identifier Illuminate refers to Laravel core","title":"Syntax"},{"location":"Web/JavaScript/#typescript","text":"Programming language developed and maintained by Microsoft, a strict syntactical superset of JavaScript which supports generic programming static typing using annotations, including number , boolean , string , and any . These declarations can be exported to a declarations file. function add ( left : number , right : number ) : number { return left + right ; }","title":"TypeScript"},{"location":"Web/JavaScript/#promises","text":"A promise is the eventual result of an asynchronous operation. Stages: Wrapping then -ing Catching Chaining Promises can be in one of 4 states: fulfilled : action relating to the promise succeeded reject : action relating to the promise failed pending : has not fulfilled or rejected yet settled : has fulfilled or rejected Promises vs. events an event listener registered after an event has occurred will never fire an action set for resolution can fire after a promise has already resolved an event can fire many times, but promises can only settle once Promise is a try/catch wrapper around code that will finish at an unpredictable time A Promise takes a function as argument. That function takes two arguments: resolve and reject . They are both callbacks that will execute when the promise has succeeded or failed any argument passed to resolve or reject , will then be received by then or catch methods, respectively resolve leads to the next then in the chain, and reject leads to the next catch","title":"Promises"},{"location":"Web/JavaScript/#glossary","text":"","title":"Glossary"},{"location":"Web/JavaScript/#babel","text":"JavaScript preprocessor","title":"Babel"},{"location":"Web/JavaScript/Express/","text":"const express = require ( 'express' ) const app = express () app . get ( '/' , ( req , res ) => res . send ( 'Hello world!' )) app . listen ( 3000 , () => console . log ( 'Server ready' ))","title":"Express"},{"location":"Web/JavaScript/React/","text":"React Most popular JavaScript library for building user interfaces, built by Facebook in 2011, competing with Angular and Vue . Every React application is a tree of components : independent, isolated, and reusable pieces of UI. React Elements map to DOM elements maintained in memory ( Virtual DOM ). React state changes are then updated to the real DOM. React is a library Angular is a framework sudo npm i -g create-react-app@1.5.2 # Create ./src/App.js create-react-app $APPNAME # Launch development server at http://localhost:3000/ npm start // index.js import React from 'react' ; import ReactDOM from 'react-dom' ; const element = < h1 > Hello World < /h1>; console . log ( element ); Counter app create-react-app counter-app Create counter-app Ctrl+` to open terminal within Code npm i bootstrap@4.1.1 Install Bootstrap into the app Cmd+P to open file explorer Find index.js Append import 'bootstrap/dist/css/bootstrap.css' to import statements at top Creating a component By convention, components are added to the components directory Using the Simple React Snippets extension for Code allows convenient expansion: - imrc : Import React/Component - cc : Class Component React is being imported because the JSX expression will be compiled into a React object. counter.jsx import React, { Component } from 'react'; class Counter extends Component { render() { return ( <h1>Hello World</h1> ); }} export default Counter; index.js ... import Counter from './components/counter'; ReactDOM.render(<Counter />, document.getElementById('root')); ... Embedding expressions A button is added into the HTML within counter.jsx counter.jsx import React, { Component } from 'react'; class Counter extends Component { render() { return ( <div><h1>Hello World</h1><button>Increment</button></div> ); } } export default Counter; Upon compile, the JSX expression within render() is converted to a statement using the React.createElement() method, which takes two arguments. Replacing <div> with <React.Fragment> - Cmd+D to select all instances - Shift+Option+F to format document Instead of hard-coding \u201cHello World\u201d, we can display a dynamic value. We do this at the component level using the state property, which is an object. counter.jsx import React, { Component } from \"react\"; class Counter extends Component { state = { count: 0 }; render() { return ( <React.Fragment> <span>{this.state.count}</span> <button>Increment</button> </React.Fragment> ); }} export default Counter; In between those curly braces, we can write any valid JavaScript expression, even arithmetic. Adding a formatCount() method which will return the string \u2018Zero\u2019 if count is zero. counter.jsx import React, { Component } from \"react\"; class Counter extends Component { state = { count: 0 }; render() { return ( <React.Fragment> <span>{this.formatCount()}</span> <button>Increment</button> </React.Fragment> ); } formatCount() { const { count } = this.state; return count === 0 ? 'Zero' : count; }} export default Counter; Alternatively, instead of returning plaintext, we can return a JSX expression with h1: return count === 0 ? <h1>Zero</h1> : count; Setting attributes HTML attributes can also render dynamically imported data: counter.jsx import React, { Component } from \"react\"; class Counter extends Component { state = { count: 0, imageUrl: 'https://picsum.photos/200' }; render() { return ( <React.Fragment> <img src={ this.state.imageUrl } alt=\"\"/> <span>{this.formatCount()}</span> <button>Increment</button> </React.Fragment> ); } formatCount() { const { count } = this.state; return count === 0 ? 'Zero' : count; } } export default Counter; Specifying HTML class requires className because Class is a reserved word in JavaScript. ... render() { return ( <React.Fragment> <span className=\"badge badge-primary m-2\">{this.formatCount()}</span> <button className=\"btn btn-secondary btn-sm\">Increment</button> </React.Fragment> ); } CSS styling can be specified in the same way, passing an object in as an HTML attribute ... styles = { fontSize: 50, fontWeight: 'bold' } render() { return ( <React.Fragment> <span style = {this.styles} className=\"badge badge-primary m-2\">{this.formatCount()}</span> <button className=\"btn btn-secondary btn-sm\">Increment</button> </React.Fragment> ); } ... Alternatively, CSS can be defined inline by using double curly braces: ...<span style={{fontSize: 30}}>... Rendering classes dynamically CSS classes can be edited using simple string concatenation techniques that key off of conditional statements ... render() { let classes = \"badge m-2 badge-\"; classes += this.state.count === 0 ? \"warning\" : \"primary\"; return ( <React.Fragment> <span className={classes}>{this.formatCount()}</span> ... The lines associated with classes cause the render() method to become bloated. We can refactor within Code using Ctrl+Shift+R, which extracts the two lines to a new method. counter.jsx ... getBadgeClasses() { let classes = \"badge m-2 badge-\"; classes += this.state.count === 0 ? \"warning\" : \"primary\"; return classes; } ... Rendering lists Let\u2019s see how to render a list of tags. Loops do not exist as a concept within JSX, but we can use the .map() method with arrays. counter.jsx ... state = { count: 0, tags: ['tag1','tag2','tag3'] }; ... <ul> { this.state.tags.map(tag => <li>{tag}</li>)} </ul> ... This produces an error in the console, because React wants each DOM element to have a distinct key value. For now, we can use the same tag as a key. <ul> { this.state.tags.map(tag => <li key ={tag}>{ tag}</li>)} </ul> Conditional rendering Unlike Angular, JSX is not a templating engine, so there are no conditionals. We can use JavaScript outside of the JSX expression and pass that into the expression. counter.jsx Another technique is to take advantage of truthy expressions and how JavaScript evaluates the logical AND operator. Handling events Properties based on standard DOM events placed within JSX expressions (note camelcase): - onClick() - onDoubleClick - onKeyDown - onKeyUp - onKeyPress counter.jsx ... handleIncrement() { console.log('Increment clicked') } ... <button onClick={this.handleIncrement} ... Attempting to log this.state.count produces an error, revealing that count is not available to the method. Binding event handlers JavaScript will return undefined if this is used in strict mode in a standalone function without an object reference. To solve this, we use the bind method. We have to add a constructor for every event handler constructor() { super(); this.handleIncrement = this.handleIncrement.bind(this); } Alternatively, an arrow function can be used because they inherit the this object. counter.jsx handleIncrement = () => { console.log(\"Increment Clicked\", this\") }; Updating state State cannot be updated directly, e.g. this.state.count++; is ineffective. React must be told explicitly what part of the DOM has been changed, unlike Angular. counter.jsx handleIncrement = () => { this.state.count++; this.setState({ count: this.state.count + 1 }); } To solve this problem we use of the setstate() method available in the Component class (from which Counter inherits). handleIncrement = () => { this.setState( { count: this.state.count + 1 }); }; What happens when state changes When state changes, an asynchronous call is made to the render method. React compares the updated virtual DOM with the old, real DOM and updates only the DOM nodes which need updating. Pass arguments using arrow functions Earlier, we saw that the onClick() method only takes a function reference, not a full function call with parameters. We can declare a new method named .doHandleIncrement() which acts as a wrapper for .handleIncrement() , passing an argument. counter.jsx doHandleIncrement = () => { this.handleIncrement({ id: 1}) } But this is wasteful. Better is defining an inline function. ... <button onClick={ () => this.handleIncrement(product)} className=\"btn btn-secondary btn-sm\" > ... Summary JSX (JavaScript XML) Rendering lists Conditional rendering Handling events Updating state Composing components Changing Counter component to Counters Every React component has a property called props Index of React commands and methods .setstate() JavaScript for React Developers let vs var vs const Variables declared with let are block-scoped. example function sayHello() { for (let i = 0; i < 5; i++) { console.log(i); }>) console.log(i) // undefined } sayHello(); Objects example const person = { name: 'Mosh', walk: function() {}, talk() {} // Alternative way of declaring a method } person.talk(); const targetMember = 'name'; person[targetMember.value] = 'John' this keyword this can be made explicit by using the bind method, which will avoid complications of using this in top-level function calls. Functions are objects in JavaScript. example const person = { name: 'Mosh', walk() { console.log(this); }, } person.walk(); // person {} const walk = person.walk.bind(person) In React, strict mode is enabled by default, so the this keyword returns undefined . example const person = { name: 'Mosh', walk() { console.log(this); }, } person.walk(); // person {} const walk = person.walk(); walk(); // undefined Arrow functions Arrow functions allow a good way to clean up code for the simplest functions. const square = function(number) { return number * number } const square = number => number * number; But they do not rebind the this object for callback functions. Array.map() One of the new array methods defined in ECMAScript 6, .map() is very useful in rendering lists. const colors = ['red','green','blue'] const items = colors.map(color => <li>${color}</li> ) }) Object destructuring const address = { street: '', city: '', country: '' } const street = address.street; const city = address.city; const country = address.country; The repetitive use of the object name address is a good object to use destructuring syntax . const {street, city, country } = address; const {street: st } = address; Classes When we have an object with at least one method, we use classes to ensure that child objects do not reduplicate code unnecessarily. const person = { name: 'Mosh', walk() { constole.log('walk') // console is mispelled }} const person2 = { name: 'Mosh', walk() { constole.log('walk') // copying reduplicates the error }} class Person { constructor(name) { this.name = name; } walk() { console.log('walk'); }} const person = new Person('Mosh'); Inheritance class Person { constructor(name) { this.name = name; } walk() { console.log('walk'); } } class Teacher { // Teacher should also be able to walk, teach() { // but we don't want to reduplicate code... console.log('teach'); } } class Teacher extends Person { constructor(name, degree) { super(name) // references parent class this.degree = degree } teach() { console.log('teach') } } const teacher = new Teacher ('Mosh', 'MSc') Modules Splitting code across multiple files, each of which is called a module . Class keyword must be prefixed by export person.js export class Person { constructor(name) { this.name = name; } walk() { console.log('walk'); } } teacher.js import { Person } from './person' // extension is not added export class Teacher extends Person { constructor(name, degree) { super(name) this.degree = degree } teach() { console.log('teach') } } index.js import Teacher from './teacher' const teacher = new Teacher ('Mosh', 'MSc') Named and default exports default keyword to export an entire object, after export and before class named exports to export functions using the export keyword named exports must be individually named and placed within braces React Native React Native is like react, but it uses native components instead of web components as building blocks, allowing true mobile apps to be programmed. import React , { Component } from 'react' ; import { Text , View } from 'react-native' ; export default class HelloWorldApp extends Component { render () { return ( < View > < Text > Hello world !< /Text> < /View> ); } } Parameters associated with customizing components are called props that appear similar to HTML attributes: < Greeting name = 'Jaina' /> < Greeting name = 'Sanjay' /> ` Two types of data control a component: 1. props set by the parent are fixed throughout the lifetime of a component 2. state data that changes","title":"React"},{"location":"Web/JavaScript/React/#react","text":"Most popular JavaScript library for building user interfaces, built by Facebook in 2011, competing with Angular and Vue . Every React application is a tree of components : independent, isolated, and reusable pieces of UI. React Elements map to DOM elements maintained in memory ( Virtual DOM ). React state changes are then updated to the real DOM. React is a library Angular is a framework sudo npm i -g create-react-app@1.5.2 # Create ./src/App.js create-react-app $APPNAME # Launch development server at http://localhost:3000/ npm start // index.js import React from 'react' ; import ReactDOM from 'react-dom' ; const element = < h1 > Hello World < /h1>; console . log ( element );","title":"React"},{"location":"Web/JavaScript/React/#counter-app","text":"create-react-app counter-app Create counter-app Ctrl+` to open terminal within Code npm i bootstrap@4.1.1 Install Bootstrap into the app Cmd+P to open file explorer Find index.js Append import 'bootstrap/dist/css/bootstrap.css' to import statements at top","title":"Counter app"},{"location":"Web/JavaScript/React/#creating-a-component","text":"By convention, components are added to the components directory Using the Simple React Snippets extension for Code allows convenient expansion: - imrc : Import React/Component - cc : Class Component React is being imported because the JSX expression will be compiled into a React object.","title":"Creating a component"},{"location":"Web/JavaScript/React/#counterjsx","text":"import React, { Component } from 'react'; class Counter extends Component { render() { return ( <h1>Hello World</h1> ); }} export default Counter;","title":"counter.jsx"},{"location":"Web/JavaScript/React/#indexjs","text":"... import Counter from './components/counter'; ReactDOM.render(<Counter />, document.getElementById('root')); ...","title":"index.js"},{"location":"Web/JavaScript/React/#embedding-expressions","text":"A button is added into the HTML within counter.jsx","title":"Embedding expressions"},{"location":"Web/JavaScript/React/#counterjsx_1","text":"import React, { Component } from 'react'; class Counter extends Component { render() { return ( <div><h1>Hello World</h1><button>Increment</button></div> ); } } export default Counter; Upon compile, the JSX expression within render() is converted to a statement using the React.createElement() method, which takes two arguments. Replacing <div> with <React.Fragment> - Cmd+D to select all instances - Shift+Option+F to format document Instead of hard-coding \u201cHello World\u201d, we can display a dynamic value. We do this at the component level using the state property, which is an object.","title":"counter.jsx"},{"location":"Web/JavaScript/React/#counterjsx_2","text":"import React, { Component } from \"react\"; class Counter extends Component { state = { count: 0 }; render() { return ( <React.Fragment> <span>{this.state.count}</span> <button>Increment</button> </React.Fragment> ); }} export default Counter; In between those curly braces, we can write any valid JavaScript expression, even arithmetic. Adding a formatCount() method which will return the string \u2018Zero\u2019 if count is zero.","title":"counter.jsx"},{"location":"Web/JavaScript/React/#counterjsx_3","text":"import React, { Component } from \"react\"; class Counter extends Component { state = { count: 0 }; render() { return ( <React.Fragment> <span>{this.formatCount()}</span> <button>Increment</button> </React.Fragment> ); } formatCount() { const { count } = this.state; return count === 0 ? 'Zero' : count; }} export default Counter; Alternatively, instead of returning plaintext, we can return a JSX expression with h1: return count === 0 ? <h1>Zero</h1> : count;","title":"counter.jsx"},{"location":"Web/JavaScript/React/#setting-attributes","text":"HTML attributes can also render dynamically imported data:","title":"Setting attributes"},{"location":"Web/JavaScript/React/#counterjsx_4","text":"import React, { Component } from \"react\"; class Counter extends Component { state = { count: 0, imageUrl: 'https://picsum.photos/200' }; render() { return ( <React.Fragment> <img src={ this.state.imageUrl } alt=\"\"/> <span>{this.formatCount()}</span> <button>Increment</button> </React.Fragment> ); } formatCount() { const { count } = this.state; return count === 0 ? 'Zero' : count; } } export default Counter; Specifying HTML class requires className because Class is a reserved word in JavaScript. ... render() { return ( <React.Fragment> <span className=\"badge badge-primary m-2\">{this.formatCount()}</span> <button className=\"btn btn-secondary btn-sm\">Increment</button> </React.Fragment> ); } CSS styling can be specified in the same way, passing an object in as an HTML attribute ... styles = { fontSize: 50, fontWeight: 'bold' } render() { return ( <React.Fragment> <span style = {this.styles} className=\"badge badge-primary m-2\">{this.formatCount()}</span> <button className=\"btn btn-secondary btn-sm\">Increment</button> </React.Fragment> ); } ... Alternatively, CSS can be defined inline by using double curly braces: ...<span style={{fontSize: 30}}>...","title":"counter.jsx"},{"location":"Web/JavaScript/React/#rendering-classes-dynamically","text":"CSS classes can be edited using simple string concatenation techniques that key off of conditional statements ... render() { let classes = \"badge m-2 badge-\"; classes += this.state.count === 0 ? \"warning\" : \"primary\"; return ( <React.Fragment> <span className={classes}>{this.formatCount()}</span> ... The lines associated with classes cause the render() method to become bloated. We can refactor within Code using Ctrl+Shift+R, which extracts the two lines to a new method.","title":"Rendering classes dynamically"},{"location":"Web/JavaScript/React/#counterjsx_5","text":"... getBadgeClasses() { let classes = \"badge m-2 badge-\"; classes += this.state.count === 0 ? \"warning\" : \"primary\"; return classes; } ...","title":"counter.jsx"},{"location":"Web/JavaScript/React/#rendering-lists","text":"Let\u2019s see how to render a list of tags. Loops do not exist as a concept within JSX, but we can use the .map() method with arrays.","title":"Rendering lists"},{"location":"Web/JavaScript/React/#counterjsx_6","text":"... state = { count: 0, tags: ['tag1','tag2','tag3'] }; ... <ul> { this.state.tags.map(tag => <li>{tag}</li>)} </ul> ... This produces an error in the console, because React wants each DOM element to have a distinct key value. For now, we can use the same tag as a key. <ul> { this.state.tags.map(tag => <li key ={tag}>{ tag}</li>)} </ul>","title":"counter.jsx"},{"location":"Web/JavaScript/React/#conditional-rendering","text":"Unlike Angular, JSX is not a templating engine, so there are no conditionals. We can use JavaScript outside of the JSX expression and pass that into the expression.","title":"Conditional rendering"},{"location":"Web/JavaScript/React/#counterjsx_7","text":"Another technique is to take advantage of truthy expressions and how JavaScript evaluates the logical AND operator.","title":"counter.jsx"},{"location":"Web/JavaScript/React/#handling-events","text":"Properties based on standard DOM events placed within JSX expressions (note camelcase): - onClick() - onDoubleClick - onKeyDown - onKeyUp - onKeyPress","title":"Handling events"},{"location":"Web/JavaScript/React/#counterjsx_8","text":"... handleIncrement() { console.log('Increment clicked') } ... <button onClick={this.handleIncrement} ... Attempting to log this.state.count produces an error, revealing that count is not available to the method.","title":"counter.jsx"},{"location":"Web/JavaScript/React/#binding-event-handlers","text":"JavaScript will return undefined if this is used in strict mode in a standalone function without an object reference. To solve this, we use the bind method. We have to add a constructor for every event handler constructor() { super(); this.handleIncrement = this.handleIncrement.bind(this); } Alternatively, an arrow function can be used because they inherit the this object.","title":"Binding event handlers"},{"location":"Web/JavaScript/React/#counterjsx_9","text":"handleIncrement = () => { console.log(\"Increment Clicked\", this\") };","title":"counter.jsx"},{"location":"Web/JavaScript/React/#updating-state","text":"State cannot be updated directly, e.g. this.state.count++; is ineffective. React must be told explicitly what part of the DOM has been changed, unlike Angular.","title":"Updating state"},{"location":"Web/JavaScript/React/#counterjsx_10","text":"handleIncrement = () => { this.state.count++; this.setState({ count: this.state.count + 1 }); } To solve this problem we use of the setstate() method available in the Component class (from which Counter inherits). handleIncrement = () => { this.setState( { count: this.state.count + 1 }); };","title":"counter.jsx"},{"location":"Web/JavaScript/React/#what-happens-when-state-changes","text":"When state changes, an asynchronous call is made to the render method. React compares the updated virtual DOM with the old, real DOM and updates only the DOM nodes which need updating.","title":"What happens when state changes"},{"location":"Web/JavaScript/React/#pass-arguments-using-arrow-functions","text":"Earlier, we saw that the onClick() method only takes a function reference, not a full function call with parameters. We can declare a new method named .doHandleIncrement() which acts as a wrapper for .handleIncrement() , passing an argument.","title":"Pass arguments using arrow functions"},{"location":"Web/JavaScript/React/#counterjsx_11","text":"doHandleIncrement = () => { this.handleIncrement({ id: 1}) } But this is wasteful. Better is defining an inline function. ... <button onClick={ () => this.handleIncrement(product)} className=\"btn btn-secondary btn-sm\" > ...","title":"counter.jsx"},{"location":"Web/JavaScript/React/#summary","text":"JSX (JavaScript XML) Rendering lists Conditional rendering Handling events Updating state","title":"Summary"},{"location":"Web/JavaScript/React/#composing-components","text":"Changing Counter component to Counters Every React component has a property called props","title":"Composing components"},{"location":"Web/JavaScript/React/#index-of-react-commands-and-methods","text":".setstate()","title":"Index of React commands and methods"},{"location":"Web/JavaScript/React/#javascript-for-react-developers","text":"","title":"JavaScript for React Developers"},{"location":"Web/JavaScript/React/#let-vs-var-vs-const","text":"Variables declared with let are block-scoped.","title":"let vs var vs const"},{"location":"Web/JavaScript/React/#example","text":"function sayHello() { for (let i = 0; i < 5; i++) { console.log(i); }>) console.log(i) // undefined } sayHello();","title":"example"},{"location":"Web/JavaScript/React/#objects","text":"","title":"Objects"},{"location":"Web/JavaScript/React/#example_1","text":"const person = { name: 'Mosh', walk: function() {}, talk() {} // Alternative way of declaring a method } person.talk(); const targetMember = 'name'; person[targetMember.value] = 'John'","title":"example"},{"location":"Web/JavaScript/React/#this-keyword","text":"this can be made explicit by using the bind method, which will avoid complications of using this in top-level function calls. Functions are objects in JavaScript.","title":"this keyword"},{"location":"Web/JavaScript/React/#example_2","text":"const person = { name: 'Mosh', walk() { console.log(this); }, } person.walk(); // person {} const walk = person.walk.bind(person) In React, strict mode is enabled by default, so the this keyword returns undefined .","title":"example"},{"location":"Web/JavaScript/React/#example_3","text":"const person = { name: 'Mosh', walk() { console.log(this); }, } person.walk(); // person {} const walk = person.walk(); walk(); // undefined","title":"example"},{"location":"Web/JavaScript/React/#arrow-functions","text":"Arrow functions allow a good way to clean up code for the simplest functions. const square = function(number) { return number * number } const square = number => number * number; But they do not rebind the this object for callback functions.","title":"Arrow functions"},{"location":"Web/JavaScript/React/#arraymap","text":"One of the new array methods defined in ECMAScript 6, .map() is very useful in rendering lists. const colors = ['red','green','blue'] const items = colors.map(color => <li>${color}</li> ) })","title":"Array.map()"},{"location":"Web/JavaScript/React/#object-destructuring","text":"const address = { street: '', city: '', country: '' } const street = address.street; const city = address.city; const country = address.country; The repetitive use of the object name address is a good object to use destructuring syntax . const {street, city, country } = address; const {street: st } = address;","title":"Object destructuring"},{"location":"Web/JavaScript/React/#classes","text":"When we have an object with at least one method, we use classes to ensure that child objects do not reduplicate code unnecessarily. const person = { name: 'Mosh', walk() { constole.log('walk') // console is mispelled }} const person2 = { name: 'Mosh', walk() { constole.log('walk') // copying reduplicates the error }} class Person { constructor(name) { this.name = name; } walk() { console.log('walk'); }} const person = new Person('Mosh');","title":"Classes"},{"location":"Web/JavaScript/React/#inheritance","text":"class Person { constructor(name) { this.name = name; } walk() { console.log('walk'); } } class Teacher { // Teacher should also be able to walk, teach() { // but we don't want to reduplicate code... console.log('teach'); } } class Teacher extends Person { constructor(name, degree) { super(name) // references parent class this.degree = degree } teach() { console.log('teach') } } const teacher = new Teacher ('Mosh', 'MSc')","title":"Inheritance"},{"location":"Web/JavaScript/React/#modules","text":"Splitting code across multiple files, each of which is called a module . Class keyword must be prefixed by export person.js export class Person { constructor(name) { this.name = name; } walk() { console.log('walk'); } } teacher.js import { Person } from './person' // extension is not added export class Teacher extends Person { constructor(name, degree) { super(name) this.degree = degree } teach() { console.log('teach') } }","title":"Modules"},{"location":"Web/JavaScript/React/#indexjs_1","text":"import Teacher from './teacher' const teacher = new Teacher ('Mosh', 'MSc')","title":"index.js"},{"location":"Web/JavaScript/React/#named-and-default-exports","text":"default keyword to export an entire object, after export and before class named exports to export functions using the export keyword named exports must be individually named and placed within braces","title":"Named and default exports"},{"location":"Web/JavaScript/React/#react-native","text":"React Native is like react, but it uses native components instead of web components as building blocks, allowing true mobile apps to be programmed. import React , { Component } from 'react' ; import { Text , View } from 'react-native' ; export default class HelloWorldApp extends Component { render () { return ( < View > < Text > Hello world !< /Text> < /View> ); } } Parameters associated with customizing components are called props that appear similar to HTML attributes: < Greeting name = 'Jaina' /> < Greeting name = 'Sanjay' /> ` Two types of data control a component: 1. props set by the parent are fixed throughout the lifetime of a component 2. state data that changes","title":"React Native"},{"location":"Web/JavaScript/Web%20Components/","text":"Web Components is a suite of technologies allowing you to create reusable custom elements 3 main technologies: Custom elements set of APIs that allow you to define custom elements and their behavior Shadow DOM set of APIs for attaching an encapsulated shadow DOM tree to an element, rendered separately from the main document DOM, to keep an element's features private. HTML templates write markup templates using template and slot elements that are not rendered in the rendered page. Basic approach: Create a class or function in which you specify web component functionality must use super() command at the top must contain a constructor() function that defines structure of Shadow DOM using appendChild() methods Use CustomElementRegistry.define() to link the component class and the custom element Attach a Shadow DOM to custom element using Element.attachShadow() and add children, event listeners, using regular DOM methods: const shadow = this.attachShadow({mode: 'open'}) Define an HTML template using template and slot Use custom element at will on page. 2 types of custom elements: Autonomous custom elements are standalone and don't inherit from standard HTML elements, e.g. <popup-info> or document.createElement('popup-info') . They almost always extend HTMLElement Customized built-in elements inherit from basic HTML elements, e.g. <p is=\"word-count\"> or document.createElement(\"p\",{ is: \"word-count\"}) Lifecycle callbacks adoptedCallback invoked each time the custom element is moved to a new document attributeChangedCallback invoked when one of the custom element's attributed is added, removed, or changed connectedCallback invoked every time the custom element is appended into a document-connected element disconnectedCallback invoked each time the custom element is disconnected from the document's DOM","title":"Web Components"},{"location":"Web/JavaScript/Web%20Components/#3-main-technologies","text":"Custom elements set of APIs that allow you to define custom elements and their behavior Shadow DOM set of APIs for attaching an encapsulated shadow DOM tree to an element, rendered separately from the main document DOM, to keep an element's features private. HTML templates write markup templates using template and slot elements that are not rendered in the rendered page.","title":"3 main technologies:"},{"location":"Web/JavaScript/Web%20Components/#basic-approach","text":"Create a class or function in which you specify web component functionality must use super() command at the top must contain a constructor() function that defines structure of Shadow DOM using appendChild() methods Use CustomElementRegistry.define() to link the component class and the custom element Attach a Shadow DOM to custom element using Element.attachShadow() and add children, event listeners, using regular DOM methods: const shadow = this.attachShadow({mode: 'open'}) Define an HTML template using template and slot Use custom element at will on page.","title":"Basic approach:"},{"location":"Web/JavaScript/Web%20Components/#2-types-of-custom-elements","text":"Autonomous custom elements are standalone and don't inherit from standard HTML elements, e.g. <popup-info> or document.createElement('popup-info') . They almost always extend HTMLElement Customized built-in elements inherit from basic HTML elements, e.g. <p is=\"word-count\"> or document.createElement(\"p\",{ is: \"word-count\"})","title":"2 types of custom elements:"},{"location":"Web/JavaScript/Web%20Components/#lifecycle-callbacks","text":"adoptedCallback invoked each time the custom element is moved to a new document attributeChangedCallback invoked when one of the custom element's attributed is added, removed, or changed connectedCallback invoked every time the custom element is appended into a document-connected element disconnectedCallback invoked each time the custom element is disconnected from the document's DOM","title":"Lifecycle callbacks"},{"location":"Windows/Active%20Directory/","text":"Active Directory Table of Contents 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 \\ 21 22 23 24 25 26 27 28 29 30 31 32 33 Terms Active Directory components Active Directory Lightweight Directory Services (AD LDS)) Instance Configuration set Replica Partition/naming context Application partition Configuration partition Schema partition Bindable object Bindable proxy object Active Directory Federated Services (ADFS) Directory Information Tree (DIT) Key tables: Data table Link table Hidden table Security descriptor table Extensible Storage Engine (ESE) Identity Management for Unix (IMU) Security Accounts Manager (SAM) Global Catalog Schema attribute syntax Active Directory concepts Domain Controller (DC) FSMO roles: Schema Master Domain Naming Master PDC Emulator RID Master Infrastructure Master Distinguished Name (DN) domain tree forest organizational unit Trust forest trust simple trust transitive trust Domain models: Multimaster Single-domain Single-master Complete trust Groups: Scopes [Domain local][domain local group] [Domain global][domain global group] [Universal][universal group] Types: Distribution Security Site topology site subnet site link connection object Windows Management Interface (WMI) CIM infrastructure CIM Repository CIMOM WMI providers Fundamentals History Active Directory has its origins in 1990 when Microsoft released Windows NT 3.0, its first Network Operating System (NOS). Limitations of NT led Microsoft to rearchitect their solution based on LDAP, a directory service that originated in 1993 as a lighter-weight alternative to X.500 . Feature NT AD Database SAM ESE Trust Simple Transitive Domain models Multimaster Single-domain Single-master Complete trust Complete trust Name resolution WINS DNS Schemas Not extensible Extensible Major components AD objects, which can be containers or non-containers (leaf nodes), are stored in a DIT file. Each object is identified by a GUID but also commonly referred to by distinguished name (i.e. dc=mycorp,dc=com ) Active Directory's structure is based on the concept of a domain , based on the following components: - Hierarchical structure of containers and objects based on X.500 - DNS domain name - Security service to provide AAA - Policies to restrict functionality for users or machines Domains can be organized into domain trees , and domain trees can be organized into forests . The most common container type is the OU . Global Catalog can be used to search for AD objects. Because Kerberos, which underlies AD, is sensitive to time differences all computers on a domain must have clocks synchronized to within 5 minutes. NTP can be useful for this. Naming contexts Predefined NCs within AD: - Domain Naming Context - Schema Naming Context - Configuration Naming Context Schema Each object in AD is an instance of a class defined in the schema . The schema version can be queried from the command-line with [ adfind ][adfind] OID SID A Windows SID is generally composed of 2 fixed fields and up to 15 additional fields, all separated by dashes: S-v- id - s1 - s2 - s3 - s4 - s5 - s6 - s7 - s8 - s9 - s10 - s11 - s12 - s13 - s14 - s15 AD LDS AD LDS offers a pared-down version of AD that is easy to set up and tear down. It was first released in November 2003 as Active Directory Application Mode (ADAM) V1.0 and offers security benefits because it doesn't enable so many services by default. It was renamed AD LDS with the release of Windows Server 2008. Differences between AD and AD LDS - AD LDS is a standalone application run from a dsamain.exe process (rather than lsass.exe ), which means it can be started or stopped on demand without rebooting and multiple instances can be run. - AD LDS lacks the global catalog functionality (removing NSPI and AB as well) Site topology A site topology is a map of the sites , subnets , site links , site link bridges, and connection objects as it relates to a forest. WMI An industry effort to develop a model for managing systems and devices for vendor use arose in the 1990s which resulted in CIM , which provides the basis for WMI . The WMI architecture is composed of two main layers: the CIM infrastructure ( CIMOM and CIM Repository ) and the WMI providers Each provider is associated with a namespace, which is similar in concept to a filesystem. .NET The .NET Framework was developed with the intention of replacing the old Win32 and COM APIs. It has two major components: - Common Language Runtime (CLR) - .NET Framework class library Searching Active Directory","title":"Active Directory"},{"location":"Windows/Active%20Directory/#active-directory","text":"","title":"Active Directory"},{"location":"Windows/Active%20Directory/#table-of-contents","text":"01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 \\ 21 22 23 24 25 26 27 28 29 30 31 32 33","title":"Table of Contents"},{"location":"Windows/Active%20Directory/#terms","text":"Active Directory components Active Directory Lightweight Directory Services (AD LDS)) Instance Configuration set Replica Partition/naming context Application partition Configuration partition Schema partition Bindable object Bindable proxy object Active Directory Federated Services (ADFS) Directory Information Tree (DIT) Key tables: Data table Link table Hidden table Security descriptor table Extensible Storage Engine (ESE) Identity Management for Unix (IMU) Security Accounts Manager (SAM) Global Catalog Schema attribute syntax Active Directory concepts Domain Controller (DC) FSMO roles: Schema Master Domain Naming Master PDC Emulator RID Master Infrastructure Master Distinguished Name (DN) domain tree forest organizational unit Trust forest trust simple trust transitive trust Domain models: Multimaster Single-domain Single-master Complete trust Groups: Scopes [Domain local][domain local group] [Domain global][domain global group] [Universal][universal group] Types: Distribution Security Site topology site subnet site link connection object Windows Management Interface (WMI) CIM infrastructure CIM Repository CIMOM WMI providers","title":"Terms"},{"location":"Windows/Active%20Directory/#fundamentals","text":"","title":"Fundamentals"},{"location":"Windows/Active%20Directory/#history","text":"Active Directory has its origins in 1990 when Microsoft released Windows NT 3.0, its first Network Operating System (NOS). Limitations of NT led Microsoft to rearchitect their solution based on LDAP, a directory service that originated in 1993 as a lighter-weight alternative to X.500 . Feature NT AD Database SAM ESE Trust Simple Transitive Domain models Multimaster Single-domain Single-master Complete trust Complete trust Name resolution WINS DNS Schemas Not extensible Extensible","title":"History"},{"location":"Windows/Active%20Directory/#major-components","text":"AD objects, which can be containers or non-containers (leaf nodes), are stored in a DIT file. Each object is identified by a GUID but also commonly referred to by distinguished name (i.e. dc=mycorp,dc=com ) Active Directory's structure is based on the concept of a domain , based on the following components: - Hierarchical structure of containers and objects based on X.500 - DNS domain name - Security service to provide AAA - Policies to restrict functionality for users or machines Domains can be organized into domain trees , and domain trees can be organized into forests . The most common container type is the OU . Global Catalog can be used to search for AD objects. Because Kerberos, which underlies AD, is sensitive to time differences all computers on a domain must have clocks synchronized to within 5 minutes. NTP can be useful for this.","title":"Major components"},{"location":"Windows/Active%20Directory/#naming-contexts","text":"Predefined NCs within AD: - Domain Naming Context - Schema Naming Context - Configuration Naming Context","title":"Naming contexts"},{"location":"Windows/Active%20Directory/#schema","text":"Each object in AD is an instance of a class defined in the schema . The schema version can be queried from the command-line with [ adfind ][adfind] OID","title":"Schema"},{"location":"Windows/Active%20Directory/#sid","text":"A Windows SID is generally composed of 2 fixed fields and up to 15 additional fields, all separated by dashes: S-v- id - s1 - s2 - s3 - s4 - s5 - s6 - s7 - s8 - s9 - s10 - s11 - s12 - s13 - s14 - s15","title":"SID"},{"location":"Windows/Active%20Directory/#ad-lds","text":"AD LDS offers a pared-down version of AD that is easy to set up and tear down. It was first released in November 2003 as Active Directory Application Mode (ADAM) V1.0 and offers security benefits because it doesn't enable so many services by default. It was renamed AD LDS with the release of Windows Server 2008. Differences between AD and AD LDS - AD LDS is a standalone application run from a dsamain.exe process (rather than lsass.exe ), which means it can be started or stopped on demand without rebooting and multiple instances can be run. - AD LDS lacks the global catalog functionality (removing NSPI and AB as well)","title":"AD LDS"},{"location":"Windows/Active%20Directory/#site-topology","text":"A site topology is a map of the sites , subnets , site links , site link bridges, and connection objects as it relates to a forest.","title":"Site topology"},{"location":"Windows/Active%20Directory/#wmi","text":"An industry effort to develop a model for managing systems and devices for vendor use arose in the 1990s which resulted in CIM , which provides the basis for WMI . The WMI architecture is composed of two main layers: the CIM infrastructure ( CIMOM and CIM Repository ) and the WMI providers Each provider is associated with a namespace, which is similar in concept to a filesystem.","title":"WMI"},{"location":"Windows/Active%20Directory/#net","text":"The .NET Framework was developed with the intention of replacing the old Win32 and COM APIs. It has two major components: - Common Language Runtime (CLR) - .NET Framework class library","title":".NET"},{"location":"Windows/Active%20Directory/#searching-active-directory","text":"","title":"Searching Active Directory"},{"location":"Windows/DNS-labs/","text":"Configuring DNS servers Hostname IP Address PLABDC01 192.168.0.1 PLABDM01 192.168.0.2 PLABSA01 192.168.0.4 Create a Domain Forest Create a new domain and forest on PLABSA01 named \"PRACTICEIT.CO.UK\". Configure PLABDC01 to forward for the new domain, and PLABSA01 to forward for the old one. Create a new DNS zone Set up alternate DNS server Configure DNS forwarders New AD forest Create a new domain and forest on PLABSA01 named \"PRACTICEIT.CO.UK\" # PLABSA01 Add-WindowsFeature dns , ad-domain-services -IncludeManagementTools $pw = ConvertTo-SecureString -Force -AsPlainText 'Passw0rd' Install-ADDSForest -DomainName PRACTICEIT . CO . UK -SafeModeAdministratorPassword $pw Forwarding Forward DNS queries for the new domain to PLABSA01 # PLABDC01 # Equivalent to `netsh interface ipv4 add dns Ethernet 192.168.0.4 index=2` Set-DNSClientServerAddress -InterfaceAlias Ethernet -ServerAddresses ( '192.168.0.1' , '192.168.0.4' ) Add-DnsServerConditionalForwarderZone -Name 'practiceit.co.uk' -MasterServers '192.168.0.4' Add-DnsServerForwarder -IpAddress '192.168.0.4' Test-NetConnection plabsa01 . practiceit . co . uk Forward DNS queries for the old domain to PLABDC01 # PLABSA01 # Equivalent to `netsh interface ipv4 add dns Ethernet 192.168.0.1 index=2` Set-DnsClientServerAddress -InterfaceIndex 13 -ServerAddresses ( '127.0.0.1' , '192.168.0.1' ) Add-DnsServerConditionalForwarderZone -Name 'practicelabs.com' -MasterServers '192.168.0.1' Test-NetConnection plabdc01 . practicelabs . com Create new zone # PLABDM01 Add-WindowsFeature dns -IncludeManagementTools Add-DnsServerPrimaryZone -ZoneFile 'apac.practicelabs.com.dns' -ZoneName 'apac.practicelabs.com' -DynamicUpdate NonsecureAndSecure Delegate to the new zone # PLABDC01 Add-DnsServerZoneDelegation -Name 'practicelabs.com' -ChildZoneName 'apac' -NameServer 'plabdm01.practicelabs.com' -IPAddress '192.168.0.2' Add-DnsServerPrimaryZone -ZoneName PLTEST . com -ReplicationScope Domain Add-DnsServerResourceRecordA -Name www -ZoneName pltest . com -IPv4Address 192 . 168 . 0 . 1 # Copying PowerShell commands from a script Add-DnsServerClientSubnet -Name EMEA -IPv4Subnet '192.168.0.0/24' Add-DnsServerClientSubnet -Name APAC -IPv4Subnet '192.168.1.0/24' Add-DnsServerZoneScope -ZoneName \"PLTEST.com\" -Name \"EMEAZoneScope\" Add-DnsServerZoneScope -ZoneName \"PLTEST.com\" -Name \"APACZoneScope\" Add-DnsServerResourceRecord -A -Name www -ZoneName 'PLTEST.com' -IPv4Address '192.168.1.1' -ZoneScope APACZoneScope Add-DnsServerResourceRecord -A -Name www -ZoneName 'PLTEST.com' -IPv4Address '192.168.0.1' -ZoneScope EMEAZoneScope Add-DnsServerQueryResolutionPolicy -Name EMEAPolicy -Action ALLOW -ClientSubnet 'eq,EMEA' -ZoneScope 'EMEAZoneScope,1' -ZoneName 'PLTEST.com' Add-DnsServerQueryResolutionPolicy -Name APACPolicy -Action ALLOW -ClientSubnet 'eq,APAC' -ZoneScope 'APACZoneScope,1' -ZoneName 'PLTEST.com' # PLABWIN10 ipconfig / flushdns nslookup www . pltest . com # PLABDC01 Add-DnsServerQueryResolutionPolicy -Name Blocklist -Action IGNORE -Fqdn 'eq,*.pltest.com' -PassThru # PLABWIN10 # Now the query does not work nslookup www . pltest . com # PLABDC01 Remove-DnsServerQueryResolutionPolicy -Name Blocklist -Confirm Configuration Configure DNS socket pool ::PLABDC01 dnscmd /info /socketpoolsize dnscmd /config /socketpoolsize 3500 net stop dns net start dns dnscmd /info /socketpoolsize Manage DNS cache locking dnscmd /info /cachelockingpercent dnscmd /config /cachelockingpercent 90 dnscmd /info /cachelockingpercent net stop dns net start dns dnscmd /info /cachelockingpercent Create a GlobalNames zone Set-DnsServerGlobalNameZone -AlwaysQueryServer $true Add-DnsServerPrimaryZone -Name GlobalNames -ReplicationScope Forest dnscmd . / config / enableglobalnamessupport 1 Create a resource record in the GlobalNames zone Add-DnsServerResourceRecordA -Name PLABDC01 -ZoneName GlobalNames -IPv4Address 192 . 168 . 0 . 1 Enable response rate limiting Get-DNSServerResponseRateLimiting Set-DNSServerResponseRateLimiting -Mode Enable -Confirm Get-DNSServerResponseRateLimiting # confirming Add-DNSServerResponseRateLimitingExceptionList -Name Whitelist1 -Fqdn 'eq, *.practicelabs.com' Set-DNSServerResponseRateLimiting -Mode Enable -Confirm Manage DNS logging Add-WindowsFeature dns -IncludeManagementTools -ComputerName PLABDM01 Set-DnsServerDiagnostics -LogFilePath C : \\ dnslog . txt # Some additional settings appear necessary in order to begin logging Enable zone transfers Set-DnsServerPrimaryZone -SecureSecondaries TransferAnyServer Create a secondary DNS zone # PLABDM01 Add-DnsServerSecondaryZone -ComputerNAme PLABDM01 -MasterServers 192 . 168 . 0 . 1 -ZoneName practicelabs . com -ZoneFile practicelabs . com . dns Check DNS logs at text file and event viewer. Manage DNS zones and resource records Convert AD-integrated zone to file-based zone Get-DNSServerZone -ZoneName practicelabs . com | ConvertTo-DnsServerPrimaryZone -ZoneFile practicelabs . com . dns Manually add an A record Add-Content -Path C : \\ Windows \\ System32 \\ dns \\ practicelabs . com . dns -Value \"PLABSVR 1200 A 192.168.0.105\" Re-integrate zone to AD ConvertTo-DnsServerPrimaryZone -ZoneName practicelabs . com -ReplicationScope Domain Promote a new DC # PLABDM01 Add-WindowsFeature dns , ad-domain-services -IncludeManagementTools Install-ADDSDomainController -Domain practicelabs . com Create a secondary zone # PLABSA01 Add-WindowsFeature dns -IncludeManagementTools Add-DnsServerSecondaryZone -ZoneName practicelabs . com -ZoneFile \"practicelabs.com.dns\" -MasterServers 192 . 168 . 0 . 1 Enable zone transfer from master # PLABDC01 Set-DnsServerPrimaryZone -Name practicelabs . com -SecureSecondaries TransferAnyServer Manually initiate a zone transfer if needed # PLABSA01 Start-DnsServerZoneTransfer -ZoneName practicelabs . com Remove secondary zone # PLABSA01 Remove-DnsServerZone -ZoneName practicelabs . com Add-DnsServerStubZone -ZoneName practicelabs . com -ZoneFile practicelabs . com . dns -MasterServers 192 . 168 . 0 . 1 Create new resource records # PLABDC01 Add-DnsServerResourceRecord -A -Name plabsa05 -IPv4Address 192 . 168 . 0 . 50 -ZoneName practicelabs . com Add-DnsServerResourceRecord -CName -Name plabdmsrv01 -ZoneName practicelabs . com -HostNameAlias plabdm01 . practicelabs . com Add-DnsServerResourceRecord -MX -Name \".\" -ZoneName practicelabs . com -MailExchange mobilemail . practicelabs . com -Preference 10 Add-DnsServerResourceRecord -A -Name mobilemail -IPv4Address 192 . 168 . 0 . 21 -ZoneName practicelabs . com","title":"DNS labs"},{"location":"Windows/DNS-labs/#configuring-dns-servers","text":"Hostname IP Address PLABDC01 192.168.0.1 PLABDM01 192.168.0.2 PLABSA01 192.168.0.4 Create a Domain Forest Create a new domain and forest on PLABSA01 named \"PRACTICEIT.CO.UK\". Configure PLABDC01 to forward for the new domain, and PLABSA01 to forward for the old one. Create a new DNS zone Set up alternate DNS server Configure DNS forwarders","title":"Configuring DNS servers"},{"location":"Windows/DNS-labs/#new-ad-forest","text":"Create a new domain and forest on PLABSA01 named \"PRACTICEIT.CO.UK\" # PLABSA01 Add-WindowsFeature dns , ad-domain-services -IncludeManagementTools $pw = ConvertTo-SecureString -Force -AsPlainText 'Passw0rd' Install-ADDSForest -DomainName PRACTICEIT . CO . UK -SafeModeAdministratorPassword $pw","title":"New AD forest"},{"location":"Windows/DNS-labs/#forwarding","text":"Forward DNS queries for the new domain to PLABSA01 # PLABDC01 # Equivalent to `netsh interface ipv4 add dns Ethernet 192.168.0.4 index=2` Set-DNSClientServerAddress -InterfaceAlias Ethernet -ServerAddresses ( '192.168.0.1' , '192.168.0.4' ) Add-DnsServerConditionalForwarderZone -Name 'practiceit.co.uk' -MasterServers '192.168.0.4' Add-DnsServerForwarder -IpAddress '192.168.0.4' Test-NetConnection plabsa01 . practiceit . co . uk Forward DNS queries for the old domain to PLABDC01 # PLABSA01 # Equivalent to `netsh interface ipv4 add dns Ethernet 192.168.0.1 index=2` Set-DnsClientServerAddress -InterfaceIndex 13 -ServerAddresses ( '127.0.0.1' , '192.168.0.1' ) Add-DnsServerConditionalForwarderZone -Name 'practicelabs.com' -MasterServers '192.168.0.1' Test-NetConnection plabdc01 . practicelabs . com","title":"Forwarding"},{"location":"Windows/DNS-labs/#create-new-zone","text":"# PLABDM01 Add-WindowsFeature dns -IncludeManagementTools Add-DnsServerPrimaryZone -ZoneFile 'apac.practicelabs.com.dns' -ZoneName 'apac.practicelabs.com' -DynamicUpdate NonsecureAndSecure Delegate to the new zone # PLABDC01 Add-DnsServerZoneDelegation -Name 'practicelabs.com' -ChildZoneName 'apac' -NameServer 'plabdm01.practicelabs.com' -IPAddress '192.168.0.2' Add-DnsServerPrimaryZone -ZoneName PLTEST . com -ReplicationScope Domain Add-DnsServerResourceRecordA -Name www -ZoneName pltest . com -IPv4Address 192 . 168 . 0 . 1 # Copying PowerShell commands from a script Add-DnsServerClientSubnet -Name EMEA -IPv4Subnet '192.168.0.0/24' Add-DnsServerClientSubnet -Name APAC -IPv4Subnet '192.168.1.0/24' Add-DnsServerZoneScope -ZoneName \"PLTEST.com\" -Name \"EMEAZoneScope\" Add-DnsServerZoneScope -ZoneName \"PLTEST.com\" -Name \"APACZoneScope\" Add-DnsServerResourceRecord -A -Name www -ZoneName 'PLTEST.com' -IPv4Address '192.168.1.1' -ZoneScope APACZoneScope Add-DnsServerResourceRecord -A -Name www -ZoneName 'PLTEST.com' -IPv4Address '192.168.0.1' -ZoneScope EMEAZoneScope Add-DnsServerQueryResolutionPolicy -Name EMEAPolicy -Action ALLOW -ClientSubnet 'eq,EMEA' -ZoneScope 'EMEAZoneScope,1' -ZoneName 'PLTEST.com' Add-DnsServerQueryResolutionPolicy -Name APACPolicy -Action ALLOW -ClientSubnet 'eq,APAC' -ZoneScope 'APACZoneScope,1' -ZoneName 'PLTEST.com' # PLABWIN10 ipconfig / flushdns nslookup www . pltest . com # PLABDC01 Add-DnsServerQueryResolutionPolicy -Name Blocklist -Action IGNORE -Fqdn 'eq,*.pltest.com' -PassThru # PLABWIN10 # Now the query does not work nslookup www . pltest . com # PLABDC01 Remove-DnsServerQueryResolutionPolicy -Name Blocklist -Confirm","title":"Create new zone"},{"location":"Windows/DNS-labs/#configuration","text":"Configure DNS socket pool ::PLABDC01 dnscmd /info /socketpoolsize dnscmd /config /socketpoolsize 3500 net stop dns net start dns dnscmd /info /socketpoolsize Manage DNS cache locking dnscmd /info /cachelockingpercent dnscmd /config /cachelockingpercent 90 dnscmd /info /cachelockingpercent net stop dns net start dns dnscmd /info /cachelockingpercent Create a GlobalNames zone Set-DnsServerGlobalNameZone -AlwaysQueryServer $true Add-DnsServerPrimaryZone -Name GlobalNames -ReplicationScope Forest dnscmd . / config / enableglobalnamessupport 1 Create a resource record in the GlobalNames zone Add-DnsServerResourceRecordA -Name PLABDC01 -ZoneName GlobalNames -IPv4Address 192 . 168 . 0 . 1 Enable response rate limiting Get-DNSServerResponseRateLimiting Set-DNSServerResponseRateLimiting -Mode Enable -Confirm Get-DNSServerResponseRateLimiting # confirming Add-DNSServerResponseRateLimitingExceptionList -Name Whitelist1 -Fqdn 'eq, *.practicelabs.com' Set-DNSServerResponseRateLimiting -Mode Enable -Confirm Manage DNS logging Add-WindowsFeature dns -IncludeManagementTools -ComputerName PLABDM01 Set-DnsServerDiagnostics -LogFilePath C : \\ dnslog . txt # Some additional settings appear necessary in order to begin logging Enable zone transfers Set-DnsServerPrimaryZone -SecureSecondaries TransferAnyServer Create a secondary DNS zone # PLABDM01 Add-DnsServerSecondaryZone -ComputerNAme PLABDM01 -MasterServers 192 . 168 . 0 . 1 -ZoneName practicelabs . com -ZoneFile practicelabs . com . dns Check DNS logs at text file and event viewer.","title":"Configuration"},{"location":"Windows/DNS-labs/#manage-dns-zones-and-resource-records","text":"Convert AD-integrated zone to file-based zone Get-DNSServerZone -ZoneName practicelabs . com | ConvertTo-DnsServerPrimaryZone -ZoneFile practicelabs . com . dns Manually add an A record Add-Content -Path C : \\ Windows \\ System32 \\ dns \\ practicelabs . com . dns -Value \"PLABSVR 1200 A 192.168.0.105\" Re-integrate zone to AD ConvertTo-DnsServerPrimaryZone -ZoneName practicelabs . com -ReplicationScope Domain Promote a new DC # PLABDM01 Add-WindowsFeature dns , ad-domain-services -IncludeManagementTools Install-ADDSDomainController -Domain practicelabs . com Create a secondary zone # PLABSA01 Add-WindowsFeature dns -IncludeManagementTools Add-DnsServerSecondaryZone -ZoneName practicelabs . com -ZoneFile \"practicelabs.com.dns\" -MasterServers 192 . 168 . 0 . 1 Enable zone transfer from master # PLABDC01 Set-DnsServerPrimaryZone -Name practicelabs . com -SecureSecondaries TransferAnyServer Manually initiate a zone transfer if needed # PLABSA01 Start-DnsServerZoneTransfer -ZoneName practicelabs . com Remove secondary zone # PLABSA01 Remove-DnsServerZone -ZoneName practicelabs . com Add-DnsServerStubZone -ZoneName practicelabs . com -ZoneFile practicelabs . com . dns -MasterServers 192 . 168 . 0 . 1 Create new resource records # PLABDC01 Add-DnsServerResourceRecord -A -Name plabsa05 -IPv4Address 192 . 168 . 0 . 50 -ZoneName practicelabs . com Add-DnsServerResourceRecord -CName -Name plabdmsrv01 -ZoneName practicelabs . com -HostNameAlias plabdm01 . practicelabs . com Add-DnsServerResourceRecord -MX -Name \".\" -ZoneName practicelabs . com -MailExchange mobilemail . practicelabs . com -Preference 10 Add-DnsServerResourceRecord -A -Name mobilemail -IPv4Address 192 . 168 . 0 . 21 -ZoneName practicelabs . com","title":"Manage DNS zones and resource records"},{"location":"Windows/Labs/","text":"Labs Server Core Configure VM Rename-VMSwitch Intel * External New-VM PLABDMCORE02 1024 1 -SwitchName External -NewVHDPath 'D:\\VHD\\PLABDMCORE02\\Virtual Hard Disks\\PLABDMCORE02.vhdx' -NewVHDSize 127gb Set-VMDvdDrive ... # Can't find way to pass through host drive Z:\\ Add DVD drive Z: of host to VM's DVD drive (no Powershell equivalent found yet). Start-VM PLABDMCORE02 Install Windows, then continue configuring VM # PLABDMCORE02 Rename-Computer PLABDMCORE02 New-NetIpAddress 192 . 168 . 0 . 7 -PrefixLength 24 -InterfaceAlias Ethernet Set-DnsClientServerAddress -InterfaceAlias Ethernet -ServerAddresses 192 . 168 . 0 . 1 Restart-Computer Join computer to domain Add-Computer -DomainName practicelabs . com -DomainCredential practicelabs \\ administrator Restart-Computer Install packages remotely # PLABDC01 Enter-PSSession PLABDMCORE02 Add-WindowsFeature -Name dns , rsat-dns-server -IncludeManagementTools Add PLABDMCORE02 as a DNS server while remotely connected to DC (no Powershell equivalent found yet). Add a secondary lookup zone to PLABDMCORE02 (doesn't work; asks for Zonefile) Add-DnsServerSecondaryZone -Name practicelabs . com -MasterServers 192 . 168 . 0 . 1 Nano Server # PLABDM01 Copy-Item Z : \\ NanoServer \\ NanoServerImageGenerator \\*. ps * C : \\ nanoserver Import-Module C : \\ nNanoserver \\ NanoServerImageGenerator . psm1 Create the VHD image New-NanoServerImage -Edition Standard -MediaPath z : \\ -Basepath c : \\ nanoserver -Targetpath c : \\ nanoserver \\ PLABNANOSRV01 . vhdx -DeploymentType Guest -ComputerName PLABNANOSRV01 -Storage -Package Microsoft-NanoServer-DNS-Package Set-VMHardDiskDrive -VMName PLABNANOSRV01 -Path C : \\ nanoserver \\ PLABNANOSRV01 . vhdx DISM # PLABSA01 Set-Location C : \\ mkdir updates , images , mount , drivers Copy-Item \\\\ plabdm01 \\ Win10 \\ sources \\ install . wim C : \\ updates ( Get-Item C : \\ updates \\ install . wim ). IsReadOnly = $false Move specified .msu installers to C:\\updates and uncompress device driver to C:\\drivers Using Powershell: Mount-WindowsImage -Path C : \\ mount -ImagePath C : \\ images \\ install . wim -Index 1 Add-WindowsPackage -Path C : \\ mount -PackagePath C : \\ updates Add-WindowsDriver -Path C : \\ mount -Driver C : \\ drivers \\ display . driver \\ nv_dispi . inf Save-WindowsImage -Path C : \\ mount Dismount-WindowsImage -Path C : \\ mount -Save Using dism.exe : dism /mount-wim /wimfile:c:\\images\\install.wim /index:1 /mountdir:c:\\mount dism /image:c:\\mount /add-package /packagepath:c:\\updates dism /image:c:\\mount /add-driver /driver:c:\\drivers\\display.driver\\nv_dispi.inf dism /commit-wim /mountdir:c:\\mount dism /unmount-wim /commit /mountdir:c:\\mount NLB lab Configure a Network Load Balancing cluster. [Practice Lab][pl:70-740] # PLABDM01 and PLABSA01 Install-WindowsFeature web-server , web-webserver , web-mgmt-tools -IncludeManagementTools # PLABDM01 New-NetIpAddress 192 . 168 . 0 . 20 -PrefixLength 24 -InterfaceAlias Ethernet1 # PLABSA01 New-NetIpAddress 192 . 168 . 0 . 40 -PrefixLength 24 -InterfaceAlias Ethernet1 Install-WindowsFeature -Name nlb -IncludeAllSubFeature -IncludeManagementTools Using the Network Load Balancing Manager , create a new cluster named PLAB-NLB1 specifying multicast operation mode at 192.168.0.25, and create an appropriate DNS record on PLABDC01 . Install NLB on PLABDM01 and add it to the cluster through the NLB Manager. Edit the image located at C:\\inetpub\\wwwroot\\iisstart.png on either server. Once saved, these edits will now be visible when that server's website is visited at its hostname. Copy the contents of C:\\inetpub\\wwwroot to new directory C:\\nlbport on PLABDM01 . Host that directory as a website on port 6789, opening the firewall appropriately. New-Website -Name nlbport -PhysicalPath \"c:\\nlbport\" -Port 6789 New-NetFirewallRule -DisplayName NLBPort -Protocol TCP -LocalPort 6789 Returning to the NLB Manager, open Cluster Properties > Port Rules and remove the single defined port rule. Add a new port rule that specifies port 80 for \"multiple host\" filtering mode, and another that specifies port 6789 for \"single host\" filtering mode. Then from the host-specific port rule dialog box, select a handling priority of 10. iSCSI Storage Network topology # PLABDC01 New-NetIpAddress 192 . 168 . 0 . 10 -PrefixLength 24 -InterfaceAlias Ethernet2 # PLABDM01 New-NetIpAddress 192 . 168 . 0 . 20 -PrefixLength 24 -InterfaceAlias Ethernet1 New-NetIpAddress 192 . 168 . 1 . 20 -PrefixLength 24 -InterfaceAlias Ethernet2 Set-DnsClientServerAddress -InterfaceAlias Ethernet1 , Ethernet2 -ServerAddresses 192 . 168 . 0 . 1 iSCSI target # PLABDC01 Add-WindowsFeature fs-iscsitarget-server -IncludeManagementTools New-iSCSIVirtualDisk -SizeBytes 5gb -Path 'C:\\CorporateHD.vhdx' New-iSCSIServerTarget -TargetName plabdc01 -InitiatorId @( 'ipaddress:192.168.0.20' , 'ipaddress:192.168.1.20' ) Add-iSCSIVirtualDiskTargetMapping -Path 'C:\\CorporateHD.vhdx' -TargetName plabdc01 iSCSI initiator # PLABDM01 Start-Service msiscsi Get-NetFirewallServiceFilter -Service msiscsi | Get-NetFirewallRule | Enable-NetFirewallRule Connect-iSCSITarget -NodeAddress ( Get-iSCSITarget ). NodeAddress Multipath ... Finally, mount the drive New-Volume -DiskNumber 5 -FriendlyName iSCSI-Volume1 -FileSystem NTFS -DriveLetter R Data deduplication # PLABDM01 Add-WindowsFeature fs-data-deduplication -IncludeManagementTools Get-DedupVolume Get-DedupStatus Enable-DedupVolume -Volume 'D:' Set-DedupVolume -Volume 'D:' -ExcludeFolder 'D:\\win81' , 'D:\\virtual machines' New-DedupSchedule $name -Days Sunday , Monday , Tuesday , Wednesday , Thursday , Friday , Saturday -DurationHours 6 -Enabled $true -Type Optimization -Memory 50 Start-DedupJob D : -Type Optimization -Memory 50 Failover cluster lab Network topology # PLABDM01 New-NetIpAddress 192 . 168 . 0 . 20 -PrefixLength 24 -InterfaceAlias Ethernet1 New-NetIpAddress 192 . 168 . 0 . 21 -PrefixLength 24 -InterfaceAlias \"vEthernet (Internal network 1)\" New-NetIpAddress 192 . 168 . 1 . 20 -PrefixLength 24 -InterfaceAlias Ethernet2 Set-DnsClientServerAddresses Ethernet1 , Ethernet2 -ServerAddresses 192 . 168 . 0 . 1 Disable-NetAdapter \"vEthernet (Internal network 1)\" # PLABSA01 New-NetIpAddress 192 . 168 . 0 . 40 -PrefixLength 24 -InterfaceAlias Ethernet1 New-NetIpAddress 192 . 168 . 0 . 41 -PrefixLength 24 -InterfaceAlias \"vEthernet (Internal network 1)\" New-NetIpAddress 192 . 168 . 1 . 40 -PrefixLength 24 -InterfaceAlias Ethernet2 Set-DnsClientServerAddress Ethernet1 , Ethernet2 -Serveraddresses 192 . 168 . 0 . 1 Disable-NetAdapter \"vEthernet (Internal network 1)\" iSCSI target # PLABDC01 Add-WindowsFeature FS-iSCSITarget-Server -IncludeManagementTools Then go into Server Manager > File and Storage Services > iSCSI and start the New iSCSI Virtual Disk Wizard . Create a fixed 500 MB iSCSI virtual disk on C:\\ named \"QuorumHD\" and assign it to the PLABDC01 iSCSI target server. Add plabdm01.practicelabs.com and plabsa01.practicelabs.com as initiators, and keep authentication disabled, and confirm the disk has been created. Create another virtual disk, 30 GB dynamic on D:\\ names \"CSVHD\", and assign it to the same PLABDC01 iSCSI target server. Configure iSCSI initiators to iSCSI Target Server On PLABDM01 , open Server Manager and click on Tools > iSCSI Initiator and connect to plabdc01.practicelabs.com . Bring disks 5 and 6 online and initialize them with [MBR][MBR] partition style. This can be done within diskmgmt.msc , where the shared volumes that have been created will appear beneath the local disks, or using PowerShell: # PLABDM01 New-Volume -DiskNumber 5 -DriveLetter Q -FileSystem NTFS -FriendlyName 'Quorum' New-Volume -DiskNumber 6 -DriveLetter V -FileSystem NTFS -FriendlyName 'CSV' Only bring the disks online on the other client # PLABSA01 Set-Disk -Number 4 , 5 -IsOffline $false Install failover clusters On both PLABDM01 and PLABSA01 : Install-WindowsFeature failover-clustering -IncludeManagementTools Then validate the installation through the Failover Cluster Manager (cluadmin.msc). Open the Validate Configuration wizard and enter plabdm01.practicelabs.com and plabsa01.practicelabs.com into the list of selected servers. Open the Create Cluster Wizard and create cluster \"PLHYPVCL01\" using those same two nodes, choosing 192.168. 0 .25 and 192.168. 1 .25 for the administrative access points. Open the Configure Cluster Quorum Wizard and configure a disk witness using the Quorum shared volume. Open Hyper-V Manager (virtmgmt.msc) and move PLABDC02 and PLABWIN811 to V:\\PLABDC02 and V:\\PLABWIN811 respectively, specifying the VM's storage . Confirm the move has taken place by inspecting each VM's hard drive in the settings. Add the Virtual Machine role to the Failover Cluster Manager , and select PLABDC02 and PLABWIN811 . This will create the \"PLABWIN811\" Role, which actually includes both selected VMs. Open PLABWIN811 Properties , then the Failover tab, and select \"Allow failback\" specifying between 1 and 2 hours. Open Hyper-V Manager and start PLABDC02 . Wait until it has booted completely, then return to Failover Cluster Manager , open Nodes, right-click on PLABDM01 and then Stop Cluster Service. This option forces a failover of the VM, which drains to PLABSA01 . This can be confirmed by returning to PLABSA01 and opening Hyper-V Manager , where it is revealed that PLABDC02 is running. Scale-Out Fileserver lab Plan network topology # PLABDM01 New-NetIpAddress 192 . 168 . 0 . 20 -PrefixLength 24 -InterfaceAlias Ethernet1 New-NetIpAddress 192 . 168 . 1 . 20 -PrefixLength 24 -InterfaceAlias Ethernet2 Set-DnsClientServerAddress -InterfaceAlias Ethernet1 , Ethernet2 -ServerAddresses 192 . 168 . 0 . 1 Disable-NetAdapter -InterfaceAlias 'vethernet (internal network 1)' # PLABSA01 New-NetIpAddress 192 . 168 . 0 . 40 -PrefixLength 24 -InterfaceAlias Ethernet1 New-NetIpAddress 192 . 168 . 1 . 40 -PrefixLength 24 -InterfaceAlias Ethernet2 Set-DnsClientServerAddress -InterfaceAlias Ethernet1 , Ethernet2 -ServerAddresses 192 . 168 . 0 . 1 Configure iSCSI target # PLABDC01 Install-WindowsFeature fs-iscsitarget-server -IncludeManagementTools New-IscsiVirtualDisk -Fixed -SizeBytes 500mb -Path 'C:\\QuorumHD.vhdx' New-IscsiVirtualDisk -SizeBytes 30gb -Path 'D:\\CSVHD.vhdx' New-IscsiServerTarget -TargetName PLABDC01 -InitiatorId @( 'iqn:iqn.1991-05.com.microsoft:plabsa01.practicelabs.com' , 'iqn:iqn.1991-05.com.microsoft:plabdm01.practicelabs.com' ) Add-IscsiVirtualDiskTargetMapping -Path 'C:\\QuorumHD.vhdx' -TargetName plabdc01 Add-IscsiVirtualDiskTargetMapping -Path 'D:\\CSVHD.vhdx' -TargetName plabdc01 Configure iSCSI initiator on both clients # PLABDM01 and PLABSA01 Start-Service msiscsi Get-NetFirewallServiceFilter -Service msiscsi | Get-NetFirewallRule | Enable-NetFirewallRule Connect-iSCSITarget -NodeAddress ( Get-iSCSITarget ). NodeAddress Install Failover Clustering role on nodes of the cluster # PLABDM01 and PLABSA01 Install-WindowsFeature failover-clustering -IncludeManagementTools Create a failover cluster through the GUI (the Powershell commands for cluster creation appear not to support adding multiple network networks) # PLABDM01 New-Cluster -Name PLABSCL01 -Node PLABDM01 , PLABSA01 -StaticAddress 192 . 168 . 0 . 25 -IgnoreNetwork 192 . 168 . 1 . 0 / 24 Set-ClusterQuorum -DiskWitness 'Cluster Disk 1' Hyper-V storage lab Create a VM with VHDX disks New-VM -Name PLABWIN102 -Generation 1 -MemoryStartupBytes 1536mb -SwitchName 'Private network 1' -NewVHDPath 'C:\\Users\\Public\\Documents\\Hyper-V\\Virtual hard disks\\PLABWIN102.vhdx' -NewVHDSizeBytes 127gb Set-VMDvdDrive -VMName PLABWIN102 -Path C : \\ Users \\ Administrator . PRACTICELABS \\ Documents \\ Eval81 . iso New-VHD -Path 'D:\\Virtual Machines\\PLAB1.vhdx' -Dynamic -SizeBytes 127gb Add-VMHardDiskDrive -VMName PLABWIN102 -ControllerType IDE -ControllerNumber 0 -Path 'D:\\Virtual Machines\\PLAB1.vhdx' Add a differencing disk New-VHD -Path 'D:\\Virtual Machines\\PLAB2.vhdx' -Differencing -ParentPath 'C:\\Users\\Public\\Documents\\Hyper-V\\Virtual hard disks\\PLABWIN102.vhdx' Add-VMHardDrive PLABWIN102 SCSI -Path 'D:\\Virtual Machines\\PLAB2.vhdx' Add a passthrough disk Set-Disk -Number 1 -IsOffline $true Add-VMHardDiskDrive PLABWIN102 IDE 1 -DiskNumber 1 Create a SAN New-VMSan -Name \"PLABS-Fc\" Add-VMFibreChannelHBA -VMName PLABDC02 -SanName \"PLABS-Fc\" Hyper-V replica lab Emphasizes VMReplication cmdlets. # PLABDM01 Enable-NetFirewallRule VIRT-HVRHTTPL-In-TCP-NoScope , VIRT-HVRHTTPSL-In-TCP-NoScope Enable incoming replication on the receiving server #PLABSA01 Enable-NetFirewallRule VIRT-HVRHTTPL-In-TCP-NoScope , VIRT-HVRHTTPSL-In-TCP-NoScope Set-Disk -Number 1 , 3 , 4 -IsOffline $false -IsReadOnly $false New-Volume -DiskNumber 1 -FileSystem NTFS -FriendlyName VMReplica -DriveLetter E Set-VmReplicationServer -ReplicationEnabled $true -AllowedAuthenticationType kerberos -ReplicationAllowedFromAnyServer $true -DefaultStorageLocation E : \\ VHD Configure one VM and initiate initial replication # PLABDM01 Test-VMReplicationConnection PLABDC02 plabsa01 . practicelabs . com 80 Kerberos Enable-VMReplication PLABDC02 plabsa01 . practicelabs . com 80 Kerberos Start-VMInitialReplication PLABDC02 Repeat for another VM Enable-VMReplication PLABWIN811 plabsa01 . practicelabs . com 80 Kerberos Start-VMInitialReplication PLABWIN811 Initiate a planned failover Start-VMFailover PLABDC02 Start-VM PLABDC02 WSUS lab Almost totally GUI configuration of WSUS server after installation # PLABDM01 Add-WindowsFeature updateservices , updateservices-widdb , updateservices-services -IncludeManagementTools mkdir C : \\ updates 'C:\\Program Files\\Update Services\\Tools\\wsusutil.exe' postinstall content_dir = C : \\ updates WSUS (Sec+) # PLABDM01 Set-Disk -Number 1 -IsOffline $false Set-Disk -Number 1 -IsReadOnly $false New-Volume -DiskNumber 1 -Filesystem NTFS - FriendlyName WSUS -DriveLetter E Add-WindowsFeature updateservices , updateservices-widdb , updateservices-services -IncludeManagementTools 'C:\\Program Files\\Update Services\\Tools\\wsusutil.exe' postinstall content_dir = E : \\ updates $wsus = Get-WsusServer $config = $wsus . GetConfiguration () Proxy $config . UseProxy = $true $config . ProxyName = 'proxy' $config . ProxyServerPort = 80 English language $config . AllUpdateLanguagesEnabled = $false $config . SetEnabledUpdateLanguages ( 'en' ) $config . Save () Upstream server is Microsoft Update Set-WsusServerSynchronization \u2013 SyncFromMU Select products and classifications Get-WSUSProduct | Where-Object { $_ . Product . Title -In ( 'Windows 10' , 'Windows 8.1' )} | Set-WSUSProduct Get-WSUSClassification | Where-Object { $_ . Classification . Title -eq 'Critical Updates' } | Set-WSUSClassification Get WSUS Subscription and perform initial synchronization $sub = $wsus . GetSubscription () $sub . StartSynchronization () $sub . SynchronizeAutomatically = $false Storage replica lab Actually from the [Practice test][mu:70-740]. # On both origin and replica servers Install-WindowsFeature storage-replica -Restart Test-SRTopology New-SRGroup -ComputerName server1 New-SRPartnership -SourceComputerName server1 -SourceRGName rg1 -SourceVolumeName D : -SourceLogVolumeName E : -DestinationComputerName server2 -DestinationRGName rg2 -DestinationVolumeName D : -DestinationLogVolumeName E : Certificates ADCSCertificationAuthority Install ? ADCSWebEnrollment [ Install ][Install-AdcsWebEnrollment] [?][msdocs:Install-AdcsWebEnrollment] WindowsFeature Add ? Add-WindowsFeature -Name ADCS-Cert-Authority , ADCS-Web-Enrollment -IncludeManagementTools Install-ADCSCertificationAuthority -CAType EnterpriseRootCA Install-ADCSWebEnrollment Make a duplicate of the \"User\" template named \"SecureUser\", with the following changes - In the Request Handling tab, select \"Prompt the user during enrollment\" - In the Security tab ensure that Authenticated Users has Autoenroll allowed - In the Superseded Templates tab, select User - In the Subject Name tab, clear checkboxes for \"Include e-mail name in subject name\" and \"E-mail name\" Add-CATemplate SecureUser Hyperconverged failover cluster [MeasureUp][mu:70-740] New-Cluster -Name HC-CLU1 -Node node1 , node2 , node3 , node4 -NoStorage Enable-ClusterStorageSpacesDirect -CacheMode Disabled -AutoConfig : 0 -SkipEligibilityChecks New-StoragePool -StorageSubSystemFriendlyName * Cluster * -FriendlyName S2DPool -ProvisioningTyupeDefault Fixed -PhysicalDisk ( Get-PhysicalDisk | Where-Object -Property CanPool -eq $true ) $pool = Get-StoragePool S2DPool New-StorageTier -StoragePoolUniqueID ( $pool ). UniqueID -FriendlyName Performance -MediaType HDD -ResiliencySettingName Mirror New-StorageTier -StoragePoolUniqueID ( $pool ). UniqueID -FriendlyName Capacity -MediaType HDD -ResiliencySettingName Parity New-Volume -StoragePool $pool -FriendlyName SharedVol1 -FileSystem CSVFS_REFS -StorageTierFriendlyNames Performance , Capacity -StorageTierSizes 2gb , 10gb","title":"Labs"},{"location":"Windows/Labs/#labs","text":"","title":"Labs"},{"location":"Windows/Labs/#server-core","text":"Configure VM Rename-VMSwitch Intel * External New-VM PLABDMCORE02 1024 1 -SwitchName External -NewVHDPath 'D:\\VHD\\PLABDMCORE02\\Virtual Hard Disks\\PLABDMCORE02.vhdx' -NewVHDSize 127gb Set-VMDvdDrive ... # Can't find way to pass through host drive Z:\\ Add DVD drive Z: of host to VM's DVD drive (no Powershell equivalent found yet). Start-VM PLABDMCORE02 Install Windows, then continue configuring VM # PLABDMCORE02 Rename-Computer PLABDMCORE02 New-NetIpAddress 192 . 168 . 0 . 7 -PrefixLength 24 -InterfaceAlias Ethernet Set-DnsClientServerAddress -InterfaceAlias Ethernet -ServerAddresses 192 . 168 . 0 . 1 Restart-Computer Join computer to domain Add-Computer -DomainName practicelabs . com -DomainCredential practicelabs \\ administrator Restart-Computer Install packages remotely # PLABDC01 Enter-PSSession PLABDMCORE02 Add-WindowsFeature -Name dns , rsat-dns-server -IncludeManagementTools Add PLABDMCORE02 as a DNS server while remotely connected to DC (no Powershell equivalent found yet). Add a secondary lookup zone to PLABDMCORE02 (doesn't work; asks for Zonefile) Add-DnsServerSecondaryZone -Name practicelabs . com -MasterServers 192 . 168 . 0 . 1","title":"Server Core"},{"location":"Windows/Labs/#nano-server","text":"# PLABDM01 Copy-Item Z : \\ NanoServer \\ NanoServerImageGenerator \\*. ps * C : \\ nanoserver Import-Module C : \\ nNanoserver \\ NanoServerImageGenerator . psm1 Create the VHD image New-NanoServerImage -Edition Standard -MediaPath z : \\ -Basepath c : \\ nanoserver -Targetpath c : \\ nanoserver \\ PLABNANOSRV01 . vhdx -DeploymentType Guest -ComputerName PLABNANOSRV01 -Storage -Package Microsoft-NanoServer-DNS-Package Set-VMHardDiskDrive -VMName PLABNANOSRV01 -Path C : \\ nanoserver \\ PLABNANOSRV01 . vhdx","title":"Nano Server"},{"location":"Windows/Labs/#dism","text":"# PLABSA01 Set-Location C : \\ mkdir updates , images , mount , drivers Copy-Item \\\\ plabdm01 \\ Win10 \\ sources \\ install . wim C : \\ updates ( Get-Item C : \\ updates \\ install . wim ). IsReadOnly = $false Move specified .msu installers to C:\\updates and uncompress device driver to C:\\drivers Using Powershell: Mount-WindowsImage -Path C : \\ mount -ImagePath C : \\ images \\ install . wim -Index 1 Add-WindowsPackage -Path C : \\ mount -PackagePath C : \\ updates Add-WindowsDriver -Path C : \\ mount -Driver C : \\ drivers \\ display . driver \\ nv_dispi . inf Save-WindowsImage -Path C : \\ mount Dismount-WindowsImage -Path C : \\ mount -Save Using dism.exe : dism /mount-wim /wimfile:c:\\images\\install.wim /index:1 /mountdir:c:\\mount dism /image:c:\\mount /add-package /packagepath:c:\\updates dism /image:c:\\mount /add-driver /driver:c:\\drivers\\display.driver\\nv_dispi.inf dism /commit-wim /mountdir:c:\\mount dism /unmount-wim /commit /mountdir:c:\\mount","title":"DISM"},{"location":"Windows/Labs/#nlb-lab","text":"Configure a Network Load Balancing cluster. [Practice Lab][pl:70-740] # PLABDM01 and PLABSA01 Install-WindowsFeature web-server , web-webserver , web-mgmt-tools -IncludeManagementTools # PLABDM01 New-NetIpAddress 192 . 168 . 0 . 20 -PrefixLength 24 -InterfaceAlias Ethernet1 # PLABSA01 New-NetIpAddress 192 . 168 . 0 . 40 -PrefixLength 24 -InterfaceAlias Ethernet1 Install-WindowsFeature -Name nlb -IncludeAllSubFeature -IncludeManagementTools Using the Network Load Balancing Manager , create a new cluster named PLAB-NLB1 specifying multicast operation mode at 192.168.0.25, and create an appropriate DNS record on PLABDC01 . Install NLB on PLABDM01 and add it to the cluster through the NLB Manager. Edit the image located at C:\\inetpub\\wwwroot\\iisstart.png on either server. Once saved, these edits will now be visible when that server's website is visited at its hostname. Copy the contents of C:\\inetpub\\wwwroot to new directory C:\\nlbport on PLABDM01 . Host that directory as a website on port 6789, opening the firewall appropriately. New-Website -Name nlbport -PhysicalPath \"c:\\nlbport\" -Port 6789 New-NetFirewallRule -DisplayName NLBPort -Protocol TCP -LocalPort 6789 Returning to the NLB Manager, open Cluster Properties > Port Rules and remove the single defined port rule. Add a new port rule that specifies port 80 for \"multiple host\" filtering mode, and another that specifies port 6789 for \"single host\" filtering mode. Then from the host-specific port rule dialog box, select a handling priority of 10.","title":"NLB lab"},{"location":"Windows/Labs/#iscsi-storage","text":"Network topology # PLABDC01 New-NetIpAddress 192 . 168 . 0 . 10 -PrefixLength 24 -InterfaceAlias Ethernet2 # PLABDM01 New-NetIpAddress 192 . 168 . 0 . 20 -PrefixLength 24 -InterfaceAlias Ethernet1 New-NetIpAddress 192 . 168 . 1 . 20 -PrefixLength 24 -InterfaceAlias Ethernet2 Set-DnsClientServerAddress -InterfaceAlias Ethernet1 , Ethernet2 -ServerAddresses 192 . 168 . 0 . 1 iSCSI target # PLABDC01 Add-WindowsFeature fs-iscsitarget-server -IncludeManagementTools New-iSCSIVirtualDisk -SizeBytes 5gb -Path 'C:\\CorporateHD.vhdx' New-iSCSIServerTarget -TargetName plabdc01 -InitiatorId @( 'ipaddress:192.168.0.20' , 'ipaddress:192.168.1.20' ) Add-iSCSIVirtualDiskTargetMapping -Path 'C:\\CorporateHD.vhdx' -TargetName plabdc01 iSCSI initiator # PLABDM01 Start-Service msiscsi Get-NetFirewallServiceFilter -Service msiscsi | Get-NetFirewallRule | Enable-NetFirewallRule Connect-iSCSITarget -NodeAddress ( Get-iSCSITarget ). NodeAddress Multipath ... Finally, mount the drive New-Volume -DiskNumber 5 -FriendlyName iSCSI-Volume1 -FileSystem NTFS -DriveLetter R","title":"iSCSI Storage"},{"location":"Windows/Labs/#data-deduplication","text":"# PLABDM01 Add-WindowsFeature fs-data-deduplication -IncludeManagementTools Get-DedupVolume Get-DedupStatus Enable-DedupVolume -Volume 'D:' Set-DedupVolume -Volume 'D:' -ExcludeFolder 'D:\\win81' , 'D:\\virtual machines' New-DedupSchedule $name -Days Sunday , Monday , Tuesday , Wednesday , Thursday , Friday , Saturday -DurationHours 6 -Enabled $true -Type Optimization -Memory 50 Start-DedupJob D : -Type Optimization -Memory 50","title":"Data deduplication"},{"location":"Windows/Labs/#failover-cluster-lab","text":"Network topology # PLABDM01 New-NetIpAddress 192 . 168 . 0 . 20 -PrefixLength 24 -InterfaceAlias Ethernet1 New-NetIpAddress 192 . 168 . 0 . 21 -PrefixLength 24 -InterfaceAlias \"vEthernet (Internal network 1)\" New-NetIpAddress 192 . 168 . 1 . 20 -PrefixLength 24 -InterfaceAlias Ethernet2 Set-DnsClientServerAddresses Ethernet1 , Ethernet2 -ServerAddresses 192 . 168 . 0 . 1 Disable-NetAdapter \"vEthernet (Internal network 1)\" # PLABSA01 New-NetIpAddress 192 . 168 . 0 . 40 -PrefixLength 24 -InterfaceAlias Ethernet1 New-NetIpAddress 192 . 168 . 0 . 41 -PrefixLength 24 -InterfaceAlias \"vEthernet (Internal network 1)\" New-NetIpAddress 192 . 168 . 1 . 40 -PrefixLength 24 -InterfaceAlias Ethernet2 Set-DnsClientServerAddress Ethernet1 , Ethernet2 -Serveraddresses 192 . 168 . 0 . 1 Disable-NetAdapter \"vEthernet (Internal network 1)\" iSCSI target # PLABDC01 Add-WindowsFeature FS-iSCSITarget-Server -IncludeManagementTools Then go into Server Manager > File and Storage Services > iSCSI and start the New iSCSI Virtual Disk Wizard . Create a fixed 500 MB iSCSI virtual disk on C:\\ named \"QuorumHD\" and assign it to the PLABDC01 iSCSI target server. Add plabdm01.practicelabs.com and plabsa01.practicelabs.com as initiators, and keep authentication disabled, and confirm the disk has been created. Create another virtual disk, 30 GB dynamic on D:\\ names \"CSVHD\", and assign it to the same PLABDC01 iSCSI target server.","title":"Failover cluster lab"},{"location":"Windows/Labs/#configure-iscsi-initiators-to-iscsi-target-server","text":"On PLABDM01 , open Server Manager and click on Tools > iSCSI Initiator and connect to plabdc01.practicelabs.com . Bring disks 5 and 6 online and initialize them with [MBR][MBR] partition style. This can be done within diskmgmt.msc , where the shared volumes that have been created will appear beneath the local disks, or using PowerShell: # PLABDM01 New-Volume -DiskNumber 5 -DriveLetter Q -FileSystem NTFS -FriendlyName 'Quorum' New-Volume -DiskNumber 6 -DriveLetter V -FileSystem NTFS -FriendlyName 'CSV' Only bring the disks online on the other client # PLABSA01 Set-Disk -Number 4 , 5 -IsOffline $false","title":"Configure iSCSI initiators to iSCSI Target Server"},{"location":"Windows/Labs/#install-failover-clusters","text":"On both PLABDM01 and PLABSA01 : Install-WindowsFeature failover-clustering -IncludeManagementTools Then validate the installation through the Failover Cluster Manager (cluadmin.msc). Open the Validate Configuration wizard and enter plabdm01.practicelabs.com and plabsa01.practicelabs.com into the list of selected servers. Open the Create Cluster Wizard and create cluster \"PLHYPVCL01\" using those same two nodes, choosing 192.168. 0 .25 and 192.168. 1 .25 for the administrative access points. Open the Configure Cluster Quorum Wizard and configure a disk witness using the Quorum shared volume. Open Hyper-V Manager (virtmgmt.msc) and move PLABDC02 and PLABWIN811 to V:\\PLABDC02 and V:\\PLABWIN811 respectively, specifying the VM's storage . Confirm the move has taken place by inspecting each VM's hard drive in the settings. Add the Virtual Machine role to the Failover Cluster Manager , and select PLABDC02 and PLABWIN811 . This will create the \"PLABWIN811\" Role, which actually includes both selected VMs. Open PLABWIN811 Properties , then the Failover tab, and select \"Allow failback\" specifying between 1 and 2 hours. Open Hyper-V Manager and start PLABDC02 . Wait until it has booted completely, then return to Failover Cluster Manager , open Nodes, right-click on PLABDM01 and then Stop Cluster Service. This option forces a failover of the VM, which drains to PLABSA01 . This can be confirmed by returning to PLABSA01 and opening Hyper-V Manager , where it is revealed that PLABDC02 is running.","title":"Install failover clusters"},{"location":"Windows/Labs/#scale-out-fileserver-lab","text":"Plan network topology # PLABDM01 New-NetIpAddress 192 . 168 . 0 . 20 -PrefixLength 24 -InterfaceAlias Ethernet1 New-NetIpAddress 192 . 168 . 1 . 20 -PrefixLength 24 -InterfaceAlias Ethernet2 Set-DnsClientServerAddress -InterfaceAlias Ethernet1 , Ethernet2 -ServerAddresses 192 . 168 . 0 . 1 Disable-NetAdapter -InterfaceAlias 'vethernet (internal network 1)' # PLABSA01 New-NetIpAddress 192 . 168 . 0 . 40 -PrefixLength 24 -InterfaceAlias Ethernet1 New-NetIpAddress 192 . 168 . 1 . 40 -PrefixLength 24 -InterfaceAlias Ethernet2 Set-DnsClientServerAddress -InterfaceAlias Ethernet1 , Ethernet2 -ServerAddresses 192 . 168 . 0 . 1 Configure iSCSI target # PLABDC01 Install-WindowsFeature fs-iscsitarget-server -IncludeManagementTools New-IscsiVirtualDisk -Fixed -SizeBytes 500mb -Path 'C:\\QuorumHD.vhdx' New-IscsiVirtualDisk -SizeBytes 30gb -Path 'D:\\CSVHD.vhdx' New-IscsiServerTarget -TargetName PLABDC01 -InitiatorId @( 'iqn:iqn.1991-05.com.microsoft:plabsa01.practicelabs.com' , 'iqn:iqn.1991-05.com.microsoft:plabdm01.practicelabs.com' ) Add-IscsiVirtualDiskTargetMapping -Path 'C:\\QuorumHD.vhdx' -TargetName plabdc01 Add-IscsiVirtualDiskTargetMapping -Path 'D:\\CSVHD.vhdx' -TargetName plabdc01 Configure iSCSI initiator on both clients # PLABDM01 and PLABSA01 Start-Service msiscsi Get-NetFirewallServiceFilter -Service msiscsi | Get-NetFirewallRule | Enable-NetFirewallRule Connect-iSCSITarget -NodeAddress ( Get-iSCSITarget ). NodeAddress Install Failover Clustering role on nodes of the cluster # PLABDM01 and PLABSA01 Install-WindowsFeature failover-clustering -IncludeManagementTools Create a failover cluster through the GUI (the Powershell commands for cluster creation appear not to support adding multiple network networks) # PLABDM01 New-Cluster -Name PLABSCL01 -Node PLABDM01 , PLABSA01 -StaticAddress 192 . 168 . 0 . 25 -IgnoreNetwork 192 . 168 . 1 . 0 / 24 Set-ClusterQuorum -DiskWitness 'Cluster Disk 1'","title":"Scale-Out Fileserver lab"},{"location":"Windows/Labs/#hyper-v-storage-lab","text":"Create a VM with VHDX disks New-VM -Name PLABWIN102 -Generation 1 -MemoryStartupBytes 1536mb -SwitchName 'Private network 1' -NewVHDPath 'C:\\Users\\Public\\Documents\\Hyper-V\\Virtual hard disks\\PLABWIN102.vhdx' -NewVHDSizeBytes 127gb Set-VMDvdDrive -VMName PLABWIN102 -Path C : \\ Users \\ Administrator . PRACTICELABS \\ Documents \\ Eval81 . iso New-VHD -Path 'D:\\Virtual Machines\\PLAB1.vhdx' -Dynamic -SizeBytes 127gb Add-VMHardDiskDrive -VMName PLABWIN102 -ControllerType IDE -ControllerNumber 0 -Path 'D:\\Virtual Machines\\PLAB1.vhdx' Add a differencing disk New-VHD -Path 'D:\\Virtual Machines\\PLAB2.vhdx' -Differencing -ParentPath 'C:\\Users\\Public\\Documents\\Hyper-V\\Virtual hard disks\\PLABWIN102.vhdx' Add-VMHardDrive PLABWIN102 SCSI -Path 'D:\\Virtual Machines\\PLAB2.vhdx' Add a passthrough disk Set-Disk -Number 1 -IsOffline $true Add-VMHardDiskDrive PLABWIN102 IDE 1 -DiskNumber 1 Create a SAN New-VMSan -Name \"PLABS-Fc\" Add-VMFibreChannelHBA -VMName PLABDC02 -SanName \"PLABS-Fc\"","title":"Hyper-V storage lab"},{"location":"Windows/Labs/#hyper-v-replica-lab","text":"Emphasizes VMReplication cmdlets. # PLABDM01 Enable-NetFirewallRule VIRT-HVRHTTPL-In-TCP-NoScope , VIRT-HVRHTTPSL-In-TCP-NoScope Enable incoming replication on the receiving server #PLABSA01 Enable-NetFirewallRule VIRT-HVRHTTPL-In-TCP-NoScope , VIRT-HVRHTTPSL-In-TCP-NoScope Set-Disk -Number 1 , 3 , 4 -IsOffline $false -IsReadOnly $false New-Volume -DiskNumber 1 -FileSystem NTFS -FriendlyName VMReplica -DriveLetter E Set-VmReplicationServer -ReplicationEnabled $true -AllowedAuthenticationType kerberos -ReplicationAllowedFromAnyServer $true -DefaultStorageLocation E : \\ VHD Configure one VM and initiate initial replication # PLABDM01 Test-VMReplicationConnection PLABDC02 plabsa01 . practicelabs . com 80 Kerberos Enable-VMReplication PLABDC02 plabsa01 . practicelabs . com 80 Kerberos Start-VMInitialReplication PLABDC02 Repeat for another VM Enable-VMReplication PLABWIN811 plabsa01 . practicelabs . com 80 Kerberos Start-VMInitialReplication PLABWIN811 Initiate a planned failover Start-VMFailover PLABDC02 Start-VM PLABDC02","title":"Hyper-V replica lab"},{"location":"Windows/Labs/#wsus-lab","text":"Almost totally GUI configuration of WSUS server after installation # PLABDM01 Add-WindowsFeature updateservices , updateservices-widdb , updateservices-services -IncludeManagementTools mkdir C : \\ updates 'C:\\Program Files\\Update Services\\Tools\\wsusutil.exe' postinstall content_dir = C : \\ updates","title":"WSUS lab"},{"location":"Windows/Labs/#wsus-sec","text":"# PLABDM01 Set-Disk -Number 1 -IsOffline $false Set-Disk -Number 1 -IsReadOnly $false New-Volume -DiskNumber 1 -Filesystem NTFS - FriendlyName WSUS -DriveLetter E Add-WindowsFeature updateservices , updateservices-widdb , updateservices-services -IncludeManagementTools 'C:\\Program Files\\Update Services\\Tools\\wsusutil.exe' postinstall content_dir = E : \\ updates $wsus = Get-WsusServer $config = $wsus . GetConfiguration () Proxy $config . UseProxy = $true $config . ProxyName = 'proxy' $config . ProxyServerPort = 80 English language $config . AllUpdateLanguagesEnabled = $false $config . SetEnabledUpdateLanguages ( 'en' ) $config . Save () Upstream server is Microsoft Update Set-WsusServerSynchronization \u2013 SyncFromMU Select products and classifications Get-WSUSProduct | Where-Object { $_ . Product . Title -In ( 'Windows 10' , 'Windows 8.1' )} | Set-WSUSProduct Get-WSUSClassification | Where-Object { $_ . Classification . Title -eq 'Critical Updates' } | Set-WSUSClassification Get WSUS Subscription and perform initial synchronization $sub = $wsus . GetSubscription () $sub . StartSynchronization () $sub . SynchronizeAutomatically = $false","title":"WSUS (Sec+)"},{"location":"Windows/Labs/#storage-replica-lab","text":"Actually from the [Practice test][mu:70-740]. # On both origin and replica servers Install-WindowsFeature storage-replica -Restart Test-SRTopology New-SRGroup -ComputerName server1 New-SRPartnership -SourceComputerName server1 -SourceRGName rg1 -SourceVolumeName D : -SourceLogVolumeName E : -DestinationComputerName server2 -DestinationRGName rg2 -DestinationVolumeName D : -DestinationLogVolumeName E :","title":"Storage replica lab"},{"location":"Windows/Labs/#certificates","text":"ADCSCertificationAuthority Install ? ADCSWebEnrollment [ Install ][Install-AdcsWebEnrollment] [?][msdocs:Install-AdcsWebEnrollment] WindowsFeature Add ? Add-WindowsFeature -Name ADCS-Cert-Authority , ADCS-Web-Enrollment -IncludeManagementTools Install-ADCSCertificationAuthority -CAType EnterpriseRootCA Install-ADCSWebEnrollment Make a duplicate of the \"User\" template named \"SecureUser\", with the following changes - In the Request Handling tab, select \"Prompt the user during enrollment\" - In the Security tab ensure that Authenticated Users has Autoenroll allowed - In the Superseded Templates tab, select User - In the Subject Name tab, clear checkboxes for \"Include e-mail name in subject name\" and \"E-mail name\" Add-CATemplate SecureUser","title":"Certificates"},{"location":"Windows/Labs/#hyperconverged-failover-cluster","text":"[MeasureUp][mu:70-740] New-Cluster -Name HC-CLU1 -Node node1 , node2 , node3 , node4 -NoStorage Enable-ClusterStorageSpacesDirect -CacheMode Disabled -AutoConfig : 0 -SkipEligibilityChecks New-StoragePool -StorageSubSystemFriendlyName * Cluster * -FriendlyName S2DPool -ProvisioningTyupeDefault Fixed -PhysicalDisk ( Get-PhysicalDisk | Where-Object -Property CanPool -eq $true ) $pool = Get-StoragePool S2DPool New-StorageTier -StoragePoolUniqueID ( $pool ). UniqueID -FriendlyName Performance -MediaType HDD -ResiliencySettingName Mirror New-StorageTier -StoragePoolUniqueID ( $pool ). UniqueID -FriendlyName Capacity -MediaType HDD -ResiliencySettingName Parity New-Volume -StoragePool $pool -FriendlyName SharedVol1 -FileSystem CSVFS_REFS -StorageTierFriendlyNames Performance , Capacity -StorageTierSizes 2gb , 10gb","title":"Hyperconverged failover cluster"},{"location":"Windows/PowerShell/","text":"\ud83d\udc1a PowerShell Control flow if if ( $condition ) { # ... } switch switch ( $reference ) { $value1 { ... } $value2 { ... } } while $Values = while ( $true ) { (++ $Tick ) if ( $Tick -gt 2 ) { break } } # => @(1,2,3) do while $Values = do { 'Hello, world!' } while ( $false ) # => @('Hello, world!') Loops are implemented with ForEach-Object . 1 .. 5 | ForEach -Object { $_ + 2 } # => @(3,4,5,6,7) When values are stored in a variable at the end of a pipeline, it will create an array. while and do while loops are available, as well as until and do until loops which operate so long as their condition is false . Variables Variables are accessed by prefixing the identifier with $ . Automatic variables ( PSVersionTable , $IsLinux , etc) are PowerShell-specific. Windows environment variables are actually accessed through the Env virtual drive the syntax $Env:VARIABLE . APPDATA LOCALAPPDATA USERNAME USERPROFILE WINDIR Typing Variables can be typed by preceding their identifier with the datatype in brackets [double] $Price [int] $Quantity [string] $Description Compatible data can be cast or converted by simply specifying the type in an assignment, but when the data cannot be converted the interpreter will throw an error. $Number = [int] '04' $FailedCast = [int] 'Hello' Filtering results can be done with 5 commands: Where-Object (aliased to where and ? ): the most commonly used such command Select-Object (aliased to sc ed to specify specific columns of information to be displayed Select-String (aliased to sls ) ForEach-Object (aliased to foreach and % ) There are two different ways to construct a ForEach-Object statement: Script block , within which the variable $_ represents the current object Operation statement , more naturalistic, where you specify a property value or call a method. Hashtable Build a hash table using literals or the Add method Literal $fruit = @{ Apple = 'red' Orange = 'orange' Eggplant = 'purple' } Literal (inline) $fruit = @{ Apple = 'red' ; Orange = 'orange' ; Eggplant = 'purple' } Add $fruit = @{} $fruit . Add ( 'Apple' , 'red' ) $fruit . Add ( 'Orange' , 'orange' ) $fruit . Add ( 'Kiwi' , 'green' ) Hashtable methods Add $fruit = @{} $fruit . Add ( 'Apple' , 'red' ) $fruit . Add ( 'Orange' , 'orange' ) $fruit . Add ( 'Kiwi' , 'green' ) Clone # Deep copy or \"clone\" of a hashtable. $fruitclone = $fruit . Clone () Keys $fruit . Keys # => @('Apple','Orange','Kiwi') Values $fruit . Values # => @('red','orange','green') Count $fruit . Count Remove $fruit . Remove ( 'One' ) Unlike Python, a hash table can be made ordered, changing its data type: $fruit = [ordered] @{ Apple = 'red' ; Orange = 'orange' ; Eggplant = 'purple' } $fruit . GetType (). Name # => OrderedDictionary Documentation Single-line comments are preceded by # and block quotes are enclosed between <# and #> . Such a block comment will be parsed by PowerShell when running Get-Help . <# .SYNOPSIS This script coordinates the process of creating new employees .DESCRIPTION This script creates new users in Active Directory... .PARAMETER UserName The official logon name for the user... .PARAMETER HomeServer The server name where the user's home folder will live... #> <# .EXAMPLE New-CorpEmployee -UserName John-Doe -HomeServer HOMESERVER1 This example creates a single new employee... #> Functions Functions are declared with the function keyword and the body in braces, following this syntax: function Verb-Noun { # ... } Positional parameters can be referenced using the $args array, which contains all arguments passed to the function on invocation. Named parameters can be declared in one of two ways. Within the function body using the param keyword, followed by the name of the variable representing the parameter's value, enclosed in $(...) : Directly after the function name in parentheses, with each parameter separated by a comma. The name of the variable becomes the named parameter used when invoking the function. Default values for parameters can be specified by placing them within the parentheses. Parameters can be made mandatory by preceding the parameter name with [Parameter(Mandatory=$true)] . Parameters can be static typed by preceding the parameter identifier with the data type in brackets. Positional parameter function Get-LargeFiles { Get-ChildItem C : \\ Users \\ Michael \\ Documents | where { $_ . Length -gt $args [ 0 ] and ! $_PSIscontainer } | Sort-Object Length -Descending } Named parameter function Get-LargeFiles ( $Size ) { # param ($Size) Get-ChildItem C : \\ Users \\ Michael \\ Documents | where { $_ . Length -gt $Size -and ! $_ . PSIsContainer } | Sort-Object Length -Descending } Using param function Get-LargeFiles { param ( $Size ) Get-ChildItem C : \\ Users \\ Michael \\ Documents | where { $_ . Length -gt $Size -and ! $_ . PSIsContainer } | Sort-Object Length -Descending } Default value function Get-LargeFiles { param ( $Size = 2000 ) Get-ChildItem C : \\ Users \\ Michael \\ Documents | where { $_ . Length -gt $Size -and ! $_ . PSIsContainer } | Sort-Object Length -Descending } Typed function Get-LargeFiles { param ( [int] $Size = 2000 ) Get-ChildItem C : \\ Users \\ Michael \\ Documents | where { $_ . Length -gt $Size -and ! $_ . PSIsContainer } | Sort-Object Length -Descending } Invocation Get-LargeFiles -Size 2000 Switch parameters are typed as a switch data type. Boolean values can be explicitly set upon invocation using this syntax: Switch-Item function Switch -Item { param ( [switch] $on ) if ( $on ) { \"Switch on\" } else { \"Switch off\" } } Invocation Switch -Item # => Switch off Switch -Item -On # => Switch on Switch -Item -On : $false # => Switch off Attach common parameters to a custom function by placing the [CmdletBinding()] within the body of a function. This allows use of options like -Verbose or -Debug with custom functions. Now, using Write-Verbose and Write-Debug within the function body serve the dual purpose of outputting additional information at the time of execution, when needed, as well as documentation. Remote administration Powershell remoting can be done explicitly or implicitly . Remoting relies on WinRM , which is Microsoft's implementation of WSMAN. Explicit remoting is also 1-to-1 remoting, where an interactive Powershell prompt is brought up on a remote computer. One-to-many or fan-out remoting is possible with implicit remoting, where a command is transmitted to many computers. Testing Pester tests are organized in a hierarchy of blocks and run with Invoke-Pester : Describe { Context # optional { It { Should # assertion statements accept a value passed in via pipe and **must** be called within a `Describe` block } } } New-Fixture deploy Foo function Foo { # ... } Describe 'Foo' { $true | Should -Be $true } The block in braces is actually an argument pass to the -Fixture parameter. Describe \"Best airports in the USA\" -Fixture { It -Name \"RDU is one of the best airports\" -Test { $Output = Get-Airport -City \"Raleigh\" $Output | Should -BeOfType System . Collections . Hashtable } } Tasks Display computer name Cmdlet Get-ComputerInfo -Property CsName Alias gin . CsName $Env $Env:computername Command prompt hostname Generate a random password 20 characters long ( src ) Add-Type -AssemblyName 'System.Web' [System.Web.Security.Membership] :: GeneratePassword ( 20 , 3 ) Store credential Interactive $cred = Get-Credential Cmdlet $pw = ConvertTo-SecureString \"Password\" -AsPlainText -Force $cred = New-Object System . Management . Automation . PSCredential ( \"FullerP\" , $pw ) Create a new file in the current working directory named filename New-Item -ItemType File -Name filename Append content to file Add-Content C : \\ path \\ to \\ file $content New domain controller [Jones][Jones] Install-WindowsFeature AD-Domain-Services , DHCP -IncludeManagementTools Install-ADDSForest -DomainName corp . packtlab . com Add-DhcpServerv4Scope -Name \"PacktLabNet\" -StartRange 10 . 0 . 0 . 50 -EndRange 10 . 0 . 0 . 100 -SubnetMask 255 . 255 . 255 . 0 Set-DhcpServerv4OptionValue -DnsDomain corp . packtlab . com Add-DhcpServerInDC -DnsName dc . corp . packtlab . com New-AdUser -SamAccountName SysAdmin -AccountPassword ( Read-Host \"Set user password\" -AsSecureString ) -Name \"SysAdmin\" -Enabled $true -PasswordNeverExpires $true -ChangePasswordAtLogon $false Add-ADPrincipalGroupMembership -Identity \"CN=SysAdmin,CN=Users,DC=corp,DC=packtlab,DC=com\" , \"CN=Domain Admins,CN=Users,DC=corp,DC=packtlab,DC=com\" Get-ADPrincipalGroupMembership sysadmin Text-to-speech Initialize text-to-speech object scriptinglibrary.com Add-Type \u2013 AssemblyName System . Speech $tts = New-Object \u2013 TypeName System . Speech . Synthesis . SpeechSynthesizer $tts . Speak ( 'Hello, World!' ) List available voices Foreach ( $voice in $SpeechSynthesizer . GetInstalledVoices ()){ $Voice . VoiceInfo | Select-Object Gender , Name , Culture , Description } Change voice $tts . SelectVoice ( \"Microsoft Zira Desktop\" ) $tts . Speak ( 'Hello, World!' ) Set output to WAV file thinkpowershell.com $WavFileOut = Join-Path -Path $env:USERPROFILE -ChildPath \"Desktop\\thinkpowershell-demo.wav\" $SpeechSynthesizer . SetOutputToWaveFile ( $WavFileOut ) VHDX file Create a new 256 GB dynamic VHDX file, mount it, initialize it, and create and format the partition [Zacker][Zacker]: 91 New-VHD -Path C : \\ Data \\ disk1 . vhdx -SizeBytes 256GB -Dynamic | Mount-VHD -Passthru | Initialize-Disk -PassThru | New-Partition -DriveLetter X -UseMaximumSize | Format-Volume -Filesystem ntfs -FileSystemLabel data1 -Confirm : $False -Force Restart Wi-Fi adapter $adaptor = Get-WmiObject -Class Win32_NetworkAdapter | Where-Object { $_ . Name -like \"*Wireless*\" } $adaptor . Disable () $adaptor . Enable () Add a member to a group Add-ADGroupMember -Identity $group -Members $user1 , $user2 Add a new local admin nlu ansible Add-LocalGroupMember Administrators ansible Configure secure remoting using a self-signed certificate Create a virtual switch with SET enabled Create a virtual switch with SET enabled. [Zacker][Zacker]: 254 New-VMSwitch -Name SETSwitch -NetAdapterName \"nic1\" , \"nic2\" -EnableEmbeddedTeaming $true Add new virtual network adapters to VMs Add-VMNetworkAdapter -VMName server1 -SwitchName setswitch -Name set1 Enable RDMA with [ Get- ][Get-NetAdapterRdma] and [ Enable-NetAdapterRdma ][Enable-NetAdapterRdma]. Implement nested virtualization Both the physical host and the nested virtual host must be running Windows Server 2016, but before installing Hyper-V on the nested host, the following configurations must be made. [Zacker][Zacker]: 181 Provide nested host's processor with access to virtualization technology on the physical host Set-VMProcessor -VMName server1 -ExposeVirtualizationExtensions $true Disable dynamic memory Set-VMMemory -VMName SRV01 -DynamicMemoryEnabled $false Configure 2 virtual processors Set-VMProcessor -VMName SVR01 -Count 2 Turn on MAC address spoofing Set-VMNetworkAdapter -VMName SVR01 -Name \"NetworkAdapter\" -MACAddressSpoofing On Enable CredSSP On the remote (managed) server [Zacker][Zacker]: 176 Enable-PSRemoting Enable-WSManCredSSP Add the fully-qualified domain name of the Hyper-V server to be managed to the local system's WSMan trusted hosts list Set-Item WSMan : \\ localhost \\ client \\ trustedhosts -Value \"hypervserver.domain.com\" Enable the use of CredSSP on the client Enable-WSManCredSSP -Role client -DelegateComputer \"hypervserver.domain.com\" Configure Server Core Manually configure network interface, if a DHCP server is unavailable [Zacker][Zacker]: 19 New-NetIPAddress 10 . 0 . 0 . 3 -InterfaceAlias \"Ethernet' -PrefixLength 24 Configure the DNS server addresses for the adapter Set-DnsClientServerAddress -InterfaceIndex 6 -ServerAddresses ( \"192.168.0.1\" , \"192.168.0.2\" ) Rename the computer and join it to a domain Add-Computer -DomainName adatum . com -NewName Server8 -Credential adatum \\ administrator Update Server Core image Mount-WindowsImage -ImagePath .\\ CoreServer . vhdx -Path .\\ MountDir -Index 1 Add-WindowsPackage -Path .\\ MountDir -PackagePath C : \\ ServicingPackages_cabs Dismount-WindowsImage -Path .\\ MountDir -Save Implement DDA Discrete Device Assignment (DDA) begins with finding the Instance ID of the device needed to be passed through. [Zacker][Zacker]: 212 Get-PnpDevice -PresentOnly Disable-PnpDevice -InstanceId # Remove host-installed drivers Get-PnpDeviceProperty # Provide `InstanceId` and `KeyName` values in order to get value for `LocationPath` parameter in next command Dismount-VmHostAssignableDevice -LocationPath # Remove the device from host control Add-VMAssignableDevice -VM -LocationPath # Attach the device to a guest Configure live migration Live migration is possible between Hyper-V hosts that are not clustered, but they must be within the same (or trusted) domains. [Zacker][Zacker]: 306 Enable-VMMigration Set-VMMigrationNetwork 192 . 168 . 4 . 0 Set-VMHost -VirtualMachineMigrationAuthenticationType Kerberos Set-VMHost -VirtualMachineMigrationPerformanceOption smbtransport Configure S2D cluster New-Cluster -Name cluster1 -node server1 , server2 , server3 , server4 -NoStorage Enable-ClusterStorageSpacesDirect Install Docker Enterprise [Zacker][Zacker]: 266 Install-Module -Name dockermsftprovider -repository psgallery -force Install-Package -Name docker -ProviderName dockermsftprovider Handle XML files Find a sample XML file here Assign the output of [ gc ][Get-Content] to a variable [xml] $xdoc = gc $xmlfile The XML tree can be viewed in VS Code using the XML Tools extension. The object itself can be treated as a first-class Powershell object using dot notation. red-gate.com $xdoc . catalog . book | Format-Table -Autosize Arrays of elements can be accessed by their index $xdoc . catalog . book [ 0 ] Nodes in the XML object can also be navigated using XPath notation with the SelectNodes and SelectSingleNode methods. $xdoc . SelectNodes ( '//author' ) This produces the same output as the command above ( in XPath nodes are 1-indexed ). ``` powershell $xdoc . SelectSingleNode ( '//book[1]' ) [ Select-Xml ][Select-Xml] wraps the returned XML node with additional metadata, including the pattern searched. However, it can accept piped input. ( Select-Xml -Xml $xdoc -Xpath '//book[1]' ). Node ( $xml | Select-Xml -Xpath '//book[1]' ). Node Update Server Core images [MeasureUp Lab][pl:70-740] Mount-WindowsImage -ImagePath .\\ CoreServer . vhdx -Path .\\ MountDir -Index 1 Add-WindowsPackage -Path .\\ MountDir PackagePath C : \\ ServicingPackages_cabs Dismount-WindowsImage -Path .\\ MountDir -Save Pass-through disk [Zacker][Zacker]: 226 Set-Disk -Number 2 -IsOffline $true Add-VMHardDiskDrive -VMName server1 -ControllerType scsi -DiskNumber 2 Site-aware failover cluster Configure failover clusters for two offices [Zacker][Zacker]: 366 New-ClusterFaultDomain -Name ny -Type site -Description \"Primary\" -Location \"New York, NY\" New-ClusterFaultDomain -Name sf -Type site -Description \"Secondary\" -Location \"San Francisco, CA\" Set-ClusterFaultDomain -Name node1 -Parent ny Set-ClusterFaultDomain -Name node2 -Parent ny Set-ClusterFaultDomain -Name node3 -Parent sf Set-ClusterFaultDomain -Name node4 -Parent sf Filter AD account information Get-aduser -filter {( SamAccountName -like \"*CA0*\" )} -properties Displayname , SaMaccountName , Enabled , EmailAddress , proxyaddresses | Where {( $_ . EmailAddress -notlike \"*@*\" )} | Where {( $_ . Enabled -eq $True )} | Select Displayname , SaMaccountName , Enabled , EmailAddress , @{ L = \u2019 ProxyAddress_1 '; E={$_.proxyaddresses[0]}}, @{L=\u2019ProxyAddress_2' ; E ={ $_ . ProxyAddresses [ 1 ]}} | Export-csv .\\ usersnoemail2 . csv -notypeinformation Create VM with installation media [Practice Lab][pl:70-740] New-VM PLABWIN102 1536mb 1 -SwitchName 'Private network 1' -NewVHDPath 'C:\\Users\\Public\\Documents\\Hyper-V\\Virtual hard disks\\PLABWIN102.vhdx' -NewVHDSizeBytes 127gb Set-VMDvdDrive -VMName PLABWIN102 -Path C : \\ Users \\ Administrator . PRACTICELABS \\ Documents \\ Eval81 . iso Registry Description Affected key Fix Windows Search bar docs.microsoft.com HKCU:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Search Remove 3D Objects howtogeek.com HKLM:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Explorer\\MyComputer\\NameSpace Display seconds in system clock howtogeek.com HKCU:\\Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\Advanced Disable Aero Shake howtogeek.com HKCU:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Explorer\\Advanced Remove 3D Objects from This PC howtogeek.com Remove-Item 'HKLM:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Explorer\\MyComputer\\NameSpace\\{0DB7E03F-FC29-4DC6-9020-FF41B59E513A}' Add seconds to clock howtogeek.com New-Item -Path HKCU : \\ Software \\ Microsoft \\ Windows \\ CurrentVersion \\ Explorer \\ Advanced -Name ShowSecondsInSystemClock -Value 1 Restart-Computer New-Item -Path HKCU : \\ SOFTWARE \\ Microsoft \\ Windows \\ CurrentVersion \\ Search -Name BingSearchEnabled -Value 0 New-Item -Path HKCU : \\ SOFTWARE \\ Microsoft \\ Windows \\ CurrentVersion \\ Search -Name CortanaConsent -Value 0 Safely combine related registry modifications using [ Start-Transaction ][Start-Transaction] and [ Complete-Transaction ][Complete-Transaction] [Holmes][Holmes]: 604 Start-Transaction New-Item TempKey -UseTransaction Complete-Transaction Remove User UAC for local users. ref Set-ItemProperty -Path HKLM : \\ Software \\ Microsoft \\ Windows \\ CurrentVersion \\ Policies \\ System -Name EnableLUA -Value 0 WinForms Pastebin # Load required assemblies [void] [System.Reflection.Assembly] :: LoadWithPartialName ( \"System.Windows.Forms\" ) # Drawing form and controls $Form_HelloWorld = New-Object System . Windows . Forms . Form $Form_HelloWorld . Text = \"Hello World\" $Form_HelloWorld . Size = New-Object System . Drawing . Size ( 272 , 160 ) $Form_HelloWorld . FormBorderStyle = \"FixedDialog\" $Form_HelloWorld . TopMost = $true $Form_HelloWorld . MaximizeBox = $false $Form_HelloWorld . MinimizeBox = $false $Form_HelloWorld . ControlBox = $true $Form_HelloWorld . StartPosition = \"CenterScreen\" $Form_HelloWorld . Font = \"Segoe UI\" # adding a label to my form $label_HelloWorld = New-Object System . Windows . Forms . Label $label_HelloWorld . Location = New-Object System . Drawing . Size ( 8 , 8 ) $label_HelloWorld . Size = New-Object System . Drawing . Size ( 240 , 32 ) $label_HelloWorld . TextAlign = \"MiddleCenter\" $label_HelloWorld . Text = \"Hello World\" $Form_HelloWorld . Controls . Add ( $label_HelloWorld ) # add a button $button_ClickMe = New-Object System . Windows . Forms . Button $button_ClickMe . Location = New-Object System . Drawing . Size ( 8 , 80 ) $button_ClickMe . Size = New-Object System . Drawing . Size ( 240 , 32 ) $button_ClickMe . TextAlign = \"MiddleCenter\" $button_ClickMe . Text = \"Click Me!\" $button_ClickMe . Add_Click ({ $button_ClickMe . Text = \"You did click me!\" Start-Process calc . exe }) $Form_HelloWorld . Controls . Add ( $button_ClickMe ) # show form $Form_HelloWorld . Add_Shown ({ $Form_HelloWorld . Activate ()}) [void] $Form_HelloWorld . ShowDialog () Modules Create a new module by placing a .psm1 file in a directory of the same name .\\Starship\\Starship.psm1 Functions defined within the module can be loaded with [ Import-Module ][Import-Module] (execution policy must allow this). ipmo .\\ Starship To import classes, a different syntax must be used source Using module .\\ Starship Sample enumeration PowerShellMagazine Add-Type -AssemblyName System . Drawing $count = [Enum] :: GetValues ( [System.Drawing.KnownColor] ). Count [System.Drawing.KnownColor] ( Get-Random -Minimum 1 -Maximum $count ) Migrate a VM Enable-VMMigration Set-VMMigrationNetwork 192 . 168 . 10 . 1 Set-VMHost -VirtualMachineMigrationAuthenticationType Kerberos Set-VMHost -VirtualMachineMigrationPerformanceOption SMBTransport Storage Spaces Direct [MeasureUp][mu:70-740] New-Cluster -Name HC-CLU1 -Node node1 , node2 , node3 , node4 -NoStorage Enable-ClusterStorageSpacesDirect -CacheMode Disabled -AutoConfig : 0 -SkipEligibilityChecks New-StoragePool -StorageSubSystemFriendlyName * Cluster * -FriendlyName S2DPool -ProvisioningTypeDefault Fixed -PhysicalDisk ( Get-PhysicalDisk | Where-Object -Property CanPool -eq $true ) $pool = Get-StoragePool S2DPool New-StorageTier -StoragePoolUniqueID ( $pool ). UniqueID -FriendlyName Performance -MediaType HDD -ResiliencySettingName Mirror New-StorageTier -StoragePoolUniqueID ( $pool ). UniqueID -FriendlyName Capacity -MediaType HDD -ResiliencySettingName Parity The next step would be the creation of a new volume New-Volume -StoragePool $pool -FriendlyName SharedVol1 -FileSystem CSVFS_REFS -StorageTiersFriendlyNames Performance , Capacity -StorageTierSizes 2GB , 10GB Scheduled task Automatically run SSH server in WSL on system start $action = New-ScheduledTaskAction -Execute C : \\ WINDOWS \\ System32 \\ bash . exe -Argument '-c sudo service ssh start' $trigger = New-ScheduledTaskTrigger -AtLogon Register-ScheduledTask -TaskName 'SSH server' -Trigger $trigger -Action $action Network connection alert Play a tone when network connection has been (re)-established. while ( $true ) { if (( Test-NetConnection 8 . 8 . 8 . 8 -WarningAction SilentlyContinue ). PingSucceeded -eq $true ) { [System.Console] :: Beep ( 1000 , 100 ) break } } while ( Test-NetConnection 8 . 8 . 8 . 8 -WarningAction SilentlyContinue ). PingSucceeded -eq $false ) { continue }","title":"\ud83d\udc1a PowerShell"},{"location":"Windows/PowerShell/#powershell","text":"","title":"\ud83d\udc1a PowerShell"},{"location":"Windows/PowerShell/#control-flow","text":"if if ( $condition ) { # ... } switch switch ( $reference ) { $value1 { ... } $value2 { ... } } while $Values = while ( $true ) { (++ $Tick ) if ( $Tick -gt 2 ) { break } } # => @(1,2,3) do while $Values = do { 'Hello, world!' } while ( $false ) # => @('Hello, world!') Loops are implemented with ForEach-Object . 1 .. 5 | ForEach -Object { $_ + 2 } # => @(3,4,5,6,7) When values are stored in a variable at the end of a pipeline, it will create an array. while and do while loops are available, as well as until and do until loops which operate so long as their condition is false .","title":"Control flow"},{"location":"Windows/PowerShell/#variables","text":"Variables are accessed by prefixing the identifier with $ . Automatic variables ( PSVersionTable , $IsLinux , etc) are PowerShell-specific. Windows environment variables are actually accessed through the Env virtual drive the syntax $Env:VARIABLE . APPDATA LOCALAPPDATA USERNAME USERPROFILE WINDIR","title":"Variables"},{"location":"Windows/PowerShell/#typing","text":"Variables can be typed by preceding their identifier with the datatype in brackets [double] $Price [int] $Quantity [string] $Description Compatible data can be cast or converted by simply specifying the type in an assignment, but when the data cannot be converted the interpreter will throw an error. $Number = [int] '04' $FailedCast = [int] 'Hello' Filtering results can be done with 5 commands: Where-Object (aliased to where and ? ): the most commonly used such command Select-Object (aliased to sc ed to specify specific columns of information to be displayed Select-String (aliased to sls ) ForEach-Object (aliased to foreach and % ) There are two different ways to construct a ForEach-Object statement: Script block , within which the variable $_ represents the current object Operation statement , more naturalistic, where you specify a property value or call a method.","title":"Typing"},{"location":"Windows/PowerShell/#hashtable","text":"Build a hash table using literals or the Add method Literal $fruit = @{ Apple = 'red' Orange = 'orange' Eggplant = 'purple' } Literal (inline) $fruit = @{ Apple = 'red' ; Orange = 'orange' ; Eggplant = 'purple' } Add $fruit = @{} $fruit . Add ( 'Apple' , 'red' ) $fruit . Add ( 'Orange' , 'orange' ) $fruit . Add ( 'Kiwi' , 'green' ) Hashtable methods Add $fruit = @{} $fruit . Add ( 'Apple' , 'red' ) $fruit . Add ( 'Orange' , 'orange' ) $fruit . Add ( 'Kiwi' , 'green' ) Clone # Deep copy or \"clone\" of a hashtable. $fruitclone = $fruit . Clone () Keys $fruit . Keys # => @('Apple','Orange','Kiwi') Values $fruit . Values # => @('red','orange','green') Count $fruit . Count Remove $fruit . Remove ( 'One' ) Unlike Python, a hash table can be made ordered, changing its data type: $fruit = [ordered] @{ Apple = 'red' ; Orange = 'orange' ; Eggplant = 'purple' } $fruit . GetType (). Name # => OrderedDictionary","title":"Hashtable"},{"location":"Windows/PowerShell/#documentation","text":"Single-line comments are preceded by # and block quotes are enclosed between <# and #> . Such a block comment will be parsed by PowerShell when running Get-Help . <# .SYNOPSIS This script coordinates the process of creating new employees .DESCRIPTION This script creates new users in Active Directory... .PARAMETER UserName The official logon name for the user... .PARAMETER HomeServer The server name where the user's home folder will live... #> <# .EXAMPLE New-CorpEmployee -UserName John-Doe -HomeServer HOMESERVER1 This example creates a single new employee... #>","title":"Documentation"},{"location":"Windows/PowerShell/#functions","text":"Functions are declared with the function keyword and the body in braces, following this syntax: function Verb-Noun { # ... } Positional parameters can be referenced using the $args array, which contains all arguments passed to the function on invocation. Named parameters can be declared in one of two ways. Within the function body using the param keyword, followed by the name of the variable representing the parameter's value, enclosed in $(...) : Directly after the function name in parentheses, with each parameter separated by a comma. The name of the variable becomes the named parameter used when invoking the function. Default values for parameters can be specified by placing them within the parentheses. Parameters can be made mandatory by preceding the parameter name with [Parameter(Mandatory=$true)] . Parameters can be static typed by preceding the parameter identifier with the data type in brackets. Positional parameter function Get-LargeFiles { Get-ChildItem C : \\ Users \\ Michael \\ Documents | where { $_ . Length -gt $args [ 0 ] and ! $_PSIscontainer } | Sort-Object Length -Descending } Named parameter function Get-LargeFiles ( $Size ) { # param ($Size) Get-ChildItem C : \\ Users \\ Michael \\ Documents | where { $_ . Length -gt $Size -and ! $_ . PSIsContainer } | Sort-Object Length -Descending } Using param function Get-LargeFiles { param ( $Size ) Get-ChildItem C : \\ Users \\ Michael \\ Documents | where { $_ . Length -gt $Size -and ! $_ . PSIsContainer } | Sort-Object Length -Descending } Default value function Get-LargeFiles { param ( $Size = 2000 ) Get-ChildItem C : \\ Users \\ Michael \\ Documents | where { $_ . Length -gt $Size -and ! $_ . PSIsContainer } | Sort-Object Length -Descending } Typed function Get-LargeFiles { param ( [int] $Size = 2000 ) Get-ChildItem C : \\ Users \\ Michael \\ Documents | where { $_ . Length -gt $Size -and ! $_ . PSIsContainer } | Sort-Object Length -Descending } Invocation Get-LargeFiles -Size 2000 Switch parameters are typed as a switch data type. Boolean values can be explicitly set upon invocation using this syntax: Switch-Item function Switch -Item { param ( [switch] $on ) if ( $on ) { \"Switch on\" } else { \"Switch off\" } } Invocation Switch -Item # => Switch off Switch -Item -On # => Switch on Switch -Item -On : $false # => Switch off Attach common parameters to a custom function by placing the [CmdletBinding()] within the body of a function. This allows use of options like -Verbose or -Debug with custom functions. Now, using Write-Verbose and Write-Debug within the function body serve the dual purpose of outputting additional information at the time of execution, when needed, as well as documentation.","title":"Functions"},{"location":"Windows/PowerShell/#remote-administration","text":"Powershell remoting can be done explicitly or implicitly . Remoting relies on WinRM , which is Microsoft's implementation of WSMAN. Explicit remoting is also 1-to-1 remoting, where an interactive Powershell prompt is brought up on a remote computer. One-to-many or fan-out remoting is possible with implicit remoting, where a command is transmitted to many computers.","title":"Remote administration"},{"location":"Windows/PowerShell/#testing","text":"Pester tests are organized in a hierarchy of blocks and run with Invoke-Pester : Describe { Context # optional { It { Should # assertion statements accept a value passed in via pipe and **must** be called within a `Describe` block } } } New-Fixture deploy Foo function Foo { # ... } Describe 'Foo' { $true | Should -Be $true } The block in braces is actually an argument pass to the -Fixture parameter. Describe \"Best airports in the USA\" -Fixture { It -Name \"RDU is one of the best airports\" -Test { $Output = Get-Airport -City \"Raleigh\" $Output | Should -BeOfType System . Collections . Hashtable } }","title":"Testing"},{"location":"Windows/PowerShell/#tasks","text":"Display computer name Cmdlet Get-ComputerInfo -Property CsName Alias gin . CsName $Env $Env:computername Command prompt hostname Generate a random password 20 characters long ( src ) Add-Type -AssemblyName 'System.Web' [System.Web.Security.Membership] :: GeneratePassword ( 20 , 3 ) Store credential Interactive $cred = Get-Credential Cmdlet $pw = ConvertTo-SecureString \"Password\" -AsPlainText -Force $cred = New-Object System . Management . Automation . PSCredential ( \"FullerP\" , $pw ) Create a new file in the current working directory named filename New-Item -ItemType File -Name filename Append content to file Add-Content C : \\ path \\ to \\ file $content","title":"Tasks"},{"location":"Windows/PowerShell/#new-domain-controller","text":"[Jones][Jones] Install-WindowsFeature AD-Domain-Services , DHCP -IncludeManagementTools Install-ADDSForest -DomainName corp . packtlab . com Add-DhcpServerv4Scope -Name \"PacktLabNet\" -StartRange 10 . 0 . 0 . 50 -EndRange 10 . 0 . 0 . 100 -SubnetMask 255 . 255 . 255 . 0 Set-DhcpServerv4OptionValue -DnsDomain corp . packtlab . com Add-DhcpServerInDC -DnsName dc . corp . packtlab . com New-AdUser -SamAccountName SysAdmin -AccountPassword ( Read-Host \"Set user password\" -AsSecureString ) -Name \"SysAdmin\" -Enabled $true -PasswordNeverExpires $true -ChangePasswordAtLogon $false Add-ADPrincipalGroupMembership -Identity \"CN=SysAdmin,CN=Users,DC=corp,DC=packtlab,DC=com\" , \"CN=Domain Admins,CN=Users,DC=corp,DC=packtlab,DC=com\" Get-ADPrincipalGroupMembership sysadmin","title":"New domain controller"},{"location":"Windows/PowerShell/#text-to-speech","text":"Initialize text-to-speech object scriptinglibrary.com Add-Type \u2013 AssemblyName System . Speech $tts = New-Object \u2013 TypeName System . Speech . Synthesis . SpeechSynthesizer $tts . Speak ( 'Hello, World!' ) List available voices Foreach ( $voice in $SpeechSynthesizer . GetInstalledVoices ()){ $Voice . VoiceInfo | Select-Object Gender , Name , Culture , Description } Change voice $tts . SelectVoice ( \"Microsoft Zira Desktop\" ) $tts . Speak ( 'Hello, World!' ) Set output to WAV file thinkpowershell.com $WavFileOut = Join-Path -Path $env:USERPROFILE -ChildPath \"Desktop\\thinkpowershell-demo.wav\" $SpeechSynthesizer . SetOutputToWaveFile ( $WavFileOut )","title":"Text-to-speech"},{"location":"Windows/PowerShell/#vhdx-file","text":"Create a new 256 GB dynamic VHDX file, mount it, initialize it, and create and format the partition [Zacker][Zacker]: 91 New-VHD -Path C : \\ Data \\ disk1 . vhdx -SizeBytes 256GB -Dynamic | Mount-VHD -Passthru | Initialize-Disk -PassThru | New-Partition -DriveLetter X -UseMaximumSize | Format-Volume -Filesystem ntfs -FileSystemLabel data1 -Confirm : $False -Force","title":"VHDX file"},{"location":"Windows/PowerShell/#restart-wi-fi-adapter","text":"$adaptor = Get-WmiObject -Class Win32_NetworkAdapter | Where-Object { $_ . Name -like \"*Wireless*\" } $adaptor . Disable () $adaptor . Enable ()","title":"Restart Wi-Fi adapter"},{"location":"Windows/PowerShell/#add-a-member-to-a-group","text":"Add-ADGroupMember -Identity $group -Members $user1 , $user2","title":"Add a member to a group"},{"location":"Windows/PowerShell/#add-a-new-local-admin","text":"nlu ansible Add-LocalGroupMember Administrators ansible","title":"Add a new local admin"},{"location":"Windows/PowerShell/#configure-secure-remoting-using-a-self-signed-certificate","text":"","title":"Configure secure remoting using a self-signed certificate"},{"location":"Windows/PowerShell/#create-a-virtual-switch-with-set-enabled","text":"Create a virtual switch with SET enabled. [Zacker][Zacker]: 254 New-VMSwitch -Name SETSwitch -NetAdapterName \"nic1\" , \"nic2\" -EnableEmbeddedTeaming $true Add new virtual network adapters to VMs Add-VMNetworkAdapter -VMName server1 -SwitchName setswitch -Name set1 Enable RDMA with [ Get- ][Get-NetAdapterRdma] and [ Enable-NetAdapterRdma ][Enable-NetAdapterRdma].","title":"Create a virtual switch with SET enabled"},{"location":"Windows/PowerShell/#implement-nested-virtualization","text":"Both the physical host and the nested virtual host must be running Windows Server 2016, but before installing Hyper-V on the nested host, the following configurations must be made. [Zacker][Zacker]: 181 Provide nested host's processor with access to virtualization technology on the physical host Set-VMProcessor -VMName server1 -ExposeVirtualizationExtensions $true Disable dynamic memory Set-VMMemory -VMName SRV01 -DynamicMemoryEnabled $false Configure 2 virtual processors Set-VMProcessor -VMName SVR01 -Count 2 Turn on MAC address spoofing Set-VMNetworkAdapter -VMName SVR01 -Name \"NetworkAdapter\" -MACAddressSpoofing On","title":"Implement nested virtualization"},{"location":"Windows/PowerShell/#enable-credssp","text":"On the remote (managed) server [Zacker][Zacker]: 176 Enable-PSRemoting Enable-WSManCredSSP Add the fully-qualified domain name of the Hyper-V server to be managed to the local system's WSMan trusted hosts list Set-Item WSMan : \\ localhost \\ client \\ trustedhosts -Value \"hypervserver.domain.com\" Enable the use of CredSSP on the client Enable-WSManCredSSP -Role client -DelegateComputer \"hypervserver.domain.com\"","title":"Enable CredSSP"},{"location":"Windows/PowerShell/#configure-server-core","text":"Manually configure network interface, if a DHCP server is unavailable [Zacker][Zacker]: 19 New-NetIPAddress 10 . 0 . 0 . 3 -InterfaceAlias \"Ethernet' -PrefixLength 24 Configure the DNS server addresses for the adapter Set-DnsClientServerAddress -InterfaceIndex 6 -ServerAddresses ( \"192.168.0.1\" , \"192.168.0.2\" ) Rename the computer and join it to a domain Add-Computer -DomainName adatum . com -NewName Server8 -Credential adatum \\ administrator","title":"Configure Server Core"},{"location":"Windows/PowerShell/#update-server-core-image","text":"Mount-WindowsImage -ImagePath .\\ CoreServer . vhdx -Path .\\ MountDir -Index 1 Add-WindowsPackage -Path .\\ MountDir -PackagePath C : \\ ServicingPackages_cabs Dismount-WindowsImage -Path .\\ MountDir -Save","title":"Update Server Core image"},{"location":"Windows/PowerShell/#implement-dda","text":"Discrete Device Assignment (DDA) begins with finding the Instance ID of the device needed to be passed through. [Zacker][Zacker]: 212 Get-PnpDevice -PresentOnly Disable-PnpDevice -InstanceId # Remove host-installed drivers Get-PnpDeviceProperty # Provide `InstanceId` and `KeyName` values in order to get value for `LocationPath` parameter in next command Dismount-VmHostAssignableDevice -LocationPath # Remove the device from host control Add-VMAssignableDevice -VM -LocationPath # Attach the device to a guest","title":"Implement DDA"},{"location":"Windows/PowerShell/#configure-live-migration","text":"Live migration is possible between Hyper-V hosts that are not clustered, but they must be within the same (or trusted) domains. [Zacker][Zacker]: 306 Enable-VMMigration Set-VMMigrationNetwork 192 . 168 . 4 . 0 Set-VMHost -VirtualMachineMigrationAuthenticationType Kerberos Set-VMHost -VirtualMachineMigrationPerformanceOption smbtransport","title":"Configure live migration"},{"location":"Windows/PowerShell/#configure-s2d-cluster","text":"New-Cluster -Name cluster1 -node server1 , server2 , server3 , server4 -NoStorage Enable-ClusterStorageSpacesDirect","title":"Configure S2D cluster"},{"location":"Windows/PowerShell/#install-docker-enterprise","text":"[Zacker][Zacker]: 266 Install-Module -Name dockermsftprovider -repository psgallery -force Install-Package -Name docker -ProviderName dockermsftprovider","title":"Install Docker Enterprise"},{"location":"Windows/PowerShell/#handle-xml-files","text":"Find a sample XML file here Assign the output of [ gc ][Get-Content] to a variable [xml] $xdoc = gc $xmlfile The XML tree can be viewed in VS Code using the XML Tools extension. The object itself can be treated as a first-class Powershell object using dot notation. red-gate.com $xdoc . catalog . book | Format-Table -Autosize Arrays of elements can be accessed by their index $xdoc . catalog . book [ 0 ] Nodes in the XML object can also be navigated using XPath notation with the SelectNodes and SelectSingleNode methods. $xdoc . SelectNodes ( '//author' ) This produces the same output as the command above ( in XPath nodes are 1-indexed ). ``` powershell $xdoc . SelectSingleNode ( '//book[1]' ) [ Select-Xml ][Select-Xml] wraps the returned XML node with additional metadata, including the pattern searched. However, it can accept piped input. ( Select-Xml -Xml $xdoc -Xpath '//book[1]' ). Node ( $xml | Select-Xml -Xpath '//book[1]' ). Node","title":"Handle XML files"},{"location":"Windows/PowerShell/#update-server-core-images","text":"[MeasureUp Lab][pl:70-740] Mount-WindowsImage -ImagePath .\\ CoreServer . vhdx -Path .\\ MountDir -Index 1 Add-WindowsPackage -Path .\\ MountDir PackagePath C : \\ ServicingPackages_cabs Dismount-WindowsImage -Path .\\ MountDir -Save","title":"Update Server Core images"},{"location":"Windows/PowerShell/#pass-through-disk","text":"[Zacker][Zacker]: 226 Set-Disk -Number 2 -IsOffline $true Add-VMHardDiskDrive -VMName server1 -ControllerType scsi -DiskNumber 2","title":"Pass-through disk"},{"location":"Windows/PowerShell/#site-aware-failover-cluster","text":"Configure failover clusters for two offices [Zacker][Zacker]: 366 New-ClusterFaultDomain -Name ny -Type site -Description \"Primary\" -Location \"New York, NY\" New-ClusterFaultDomain -Name sf -Type site -Description \"Secondary\" -Location \"San Francisco, CA\" Set-ClusterFaultDomain -Name node1 -Parent ny Set-ClusterFaultDomain -Name node2 -Parent ny Set-ClusterFaultDomain -Name node3 -Parent sf Set-ClusterFaultDomain -Name node4 -Parent sf","title":"Site-aware failover cluster"},{"location":"Windows/PowerShell/#filter-ad-account-information","text":"Get-aduser -filter {( SamAccountName -like \"*CA0*\" )} -properties Displayname , SaMaccountName , Enabled , EmailAddress , proxyaddresses | Where {( $_ . EmailAddress -notlike \"*@*\" )} | Where {( $_ . Enabled -eq $True )} | Select Displayname , SaMaccountName , Enabled , EmailAddress , @{ L = \u2019 ProxyAddress_1 '; E={$_.proxyaddresses[0]}}, @{L=\u2019ProxyAddress_2' ; E ={ $_ . ProxyAddresses [ 1 ]}} | Export-csv .\\ usersnoemail2 . csv -notypeinformation","title":"Filter AD account information"},{"location":"Windows/PowerShell/#create-vm-with-installation-media","text":"[Practice Lab][pl:70-740] New-VM PLABWIN102 1536mb 1 -SwitchName 'Private network 1' -NewVHDPath 'C:\\Users\\Public\\Documents\\Hyper-V\\Virtual hard disks\\PLABWIN102.vhdx' -NewVHDSizeBytes 127gb Set-VMDvdDrive -VMName PLABWIN102 -Path C : \\ Users \\ Administrator . PRACTICELABS \\ Documents \\ Eval81 . iso","title":"Create VM with installation media"},{"location":"Windows/PowerShell/#registry","text":"Description Affected key Fix Windows Search bar docs.microsoft.com HKCU:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Search Remove 3D Objects howtogeek.com HKLM:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Explorer\\MyComputer\\NameSpace Display seconds in system clock howtogeek.com HKCU:\\Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\Advanced Disable Aero Shake howtogeek.com HKCU:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Explorer\\Advanced Remove 3D Objects from This PC howtogeek.com Remove-Item 'HKLM:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Explorer\\MyComputer\\NameSpace\\{0DB7E03F-FC29-4DC6-9020-FF41B59E513A}' Add seconds to clock howtogeek.com New-Item -Path HKCU : \\ Software \\ Microsoft \\ Windows \\ CurrentVersion \\ Explorer \\ Advanced -Name ShowSecondsInSystemClock -Value 1 Restart-Computer New-Item -Path HKCU : \\ SOFTWARE \\ Microsoft \\ Windows \\ CurrentVersion \\ Search -Name BingSearchEnabled -Value 0 New-Item -Path HKCU : \\ SOFTWARE \\ Microsoft \\ Windows \\ CurrentVersion \\ Search -Name CortanaConsent -Value 0 Safely combine related registry modifications using [ Start-Transaction ][Start-Transaction] and [ Complete-Transaction ][Complete-Transaction] [Holmes][Holmes]: 604 Start-Transaction New-Item TempKey -UseTransaction Complete-Transaction Remove User UAC for local users. ref Set-ItemProperty -Path HKLM : \\ Software \\ Microsoft \\ Windows \\ CurrentVersion \\ Policies \\ System -Name EnableLUA -Value 0","title":"Registry"},{"location":"Windows/PowerShell/#winforms","text":"Pastebin # Load required assemblies [void] [System.Reflection.Assembly] :: LoadWithPartialName ( \"System.Windows.Forms\" ) # Drawing form and controls $Form_HelloWorld = New-Object System . Windows . Forms . Form $Form_HelloWorld . Text = \"Hello World\" $Form_HelloWorld . Size = New-Object System . Drawing . Size ( 272 , 160 ) $Form_HelloWorld . FormBorderStyle = \"FixedDialog\" $Form_HelloWorld . TopMost = $true $Form_HelloWorld . MaximizeBox = $false $Form_HelloWorld . MinimizeBox = $false $Form_HelloWorld . ControlBox = $true $Form_HelloWorld . StartPosition = \"CenterScreen\" $Form_HelloWorld . Font = \"Segoe UI\" # adding a label to my form $label_HelloWorld = New-Object System . Windows . Forms . Label $label_HelloWorld . Location = New-Object System . Drawing . Size ( 8 , 8 ) $label_HelloWorld . Size = New-Object System . Drawing . Size ( 240 , 32 ) $label_HelloWorld . TextAlign = \"MiddleCenter\" $label_HelloWorld . Text = \"Hello World\" $Form_HelloWorld . Controls . Add ( $label_HelloWorld ) # add a button $button_ClickMe = New-Object System . Windows . Forms . Button $button_ClickMe . Location = New-Object System . Drawing . Size ( 8 , 80 ) $button_ClickMe . Size = New-Object System . Drawing . Size ( 240 , 32 ) $button_ClickMe . TextAlign = \"MiddleCenter\" $button_ClickMe . Text = \"Click Me!\" $button_ClickMe . Add_Click ({ $button_ClickMe . Text = \"You did click me!\" Start-Process calc . exe }) $Form_HelloWorld . Controls . Add ( $button_ClickMe ) # show form $Form_HelloWorld . Add_Shown ({ $Form_HelloWorld . Activate ()}) [void] $Form_HelloWorld . ShowDialog ()","title":"WinForms"},{"location":"Windows/PowerShell/#modules","text":"Create a new module by placing a .psm1 file in a directory of the same name .\\Starship\\Starship.psm1 Functions defined within the module can be loaded with [ Import-Module ][Import-Module] (execution policy must allow this). ipmo .\\ Starship To import classes, a different syntax must be used source Using module .\\ Starship","title":"Modules"},{"location":"Windows/PowerShell/#sample-enumeration","text":"PowerShellMagazine Add-Type -AssemblyName System . Drawing $count = [Enum] :: GetValues ( [System.Drawing.KnownColor] ). Count [System.Drawing.KnownColor] ( Get-Random -Minimum 1 -Maximum $count )","title":"Sample enumeration"},{"location":"Windows/PowerShell/#migrate-a-vm","text":"Enable-VMMigration Set-VMMigrationNetwork 192 . 168 . 10 . 1 Set-VMHost -VirtualMachineMigrationAuthenticationType Kerberos Set-VMHost -VirtualMachineMigrationPerformanceOption SMBTransport","title":"Migrate a VM"},{"location":"Windows/PowerShell/#storage-spaces-direct","text":"[MeasureUp][mu:70-740] New-Cluster -Name HC-CLU1 -Node node1 , node2 , node3 , node4 -NoStorage Enable-ClusterStorageSpacesDirect -CacheMode Disabled -AutoConfig : 0 -SkipEligibilityChecks New-StoragePool -StorageSubSystemFriendlyName * Cluster * -FriendlyName S2DPool -ProvisioningTypeDefault Fixed -PhysicalDisk ( Get-PhysicalDisk | Where-Object -Property CanPool -eq $true ) $pool = Get-StoragePool S2DPool New-StorageTier -StoragePoolUniqueID ( $pool ). UniqueID -FriendlyName Performance -MediaType HDD -ResiliencySettingName Mirror New-StorageTier -StoragePoolUniqueID ( $pool ). UniqueID -FriendlyName Capacity -MediaType HDD -ResiliencySettingName Parity The next step would be the creation of a new volume New-Volume -StoragePool $pool -FriendlyName SharedVol1 -FileSystem CSVFS_REFS -StorageTiersFriendlyNames Performance , Capacity -StorageTierSizes 2GB , 10GB","title":"Storage Spaces Direct"},{"location":"Windows/PowerShell/#scheduled-task","text":"Automatically run SSH server in WSL on system start $action = New-ScheduledTaskAction -Execute C : \\ WINDOWS \\ System32 \\ bash . exe -Argument '-c sudo service ssh start' $trigger = New-ScheduledTaskTrigger -AtLogon Register-ScheduledTask -TaskName 'SSH server' -Trigger $trigger -Action $action","title":"Scheduled task"},{"location":"Windows/PowerShell/#network-connection-alert","text":"Play a tone when network connection has been (re)-established. while ( $true ) { if (( Test-NetConnection 8 . 8 . 8 . 8 -WarningAction SilentlyContinue ). PingSucceeded -eq $true ) { [System.Console] :: Beep ( 1000 , 100 ) break } } while ( Test-NetConnection 8 . 8 . 8 . 8 -WarningAction SilentlyContinue ). PingSucceeded -eq $false ) { continue }","title":"Network connection alert"},{"location":"Windows/WDS-lab/","text":"# PLABDC01 Set-DHCPServerv4Scope -ScopeId '192.168.0.0' -Type Both -State Active wdsutil . exe / initialize-server / remInst : \"D:\\RemoteInstall\" wdsutil . exe / enable-server wdsutil . exe / start-server Import install and boot images New-WDSInstallImageGroup -Name Win10-DVDImage Get-WindowsImage -ImagePath \\\\ plabdm01 \\ win10 \\ sources \\ install . wim Import-WDSInstallImage -Path \\\\ plabdm01 \\ win10 \\ sources \\ install . wim -ImageGroup Win10-DVDImage -ImageName 'Windows 10 Enterprise' Import-WdsBootImage -Path \\\\ plabdm01 \\ win10 \\ sources \\ boot . wim -NewImageName 'Microsoft Windows Setup (x64)' -NewDescription 'Microsoft Windows Setup (x64)' Configure server to accept clients ::wdsutil.exe /start-transportserver wdsutil.exe /set-server /answerclients:all wdsutil.exe /set-server /authorize:yes wdsutil.exe /set-server /NewMachineNamingPolicy:PLABSA%#03 Configure VM to boot from PXE # PLABDM01 Rename-VMSwitch Intel * External Add-VMNetworkAdapter -IsLegacy $true -VMName PLABSA02 -SwitchName External Remove-VMAssignableDevice -VMName PLABSA02 Start-VM PLABSA02 # While connected to VM, when prompted to \"press F12 for network service boot\", press F12.","title":"WDS lab"},{"location":"Windows/WS2016/","text":"Windows Server Certification exams Number Title 70-740 Installation, storage and Compute with Windows Server 2016 70-741 Networking with Windows Server 2016 70-742 Identity with Windows Server 2016 Find notes on labs here . Installation Windows Server 2016 installations are determined by the most suitable installation method, option, and edition. Installation methods : An upgrade is an installation performed in-place with existing data intact and is opposed to a clean installation . A migration is a clean installation with old data transferred over. Migrations are facilitated by Powershell and [command prompt][SmigDeploy.exe] tools Installation options include Desktop Experience, [Server Core][Server Core], and Nano Server . The most important installation edition is Windows Server 2016 Datacenter edition , which is the only edition to have several important features that figure prominently in the exam. Storage Spaces Direct , Storage Replica Shielded VMs Network controller . Various other installation options exist, including: Windows Server 2016 Standard , Essentials , Multipoint Premium , Storage , and Hyper-V editions. Server installations are also influenced by choice of activation model . Licensing Servicing channels provide a way of separating users into deployment groups for feature and quality updates. Semi-Annual Channel - previously known as Current Branch for Business (CBB) - features updates twice a year. It is more appropriate for non-infrastructure workloads that can be deployed through automation. Long Term Servicing Channel (LTSC) has a minimum servicing lifetime of 10 years and was designed to be used only for specialized devices such as those that control medical equipment or ATM machines, receiving new feature releases every 2-3 years Server Core Installing the Windows Server 2016 Server Core foregoes the possibility of later switching back to Desktop Experience, as had been possible in previous editions. Notably, WDS is incompatible with Server Core installations. Server Core installations can be managed with a GUI with the use of MMC snap-ins . Because MMC is reliant on Distributed Component Object Model (DCOM) technology, firewall rules have to be enabled to allow DCOM traffic (ref. Set-NetFirewallRule ). Nano Server Nano Server , a new installation option introduced in Windows Server 2016, provides a much smaller footprint and attack surface than even Server Core, but supports only some roles and features. Installation is done by building a VHD image via PowerShell on another computer. That VHD is then deployed as a VM or used as a boot drive for a physical server. Booting a Nano Server VM produces a text-based interface called the Nano Server Recovery Console , a menu system that allows configuration of static network options (DHCP is enabled by default). The DNS server may not be configured interactively, but must be specified when building the image with the Ipv4Dns parameter. If a Nano Server is domain-joined a remote Powershell session will authenticate via Kerberos. If not, its name or IP address must be added to the Trusted Hosts list. The Windows Server 2016 installation media contains a NanoServer directory, from which the NanoServerImageGenerator Powershell module must be imported. It also contains a Packages subdirectory, with CAB files containing roles and features that correspond to named parameters or packages that are specified as values to the Packages named parameter when building a Nano Server image. Cmdlet Description Edit-NanoServerImage Add a role or feature to an existing Nano Server VHD file New-NanoServerImage Used to create a Nano Server VHD file for Nano Server installation Activation Server installations are influenced by choice of activation model. MAK is suitable for small networks, but large enterprises may opt for KMS . - MAK activations are subdivided into Independent and Proxy , based on whether or not a VAMT is used. - KMS activations, which distribute GVLK s, are valid for a period of time and require the installation of a role and management tools . KMS operates on TCP port 1688. - AVMA simplifies the process of activating Hyper-V VMs running Windows Server 2012 or 2016. Active Directory-based activation is an alternative for enterprises who opt to activate licenses through the existing AD DS domain infrastructure. Any domain-joined computers running a supported OS with a GVLK will be activated automatically and transparently. The domain must be extended to the Windows Server 2012 R2 or higher schema level, and a KMS host key must be added using the VAMT. After Microsoft verifies the KMS host key, client computers are activated by receiving an activation object from the DC. MS Docs Images Many enterprises have begun virtualizing their server environments to take advantage of the many cost, reliability, and performance benefits that this change creates. Migrations should start with systems that are peripheral to main business interests before moving on to those that are more vital. A carefully documented protocol should be developed to facilitate the conversion of physical hard disks to VHDs for use in Hyper-V guests. Supported guest OSes include Linux and FreeBSD. The Microsoft Assessment and Planning (MAP) Toolkit is a free software tool that intelligently constructs a database of the hardware, software, and performance of computers on a network to plan for an operating system upgrade or virtualization. MAP supports the following discovery methods: Active Directory Domain Services Windows networking protocols System Center Configuration Manager IP address range scanning Computer names entered manually or imported from a file Server Core relies on the command-line for system maintenance, including updates which must be installed directly to the image using dism.exe or equivalent Powershell commands. Containers Containers run applications in an isolated namespace , meaning it only has access to resources that are made available to it by the container runtime. Resource governance means that a container has access only to a specified number of processor cycles, system memory, and other resources. Containers allow applications to be packaged with their dependencies in container images , which will run the same regardless of underlying operating system or infrastructure and are downloaded from container registries like Docker Hub . Container registries are not to be confused with repositories , which are subcomponents of registries. Windows Server 2016 suports Windows Server Containers and Hyper-V Containers , which create a separate copy of the operating system kernel for each container. The \"Containers\" feature must be installed on Windows Server 2016 hosts, and to create Hyper-V containers the Hyper-V role must also be installed (although the Hyper-V management tools are not necessary if VMs are not going to created). Windows container hosts need to have Windows installed to C:. Nano Server once could serve as Docker hosts, but no longer; Nano Servers are now intended to be deployed as containers themselves. The Powershell Docker module has been deprecated for years. Docker has several options for containers to store files in a persistent manner: - Volumes are stored in a part of the host filesystem which is managed by Docker (/var/lib/docker/volumes/ on Docker). - Bind mounts may be stored anywhere on the host system and are specified by [ docker run --volume ][docker run -v]. - tmpfs mounts are stored in the host system's memory only, and are available only on Linux. DNS Installation DNS server role requiremenets: - Statically assigned IP - Signed-in user must be member of local Administrators group There are several recommended DNS deployment scenarios, all of which involve installing DNS on a Server Core or Nano Server instance. This is because these installation options offer a reduced attack surface, a reduced resource footprint, and reduced patching requirements. - DNS on DC : All DNS features are available and supports AD-integrated, primary, secondary, and stub zones. - DNS on RODC : Passes DNS zone updates to a writeable DC - DNS on standalone member server : Supports file-based primary, secondary, and stub zones but requiring zone replication because there is no integration over AD. Nano Server Installing DNS on a running Nano Server image requires running Install-NanoServerPackage as well as enabling the \"DNS-Server-Full-Role\" optional feature using Enable-WindowsOptionalFeature . As of early 2017, Nano Server only supported a few roles, including DNS , but was only able to do so with some limitations - Nano Server can only support file-based DNS and cannot host AD-integrated zones. - Nano Server only supports the Semi-Annual servicing channel license. - Nano Server is not suitable for primary zones, only caching-only, forwarder, or secondary zone DNS servers Zones Zones can be considered one or more DNS domains or subdomains, associated with zone files , which compose the DNS database itself and contain two types of entries: Parser commands , which provide shorthand ways to enter records: $ORIGIN , $INCLUDE , and $TTL Resource records are whitespace delimited text files with columns for name, time to live, class, type, and data The copies of zone files local to individual DNS servers can be primary (read/write) or secondary (read-only). A primary zone is a writable copy of a DNS zone that exists on a DNS server. A secondary zone is a read-only replica of a primary zone and necessitates the presence of a primary zone for the same zone. Defining a secondary zone via PowerShell requires specifying that zone's MasterServers . In Windows Server, zone files can also be integrated with Active Directory, making what is called an Active Directory Integrated Zone . These allow multi-master zones , meaning any DC can process zone updates and the zone can be replicated to any DC in the domain or forest. An AD-integrated zone can be specified by passing the ReplicationScope parameter to the Add-DnsServerPrimaryZone cmdlet. Stub zones contains only name server (NS) records of another zone, but unlike a forwarder is able to update when name servers in a target zone change. Reverse Lookup zones are used to resolve IP addresses to FQDNs. Reverse lookup zones for public IP address space are often administered by ISPs, and they are useful in spam filtering to double-check the source domain name with the IP address. GlobalNames zones provide \"single label name resolution\" (as opposed to a FQDN) and are intended to replace WINS servers. Query traffic The process of resolving a query by querying other DNS servers is called recursion . Recursion can be disabled outright but Windows Server 2016 supports recursion scopes which will allow recursion to be disabled unless certain conditions are met (such as receiving the request on a particular interface). There are two types of query in the context of recursion: - Recursive query sent by the petitioner: that is, the original query which begins recursion. - Iterative query : individual queries sent out to authoritative name servers in order to resolve a recursive query. Root hints are preconfigured root servers that are necessary to begin the recursion process. The DNS Server service stores root hints in %systemroot%\\System32\\dns\\CACHE.DNS . These can be edited through the GUI or by using the PowerShell commands Add- , Import- , Remove- , and Set-DnsServerRootHint . Forwarding of a request occurs when a petitioned DNS server is unable to resolve the query because it is both: - Non-authoritative for the specified zone, and - Does not have the response cached. Two actions are possible when forwarding: - Configure a DNS server only to respond to queries it can satisfy by referencing locally-stored zone information, forwarding all other requests. - Configure forwarding for specific zones through conditional forwarding A secondary zone is not to be confused with delegation , where a DNS server delegates authority over part of its namespace (i.e. a subdomain) to one or more other servers. Windows Server 2016 supports a DNS GlobalNames zone meant to supercede WINS, which served a role similar to DNS for the old NetBIOS naming standard. NetBIOS names use a nonhierarchical structure (i.e. are a single name and not divisible into sub-domains) based on a name up to 16 characters long (although the 16th character defines a particular service running on the host defined by the previous 15). An organization must share a single GlobalNames zone, which must be created in PowerShell manually. Resource records Zone scavenging allows servers with stale records to remove them. This feature is disabled by default, but can be set at the server or zone level. Type Description A IPv4 address record AAAA IPv6 address record CNAME Hostname or alias for hosts in the domain MX Where mail for the domain should be delivered NS Name servers PTR Reverse lookup SOA Each zone contains a single SOA record SRV Generalized service location record, used for newer protocols instead of protocol-specific records TXT Typically holds machine-readable data Security DNSSEC offers security features using public key certificates. A socket pool can be used to configure the DNS server to use a random source port when issuing DNS queries. Response rate limiting can pose a defense against DNS DoS attacks by ignoring potentially malicious, repetitive requests. DNS-based Authentication of Named Entities (DANE) is supported by Windows Server 2016 to reduce man-in-the-middle attacks. DANE works by informing DNS clients requesting records from the domain which Certification Authoority they must expect digital certificates to be issued from. Policies Zone transfer policies can prevent or allow zone transfers to any server, to name servers, or to servers specified by FQDN or IP address. DNS Policy is a new feature in Windows Server 2016 that can control DNS server behavior depending on certain criteria. These criteria include: Client subnet Recursion scope Zone scope DNSSEC DNSSEC is a security setting for DNS that enables all DNS records in a zone to be digitally signed by a trust anchor which validates DNSKEY resource records. Root and top-level domain zones already have trust anchors configured and merely have to have it enabled. To implement trust anchors: - A TrustAnchors zone must be created, which will store public keys associated with specific zones. A trust anchor from the secured zone must be created on every DNS server that hosts the zone. - A Name Resolution Policy Table (NRPT) GPO must be created (Windows Settings\\Name Resolution Policy) This option can require DNSSEC based on computer name prefix or suffix, FQDN, or subnet. - DNSSEC key master is a special DNS server that generates and manages signing keys for DNSSEC protected zones. DANE allows you to publish certificate information within the DNS zone, rather than one of the thousands of trusted CAs. This protects against rogue/compromised CAs issuing illegitimate TLS certificates. Two cryptographic keys: - Zone Signing Key (ZSK) signs zone data including individual resource records other than DNSKEY. It is also used to create the KSK. - Key Signing Key (KSK) is used to sign all DNSKEY records at the zone root. DNSSEC record types: - RRSIG \"resource record signature\" each of which matches and provides a signature for an existing record in a zone - NSEC proves nonexistence of a record - NSEC3 NSEC replacement that prevents zone walking - NSEC3PARAM specifies the NSEC3 records included in response for DNS names that don't exist - DNSKEY stores public key used to verify a signature - DS delegation signer records secure delegations DSC IaaS management of servers is possible with Desired State Configuration (DSC) , a feature of Windows PowerShell where script files stored on a central server can apply specific a specific configuration to nodes. These scripts are idempotent , meaning that they can be applied repeatedly without generating errors. The DSC model is composed of phases 1. Authoring Phase, where MOF definitions are created 2. Staging Phase, where declarative MOFs are staged and a Configuration calculated per node 3. \"Make It So\" Phase, where declarative Configurations are implemented through imperative providers Components of DSC scripts include: - Local Configuration Manager : engine running on the client system that received configurations from the DSC server and applies them to the target. - Node block specifies the names of target computers. - Resource block specifies settings or components and the values that the configuration script should assign to them. DSC configurations can be deployed in two different refresh modes Pull architecture : target LCM periodically retrieves configuration from a Pull Server , which consolidates MOF files. Push architecture : configuration is sent to target in response to explicit invocation of Start-DSCConfiguration on the server. LCM has to be configured to accept Configurations of either refresh mode. Tasks Set LCM to push mode [ DSCLocalConfigurationManager ()] Configuration LCMConfig { Node localhost { Settings { RefreshMode = 'Push' } } } Install Telnet client Configuration InstallTelnetLocal { Import-DscResource -ModuleName 'PSDesiredStateConfiguration' Node localhost { WindowsOptionalFeature InstallTelnet { Name = \"Telnet-Client\" Ensure = \"Present\" } } } Install WSL Configuration InstallWSLLocal { Import-DscResource -ModuleName 'PSDesiredStateConfiguration' Node localhost { WindowsOptionalFeature InstallWSL { Name = \"Microsoft-Windows-Subsystem-Linux\" Ensure = \"Present\" } } } Failover clusters Failover clusters are composed of computers called nodes and can be created using New-Cluster . which typically possess a secondary network adapter, used for cluster communications. Before Windows Server 2016, all cluster nodes had to belong to the same domain, but now this is but one of several possible cluster types called a single-domain cluster. A failover cluster can also be multi-domain , or workgroup , depending on how or if the servers are joined to domains. A cluster can also be detached from AD, even though its nodes are joined. A cluster whose servers are joined to a single domain is typically associated with a cluster name object in Active Directory, which serves as its administrative access point . A workgroup cluster or a detached cluster need to have the cluster's network name registered in DNS as its administrative access point, which can be specified in Powershell with the AdministrativeAccessPoint named parameter. Additionally, on a workgroup cluster the same local administrator account must be created on every node, preferably the builtin Administrator account, although a different account can be configured if a particular Registry key is [created][New-ItemProperty] on each node. Nodes that are domain-joined support CredSSP or Kerberos authentication, but workgroup nodes support NTLM authentication only. Three types of witness resources can help to ensure a quorum takes place in clusters. This is necessary to prevent a split-brain situation, where communication failures between nodes cause separate segments of the clusters to continue operating independently of each other. A witness is created when a cluster has an even number of nodes, and only one can be configured. [pwsh][Set-ClusterQuorum] - Disk witness : dedicated disk in shared storage that contains a copy of the cluster database - File Share witness : SMB file share containing a Witness.log file with information about the cluster - Cloud witness : blob stored in Azure Scale-out File Server (SoFS) is a clustered role providing highly available storage to cluster nodes. SoFS ensures continuous availability in the case of a node failure. Using SoFS, multiple nodes can also access the same block of storage at the same time, and for this reason is is an active/active or dual active system, as opposed to one where only one node provides accessible shares, or an active/passive system. SoFS is specifically recommended for use on Hyper-V and SQL Server clusters and can be installed with Add-ClusterScaleOutFileServer . SoFS shares are created with the [ New-SmbShare ][New-SmbShare] PowerShell cmdlet. SoFS shares are located on Cluster Shared Volumes (CSV) , a shared disk containing an NTFS or ReFS volume that is made accessible for read and write operations by all nodes within a failover cluster. CSVs solved a historical problem with using NTFS volumes with VMs in previous versions of Windows Server. NTFS is designed to be accessed by only one operating system instance at a time. In Windows Server 2008 and earlier, this meant that only one node could access a disk at a time, which had to be mounted and dismounted for every VM. The solution was to create a pseudo-file system called CSVFS , sitting on top of NTFS, that enables multiple drives to modify a disk's content at the same time, but restricting access to the metada to the owner or coordinator . The coordinator node refers to the cluster node where NTFS for the clustered CSV disk is mounted, any other node is called a Data Server (DS) . VM resiliency can be configured by adjusting settings in response to changes in VM state: - Unmonitored : VM owning a role is not being monitored by the Cluster Service - Isolated : Node is not currently an active member of the cluster, but still possess the role - Quarantine : Node has been drained of its roles and removed from the cluster for a specified length of time. Cluster Operating System Rolling Upgrade is a new feature that reduces downtime by making it possible for a cluster to have nodes running both Windows Server 2012 R2 and Window Server 2016. Using this feature, nodes can be brought down for an upgrade. When [Storage Spaces][Storage Spaces] is combined with a failover cluster, the solution is known as Clustered Storage Spaces . High availability Hyper-V Replica allows simple failover to occur between Hyper-V hosts, without the need for a cluster. To configure a simple one-way failover solution using Hyper-V Replica, configure the destination VM as a replica server , either in Hyper-V Manager or PowerShell. [ Set-VMReplicationServer ][Set-VMReplicationServer] lab The destination host must also have firewall ports opened corresponding to the authentication method chosen. The source VM, which is to be replicated, must have its options configured through the Enable Replication wizard . [ Enable-VMReplication ][Enable-VMReplication] To use Hyper-V Replica as a (two-way) failover solution, configure both VMs as replica servers. Migrations can take place one of three methods: - Live Migration moves only the system state and live memory contents, not data files. Live migration requires that the hosts be, if not clustered, at least part of the same (or a trusted) domain. Live Migration requires that VHD files be placed on shared storage and both hosts have appropriate permissions to access said storage. An unpopulated VM is created on the destination with the same resources as the source before transferring memory pages. Once the servers have an identical memory state, the source VM is suspended and the destination takes over. Hyper-V notifies the network switch of the change, diverting network traffic to the destination. Authentication can be made by [CredSSP][CredSSP] or Kerberos. When a Hyper-V cluster is created, the Failover Cluster Manager launches the High Availability Wizard, which configures the VM to support Live Migration. The same thing can be done with the PowerShell cmdlet Add-ClusterVirtualMachineRole . Additionally, using Kerberos authentication for live migration requires constrained delegation , which enables a server to act on behalf of a user for only certain defined services. This must be configured within Active Directory Users and Computers , by opening the Properties of the source Computer object, and changing the setting under the Delegation tab. - An additional, outdated method of migration is quick migration , which was present in Windows Server prior to the introduction of live migration and persists in Windows Server 2016 for backward compatibility. A quick migration involves pausing the VM, saving its state, moving the VM to the new owner, and starting it again. A quick migration always involves a short period of VM downtime. Shared Nothing Live Migration requires that source and destination VMs be members of the same (or trusted) domain, and source and domain servers must be running the same processor family (Intel or AMD) and linked by an Ethernet network running a minimum of 1 Gbps. Additionally, both Hyper-V hosts must be running idential virtual switches that use the same name; otherwise the migration process will be interrupted to prompt the operator to select a switch on the destination server. The process of migrating is almost identical to a Live Migration, except that you select the \"Move the Virtual Machine's Data To a Single Location\" option on the Choose Move Options page of the Move Wizard. Storage Migration works by first creating new VHDs on the destination corresponding to those on the source server. While the source server continues to operate using local files, Hyper-V begins mirroring disk writes to the destination server and begins a single-pass copy of the source disks to the destination begins, skipping blocks that have already been copied. Once the copy has completed, the VM begins working from the destination server and the source files are deleted. For a VM that is shut off, storage migration is equivalent to simply copying files from source to destination. Site-aware clusters have failover affinity . Node fairnes evalutes memory and CPU loads on cluster nodes over time. Cluster management VM Monitoring allows specific services to be restarted or failed-over when a problem occurs. To use VM Monitoring: - The guest must be joined to the same domain as the host - The host administrator must be a member of the guest's local Administrators group - And Windows Firewall rules in the Virtual Machine Monitoring group must be enabled. The service can then be monitored using [ Add-ClusterVMMonitoredItem ][Add-ClusterVMMonitoredItem]. Migration VMs can be moved from node to node of a cluster using live , storage , or quick migrations. VM network health protection is a feature (enabled by default) that detects whether a VM on a cluster node has a functional connection to a designated network. If not, the cluster live migrates the VM role to another node that does have such a connection. This setting can be controlled in Hyper-V Manager > VM Settings > Advanced Features > Protected network GPO Group Policy Objects (GPO) facilitate the uniform administration of large numbers of users and computers. GPOs can be local or domain-based . Local GPOs come in several varieties, applied in the following order (last takes highest precedence): - Local Group Policy applied to computers - Administrators and Non-Administrators Local Group Policy applied to users based on their membership in local Administrators group. - User-specific Local Group Policy : Domain-based GPOs consist of two components a [ container ][Group Policy container] and a [ template ][Group Policy template]. These are stored in different locations and replicated by different means. - Containers define the fundamental attributes of a GPO, each of which is assigned a GUID, and are stored in the AD DS database and replicated to other domain controllers using intrasite or intersite AD DS replication schedule. - Templates, a collection of files and folders that define the actual GPO settings, are stored in the SYSVOL shared folder ( %SystemRoot%\\SYSVOL\\Domain\\Poligicies\\{GUID} ) on all DCs. SYSVOL replication is handled by the DFS Replication Agent since Windows Server 2008. A GPO consists of 2 top-level nodes: - Computer Configuration contains settings that are applied to computer objects to which the GPO is linked - User Configuration containers user-related settings, applied when a user signs in and thereafter and automatically refreshed every 90-120 minutes Beneath each of these nodes are folders that group settings - Software Settings - Windows Settings allows basic configuration for computers or users - Administrative Templates contains Registry settings that control user, computer, and app behavior and settings, grouped logically into folders Although domain controllers store and serve GPOs, the client computer itself must request and apply the GPOs using the Group Policy Client service. Client-side extensions process the GPOs once downloaded Starter GPOs are intended for use in large organizations with a proliferation of GPOs that share settings. Starter GPOs can be imported from, and exported to, a .CAB file. Once a GPO is created it must be linked to a container object in AD DS for it to apply to objects, a process known as scoping . GPOs can be linked to Sites, Domains, and OUs. If multiple GPOs are linked to the same container, the link order must be configured. There are 2 default GPOs in an AD DS domain, which can be reset using arguments to the dcgpofix command. - Default Domain Policy, linked to the domain object - Default Domain Controllers Policy, linked to the Domain Controllers OU Although it is possible to link the same GPO to multiple containers, it is recommended to import (i.e. copy) a GPO from another domain. This process effecitvely restores the settings of another GPO into a newly created GPO, which is then linked to another container. Hyper-V [Hyper-V][Hyper-V] is a Type I hypervisor and role that allows a Windows Server 2016 host to create VMs, called guests . In Type I virtualization, the hypervisor forms an abstraction layer that interacts directly with the host hardware. In this model, the individual environments created by the hypervisor, including the host operating system and guest VMs, are called [partitions][partition]. Hyper-V Server, a free product available for download is limited to the command-line Server Core interface, however it does include Sconfig to aid configuration. Hyper-V can be managed remotely using the GUI (Hyper-V Manager, hyper-v-tools ), or Powershell ( hyper-v-powershell ). Authentication can be via Kerberos or Credential Security Support Provider (CredSSP) , which must be enabled on both server and client. PowerShell remoting - Explicit remoting involves opening a PowerShell session to a remote session - Implicit remoting involves running a cmdlet specifying the ComputerName parameter. PowerShell Direct allows easy remoting to VMs by using the -VmName Powershell parameter using a PowerShell session. [Nested virtualization][Nested virtualization] is a new capability where a virtual host running Windows Server 2016 on a physical host also running Windows Server 2016 can host nested VMs. Host configuration [Hyper-V][Hyper-V] is a Type I hypervisor and role that allows a Windows Server 2016 host to create VMs, called guests . In Type I virtualization, the hypervisor forms an abstraction layer that interacts directly with the host hardware. In this model, the individual environments created by the hypervisor, including the host operating system and guest VMs, are called [partitions][partition]. Hyper-V Server, a free product available for download is limited to the command-line Server Core interface, however it does include Sconfig to aid configuration. Hyper-V can be managed remotely using the GUI (Hyper-V Manager, hyper-v-tools ), or Powershell ( hyper-v-powershell ). Authentication can be via Kerberos or Credential Security Support Provider (CredSSP) , which must be enabled on both server and client. PowerShell remoting - Explicit remoting involves opening a PowerShell session to a remote session - Implicit remoting involves running a cmdlet specifying the ComputerName parameter. PowerShell Direct allows easy remoting to VMs by using the -VmName Powershell parameter using a PowerShell session. [Nested virtualization][Nested virtualization] is a new capability where a virtual host running Windows Server 2016 on a physical host also running Windows Server 2016 can host nested VMs. Networking Virtual switches can be external , internal , or private (in order of decreasing access). Up to 8 network adapters can be [added][Add-VMNetworkAdapter] to a Windows Server 2016 Hyper-V VM. Hyper-V maintains a pool of MAC addresses which are assigned to virtual network adapters as they are created. Hyper-V MAC addresses begin with 00-15-5D , followed by the last two bytes of the IP address assigned to the server's physical network adapter (i.e. last two octets), then a final byte. Generation 1 VMs supported synthetic and legacy virtual network adapters, but in Generation 2 VMs only synthetic adapters are used. Generation 1 VMs can only boot from network (PXE) when using a legacy adapter. Physical hosts running Windows Server 2016 can support teams of up to 32 NICs, but Hyper-V VMs are limited to teams of two. The team must first be configured in the host operating system and appears as a single interface in the Virtual Switch Manager. High-performance embedded teaming , reliant on RDMA , can only be configured with Powershell . - Teaming Mode - Switch Independent : switch is unaware of presence of NIC team and does not load balance to members; Windows is performing the teaming - Switch Dependent : switch determines how to distribute inbound network traffic; only supported by specialty hardware - Static Teaming : switch and host are manually configured (typically supported by server-class switches) - Link Aggregation Control Protocol (LACP) : dynamically identifies links that are connected between the host and the switch - Load Balancing Mode - Address Hash : a hash is created based on address components of the packet, which is used to reasonably balance adapters - Hyper-V Port : NIC teams configured on Hyper-V hosts give VMs independent MAC addresses - Dynamic : outbound loads are distributed based on a hash of the TCP ports and IP addresses Virtual machine queuing will enhance performance if a physical host supports it and it is enabled . Bandwidth management is achieved by setting limits on the virtual network adapter, in the GUI or in [Powershell][Set-VMNetworkAdapter]. Storage The New Virtual Machine Wizard presents different options for Generation 1 vs. Generation 2 VMs. - Generation 1 VMs provide two IDE controllers, which host the hard drive and a DVD drive, and an unpopulated SCSI controller which can host additional disks. - Generation 2 VMs, however, have only a single SCSI controller, which hosts all virtual drives. A new VHD can be created using - Hyper-V Manager through the New Virtual Hard Disk Wizard - Disk Management ( diskmgmt.msc ), however the option to create a differencing disk is not available, nor can specific block or sector size be specified - PowerShell Shared virtual disk files are preferably created as VHD sets . Pass-through disks make exclusive use of a physical disk. pwsh Standard checkpoints (previously known as \"snapshots\" in Windows Server 2012 and before) with the extensions AVHD or AVHDX save the state, data, and hardware configuration of a VM. They are recommended for development and testing but are not a replacement for backup software nor recommended for production environmentsj, because restoring them in a production environment will interrupt running services. Production checkpoints do not save memory state, but use Volume Shadow Copy Service (Windows) or File System Freeze (Linux) inside the guest to create \"point in time\" images of the VM. Shielded VMs Shielded VMs are a feature exclusive to the Datacenter Edition of Windows Server 2016. As a result of increased virtualization, physical servers that were once secured physically were migrated to Hyper-V hosts that are less secure because they are accessible to fabric administrators . Shielded VMs were introduced to protect tenant workloads from inspection, theft, and tampering as a result of being run on potentially compromised hosts. A security concept closely associated to shielded VMs is the guarded fabric , which is a collection of nodes cooperating to protect shielded Hyper-V guests. The guarded fabric consists of: - Host Guardian Service (HGS) utilizes remote attestation to confirm that a node is trusted; if so, it releases a key enabling the shielded VM to be started. HGS is typically a cluster of 3 nodes. - Guarded hosts : Windows Server 2016 Datacenter edition Hyper-V hosts that can run shielded VMs only if they can prove they are running in a known, trusted state to the Host Guardian Service. - Shielded VMs In a production environment, a fabric manager like Virtual Machine Manager would be used to deploy shielded VMs (which are signified by a shield icon). Shielded VMs must run Windows (8+) or Windows Server (2012+), although Linux shielded VMs are now also supported since version Windows Server version 1709. Shielded VMs are produced by a three-stage process (VHD -> Shielded template -> Shielded VMs) 1. Preparation : Install and configure an OS onto a virtual disk file 2. Templatization : Convert virtual disk file into a shielded template 3. Provisioning : Create one or more shielded VMs from the shielded template Configure HGS in its own new forest YouTube Install-WindowsFeature HostGuardianServiceRole -Restart Install-HgsServer -HgsDomainName 'savtechhgs.net' -SafeModeAdministratorPassword $adminPassword -Restart Shielding Data is created and owned by tenant VM owners and contains secrets needed to create shielded VMs that must be protected from the fabric admin. Resources: Intro to shielded VMs Create a shielded VM using Powershell Linux Shielded VM How To Shielded VM Demonstration and Quick Setup Guarded Fabric Deployment Guide for Windows Server 2016 Deploying Shielded VMs and a Guarded Fabric with Windows Server 2016 Attestation There are two modes of attestation supported by HGS: Hardware-trusted attestation Hardware-trusted attestation mode requires : Measured boot : TPMv2 to seal software and hardware configuration details measured at boot Code integrity enforcement to strictly define permissible software Platform Identity Verification : Active Directory is not sufficient to identify the host. Rather, an identity key rooted in the host TPM is used for identity. Remote attestation based on asymmetric key pairs Admin-trusted attestation was previously based on guarded host membership in a designated AD DS security group, but is deprecated beginning with Windows Server 2019. Host identity is [verified]](https://youtu.be/B2vFrdXd5jg?t=525) by checking security group permission No Measured Boot or Code Integrity Validation Intended to aid transition to Hardware-trusted attestation mode for hosts produced before TPMv2 VM configuration VMs are associated with a variety of file types: Extension Description .vmc XML-format VM configuration .vhd, .vhdx Virtual hard disks .vsv Saved-state files VMs can be created in Hyper-V, and a machine's RAM can even be changed dynamically. Hyper-V guests can take advantage of a suite of features to enhance performance and functionality. - Virtualization of NUMA architecture - Smart paging for when VMs that use dynamic memory restart and temporarily need more memory than is available on the host, for example at boot - Monitoring resource usage, to minimize cost overruns when guests run in the cloud - Disk and GPU passthrough, and other PCI-x devices, with DDA pwsh - Increased performance of interactive sessions that use [VMConnect][VMConnect.exe] Microsoft supports some Linux distributions, like Ubuntu, with built-in Linux Integration Services , which improve performance by providing custom drivers to interface with Hyper-V. Some distributions like CentOS and Oracle come with integrated LIS packages, but free LIS packages provided by Microsoft for download from the Microsoft Download Center support additional features and come with the additional benefit of being versioned. These packages are provided as tarballs or ISO images, and must be loaded directly into the running guest operating system. FreeBSD has included full support for FreeBSD Integration Services (BIS) since version 10. Secure Boot has to be disabled when loading Hyper-V VMs running Linux distributions, since UEFI does not have certificates for non-Windows operating systems by default. Some distributions supported by Microsoft do have certificates in the UEFI Certificate Authority . Different versions of Hyper-V create VMs associated with that version (Windows Server 2016 uses Hyper-V 8.0). VMs created by older versions of Hyper-V can be [updated][Update-VMVersion], but once updated they may no longer run on a host of a previous version. Importing an exported VM can be done in three ways: - Register : exported files are left as-is and the guest's ID is maintained; - Restore : exported files copied to the host's default locations or ones that are otherwise specified; ID is maintained - Copy : exported files are copied; new ID generated PXE boot is supported in two scenarios: - Generation 1 VMs with a legacy virtual network adapter support PXE boot (but not synthetic ). Generation 1 VMs are limited to 2 TB in size and do not support many of the advanced features that Generation 2 VMs do. But PXE Boot remains one of the primary reasons to continue using a Generation 1 VM. - Generation 2 VMs with a synthetic network adapter also support PXE boot. would also support bandwidth management and VMQ. Generation 2 VMs also do not support 32-bit OSes, including: - Windows Server 2008, R2 - Windows 7 - Older Linux distros - FreeBSD (all) VMs cannot be upgraded from Generation 1 to Generation 2 easily, although a script named Convert-VMGeneration was once provided by Microsoft and can still be found. But the VM's version , referring to the version of Hyper-V used to create it, can be upgraded with Upgrade-VMVersion . Monitoring Performance Monitor is a program that allows realtime monitoring of hundreds of different system performance statistics, called performance counters . Counters can be viewed in several ways, including line graph, histogram bar graph, and report views. Every counter added to a graph is associated with a computer, a performance object (hardware or software component to be monitored), a performance counter (statistic), and an instance. A data collector set captures counter statistics for later review. A single data collector set can gather performance counter data from multiple VMs. Event trace data cannot be combined with performance data in the same data collector set. Expiration dates can be set for data collector sets, but if actively collecting data the expiration date will not stop collection. A performance alert is a type of data collector set that can track system performance and log events in the application event log. Alerts can be triggered when a performance counter value exceeds a certain threshold. Only members of the local groups Administrators and Performance Log Users can create alerts, but the Log on as a batch user right must be granted to members of Performance Log Users. A hard fault occurs when data is swapped between memory and disk. Performance counters Counter Acceptable values Processor: % Processor Time <85% Processor: Interrupts/Sec cf. baseline System: Processor Queue Length <2 Server Work Queues: Queue Length <4 Memory: Page Faults/Sec <5 Memory: Pages/Sec <20 Memory: Available MBytes >5% of physical memory Memory: Committed Bytes < physical memory Memory: Pool Non-Paged Bytes Stable PhysicalDisk: Disk Bytes/Sec cf. baseline PhysicalDisk: Avg. Disk Bytes/Transfer cf. baseline PhysicalDisk: Current Disk Queue Length <2 per spindle PhysicalDisk: % Disk Time <90% LogicalDisk: % Free Space >20% Network Interface: Bytes Total/Sec cf. baseline Network Interface: Output Queue Length <2 Server: Bytes Total/Sec 50% of total bandwidth Network Load Balancing Cluster VMs can be configured to drain their workloads to other nodes when being shutdown using Suspend-ClusterNode NLB Clusters are made of hosts , while Failover Clusters are made of nodes . NLB port rules control how the cluster functions and are defined by two operational parameters: Affinity : associate client requests to cluster hosts. When no affinity is specified, all network requests are load-balanced across the cluster without regard to their source. Filtering mode : specify how the cluster handles traffic described by port range and protocols; can be single or multiple hosts. When a port rule is not configured, the default host will receive all network traffic. Windows Server NLB Clusters can be upgraded to Windows Server 2016 in two ways: - Rolling upgrade brings only a single host down at a time, upgrading it before adding it and proceeding to the next one - Simultaneous upgrade brings the entire NLB cluster goes down NLB clusters have a Cluster Operation Mode setting specifying what kind of TCP/IP traffic the cluster hosts should use - Unicast : NLB replaces the MAC address on the interface with the cluster's virtual MAC address, causing traffic to go to all hosts. Cluster hosts are prevented from communicating with each other in this mode. In this case, a second network adapter must be installed in order to facilitate normal communication between NLB cluster hosts. - Multicast : NLB adds a multicast MAC address to the network interface on each host that does not replace the original. Storage Every track of a hard drive platter is split into disk sectors , traditionally 512 bytes. A block is commonly called an \"allocation unit\" in Windows, but also commonly called a cluster. Storage left over unused in partially unused blocks is known as slack space . A new disk must first be initialized , that is, a partition table style must be chosen: - GPT : 128 partitions per disk, maximum volume size of 18 exabytes (2 60 bytes). Booting from a GPT drive is not possible unless the computer architecture supports EFI-based boot partitions. - MBR : older format that is commonly used for removable media, supporting volumes up to 2 TB with up to 4 primary partitions , although a common workaround is to make one of these partitions an extended partition , which can be be further subdivided into logical drives Mounting a partition as a single filesystem produces a volume , although the distinction can often be lost. The exception would be a case where a volume spans multiple partitions or physical disks, as is possible with software RAID. Virtual hard disks can be created with [Powershell][New-VHD] or in diskmgmt.msc and come in two formats: - VHD - VHDX Only 2 filesystem options are available for modern servers: - NTFS supports volumes up to 16 TB with the default 4 KB allocation unit size (but 256 TB with the 64 KB allocation unit size) and is required by some Windows Server services like AD DS , File Replication Service , Volume Shadow Copy Service , and Distributed File System - ReFS uses the same system of permissions as NTFS and offers error checking and repair capabilities that NTFS does not, but it does not support NTFS features like file compression, Encrypted File System , and disk quotas. ReFS supports a maximum file size of 16 exabytes and volumes up to 1 yobibyte (2 80 bytes) Software RAID can be implemented by creating Spanned , Striped , or RAID-5 volumes in diskmgmt.msc . A more modern and preferred technique is to create storage pools in [Storage Spaces][Storage Spaces]. Dedup Data deduplication (\"dedup\") is a role service that conserves storage space by storing only one copy of redundant chunks of files. Data duplication is appropriate to specific workloads, like backup volumes and file servers. It is not appropriate for database storage or operating system data or boot volumes. Data deduplication had required NTFS , although ReFS is supported since 1709. Data deduplication runs as a low-priority background process when the system is idle, by default; however its behavior can be configured based on its intended usage. Deduplication works by scanning files, and breaking them into unique chunks of various sizes that are collected in a chunk store . The original locations of chunks are replaced by reparse points . When a file is recently written, it is written in the standard, unoptimized form; the accumulation of such files is known as churn . Other jobs associated with deduplication include garbage collection , integrity scrubbing , and (when disabling deduplication) unoptimization . There are several deployment scenarios considered for data deduplication: - General purpose file servers Users often store multiple copies of the same, or similar, documents and files. Up to 30-50% of this space can be reclaimed using deduplication. - Virtualized Desktop Infrastructre (VDI) deployments Virtual hard disks that are used for remote desktops are essentially identical. Data Deduplication can also amelioriate the drop in storage performance when many users simultaneously log in at the start of the day, called a VDI boot storm . - Backup snapshots are an ideal deployment scenario because of the data is so duplicative. Deduplication is especially useful for disk drive backups, since snapshots typically differ little from each other. File shares Windows Server 2016 supports file shares via two protocols, both of which require the fs-fileserver role service: - SMB , long the standard for Windows networks - NFS , typically used in Linux, requires the installation of fs-nfs-service role service BranchCache enables client computers at remote locations to cache files accessed from shares, so that other computers at the same location can access them. Install the FS-BranchCache feature and enable the File and Printer Sharing and Branchcache - Hosted Cache Server (uses HTTPS) firewall display groups. Media Every track of a hard drive platter is split into disk sectors , traditionally 512 bytes. A block is commonly called an \"allocation unit\" in Windows, but also commonly called a cluster. Storage left over unused in partially unused blocks is known as slack space . A new disk must first be initialized , that is, a partition table style must be chosen: - GPT : 128 partitions per disk, maximum volume size of 18 exabytes (2 60 bytes). Booting from a GPT drive is not possible unless the computer architecture supports EFI-based boot partitions. - MBR : older format that is commonly used for removable media, supporting volumes up to 2 TB with up to 4 primary partitions , although a common workaround is to make one of these partitions an extended partition , which can be be further subdivided into logical drives Mounting a partition as a single filesystem produces a volume , although the distinction can often be lost. The exception would be a case where a volume spans multiple partitions or physical disks, as is possible with software RAID. Virtual hard disks can be created with [Powershell][New-VHD] or in diskmgmt.msc and come in two formats: - VHD - VHDX Only 2 filesystem options are available for modern servers: - NTFS supports volumes up to 16 TB with the default 4 KB allocation unit size (but 256 TB with the 64 KB allocation unit size) and is required by some Windows Server services like AD DS , File Replication Service , Volume Shadow Copy Service , and Distributed File System - ReFS uses the same system of permissions as NTFS and offers error checking and repair capabilities that NTFS does not, but it does not support NTFS features like file compression, Encrypted File System , and disk quotas. ReFS supports a maximum file size of 16 exabytes and volumes up to 1 yobibyte (2 80 bytes) Software RAID can be implemented by creating Spanned , Striped , or RAID-5 volumes in diskmgmt.msc . A more modern and preferred technique is to create storage pools in [Storage Spaces][Storage Spaces]. S2D Although a cluster can normally be created in the GUI Failover Cluster Manager , in order to use Storage Spaces Direct the system must be prevented from automatically creating storage, which necessitates creation in PowerShell with the NoStorage switch parameter, and then S2D must be enabled using Enable-ClusterStorageSpacesDirect . This command scans all cluster nodes for local, unpartitioned disks , which are added to a single storage pool and classified by media type in order to use the fastest disks for caching. The recommended drive configuration for a node in an S2D cluster is a minimum of six drives, with at least 2 SSDs and at least 4 HDDs, with no RAID or other intelligence that cannot be disabled. Caching is configured automatically, depending on the combination of drives present - NVMe + SSD : NVMe drives are configured as a write-only cache for the SSD drives - NVMe + HDD : NVMe drives are read/write cache - NVME + SSD + HDD : NVME are write-only for the SSD drives and read/write for HDD drives - SSD + HDD : SSD drives are read/write cache Microsoft defined two deployment scenarios for Storage Spaces Direct: - Disaggregated which creates two separate clusters, one of which is a Scale-out File Server dedicated to storage, essentially functioning as a SAN. This solution requires the [DCB][DCB] role for traffic management. At least two 10Gbps Ethernet adapters are recommended per node, preferably adapters that use RDMA. - Hyper-converged , where a single cluster node hosts VMs and storage. This solution is much less expensive because it requires less hardware and generates much less network traffic, but storage and compute can't scale independently: adding a node to storage necessarily entails adding one to the Hyper-V hosts, and vice versa. Storage Replica Storage Replica supports one-way replication between standalone servers, between clusters, and between storage devices within an [ asymmetric (stretch) cluster ][asymmetric cluster]. - Synchronous replication is possible when the replicated volumes can mirror data immediately, ensuring no data loss in case of failover - Asynchronous replication is preferable when the replication partner is located over a WAN link Storage Replica improves on DFS Replication, which is exclusively asynchronous and file-based, by using SMBv3 (port 445). Storage Replica requires two virtual disks, one for logs and one for data, which are the same size for each replication partner, and all the physical disks must use the same sector size. WSUS Windows Server Update Services (WSUS) can be configured from the command-line with wsusutil.exe. There are 5 basic WSUS architecture configurations Single WSUS Server downloads updates from Microsoft Update, and all the computers on the network download updates from it. A single server can usupport up to 25,000 clients. Replica WSUS Servers: a central WSUS server downloads from Microsoft Update, and after approval the updates are distributed to downstream servers at remote locations. Autonomous WSUS Servers: a central WSUS server downloads from Microsoft Update, all of which are distributed to remote servers; each remote site's administrators are individually responsible for evaluating and approving updates. Low-bandwidth WSUS Servers at remote sites download only the list of approved updates, which are then retrieved from Microsoft Update over the Internet, minimizing WAN traffic. Disconnected WSUS Servers have updates imported from offline media (DVD-ROMs, portable drives, etc), utilizing no WAN or Internet bandwidth whatsoever. When a computer first communicates with a WSUS server, it is added to the All Computers and and Unassigned Computers group automatically, which is created by default. Windows Server Backup To back up a VM without any downtime, integration services must be installed and enabled, and all disks must be basic disks formatted with NTFS . Windows Server Backup - System state includes boot files, Active Directory files, SYSVOL (when run on a DC), the registry, and other data. - System reserved is a special partition containing Boot Manager and Boot Configuration data. \ud83d\udcd8 Glossary adfind Query the schema version associated with Active Directory [Desmond][Desmond2009]: 53 adfind -schema -s base objectVersion adprep Prepare Active Directory for Windows Server upgrades. Must be run on the Infrastructure Master role owner with the flag /domainprep . [Desmond][Desmond2009]: 29 arp a d s bcdedit Change Windows bootloader to Linux, while dual booting ::Manjaro bcdedit /set {bootmgr} path \\EFI\\manjaro\\grubx64.efi ::Fedora bcdedit /set {bootmgr} path \\EFI\\fedora\\shim.efi Enable or disable Test Signing Mode ref bcdedit /set testsign on bcdedit /set testsign off bootrec Windows Recovery Environment command that repairs a system partition Use when boot sector not found bootrec /fixboot Use when BCD file has been corrupted bootrec /rebuildbcd cmdkey add delete generic list pass smartcard user Add a user name and password for user Mikedan to access computer Server01 with the password Kleo docs.microsoft.com cmdkey /add:server01 /user:mikedan /pass:Kleo dism Add-Driver Add-Package Add-ProvisionedAppxPackage Append-Image Apply-Image Apply-Unattend Capture-Image Cleanup-Image Commit-Image Disable-Feature Enable-Feature Export-Driver Export-Image Get-Driverinfo Get-Drivers Get-Featureinfo Get-Features Get-ImageInfo Get-MountedImageInfo Get-Packageinfo Get-Packages Get-ProvisionedAppxPackages List-Image Remount-Image Remove-Driver Remove-Image Remove-Package Remove-ProvisionedAppxPackage Set-ProvisionedAppxDataFile Unmount-Image Mount an image Zacker : 71 dism /mount-image /imagefile:$FILENAME /index:$N /name:$IMAGENAME /mountdir:$PATH Practice Labs dism /mount-wim /wimfile:c:\\images\\install.wim /index:1 /mountdir:c:\\mount Add a driver to an image file that you have already mounted Zacker : 72 dism /image:$FOLDERNAME /add-driver /driver:$DRIVERNAME /recurse Commit changes and unmount the image Zacker : 75 dism /unmount-image /mountdir:c:\\mount /commit Determine exact name of Windows features that can be enabled and disabled Zacker : 75 dism /image:c:\\mount /get-features Scan an image, checking for corruption dism /Online /Cleanup-Image /ScanHealth Check an image to see whether any corruption has been detected dism /Online /Cleanup-Image /CheckHealth Repair an offline dicsk using a mounted image as a repair source dism /Image:C:\\offline /Cleanup-Image /RestoreHealth /Source:C:\\test\\mount\\windows Zacker: 71-75 dism /mount-image /imagefile:C:\\images\\install.wim /index:1 /mountdir:C:\\mount dism /add-package /image:C:\\mount /packagepath:C:\\updates dism /add-driver /image:C:\\mount /driver:C:\\drivers\\display.driver\\nv_dispi.inf dism /commit-image /image:C:\\mount dism /unmount-image /image:C:\\mount djoin Perform an offline domain join for a Nano Server Zacker: 46 djoin /provision /domain practicelabs /machine PLABNANOSRV01 /savefile .\\odjblob Load the odjblob file created offline on the Nano Server. djoin /requestodj /loadfile c:\\odjblob /windowspath c:\\windows /localos dnscmd Replicate an AD-integrated DNS zone to specific DCs ref dnscmd . /CreateDirectoryPartition FQDN Enable GlobalNames zone support dnscmd <servername> /config /enableglobalnamessupport 1 Observe status of socket pool dnscmd /info /socketpoolsize Configure DNS socket pool size (0 through 10,000) dnscmd /Config /SocketPoolSize <value> dsquery Find the Active Directory Schema version from the command-line ref pwsh dsquery * cn=schema,cn=configuration,dc=domain,dc=com -scope base -attr objectVersion\" JEA Just Enough Administration (JEA) allows special remote sessions that limit which cmdlets and parameters can be used in a remote PowerShell session. These are implemented as restricted endpoints , to which only members of a specific security group can gain access. This offers a way to administer remote servers and move away from the traditional method using RDP. net Map a network location to a drive letter Practice Lab net use x: \\\\192.168.0.35\\c$ Stop/start a service net stop dns net start dns netdom Rename a computer netdom renamecomputer %computername% /newname: newcomputername Join a computer to a domain cf. Add-Computer , Zacker: 21 netdom join %computername% /domain: domainname /userd: username /password:* netsh Enable port forwarding (\" portproxy \") to a WSL2 distribution ( src ) netsh interface portproxy add v4tov4 listenaddress=0.0.0.0 listenport=2222 connectaddress=172.23.129.80 connectport=2222 Configure DNS to be dynamically assigned netsh interface ip set dns \"Wi-Fi\" dhcp Delete Wi-Fi profiles netsh wlan delete profile name=* Turn off Windows firewall netsh advfirewall set allprofiles state off Enable firewall rule group netsh advfirewall firewall set rule group=\u201dFile and Printer Sharing\u201d new enable=yes Show Wi-Fi passwords ( src netsh wlan show profile wifi key=clear Check/reset WinHTTP proxy netsh winhttp show proxy netsh winhttp reset proxy ntdsutil Used to transfer FSMO roles between domain controllers. [ Desmond: 30 ][Desmond2009] regsvr32 Register a DLL dependency in order to enable the Active Directory Schema MMC snap-in on a DC [Desmond][Desmond2009]: 54 regsvr32 schmmgmt.dll route p print add change delete Basic usage route add 192 .168.2.1 mask ( 255 .255.255.0 ) 192 .168.2.4 runas env netonly profile / no profile savecred showtrustlevels smartcard trustlevel user: Settings appsfeatures personalization printers windowsupdate about activation apps-volume appsforwebsites assignedaccess autoplay backup batterysaver bluetooth camera clipboard colors connecteddevices cortana crossdevice datausage dateandtime defaultapps delivery-optimization developers deviceencryption devices-touchpad display easeofaccess-display emailandaccounts findmydevice fonts keyboard lockscreen maps messaging mobile-devices mousetouchpad multitasking network network-wifi nfctransactions nightlight notifications optionalfeatures otherusers pen personalization-background personalization-colors personalization-start personalization-start-places phone powersleep privacy project proximity quiethours quietmomentsgame quietmomentspresentation quietmomentsscheduled recovery regionformatting regionlanguage remotedesktop savelocations screenrotation signinoptions signinoptions-launchfaceenrollment sound speech speech startupapps storagepolicies storagesense surfacehub-accounts surfacehub-calling surfacehub-devicemanagenent surfacehub-sessioncleanup surfacehub-welcome sync tabletmode taskbar themes troubleshoot typing usb videoplayback wheel windowsdefender windowsinsider workplace yourinfo sfc sfc /scannow shutdown Immediate restart shutdown /r /t 0 Log off shutdown /L slmgr ato dli dlv ipk rearm upk xpr sysdm 2 3 4 5 tracert On Windows, this command is aliased to traceroute which is the Linux command. [Lammle][Lammle]: 112 wbadmin enable backup get items get versions start backup start recovery start systemstaterecovery -backupTarget -hyperv -vsscopy | -vssFull Backup the entire drive, excluding some VMs wbadmin enable backup -backupTarget \\\\backups\\hostdr\\temp\\ -include:c: -exclude: C:\\VMs\\VM1.vhdx, C:\\VMs\\VMAR.vhd -vsscopy -quiet Zacker : 325-326 wbadmin get versions wbadmin get items -version: 11/14/2016:05:09 wbadmin start recovery -itemtype:app items:cluster -version:01/01/2008-00:00 Zacker : 422 wbadmin start systemstaterecovery -version:11/27/2016-11:07 wbadmin get versions wdsutil initialize-server remInst wdsutil /initialize-server /remInst:\"D:\\RemoteInstall\" winrm List all WinRM listeners winrm enumerate winrm/config/listener Display WinRM configuration winrm get winrm/config Add an address to Trusted Hosts list Zacker : 56 winrm set winrm/config/client @{TrustedHosts=\"192.168.10.41\"} winver wmic bios logicaldisk memorychip os path Recover Windows product key [fossbytes.com][https://fossbytes.com/how-to-find-windows-product-key-lost-cmd-powershell-registry/] wmic path softwarelicensingservice get OA3xOriginalProductKey Display information about installed RAM wmic memorychip list full List all objects of type Win32_LogicalDisk using that class's alias logicaldisk . [Desmond][Desmond2009]: 642 pwsh wmic logicaldisk list brief Recover serial number of a Lenovo laptop [pcsupport.lenovo.com][https://pcsupport.lenovo.com/us/en/solutions/find-product-name] wmic bios get serialnumber Display BIOS version wmic bios get biosversion Display operating system architecture wmic os get osarchitecture Display operating system type (48 is Windows 10) wmic os get operatingsystemsku [wsl][msdocs:wsl.exe] l s t \\ export import set-default-version [wsusutil]][msdocs:wsusutil.exe] Specify a location for downloaded updates Zacker : 393 C:\\Program Files\\Update Services\\Tools\\wsusutil.exe postinstall content_dir=d:\\wsus Specify SQL server, when not using the default WID database C:\\Program Files\\Update Services\\Tools\\wsusutil.exe postinstall sql_instance_name=\"db1\\sqlinstance1\"- content_dir=d:\\wsus wt d p split-pane focus-tab Open the default Windows Terminal profile and also an Ubuntu WSL tab [bleepingcomputer.com][https://www.bleepingcomputer.com/news/microsoft/windows-terminal-09-released-with-command-line-arguments-and-more/] wt; new-tab -p \"Ubuntu-18.04\" Open a split pane of the default profile in the D:\\ folder and the cmd profile in the E:\\ folder [bleepingcomputer.com][https://www.bleepingcomputer.com/news/microsoft/windows-terminal-09-released-with-command-line-arguments-and-more/] wt -d d:\\ ; split-pane -p \"cmd\" -d e: Open the default profile and an Ubuntu WSL profile, but with the first tab focused [bleepingcomputer.com][https://www.bleepingcomputer.com/news/microsoft/windows-terminal-09-released-with-command-line-arguments-and-more/] wt ; new-tab -p \"Ubuntu-18.04\"; focus-tab -t0 xcopy Copy from one directory to another Practice Lab xcopy /s c:\\inetpub\\wwwroot c:\\nlbport","title":"Windows Server"},{"location":"Windows/WS2016/#windows-server","text":"","title":"Windows Server"},{"location":"Windows/WS2016/#certification-exams","text":"Number Title 70-740 Installation, storage and Compute with Windows Server 2016 70-741 Networking with Windows Server 2016 70-742 Identity with Windows Server 2016 Find notes on labs here .","title":"Certification exams"},{"location":"Windows/WS2016/#installation","text":"Windows Server 2016 installations are determined by the most suitable installation method, option, and edition. Installation methods : An upgrade is an installation performed in-place with existing data intact and is opposed to a clean installation . A migration is a clean installation with old data transferred over. Migrations are facilitated by Powershell and [command prompt][SmigDeploy.exe] tools Installation options include Desktop Experience, [Server Core][Server Core], and Nano Server . The most important installation edition is Windows Server 2016 Datacenter edition , which is the only edition to have several important features that figure prominently in the exam. Storage Spaces Direct , Storage Replica Shielded VMs Network controller . Various other installation options exist, including: Windows Server 2016 Standard , Essentials , Multipoint Premium , Storage , and Hyper-V editions. Server installations are also influenced by choice of activation model .","title":"Installation"},{"location":"Windows/WS2016/#licensing","text":"Servicing channels provide a way of separating users into deployment groups for feature and quality updates. Semi-Annual Channel - previously known as Current Branch for Business (CBB) - features updates twice a year. It is more appropriate for non-infrastructure workloads that can be deployed through automation. Long Term Servicing Channel (LTSC) has a minimum servicing lifetime of 10 years and was designed to be used only for specialized devices such as those that control medical equipment or ATM machines, receiving new feature releases every 2-3 years","title":"Licensing"},{"location":"Windows/WS2016/#server-core","text":"Installing the Windows Server 2016 Server Core foregoes the possibility of later switching back to Desktop Experience, as had been possible in previous editions. Notably, WDS is incompatible with Server Core installations. Server Core installations can be managed with a GUI with the use of MMC snap-ins . Because MMC is reliant on Distributed Component Object Model (DCOM) technology, firewall rules have to be enabled to allow DCOM traffic (ref. Set-NetFirewallRule ).","title":"Server Core"},{"location":"Windows/WS2016/#nano-server","text":"Nano Server , a new installation option introduced in Windows Server 2016, provides a much smaller footprint and attack surface than even Server Core, but supports only some roles and features. Installation is done by building a VHD image via PowerShell on another computer. That VHD is then deployed as a VM or used as a boot drive for a physical server. Booting a Nano Server VM produces a text-based interface called the Nano Server Recovery Console , a menu system that allows configuration of static network options (DHCP is enabled by default). The DNS server may not be configured interactively, but must be specified when building the image with the Ipv4Dns parameter. If a Nano Server is domain-joined a remote Powershell session will authenticate via Kerberos. If not, its name or IP address must be added to the Trusted Hosts list. The Windows Server 2016 installation media contains a NanoServer directory, from which the NanoServerImageGenerator Powershell module must be imported. It also contains a Packages subdirectory, with CAB files containing roles and features that correspond to named parameters or packages that are specified as values to the Packages named parameter when building a Nano Server image. Cmdlet Description Edit-NanoServerImage Add a role or feature to an existing Nano Server VHD file New-NanoServerImage Used to create a Nano Server VHD file for Nano Server installation","title":"Nano Server"},{"location":"Windows/WS2016/#activation","text":"Server installations are influenced by choice of activation model. MAK is suitable for small networks, but large enterprises may opt for KMS . - MAK activations are subdivided into Independent and Proxy , based on whether or not a VAMT is used. - KMS activations, which distribute GVLK s, are valid for a period of time and require the installation of a role and management tools . KMS operates on TCP port 1688. - AVMA simplifies the process of activating Hyper-V VMs running Windows Server 2012 or 2016. Active Directory-based activation is an alternative for enterprises who opt to activate licenses through the existing AD DS domain infrastructure. Any domain-joined computers running a supported OS with a GVLK will be activated automatically and transparently. The domain must be extended to the Windows Server 2012 R2 or higher schema level, and a KMS host key must be added using the VAMT. After Microsoft verifies the KMS host key, client computers are activated by receiving an activation object from the DC. MS Docs","title":"Activation"},{"location":"Windows/WS2016/#images","text":"Many enterprises have begun virtualizing their server environments to take advantage of the many cost, reliability, and performance benefits that this change creates. Migrations should start with systems that are peripheral to main business interests before moving on to those that are more vital. A carefully documented protocol should be developed to facilitate the conversion of physical hard disks to VHDs for use in Hyper-V guests. Supported guest OSes include Linux and FreeBSD. The Microsoft Assessment and Planning (MAP) Toolkit is a free software tool that intelligently constructs a database of the hardware, software, and performance of computers on a network to plan for an operating system upgrade or virtualization. MAP supports the following discovery methods: Active Directory Domain Services Windows networking protocols System Center Configuration Manager IP address range scanning Computer names entered manually or imported from a file Server Core relies on the command-line for system maintenance, including updates which must be installed directly to the image using dism.exe or equivalent Powershell commands.","title":"Images"},{"location":"Windows/WS2016/#containers","text":"Containers run applications in an isolated namespace , meaning it only has access to resources that are made available to it by the container runtime. Resource governance means that a container has access only to a specified number of processor cycles, system memory, and other resources. Containers allow applications to be packaged with their dependencies in container images , which will run the same regardless of underlying operating system or infrastructure and are downloaded from container registries like Docker Hub . Container registries are not to be confused with repositories , which are subcomponents of registries. Windows Server 2016 suports Windows Server Containers and Hyper-V Containers , which create a separate copy of the operating system kernel for each container. The \"Containers\" feature must be installed on Windows Server 2016 hosts, and to create Hyper-V containers the Hyper-V role must also be installed (although the Hyper-V management tools are not necessary if VMs are not going to created). Windows container hosts need to have Windows installed to C:. Nano Server once could serve as Docker hosts, but no longer; Nano Servers are now intended to be deployed as containers themselves. The Powershell Docker module has been deprecated for years. Docker has several options for containers to store files in a persistent manner: - Volumes are stored in a part of the host filesystem which is managed by Docker (/var/lib/docker/volumes/ on Docker). - Bind mounts may be stored anywhere on the host system and are specified by [ docker run --volume ][docker run -v]. - tmpfs mounts are stored in the host system's memory only, and are available only on Linux.","title":"Containers"},{"location":"Windows/WS2016/#dns","text":"","title":"DNS"},{"location":"Windows/WS2016/#installation_1","text":"DNS server role requiremenets: - Statically assigned IP - Signed-in user must be member of local Administrators group There are several recommended DNS deployment scenarios, all of which involve installing DNS on a Server Core or Nano Server instance. This is because these installation options offer a reduced attack surface, a reduced resource footprint, and reduced patching requirements. - DNS on DC : All DNS features are available and supports AD-integrated, primary, secondary, and stub zones. - DNS on RODC : Passes DNS zone updates to a writeable DC - DNS on standalone member server : Supports file-based primary, secondary, and stub zones but requiring zone replication because there is no integration over AD.","title":"Installation"},{"location":"Windows/WS2016/#nano-server_1","text":"Installing DNS on a running Nano Server image requires running Install-NanoServerPackage as well as enabling the \"DNS-Server-Full-Role\" optional feature using Enable-WindowsOptionalFeature . As of early 2017, Nano Server only supported a few roles, including DNS , but was only able to do so with some limitations - Nano Server can only support file-based DNS and cannot host AD-integrated zones. - Nano Server only supports the Semi-Annual servicing channel license. - Nano Server is not suitable for primary zones, only caching-only, forwarder, or secondary zone DNS servers","title":"Nano Server"},{"location":"Windows/WS2016/#zones","text":"Zones can be considered one or more DNS domains or subdomains, associated with zone files , which compose the DNS database itself and contain two types of entries: Parser commands , which provide shorthand ways to enter records: $ORIGIN , $INCLUDE , and $TTL Resource records are whitespace delimited text files with columns for name, time to live, class, type, and data The copies of zone files local to individual DNS servers can be primary (read/write) or secondary (read-only). A primary zone is a writable copy of a DNS zone that exists on a DNS server. A secondary zone is a read-only replica of a primary zone and necessitates the presence of a primary zone for the same zone. Defining a secondary zone via PowerShell requires specifying that zone's MasterServers . In Windows Server, zone files can also be integrated with Active Directory, making what is called an Active Directory Integrated Zone . These allow multi-master zones , meaning any DC can process zone updates and the zone can be replicated to any DC in the domain or forest. An AD-integrated zone can be specified by passing the ReplicationScope parameter to the Add-DnsServerPrimaryZone cmdlet. Stub zones contains only name server (NS) records of another zone, but unlike a forwarder is able to update when name servers in a target zone change. Reverse Lookup zones are used to resolve IP addresses to FQDNs. Reverse lookup zones for public IP address space are often administered by ISPs, and they are useful in spam filtering to double-check the source domain name with the IP address. GlobalNames zones provide \"single label name resolution\" (as opposed to a FQDN) and are intended to replace WINS servers.","title":"Zones"},{"location":"Windows/WS2016/#query-traffic","text":"The process of resolving a query by querying other DNS servers is called recursion . Recursion can be disabled outright but Windows Server 2016 supports recursion scopes which will allow recursion to be disabled unless certain conditions are met (such as receiving the request on a particular interface). There are two types of query in the context of recursion: - Recursive query sent by the petitioner: that is, the original query which begins recursion. - Iterative query : individual queries sent out to authoritative name servers in order to resolve a recursive query. Root hints are preconfigured root servers that are necessary to begin the recursion process. The DNS Server service stores root hints in %systemroot%\\System32\\dns\\CACHE.DNS . These can be edited through the GUI or by using the PowerShell commands Add- , Import- , Remove- , and Set-DnsServerRootHint . Forwarding of a request occurs when a petitioned DNS server is unable to resolve the query because it is both: - Non-authoritative for the specified zone, and - Does not have the response cached. Two actions are possible when forwarding: - Configure a DNS server only to respond to queries it can satisfy by referencing locally-stored zone information, forwarding all other requests. - Configure forwarding for specific zones through conditional forwarding A secondary zone is not to be confused with delegation , where a DNS server delegates authority over part of its namespace (i.e. a subdomain) to one or more other servers. Windows Server 2016 supports a DNS GlobalNames zone meant to supercede WINS, which served a role similar to DNS for the old NetBIOS naming standard. NetBIOS names use a nonhierarchical structure (i.e. are a single name and not divisible into sub-domains) based on a name up to 16 characters long (although the 16th character defines a particular service running on the host defined by the previous 15). An organization must share a single GlobalNames zone, which must be created in PowerShell manually.","title":"Query traffic"},{"location":"Windows/WS2016/#resource-records","text":"Zone scavenging allows servers with stale records to remove them. This feature is disabled by default, but can be set at the server or zone level. Type Description A IPv4 address record AAAA IPv6 address record CNAME Hostname or alias for hosts in the domain MX Where mail for the domain should be delivered NS Name servers PTR Reverse lookup SOA Each zone contains a single SOA record SRV Generalized service location record, used for newer protocols instead of protocol-specific records TXT Typically holds machine-readable data","title":"Resource records"},{"location":"Windows/WS2016/#security","text":"DNSSEC offers security features using public key certificates. A socket pool can be used to configure the DNS server to use a random source port when issuing DNS queries. Response rate limiting can pose a defense against DNS DoS attacks by ignoring potentially malicious, repetitive requests. DNS-based Authentication of Named Entities (DANE) is supported by Windows Server 2016 to reduce man-in-the-middle attacks. DANE works by informing DNS clients requesting records from the domain which Certification Authoority they must expect digital certificates to be issued from.","title":"Security"},{"location":"Windows/WS2016/#policies","text":"Zone transfer policies can prevent or allow zone transfers to any server, to name servers, or to servers specified by FQDN or IP address. DNS Policy is a new feature in Windows Server 2016 that can control DNS server behavior depending on certain criteria. These criteria include: Client subnet Recursion scope Zone scope","title":"Policies"},{"location":"Windows/WS2016/#dnssec","text":"DNSSEC is a security setting for DNS that enables all DNS records in a zone to be digitally signed by a trust anchor which validates DNSKEY resource records. Root and top-level domain zones already have trust anchors configured and merely have to have it enabled. To implement trust anchors: - A TrustAnchors zone must be created, which will store public keys associated with specific zones. A trust anchor from the secured zone must be created on every DNS server that hosts the zone. - A Name Resolution Policy Table (NRPT) GPO must be created (Windows Settings\\Name Resolution Policy) This option can require DNSSEC based on computer name prefix or suffix, FQDN, or subnet. - DNSSEC key master is a special DNS server that generates and manages signing keys for DNSSEC protected zones. DANE allows you to publish certificate information within the DNS zone, rather than one of the thousands of trusted CAs. This protects against rogue/compromised CAs issuing illegitimate TLS certificates. Two cryptographic keys: - Zone Signing Key (ZSK) signs zone data including individual resource records other than DNSKEY. It is also used to create the KSK. - Key Signing Key (KSK) is used to sign all DNSKEY records at the zone root. DNSSEC record types: - RRSIG \"resource record signature\" each of which matches and provides a signature for an existing record in a zone - NSEC proves nonexistence of a record - NSEC3 NSEC replacement that prevents zone walking - NSEC3PARAM specifies the NSEC3 records included in response for DNS names that don't exist - DNSKEY stores public key used to verify a signature - DS delegation signer records secure delegations","title":"DNSSEC"},{"location":"Windows/WS2016/#dsc","text":"IaaS management of servers is possible with Desired State Configuration (DSC) , a feature of Windows PowerShell where script files stored on a central server can apply specific a specific configuration to nodes. These scripts are idempotent , meaning that they can be applied repeatedly without generating errors. The DSC model is composed of phases 1. Authoring Phase, where MOF definitions are created 2. Staging Phase, where declarative MOFs are staged and a Configuration calculated per node 3. \"Make It So\" Phase, where declarative Configurations are implemented through imperative providers Components of DSC scripts include: - Local Configuration Manager : engine running on the client system that received configurations from the DSC server and applies them to the target. - Node block specifies the names of target computers. - Resource block specifies settings or components and the values that the configuration script should assign to them. DSC configurations can be deployed in two different refresh modes Pull architecture : target LCM periodically retrieves configuration from a Pull Server , which consolidates MOF files. Push architecture : configuration is sent to target in response to explicit invocation of Start-DSCConfiguration on the server. LCM has to be configured to accept Configurations of either refresh mode.","title":"DSC"},{"location":"Windows/WS2016/#tasks","text":"Set LCM to push mode [ DSCLocalConfigurationManager ()] Configuration LCMConfig { Node localhost { Settings { RefreshMode = 'Push' } } } Install Telnet client Configuration InstallTelnetLocal { Import-DscResource -ModuleName 'PSDesiredStateConfiguration' Node localhost { WindowsOptionalFeature InstallTelnet { Name = \"Telnet-Client\" Ensure = \"Present\" } } } Install WSL Configuration InstallWSLLocal { Import-DscResource -ModuleName 'PSDesiredStateConfiguration' Node localhost { WindowsOptionalFeature InstallWSL { Name = \"Microsoft-Windows-Subsystem-Linux\" Ensure = \"Present\" } } }","title":"Tasks"},{"location":"Windows/WS2016/#failover-clusters","text":"Failover clusters are composed of computers called nodes and can be created using New-Cluster . which typically possess a secondary network adapter, used for cluster communications. Before Windows Server 2016, all cluster nodes had to belong to the same domain, but now this is but one of several possible cluster types called a single-domain cluster. A failover cluster can also be multi-domain , or workgroup , depending on how or if the servers are joined to domains. A cluster can also be detached from AD, even though its nodes are joined. A cluster whose servers are joined to a single domain is typically associated with a cluster name object in Active Directory, which serves as its administrative access point . A workgroup cluster or a detached cluster need to have the cluster's network name registered in DNS as its administrative access point, which can be specified in Powershell with the AdministrativeAccessPoint named parameter. Additionally, on a workgroup cluster the same local administrator account must be created on every node, preferably the builtin Administrator account, although a different account can be configured if a particular Registry key is [created][New-ItemProperty] on each node. Nodes that are domain-joined support CredSSP or Kerberos authentication, but workgroup nodes support NTLM authentication only. Three types of witness resources can help to ensure a quorum takes place in clusters. This is necessary to prevent a split-brain situation, where communication failures between nodes cause separate segments of the clusters to continue operating independently of each other. A witness is created when a cluster has an even number of nodes, and only one can be configured. [pwsh][Set-ClusterQuorum] - Disk witness : dedicated disk in shared storage that contains a copy of the cluster database - File Share witness : SMB file share containing a Witness.log file with information about the cluster - Cloud witness : blob stored in Azure Scale-out File Server (SoFS) is a clustered role providing highly available storage to cluster nodes. SoFS ensures continuous availability in the case of a node failure. Using SoFS, multiple nodes can also access the same block of storage at the same time, and for this reason is is an active/active or dual active system, as opposed to one where only one node provides accessible shares, or an active/passive system. SoFS is specifically recommended for use on Hyper-V and SQL Server clusters and can be installed with Add-ClusterScaleOutFileServer . SoFS shares are created with the [ New-SmbShare ][New-SmbShare] PowerShell cmdlet. SoFS shares are located on Cluster Shared Volumes (CSV) , a shared disk containing an NTFS or ReFS volume that is made accessible for read and write operations by all nodes within a failover cluster. CSVs solved a historical problem with using NTFS volumes with VMs in previous versions of Windows Server. NTFS is designed to be accessed by only one operating system instance at a time. In Windows Server 2008 and earlier, this meant that only one node could access a disk at a time, which had to be mounted and dismounted for every VM. The solution was to create a pseudo-file system called CSVFS , sitting on top of NTFS, that enables multiple drives to modify a disk's content at the same time, but restricting access to the metada to the owner or coordinator . The coordinator node refers to the cluster node where NTFS for the clustered CSV disk is mounted, any other node is called a Data Server (DS) . VM resiliency can be configured by adjusting settings in response to changes in VM state: - Unmonitored : VM owning a role is not being monitored by the Cluster Service - Isolated : Node is not currently an active member of the cluster, but still possess the role - Quarantine : Node has been drained of its roles and removed from the cluster for a specified length of time. Cluster Operating System Rolling Upgrade is a new feature that reduces downtime by making it possible for a cluster to have nodes running both Windows Server 2012 R2 and Window Server 2016. Using this feature, nodes can be brought down for an upgrade. When [Storage Spaces][Storage Spaces] is combined with a failover cluster, the solution is known as Clustered Storage Spaces .","title":"Failover clusters"},{"location":"Windows/WS2016/#high-availability","text":"Hyper-V Replica allows simple failover to occur between Hyper-V hosts, without the need for a cluster. To configure a simple one-way failover solution using Hyper-V Replica, configure the destination VM as a replica server , either in Hyper-V Manager or PowerShell. [ Set-VMReplicationServer ][Set-VMReplicationServer] lab The destination host must also have firewall ports opened corresponding to the authentication method chosen. The source VM, which is to be replicated, must have its options configured through the Enable Replication wizard . [ Enable-VMReplication ][Enable-VMReplication] To use Hyper-V Replica as a (two-way) failover solution, configure both VMs as replica servers. Migrations can take place one of three methods: - Live Migration moves only the system state and live memory contents, not data files. Live migration requires that the hosts be, if not clustered, at least part of the same (or a trusted) domain. Live Migration requires that VHD files be placed on shared storage and both hosts have appropriate permissions to access said storage. An unpopulated VM is created on the destination with the same resources as the source before transferring memory pages. Once the servers have an identical memory state, the source VM is suspended and the destination takes over. Hyper-V notifies the network switch of the change, diverting network traffic to the destination. Authentication can be made by [CredSSP][CredSSP] or Kerberos. When a Hyper-V cluster is created, the Failover Cluster Manager launches the High Availability Wizard, which configures the VM to support Live Migration. The same thing can be done with the PowerShell cmdlet Add-ClusterVirtualMachineRole . Additionally, using Kerberos authentication for live migration requires constrained delegation , which enables a server to act on behalf of a user for only certain defined services. This must be configured within Active Directory Users and Computers , by opening the Properties of the source Computer object, and changing the setting under the Delegation tab. - An additional, outdated method of migration is quick migration , which was present in Windows Server prior to the introduction of live migration and persists in Windows Server 2016 for backward compatibility. A quick migration involves pausing the VM, saving its state, moving the VM to the new owner, and starting it again. A quick migration always involves a short period of VM downtime. Shared Nothing Live Migration requires that source and destination VMs be members of the same (or trusted) domain, and source and domain servers must be running the same processor family (Intel or AMD) and linked by an Ethernet network running a minimum of 1 Gbps. Additionally, both Hyper-V hosts must be running idential virtual switches that use the same name; otherwise the migration process will be interrupted to prompt the operator to select a switch on the destination server. The process of migrating is almost identical to a Live Migration, except that you select the \"Move the Virtual Machine's Data To a Single Location\" option on the Choose Move Options page of the Move Wizard. Storage Migration works by first creating new VHDs on the destination corresponding to those on the source server. While the source server continues to operate using local files, Hyper-V begins mirroring disk writes to the destination server and begins a single-pass copy of the source disks to the destination begins, skipping blocks that have already been copied. Once the copy has completed, the VM begins working from the destination server and the source files are deleted. For a VM that is shut off, storage migration is equivalent to simply copying files from source to destination. Site-aware clusters have failover affinity . Node fairnes evalutes memory and CPU loads on cluster nodes over time.","title":"High availability"},{"location":"Windows/WS2016/#cluster-management","text":"VM Monitoring allows specific services to be restarted or failed-over when a problem occurs. To use VM Monitoring: - The guest must be joined to the same domain as the host - The host administrator must be a member of the guest's local Administrators group - And Windows Firewall rules in the Virtual Machine Monitoring group must be enabled. The service can then be monitored using [ Add-ClusterVMMonitoredItem ][Add-ClusterVMMonitoredItem].","title":"Cluster management"},{"location":"Windows/WS2016/#migration","text":"VMs can be moved from node to node of a cluster using live , storage , or quick migrations. VM network health protection is a feature (enabled by default) that detects whether a VM on a cluster node has a functional connection to a designated network. If not, the cluster live migrates the VM role to another node that does have such a connection. This setting can be controlled in Hyper-V Manager > VM Settings > Advanced Features > Protected network","title":"Migration"},{"location":"Windows/WS2016/#gpo","text":"Group Policy Objects (GPO) facilitate the uniform administration of large numbers of users and computers. GPOs can be local or domain-based . Local GPOs come in several varieties, applied in the following order (last takes highest precedence): - Local Group Policy applied to computers - Administrators and Non-Administrators Local Group Policy applied to users based on their membership in local Administrators group. - User-specific Local Group Policy : Domain-based GPOs consist of two components a [ container ][Group Policy container] and a [ template ][Group Policy template]. These are stored in different locations and replicated by different means. - Containers define the fundamental attributes of a GPO, each of which is assigned a GUID, and are stored in the AD DS database and replicated to other domain controllers using intrasite or intersite AD DS replication schedule. - Templates, a collection of files and folders that define the actual GPO settings, are stored in the SYSVOL shared folder ( %SystemRoot%\\SYSVOL\\Domain\\Poligicies\\{GUID} ) on all DCs. SYSVOL replication is handled by the DFS Replication Agent since Windows Server 2008. A GPO consists of 2 top-level nodes: - Computer Configuration contains settings that are applied to computer objects to which the GPO is linked - User Configuration containers user-related settings, applied when a user signs in and thereafter and automatically refreshed every 90-120 minutes Beneath each of these nodes are folders that group settings - Software Settings - Windows Settings allows basic configuration for computers or users - Administrative Templates contains Registry settings that control user, computer, and app behavior and settings, grouped logically into folders Although domain controllers store and serve GPOs, the client computer itself must request and apply the GPOs using the Group Policy Client service. Client-side extensions process the GPOs once downloaded Starter GPOs are intended for use in large organizations with a proliferation of GPOs that share settings. Starter GPOs can be imported from, and exported to, a .CAB file. Once a GPO is created it must be linked to a container object in AD DS for it to apply to objects, a process known as scoping . GPOs can be linked to Sites, Domains, and OUs. If multiple GPOs are linked to the same container, the link order must be configured. There are 2 default GPOs in an AD DS domain, which can be reset using arguments to the dcgpofix command. - Default Domain Policy, linked to the domain object - Default Domain Controllers Policy, linked to the Domain Controllers OU Although it is possible to link the same GPO to multiple containers, it is recommended to import (i.e. copy) a GPO from another domain. This process effecitvely restores the settings of another GPO into a newly created GPO, which is then linked to another container.","title":"GPO"},{"location":"Windows/WS2016/#hyper-v","text":"[Hyper-V][Hyper-V] is a Type I hypervisor and role that allows a Windows Server 2016 host to create VMs, called guests . In Type I virtualization, the hypervisor forms an abstraction layer that interacts directly with the host hardware. In this model, the individual environments created by the hypervisor, including the host operating system and guest VMs, are called [partitions][partition]. Hyper-V Server, a free product available for download is limited to the command-line Server Core interface, however it does include Sconfig to aid configuration. Hyper-V can be managed remotely using the GUI (Hyper-V Manager, hyper-v-tools ), or Powershell ( hyper-v-powershell ). Authentication can be via Kerberos or Credential Security Support Provider (CredSSP) , which must be enabled on both server and client. PowerShell remoting - Explicit remoting involves opening a PowerShell session to a remote session - Implicit remoting involves running a cmdlet specifying the ComputerName parameter. PowerShell Direct allows easy remoting to VMs by using the -VmName Powershell parameter using a PowerShell session. [Nested virtualization][Nested virtualization] is a new capability where a virtual host running Windows Server 2016 on a physical host also running Windows Server 2016 can host nested VMs.","title":"Hyper-V"},{"location":"Windows/WS2016/#host-configuration","text":"[Hyper-V][Hyper-V] is a Type I hypervisor and role that allows a Windows Server 2016 host to create VMs, called guests . In Type I virtualization, the hypervisor forms an abstraction layer that interacts directly with the host hardware. In this model, the individual environments created by the hypervisor, including the host operating system and guest VMs, are called [partitions][partition]. Hyper-V Server, a free product available for download is limited to the command-line Server Core interface, however it does include Sconfig to aid configuration. Hyper-V can be managed remotely using the GUI (Hyper-V Manager, hyper-v-tools ), or Powershell ( hyper-v-powershell ). Authentication can be via Kerberos or Credential Security Support Provider (CredSSP) , which must be enabled on both server and client. PowerShell remoting - Explicit remoting involves opening a PowerShell session to a remote session - Implicit remoting involves running a cmdlet specifying the ComputerName parameter. PowerShell Direct allows easy remoting to VMs by using the -VmName Powershell parameter using a PowerShell session. [Nested virtualization][Nested virtualization] is a new capability where a virtual host running Windows Server 2016 on a physical host also running Windows Server 2016 can host nested VMs.","title":"Host configuration"},{"location":"Windows/WS2016/#networking","text":"Virtual switches can be external , internal , or private (in order of decreasing access). Up to 8 network adapters can be [added][Add-VMNetworkAdapter] to a Windows Server 2016 Hyper-V VM. Hyper-V maintains a pool of MAC addresses which are assigned to virtual network adapters as they are created. Hyper-V MAC addresses begin with 00-15-5D , followed by the last two bytes of the IP address assigned to the server's physical network adapter (i.e. last two octets), then a final byte. Generation 1 VMs supported synthetic and legacy virtual network adapters, but in Generation 2 VMs only synthetic adapters are used. Generation 1 VMs can only boot from network (PXE) when using a legacy adapter. Physical hosts running Windows Server 2016 can support teams of up to 32 NICs, but Hyper-V VMs are limited to teams of two. The team must first be configured in the host operating system and appears as a single interface in the Virtual Switch Manager. High-performance embedded teaming , reliant on RDMA , can only be configured with Powershell . - Teaming Mode - Switch Independent : switch is unaware of presence of NIC team and does not load balance to members; Windows is performing the teaming - Switch Dependent : switch determines how to distribute inbound network traffic; only supported by specialty hardware - Static Teaming : switch and host are manually configured (typically supported by server-class switches) - Link Aggregation Control Protocol (LACP) : dynamically identifies links that are connected between the host and the switch - Load Balancing Mode - Address Hash : a hash is created based on address components of the packet, which is used to reasonably balance adapters - Hyper-V Port : NIC teams configured on Hyper-V hosts give VMs independent MAC addresses - Dynamic : outbound loads are distributed based on a hash of the TCP ports and IP addresses Virtual machine queuing will enhance performance if a physical host supports it and it is enabled . Bandwidth management is achieved by setting limits on the virtual network adapter, in the GUI or in [Powershell][Set-VMNetworkAdapter].","title":"Networking"},{"location":"Windows/WS2016/#storage","text":"The New Virtual Machine Wizard presents different options for Generation 1 vs. Generation 2 VMs. - Generation 1 VMs provide two IDE controllers, which host the hard drive and a DVD drive, and an unpopulated SCSI controller which can host additional disks. - Generation 2 VMs, however, have only a single SCSI controller, which hosts all virtual drives. A new VHD can be created using - Hyper-V Manager through the New Virtual Hard Disk Wizard - Disk Management ( diskmgmt.msc ), however the option to create a differencing disk is not available, nor can specific block or sector size be specified - PowerShell Shared virtual disk files are preferably created as VHD sets . Pass-through disks make exclusive use of a physical disk. pwsh Standard checkpoints (previously known as \"snapshots\" in Windows Server 2012 and before) with the extensions AVHD or AVHDX save the state, data, and hardware configuration of a VM. They are recommended for development and testing but are not a replacement for backup software nor recommended for production environmentsj, because restoring them in a production environment will interrupt running services. Production checkpoints do not save memory state, but use Volume Shadow Copy Service (Windows) or File System Freeze (Linux) inside the guest to create \"point in time\" images of the VM.","title":"Storage"},{"location":"Windows/WS2016/#shielded-vms","text":"Shielded VMs are a feature exclusive to the Datacenter Edition of Windows Server 2016. As a result of increased virtualization, physical servers that were once secured physically were migrated to Hyper-V hosts that are less secure because they are accessible to fabric administrators . Shielded VMs were introduced to protect tenant workloads from inspection, theft, and tampering as a result of being run on potentially compromised hosts. A security concept closely associated to shielded VMs is the guarded fabric , which is a collection of nodes cooperating to protect shielded Hyper-V guests. The guarded fabric consists of: - Host Guardian Service (HGS) utilizes remote attestation to confirm that a node is trusted; if so, it releases a key enabling the shielded VM to be started. HGS is typically a cluster of 3 nodes. - Guarded hosts : Windows Server 2016 Datacenter edition Hyper-V hosts that can run shielded VMs only if they can prove they are running in a known, trusted state to the Host Guardian Service. - Shielded VMs In a production environment, a fabric manager like Virtual Machine Manager would be used to deploy shielded VMs (which are signified by a shield icon). Shielded VMs must run Windows (8+) or Windows Server (2012+), although Linux shielded VMs are now also supported since version Windows Server version 1709. Shielded VMs are produced by a three-stage process (VHD -> Shielded template -> Shielded VMs) 1. Preparation : Install and configure an OS onto a virtual disk file 2. Templatization : Convert virtual disk file into a shielded template 3. Provisioning : Create one or more shielded VMs from the shielded template Configure HGS in its own new forest YouTube Install-WindowsFeature HostGuardianServiceRole -Restart Install-HgsServer -HgsDomainName 'savtechhgs.net' -SafeModeAdministratorPassword $adminPassword -Restart Shielding Data is created and owned by tenant VM owners and contains secrets needed to create shielded VMs that must be protected from the fabric admin. Resources: Intro to shielded VMs Create a shielded VM using Powershell Linux Shielded VM How To Shielded VM Demonstration and Quick Setup Guarded Fabric Deployment Guide for Windows Server 2016 Deploying Shielded VMs and a Guarded Fabric with Windows Server 2016","title":"Shielded VMs"},{"location":"Windows/WS2016/#attestation","text":"There are two modes of attestation supported by HGS: Hardware-trusted attestation Hardware-trusted attestation mode requires : Measured boot : TPMv2 to seal software and hardware configuration details measured at boot Code integrity enforcement to strictly define permissible software Platform Identity Verification : Active Directory is not sufficient to identify the host. Rather, an identity key rooted in the host TPM is used for identity. Remote attestation based on asymmetric key pairs Admin-trusted attestation was previously based on guarded host membership in a designated AD DS security group, but is deprecated beginning with Windows Server 2019. Host identity is [verified]](https://youtu.be/B2vFrdXd5jg?t=525) by checking security group permission No Measured Boot or Code Integrity Validation Intended to aid transition to Hardware-trusted attestation mode for hosts produced before TPMv2","title":"Attestation"},{"location":"Windows/WS2016/#vm-configuration","text":"VMs are associated with a variety of file types: Extension Description .vmc XML-format VM configuration .vhd, .vhdx Virtual hard disks .vsv Saved-state files VMs can be created in Hyper-V, and a machine's RAM can even be changed dynamically. Hyper-V guests can take advantage of a suite of features to enhance performance and functionality. - Virtualization of NUMA architecture - Smart paging for when VMs that use dynamic memory restart and temporarily need more memory than is available on the host, for example at boot - Monitoring resource usage, to minimize cost overruns when guests run in the cloud - Disk and GPU passthrough, and other PCI-x devices, with DDA pwsh - Increased performance of interactive sessions that use [VMConnect][VMConnect.exe] Microsoft supports some Linux distributions, like Ubuntu, with built-in Linux Integration Services , which improve performance by providing custom drivers to interface with Hyper-V. Some distributions like CentOS and Oracle come with integrated LIS packages, but free LIS packages provided by Microsoft for download from the Microsoft Download Center support additional features and come with the additional benefit of being versioned. These packages are provided as tarballs or ISO images, and must be loaded directly into the running guest operating system. FreeBSD has included full support for FreeBSD Integration Services (BIS) since version 10. Secure Boot has to be disabled when loading Hyper-V VMs running Linux distributions, since UEFI does not have certificates for non-Windows operating systems by default. Some distributions supported by Microsoft do have certificates in the UEFI Certificate Authority . Different versions of Hyper-V create VMs associated with that version (Windows Server 2016 uses Hyper-V 8.0). VMs created by older versions of Hyper-V can be [updated][Update-VMVersion], but once updated they may no longer run on a host of a previous version. Importing an exported VM can be done in three ways: - Register : exported files are left as-is and the guest's ID is maintained; - Restore : exported files copied to the host's default locations or ones that are otherwise specified; ID is maintained - Copy : exported files are copied; new ID generated PXE boot is supported in two scenarios: - Generation 1 VMs with a legacy virtual network adapter support PXE boot (but not synthetic ). Generation 1 VMs are limited to 2 TB in size and do not support many of the advanced features that Generation 2 VMs do. But PXE Boot remains one of the primary reasons to continue using a Generation 1 VM. - Generation 2 VMs with a synthetic network adapter also support PXE boot. would also support bandwidth management and VMQ. Generation 2 VMs also do not support 32-bit OSes, including: - Windows Server 2008, R2 - Windows 7 - Older Linux distros - FreeBSD (all) VMs cannot be upgraded from Generation 1 to Generation 2 easily, although a script named Convert-VMGeneration was once provided by Microsoft and can still be found. But the VM's version , referring to the version of Hyper-V used to create it, can be upgraded with Upgrade-VMVersion .","title":"VM configuration"},{"location":"Windows/WS2016/#monitoring","text":"Performance Monitor is a program that allows realtime monitoring of hundreds of different system performance statistics, called performance counters . Counters can be viewed in several ways, including line graph, histogram bar graph, and report views. Every counter added to a graph is associated with a computer, a performance object (hardware or software component to be monitored), a performance counter (statistic), and an instance. A data collector set captures counter statistics for later review. A single data collector set can gather performance counter data from multiple VMs. Event trace data cannot be combined with performance data in the same data collector set. Expiration dates can be set for data collector sets, but if actively collecting data the expiration date will not stop collection. A performance alert is a type of data collector set that can track system performance and log events in the application event log. Alerts can be triggered when a performance counter value exceeds a certain threshold. Only members of the local groups Administrators and Performance Log Users can create alerts, but the Log on as a batch user right must be granted to members of Performance Log Users. A hard fault occurs when data is swapped between memory and disk.","title":"Monitoring"},{"location":"Windows/WS2016/#performance-counters","text":"Counter Acceptable values Processor: % Processor Time <85% Processor: Interrupts/Sec cf. baseline System: Processor Queue Length <2 Server Work Queues: Queue Length <4 Memory: Page Faults/Sec <5 Memory: Pages/Sec <20 Memory: Available MBytes >5% of physical memory Memory: Committed Bytes < physical memory Memory: Pool Non-Paged Bytes Stable PhysicalDisk: Disk Bytes/Sec cf. baseline PhysicalDisk: Avg. Disk Bytes/Transfer cf. baseline PhysicalDisk: Current Disk Queue Length <2 per spindle PhysicalDisk: % Disk Time <90% LogicalDisk: % Free Space >20% Network Interface: Bytes Total/Sec cf. baseline Network Interface: Output Queue Length <2 Server: Bytes Total/Sec 50% of total bandwidth","title":"Performance counters"},{"location":"Windows/WS2016/#network-load-balancing","text":"Cluster VMs can be configured to drain their workloads to other nodes when being shutdown using Suspend-ClusterNode NLB Clusters are made of hosts , while Failover Clusters are made of nodes . NLB port rules control how the cluster functions and are defined by two operational parameters: Affinity : associate client requests to cluster hosts. When no affinity is specified, all network requests are load-balanced across the cluster without regard to their source. Filtering mode : specify how the cluster handles traffic described by port range and protocols; can be single or multiple hosts. When a port rule is not configured, the default host will receive all network traffic. Windows Server NLB Clusters can be upgraded to Windows Server 2016 in two ways: - Rolling upgrade brings only a single host down at a time, upgrading it before adding it and proceeding to the next one - Simultaneous upgrade brings the entire NLB cluster goes down NLB clusters have a Cluster Operation Mode setting specifying what kind of TCP/IP traffic the cluster hosts should use - Unicast : NLB replaces the MAC address on the interface with the cluster's virtual MAC address, causing traffic to go to all hosts. Cluster hosts are prevented from communicating with each other in this mode. In this case, a second network adapter must be installed in order to facilitate normal communication between NLB cluster hosts. - Multicast : NLB adds a multicast MAC address to the network interface on each host that does not replace the original.","title":"Network Load Balancing"},{"location":"Windows/WS2016/#storage_1","text":"Every track of a hard drive platter is split into disk sectors , traditionally 512 bytes. A block is commonly called an \"allocation unit\" in Windows, but also commonly called a cluster. Storage left over unused in partially unused blocks is known as slack space . A new disk must first be initialized , that is, a partition table style must be chosen: - GPT : 128 partitions per disk, maximum volume size of 18 exabytes (2 60 bytes). Booting from a GPT drive is not possible unless the computer architecture supports EFI-based boot partitions. - MBR : older format that is commonly used for removable media, supporting volumes up to 2 TB with up to 4 primary partitions , although a common workaround is to make one of these partitions an extended partition , which can be be further subdivided into logical drives Mounting a partition as a single filesystem produces a volume , although the distinction can often be lost. The exception would be a case where a volume spans multiple partitions or physical disks, as is possible with software RAID. Virtual hard disks can be created with [Powershell][New-VHD] or in diskmgmt.msc and come in two formats: - VHD - VHDX Only 2 filesystem options are available for modern servers: - NTFS supports volumes up to 16 TB with the default 4 KB allocation unit size (but 256 TB with the 64 KB allocation unit size) and is required by some Windows Server services like AD DS , File Replication Service , Volume Shadow Copy Service , and Distributed File System - ReFS uses the same system of permissions as NTFS and offers error checking and repair capabilities that NTFS does not, but it does not support NTFS features like file compression, Encrypted File System , and disk quotas. ReFS supports a maximum file size of 16 exabytes and volumes up to 1 yobibyte (2 80 bytes) Software RAID can be implemented by creating Spanned , Striped , or RAID-5 volumes in diskmgmt.msc . A more modern and preferred technique is to create storage pools in [Storage Spaces][Storage Spaces].","title":"Storage"},{"location":"Windows/WS2016/#dedup","text":"Data deduplication (\"dedup\") is a role service that conserves storage space by storing only one copy of redundant chunks of files. Data duplication is appropriate to specific workloads, like backup volumes and file servers. It is not appropriate for database storage or operating system data or boot volumes. Data deduplication had required NTFS , although ReFS is supported since 1709. Data deduplication runs as a low-priority background process when the system is idle, by default; however its behavior can be configured based on its intended usage. Deduplication works by scanning files, and breaking them into unique chunks of various sizes that are collected in a chunk store . The original locations of chunks are replaced by reparse points . When a file is recently written, it is written in the standard, unoptimized form; the accumulation of such files is known as churn . Other jobs associated with deduplication include garbage collection , integrity scrubbing , and (when disabling deduplication) unoptimization . There are several deployment scenarios considered for data deduplication: - General purpose file servers Users often store multiple copies of the same, or similar, documents and files. Up to 30-50% of this space can be reclaimed using deduplication. - Virtualized Desktop Infrastructre (VDI) deployments Virtual hard disks that are used for remote desktops are essentially identical. Data Deduplication can also amelioriate the drop in storage performance when many users simultaneously log in at the start of the day, called a VDI boot storm . - Backup snapshots are an ideal deployment scenario because of the data is so duplicative. Deduplication is especially useful for disk drive backups, since snapshots typically differ little from each other.","title":"Dedup"},{"location":"Windows/WS2016/#file-shares","text":"Windows Server 2016 supports file shares via two protocols, both of which require the fs-fileserver role service: - SMB , long the standard for Windows networks - NFS , typically used in Linux, requires the installation of fs-nfs-service role service BranchCache enables client computers at remote locations to cache files accessed from shares, so that other computers at the same location can access them. Install the FS-BranchCache feature and enable the File and Printer Sharing and Branchcache - Hosted Cache Server (uses HTTPS) firewall display groups.","title":"File shares"},{"location":"Windows/WS2016/#media","text":"Every track of a hard drive platter is split into disk sectors , traditionally 512 bytes. A block is commonly called an \"allocation unit\" in Windows, but also commonly called a cluster. Storage left over unused in partially unused blocks is known as slack space . A new disk must first be initialized , that is, a partition table style must be chosen: - GPT : 128 partitions per disk, maximum volume size of 18 exabytes (2 60 bytes). Booting from a GPT drive is not possible unless the computer architecture supports EFI-based boot partitions. - MBR : older format that is commonly used for removable media, supporting volumes up to 2 TB with up to 4 primary partitions , although a common workaround is to make one of these partitions an extended partition , which can be be further subdivided into logical drives Mounting a partition as a single filesystem produces a volume , although the distinction can often be lost. The exception would be a case where a volume spans multiple partitions or physical disks, as is possible with software RAID. Virtual hard disks can be created with [Powershell][New-VHD] or in diskmgmt.msc and come in two formats: - VHD - VHDX Only 2 filesystem options are available for modern servers: - NTFS supports volumes up to 16 TB with the default 4 KB allocation unit size (but 256 TB with the 64 KB allocation unit size) and is required by some Windows Server services like AD DS , File Replication Service , Volume Shadow Copy Service , and Distributed File System - ReFS uses the same system of permissions as NTFS and offers error checking and repair capabilities that NTFS does not, but it does not support NTFS features like file compression, Encrypted File System , and disk quotas. ReFS supports a maximum file size of 16 exabytes and volumes up to 1 yobibyte (2 80 bytes) Software RAID can be implemented by creating Spanned , Striped , or RAID-5 volumes in diskmgmt.msc . A more modern and preferred technique is to create storage pools in [Storage Spaces][Storage Spaces].","title":"Media"},{"location":"Windows/WS2016/#s2d","text":"Although a cluster can normally be created in the GUI Failover Cluster Manager , in order to use Storage Spaces Direct the system must be prevented from automatically creating storage, which necessitates creation in PowerShell with the NoStorage switch parameter, and then S2D must be enabled using Enable-ClusterStorageSpacesDirect . This command scans all cluster nodes for local, unpartitioned disks , which are added to a single storage pool and classified by media type in order to use the fastest disks for caching. The recommended drive configuration for a node in an S2D cluster is a minimum of six drives, with at least 2 SSDs and at least 4 HDDs, with no RAID or other intelligence that cannot be disabled. Caching is configured automatically, depending on the combination of drives present - NVMe + SSD : NVMe drives are configured as a write-only cache for the SSD drives - NVMe + HDD : NVMe drives are read/write cache - NVME + SSD + HDD : NVME are write-only for the SSD drives and read/write for HDD drives - SSD + HDD : SSD drives are read/write cache Microsoft defined two deployment scenarios for Storage Spaces Direct: - Disaggregated which creates two separate clusters, one of which is a Scale-out File Server dedicated to storage, essentially functioning as a SAN. This solution requires the [DCB][DCB] role for traffic management. At least two 10Gbps Ethernet adapters are recommended per node, preferably adapters that use RDMA. - Hyper-converged , where a single cluster node hosts VMs and storage. This solution is much less expensive because it requires less hardware and generates much less network traffic, but storage and compute can't scale independently: adding a node to storage necessarily entails adding one to the Hyper-V hosts, and vice versa.","title":"S2D"},{"location":"Windows/WS2016/#storage-replica","text":"Storage Replica supports one-way replication between standalone servers, between clusters, and between storage devices within an [ asymmetric (stretch) cluster ][asymmetric cluster]. - Synchronous replication is possible when the replicated volumes can mirror data immediately, ensuring no data loss in case of failover - Asynchronous replication is preferable when the replication partner is located over a WAN link Storage Replica improves on DFS Replication, which is exclusively asynchronous and file-based, by using SMBv3 (port 445). Storage Replica requires two virtual disks, one for logs and one for data, which are the same size for each replication partner, and all the physical disks must use the same sector size.","title":"Storage Replica"},{"location":"Windows/WS2016/#wsus","text":"Windows Server Update Services (WSUS) can be configured from the command-line with wsusutil.exe. There are 5 basic WSUS architecture configurations Single WSUS Server downloads updates from Microsoft Update, and all the computers on the network download updates from it. A single server can usupport up to 25,000 clients. Replica WSUS Servers: a central WSUS server downloads from Microsoft Update, and after approval the updates are distributed to downstream servers at remote locations. Autonomous WSUS Servers: a central WSUS server downloads from Microsoft Update, all of which are distributed to remote servers; each remote site's administrators are individually responsible for evaluating and approving updates. Low-bandwidth WSUS Servers at remote sites download only the list of approved updates, which are then retrieved from Microsoft Update over the Internet, minimizing WAN traffic. Disconnected WSUS Servers have updates imported from offline media (DVD-ROMs, portable drives, etc), utilizing no WAN or Internet bandwidth whatsoever. When a computer first communicates with a WSUS server, it is added to the All Computers and and Unassigned Computers group automatically, which is created by default.","title":"WSUS"},{"location":"Windows/WS2016/#windows-server-backup","text":"To back up a VM without any downtime, integration services must be installed and enabled, and all disks must be basic disks formatted with NTFS . Windows Server Backup - System state includes boot files, Active Directory files, SYSVOL (when run on a DC), the registry, and other data. - System reserved is a special partition containing Boot Manager and Boot Configuration data.","title":"Windows Server Backup"},{"location":"Windows/WS2016/#glossary","text":"","title":"\ud83d\udcd8 Glossary"},{"location":"Windows/WS2016/#adfind","text":"Query the schema version associated with Active Directory [Desmond][Desmond2009]: 53 adfind -schema -s base objectVersion","title":"adfind"},{"location":"Windows/WS2016/#adprep","text":"Prepare Active Directory for Windows Server upgrades. Must be run on the Infrastructure Master role owner with the flag /domainprep . [Desmond][Desmond2009]: 29","title":"adprep"},{"location":"Windows/WS2016/#arp","text":"a d s","title":"arp"},{"location":"Windows/WS2016/#bcdedit","text":"Change Windows bootloader to Linux, while dual booting ::Manjaro bcdedit /set {bootmgr} path \\EFI\\manjaro\\grubx64.efi ::Fedora bcdedit /set {bootmgr} path \\EFI\\fedora\\shim.efi Enable or disable Test Signing Mode ref bcdedit /set testsign on bcdedit /set testsign off","title":"bcdedit"},{"location":"Windows/WS2016/#bootrec","text":"Windows Recovery Environment command that repairs a system partition Use when boot sector not found bootrec /fixboot Use when BCD file has been corrupted bootrec /rebuildbcd","title":"bootrec"},{"location":"Windows/WS2016/#cmdkey","text":"add delete generic list pass smartcard user Add a user name and password for user Mikedan to access computer Server01 with the password Kleo docs.microsoft.com cmdkey /add:server01 /user:mikedan /pass:Kleo","title":"cmdkey"},{"location":"Windows/WS2016/#dism","text":"Add-Driver Add-Package Add-ProvisionedAppxPackage Append-Image Apply-Image Apply-Unattend Capture-Image Cleanup-Image Commit-Image Disable-Feature Enable-Feature Export-Driver Export-Image Get-Driverinfo Get-Drivers Get-Featureinfo Get-Features Get-ImageInfo Get-MountedImageInfo Get-Packageinfo Get-Packages Get-ProvisionedAppxPackages List-Image Remount-Image Remove-Driver Remove-Image Remove-Package Remove-ProvisionedAppxPackage Set-ProvisionedAppxDataFile Unmount-Image Mount an image Zacker : 71 dism /mount-image /imagefile:$FILENAME /index:$N /name:$IMAGENAME /mountdir:$PATH Practice Labs dism /mount-wim /wimfile:c:\\images\\install.wim /index:1 /mountdir:c:\\mount Add a driver to an image file that you have already mounted Zacker : 72 dism /image:$FOLDERNAME /add-driver /driver:$DRIVERNAME /recurse Commit changes and unmount the image Zacker : 75 dism /unmount-image /mountdir:c:\\mount /commit Determine exact name of Windows features that can be enabled and disabled Zacker : 75 dism /image:c:\\mount /get-features Scan an image, checking for corruption dism /Online /Cleanup-Image /ScanHealth Check an image to see whether any corruption has been detected dism /Online /Cleanup-Image /CheckHealth Repair an offline dicsk using a mounted image as a repair source dism /Image:C:\\offline /Cleanup-Image /RestoreHealth /Source:C:\\test\\mount\\windows Zacker: 71-75 dism /mount-image /imagefile:C:\\images\\install.wim /index:1 /mountdir:C:\\mount dism /add-package /image:C:\\mount /packagepath:C:\\updates dism /add-driver /image:C:\\mount /driver:C:\\drivers\\display.driver\\nv_dispi.inf dism /commit-image /image:C:\\mount dism /unmount-image /image:C:\\mount","title":"dism"},{"location":"Windows/WS2016/#djoin","text":"Perform an offline domain join for a Nano Server Zacker: 46 djoin /provision /domain practicelabs /machine PLABNANOSRV01 /savefile .\\odjblob Load the odjblob file created offline on the Nano Server. djoin /requestodj /loadfile c:\\odjblob /windowspath c:\\windows /localos","title":"djoin"},{"location":"Windows/WS2016/#dnscmd","text":"Replicate an AD-integrated DNS zone to specific DCs ref dnscmd . /CreateDirectoryPartition FQDN Enable GlobalNames zone support dnscmd <servername> /config /enableglobalnamessupport 1 Observe status of socket pool dnscmd /info /socketpoolsize Configure DNS socket pool size (0 through 10,000) dnscmd /Config /SocketPoolSize <value>","title":"dnscmd"},{"location":"Windows/WS2016/#dsquery","text":"Find the Active Directory Schema version from the command-line ref pwsh dsquery * cn=schema,cn=configuration,dc=domain,dc=com -scope base -attr objectVersion\"","title":"dsquery"},{"location":"Windows/WS2016/#jea","text":"Just Enough Administration (JEA) allows special remote sessions that limit which cmdlets and parameters can be used in a remote PowerShell session. These are implemented as restricted endpoints , to which only members of a specific security group can gain access. This offers a way to administer remote servers and move away from the traditional method using RDP.","title":"JEA"},{"location":"Windows/WS2016/#net","text":"Map a network location to a drive letter Practice Lab net use x: \\\\192.168.0.35\\c$ Stop/start a service net stop dns net start dns","title":"net"},{"location":"Windows/WS2016/#netdom","text":"Rename a computer netdom renamecomputer %computername% /newname: newcomputername Join a computer to a domain cf. Add-Computer , Zacker: 21 netdom join %computername% /domain: domainname /userd: username /password:*","title":"netdom"},{"location":"Windows/WS2016/#netsh","text":"Enable port forwarding (\" portproxy \") to a WSL2 distribution ( src ) netsh interface portproxy add v4tov4 listenaddress=0.0.0.0 listenport=2222 connectaddress=172.23.129.80 connectport=2222 Configure DNS to be dynamically assigned netsh interface ip set dns \"Wi-Fi\" dhcp Delete Wi-Fi profiles netsh wlan delete profile name=* Turn off Windows firewall netsh advfirewall set allprofiles state off Enable firewall rule group netsh advfirewall firewall set rule group=\u201dFile and Printer Sharing\u201d new enable=yes Show Wi-Fi passwords ( src netsh wlan show profile wifi key=clear Check/reset WinHTTP proxy netsh winhttp show proxy netsh winhttp reset proxy","title":"netsh"},{"location":"Windows/WS2016/#ntdsutil","text":"Used to transfer FSMO roles between domain controllers. [ Desmond: 30 ][Desmond2009]","title":"ntdsutil"},{"location":"Windows/WS2016/#regsvr32","text":"Register a DLL dependency in order to enable the Active Directory Schema MMC snap-in on a DC [Desmond][Desmond2009]: 54 regsvr32 schmmgmt.dll","title":"regsvr32"},{"location":"Windows/WS2016/#route","text":"p print add change delete Basic usage route add 192 .168.2.1 mask ( 255 .255.255.0 ) 192 .168.2.4","title":"route"},{"location":"Windows/WS2016/#runas","text":"env netonly profile / no profile savecred showtrustlevels smartcard trustlevel user:","title":"runas"},{"location":"Windows/WS2016/#settings","text":"appsfeatures personalization printers windowsupdate about activation apps-volume appsforwebsites assignedaccess autoplay backup batterysaver bluetooth camera clipboard colors connecteddevices cortana crossdevice datausage dateandtime defaultapps delivery-optimization developers deviceencryption devices-touchpad display easeofaccess-display emailandaccounts findmydevice fonts keyboard lockscreen maps messaging mobile-devices mousetouchpad multitasking network network-wifi nfctransactions nightlight notifications optionalfeatures otherusers pen personalization-background personalization-colors personalization-start personalization-start-places phone powersleep privacy project proximity quiethours quietmomentsgame quietmomentspresentation quietmomentsscheduled recovery regionformatting regionlanguage remotedesktop savelocations screenrotation signinoptions signinoptions-launchfaceenrollment sound speech speech startupapps storagepolicies storagesense surfacehub-accounts surfacehub-calling surfacehub-devicemanagenent surfacehub-sessioncleanup surfacehub-welcome sync tabletmode taskbar themes troubleshoot typing usb videoplayback wheel windowsdefender windowsinsider workplace yourinfo","title":"Settings"},{"location":"Windows/WS2016/#sfc","text":"sfc /scannow","title":"sfc"},{"location":"Windows/WS2016/#shutdown","text":"Immediate restart shutdown /r /t 0 Log off shutdown /L","title":"shutdown"},{"location":"Windows/WS2016/#slmgr","text":"ato dli dlv ipk rearm upk xpr","title":"slmgr"},{"location":"Windows/WS2016/#sysdm","text":"2 3 4 5","title":"sysdm"},{"location":"Windows/WS2016/#tracert","text":"On Windows, this command is aliased to traceroute which is the Linux command. [Lammle][Lammle]: 112","title":"tracert"},{"location":"Windows/WS2016/#wbadmin","text":"enable backup get items get versions start backup start recovery start systemstaterecovery -backupTarget -hyperv -vsscopy | -vssFull Backup the entire drive, excluding some VMs wbadmin enable backup -backupTarget \\\\backups\\hostdr\\temp\\ -include:c: -exclude: C:\\VMs\\VM1.vhdx, C:\\VMs\\VMAR.vhd -vsscopy -quiet Zacker : 325-326 wbadmin get versions wbadmin get items -version: 11/14/2016:05:09 wbadmin start recovery -itemtype:app items:cluster -version:01/01/2008-00:00 Zacker : 422 wbadmin start systemstaterecovery -version:11/27/2016-11:07 wbadmin get versions","title":"wbadmin"},{"location":"Windows/WS2016/#wdsutil","text":"initialize-server remInst wdsutil /initialize-server /remInst:\"D:\\RemoteInstall\"","title":"wdsutil"},{"location":"Windows/WS2016/#winrm","text":"List all WinRM listeners winrm enumerate winrm/config/listener Display WinRM configuration winrm get winrm/config Add an address to Trusted Hosts list Zacker : 56 winrm set winrm/config/client @{TrustedHosts=\"192.168.10.41\"}","title":"winrm"},{"location":"Windows/WS2016/#winver","text":"","title":"winver"},{"location":"Windows/WS2016/#wmic","text":"bios logicaldisk memorychip os path Recover Windows product key [fossbytes.com][https://fossbytes.com/how-to-find-windows-product-key-lost-cmd-powershell-registry/] wmic path softwarelicensingservice get OA3xOriginalProductKey Display information about installed RAM wmic memorychip list full List all objects of type Win32_LogicalDisk using that class's alias logicaldisk . [Desmond][Desmond2009]: 642 pwsh wmic logicaldisk list brief Recover serial number of a Lenovo laptop [pcsupport.lenovo.com][https://pcsupport.lenovo.com/us/en/solutions/find-product-name] wmic bios get serialnumber Display BIOS version wmic bios get biosversion Display operating system architecture wmic os get osarchitecture Display operating system type (48 is Windows 10) wmic os get operatingsystemsku","title":"wmic"},{"location":"Windows/WS2016/#wslmsdocswslexe","text":"l s t \\ export import set-default-version","title":"[wsl][msdocs:wsl.exe]"},{"location":"Windows/WS2016/#wsusutilmsdocswsusutilexe","text":"Specify a location for downloaded updates Zacker : 393 C:\\Program Files\\Update Services\\Tools\\wsusutil.exe postinstall content_dir=d:\\wsus Specify SQL server, when not using the default WID database C:\\Program Files\\Update Services\\Tools\\wsusutil.exe postinstall sql_instance_name=\"db1\\sqlinstance1\"- content_dir=d:\\wsus","title":"[wsusutil]][msdocs:wsusutil.exe]"},{"location":"Windows/WS2016/#wt","text":"d p split-pane focus-tab Open the default Windows Terminal profile and also an Ubuntu WSL tab [bleepingcomputer.com][https://www.bleepingcomputer.com/news/microsoft/windows-terminal-09-released-with-command-line-arguments-and-more/] wt; new-tab -p \"Ubuntu-18.04\" Open a split pane of the default profile in the D:\\ folder and the cmd profile in the E:\\ folder [bleepingcomputer.com][https://www.bleepingcomputer.com/news/microsoft/windows-terminal-09-released-with-command-line-arguments-and-more/] wt -d d:\\ ; split-pane -p \"cmd\" -d e: Open the default profile and an Ubuntu WSL profile, but with the first tab focused [bleepingcomputer.com][https://www.bleepingcomputer.com/news/microsoft/windows-terminal-09-released-with-command-line-arguments-and-more/] wt ; new-tab -p \"Ubuntu-18.04\"; focus-tab -t0","title":"wt"},{"location":"Windows/WS2016/#xcopy","text":"Copy from one directory to another Practice Lab xcopy /s c:\\inetpub\\wwwroot c:\\nlbport","title":"xcopy"},{"location":"Windows/diskpart/","text":"ACTIVE ADD ASSIGN AUTOMOUNT BREAK CLEAN CONVERT CREATE DELETE DETAIL EXIT EXTEND GPT HELP IMPORT INACTIVE LIST ONLINE REM REMOVE REPAIR RESCAN RETAIN SELECT Moffitt disk formatting script SELECT DISK 0 CLEAN CONVERT gpt CREATE PARTITION primary SIZE=1024 FORMAT QUICK FS=NTFS LABEL=\"Recovery\" SET ID=\"de94bba4-06d1-4d40-a16a-bfd50179d6ac\" CREATE PARTITION efi SIZE=750 ASSIGN LETTER=K FORMAT QUICK FS=FAT32 LABEL=\"System\" CREATE PARTITION MSR SIZE=128 CREATE PARTITION PRIMARY ASSIGN LETTER=C FORMAT QUICK FS=NTFS EXIT","title":"Diskpart"}]}