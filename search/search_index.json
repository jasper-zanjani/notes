{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"Coding/","text":"Overview Text Hello, world! Hello, world! Parameterized Interactive CLI framework \"Hello, world!\" Console . WriteLine ( \"Hello World!\" ); import sys def main (): name : str = sys . argv [ 1 ] print ( f \"Hello, { name } !\" ) if __name__ == \"__main__\" : main () using System.CommandLine ; using System.CommandLine.Invocation ; static int Main ( string [] args ) { var cmd = new RootCommand { new Argument < string >( \"name\" ), //, \"Your name\"), new Option < string? >( new [] { \"--greeting\" , \"-g\" }, \"The greeting to use\" ), }; cmd . Handler = CommandHandler . Create < string , string? >( HandleGreeting ); return cmd . Invoke ( args ); } static void HandleGreeting ( string? greeting , string name ) { Console . WriteLine ( $\"{greeting}, {name}\" ); } def main (): name : str = input ( \"What is your name? \" ) print ( f \"Hello, { name } !\" ) if __name__ == \"__main__\" : main () argparse import argparse def main (): parser = argparse . ArgumentParser ( description = \"Say hello\" ) parser . add_argument ( dest = \"name\" , metavar = \"name\" , default = \"World\" , help = \"Name to greet\" ) args = parser . parse_args () print ( f \"Hello, { args . name } !\" ) if __name__ == \"__main__\" : main () clap Numbers Weight on Mars Double array def main (): weight = input ( \"Enter weight in kilograms: \\n \" ) try : mars_weight = ( int ( weight ) / 9.81 ) * 3.711 except ValueError : mars_weight = 0.0 print ( \"Weight on Mars: {} kg\" . format ( mars_weight )) if __name__ == '__main__' : main () List comprehension map() [ 2 * el for el in primes ] primes = [ 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 ] double = lambda x : 2 * x list ( map ( double , primes )) Parse a date string TryParse try/catch 1 2 3 4 5 6 7 8 9 10 11 12 namespace Program { class Program { static void Main () { string rawDate = \"07/04/1776\" ; DateTime . TryParse ( rawDate , out parsedDate ); Console . WriteLine ( parsedDate . ToLongDateString ()); // => \"July 4, 1776\" } } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 namespace Program { class Program { static void Main () { string rawDate = \"07/04/1776\" ; try { DateTime parsedDate = DateTime . Parse ( rawDate ); } catch ( FormatException ) { Console . WriteLine ( \"Unparsable!\" ) } } } } File operations Create Read Copy Move using ( StreamWriter writer = File . CreateText ( \"test.txt\" )) { writer . WriteLine ( \"Hello, world!\" ); } with open ( 'text' , 'w' ) as f : f . write ( 'Hello, world!' ) ReadAllText ReadAllLines using System.IO ; string raven = File . ReadAllText ( \"raven\" ); using System.IO ; string [] raven = File . ReadAllLines ( \"raven\" ); Using StreamReader objects ReadToEnd Loop using System.IO ; using ( StreamReader reader = File . OpenText ( \"raven\" )) { reader . ReadToEnd (); } using System.IO ; using ( StreamReader reader = File . OpenText ( \"raven\" )) { string s ; while (( s = sr . ReadLine ()) != null ) { Console . WriteLine ( s ); } } with open ( 'raven' ) as f : f . readlines () using System.IO ; File . Copy ( ' raven ' , ' raven . bak ' , true ) import shutil shutil . copyfile ( 'raven' , 'raven.bak' ) using System.IO ; File . Move ( ' raven ' , ' raven . bak ' ); Text output fn main () { let args : Vec < String > = std :: env :: args (). collect (); let filename = & args [ 1 ]; let contents = std :: fs :: read_to_string ( filename ) . expect ( \"Couldn't read file\" ); println! ( \"{}\" , contents ); } Data file formats CSV JSON import csv with open ( \"greeks.csv\" ) as f : r = csv . reader ( f ) headers = next ( r ) data = [ row for row in r ] using System ; using System.IO ; using CsvHelper ; struct Greek { public string name { get ; set ; } public string city { get ; set ; } public string dob { get ; set ; } } class Program { static void Main ( string [] args ) { using ( StreamReader reader = new StreamReader ( \"greeks.csv\" )) { CsvReader csvreader = new CsvReader ( reader , System . Globalization . CultureInfo . InvariantCulture ); var data = csvreader . GetRecords < Greek >(); foreach ( Greek item in data ) { Console . WriteLine ( $\"{item.name,-15} {item.city,-15} {item.dob,-15}\" ); } } } } using ( var stream = await storageFile . OpenAsync ( FileAccessMode . Read )) { using ( var dataReader = new DataReader ( stream )) { await dataReader . LoadAsync (( uint ) stream . Size ); var json = dataReader . ReadString (( uint ) stream . Size ); customerList = JsonConvert . DeserializeObject < List < Customer >>( json ); } } Random numbers Integer Real number Random r = new System . Random (); int result = r . Next ( 1 , 6 ); import random random . randrange ( 1 , 6 ) Random r = new System . Random (); int result = r . NextDouble (); import random random . random () String formatting Socrates Athens 470 BC Plato Athens 428 BC Aristotle Stagira 384 BC Euclid Alexandria 325 BC Pythagoras Samos 570 BC In C#, multidimensional arrays cannot be traversed with the foreach loops which appear to flatten its structure. for ( int i = 0 ; i <= greeks . GetUpperBound ( 0 ); i ++) { Console . WriteLine ( \"{0,-10} {1,-10} {2,10}\" , greeks [ i , 0 ], greeks [ i , 1 ], greeks [ i , 2 ]); } for r in greeks : print ( \" {0:10} {1:10} {2:>10} \" . format ( r [ 0 ], r [ 1 ], r [ 2 ])) Currency formatting Console . WriteLine ( $\"{123456.789:C }\" ); // $123,456.79 Console . WriteLine ( 123456.789d . ToString ( \"C\" )); // $123,456.79 f-string locale module f \"$ { 123456.789 : ,.2f } \" # $123,456.79 Formatting a number in currency requires use of the locale module, and for the locale environment variables to be set. import locale locale . setlocale ( locale . LC_ALL , 'en_US.UTF-8' ) locale . currency ( 123456.789 ) # $123456.79 Network Berkeley sockets have formed the basis of modern network communication since their introduction in the early 1980s. The use of the term \"socket\" to refer to an endpoint for communication began as early as 1971 with ARPANET. As a concept it belongs in the Session layer of the OSI model. The original Berkeley sockets API, written in C, has been maintained in implementations of other languages Python's socket module Rust's net module The API: socket() creates a new socket in the operating system, identified by an integer. It returns a file descriptor bind() associates a socket with an address structure : IP address and port number listen() blocks for incoming connections accept() creates a new TCP connection from the remote client Echo server An echo server simply reflects text sent to it over a TCP connection Both Python's socket object and Rust's TcpListener object expose a bind() method, although in Rust host and port are combined in a string, whereas in Python they are passed as a (str,int) tuple. Both implementations expose an accept() method that returns a tuple, but in Python the tuple returned contains the socket (named \"conn\") object and a nested tuple that contains the address of the client. In Rust, the equivalent to the connection object seems to be a TcpStream object ... This object contains binary data which must be put into a buffer. In Rust the buffer is passed as a mutable reference to the read() method (i.e. read(&mut buffer) ). The buffer size is determined by the size of the initialized buffer. In Python the binary information is assigned to a variable using the recv() method, which does take an integer argument specifying the buffer size. echo \"Hello, world!\" | netcat localhost 8080 use std :: net :: TcpListener ; use std :: io :: Read ; fn main () { let server = Server :: new ( \"127.0.0.1:8000\" . to_string ()); server . run (); } pub struct Server { addr : String , } impl Server { pub fn new ( addr : String ) -> Self { Self { addr } } pub fn run ( self ) { println! ( \"Listening on {}\" , self . addr ); let listener = TcpListener :: bind ( & self . addr ). unwrap (); loop { match listener . accept () { Ok (( mut stream , _ )) => { let mut buffer = [ 0 ; 1024 ]; match stream . read ( & mut buffer ) { Ok ( _ ) => { println! ( \"Received request: {}\" , String :: from_utf8_lossy ( & buffer )); }, Err ( e ) => println! ( \"Failed to read from connection: {}\" , e ), } } Err ( e ) => println! ( \"Failed to establish a connection: {}\" , e ), } } } } import socket class Echo_Server : def __init__ ( self , address : str ): self . addr = address def run ( self ): HOST = self . addr [: self . addr . find ( \":\" )] PORT = int ( self . addr [ self . addr . find ( \":\" ) + 1 :]) with socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) as s : s . bind (( HOST , PORT )) s . listen () conn , addr = s . accept () with conn : print ( f \"Received connection from { addr } \" ) while True : data = conn . recv ( 1024 ) if not data : break print ( data . decode ( \"utf-8\" )) # conn.sendall(data) # Echo server def main (): server = Echo_Server ( \"127.0.0.1:8000\" ) server . run () if __name__ == '__main__' : main () Terminal Input validation loop Such a loop will continuously prompt for valid input, in this case an integer. TryParse try/catch while ( true ): { int option ; int . TryParse ( Console . ReadLine (), out option ); if ( option != null ) { // ... } else { break ;} } while ( true ): { try { int option = Int32 . Parse ( Console . ReadLine ()); } catch ( FormatException ) { // Input was not an integer } catch ( OverflowException ) { // Number was too big } } while True : try : option = int ( input ()) except ValueError : # Integer was not able to be parsed Guessing game Oxford comma using System.CommandLine ; using System.CommandLine.Invocation ; using System.Linq ; static int Main ( string [] args ) { var cmd = new RootCommand { new Argument < string []>( \"names\" ) }; cmd . Handler = CommandHandler . Create < string []>( Handler ); return cmd . Invoke ( args ); } static void Handler ( string [] names ) { Console . WriteLine ( $\"{String.Join(\" , \", names.Take(names.Length -1))}, and {names.Last<string>()}\" ); } import argparse def get_args (): p = argparse . ArgumentParser ( description = \"Listing args with Oxford comma\" ) p . add_argument ( \"words\" , nargs = \"+\" , help = \"Words to concatenate using Oxford comma\" ) return p . parse_args () def oxford_commafy ( words ): l = len ( words ) if l > 2 : words [ - 1 ] = f \"and { words [ - 1 ] } \" print ( \", \" . join ( words )) elif l == 2 : print ( f \" { words [ 0 ] } and { words [ 1 ] } \" ) else : print ( words [ 0 ]) def main (): args = get_args () . words oxford_commafy ( args ) if __name__ == \"__main__\" : main () Color output Console . Color = ConsoleColor . Red ; Console . WriteLine ( \"Red!\" ) Console . ResetColor (); print ( f \" { colorama . Fore . RED } Red! { colorama . Style . RESET_ALL } \" ) Console . Color = ConsoleColor . Green ; Console . WriteLine ( \"Green!\" ) Console . ResetColor (); print ( f \" { colorama . Fore . GREEN } Green! { colorama . Style . RESET_ALL } \" ) Console . Color = ConsoleColor . Yellow ; Console . WriteLine ( \"Yellow!\" ) Console . ResetColor (); print ( f \" { colorama . Fore . YELLOW } Yellow! { colorama . Style . RESET_ALL } \" ) Console . Color = ConsoleColor . Blue ; Console . WriteLine ( \"Blue!\" ) Console . ResetColor (); print ( f \" { colorama . Fore . BLUE } Blue! { colorama . Style . RESET_ALL } \" ) Console . Color = ConsoleColor . Magenta ; Console . WriteLine ( \"Magenta!\" ) Console . ResetColor (); print ( f \" { colorama . Fore . MAGENTA } Magenta! { colorama . Style . RESET_ALL } \" ) Calculator (argparse) import argparse def get_args (): parser = argparse . ArgumentParser () parser . add_argument ( \"operand1\" , type = float ) parser . add_argument ( \"operand2\" , type = float ) op = parser . add_mutually_exclusive_group () op . add_argument ( \"-a\" , \"--add\" , dest = \"add\" , action = \"store_true\" ) op . add_argument ( \"-s\" , \"--subtract\" , action = \"store_true\" ) op . add_argument ( \"-d\" , \"--divide\" , action = \"store_true\" ) op . add_argument ( \"-m\" , \"--multiply\" , action = \"store_true\" ) return parser . parse_args () def main (): args = get_args () if args . add : print ( \"Adding\" ) print ( args . operand1 , \" + \" , args . operand2 , \" = \" , args . operand1 + args . operand2 ) elif args . subtract : print ( \"Subtracting\" ) print ( args . operand1 , \" - \" , args . operand2 , \" = \" , args . operand1 - args . operand2 ) elif args . divide : print ( \"Dividing\" ) print ( args . operand1 , \" / \" , args . operand2 , \" = \" , args . operand1 / args . operand2 ) elif args . multiply : print ( \"Multiplying\" ) print ( args . operand1 , \" * \" , args . operand2 , \" = \" , args . operand1 * args . operand2 ) else : print ( \"Unknown operation!\" ) if __name__ == \"__main__\" : main () Input validation string input ; int inputParsed ; while ( true ) { input = System . Console . ReadLine (); try { inputParsed = int . Parse ( input ); break ; } catch { System . Console . WriteLine ( \"Please input a number...\" ); } } System . Console . WriteLine ( $\"Number provided: {inputParsed}\" ); Subcommands ./app command subcommand argument static int Main ( string [] args ) { var rootCommand = new RootCommand ( \"command\" ); var command = new Command ( \"subcommand\" ) { new Argument < string >( \"argument\" ); }; command . Handler = new CommandHandler . Create < string >( argumentHandler ); rootCommand . Add ( command ); rootCommand . Invoke ( args ); } private static void argumentHandler ( string argument ) { /* ... */ } To-do app Notably, the dstask Go application is a very sophisticated evolution of a terminal-based to-do app featuring subcommands, YAML-formatted tasks, and Git integration. TDD Test fixture OOP DnD character Generating a Dungeons 'n Dragons character provides the opportunity to exercise a variety of OOP techniques: public and private fields and properties and methods using simple arithmetic. A Dungeons 'n Dragons character has six character attributes that can be randomly assigned. This process, called an ability roll , is calculated by rolling four six-sided dice (d6) and summing the highest three values, discarding the lowest. The raw ability score is then modified according to a table to produce a final ability score. In the implementations below, all ability scores are dynamically calculated using getter functions that sum the raw ability score (stored as a private field) and modifier. Both the ability roll and modifier lookup are implemented as public static functions. Constructor Properties Methods class Character : def __init__ ( self , race : Race = Race . HUMAN ): self . _strength_ability = self . ability_roll () self . _dexterity_ability = self . ability_roll () self . _constitution_ability = self . ability_roll () self . _intelligence_ability = self . ability_roll () self . _wisdom_ability = self . ability_roll () self . _charisma_ability = self . ability_roll () self . _race = race class Character : @property def Strength ( self ): return self . _strength_ability + self . get_modifier ( self . _strength_ability ) @property def Dexterity ( self ): return self . _dexterity_ability + self . get_modifier ( self . _dexterity_ability ) @property def Constitution ( self ): return self . _constitution_ability + self . get_modifier ( self . _constitution_ability ) @property def Intelligence ( self ): return self . _intelligence_ability + self . get_modifier ( self . _intelligence_ability ) @property def Wisdom ( self ): return self . _wisdom_ability + self . get_modifier ( self . _wisdom_ability ) @property def Charisma ( self ): return self . _charisma_ability + self . get_modifier ( self . _charisma_ability ) @staticmethod def Roll ( range : int = 6 ): return random . randrange ( range ) + 1 @staticmethod def get_modifier ( score : int ): return math . floor (( score - 10 ) / 2 ) @classmethod def ability_roll ( cls ): rolls = [ cls . Roll (), cls . Roll (), cls . Roll (), cls . Roll ()] rolls . remove ( min ( rolls )) return sum ( rolls ) def report ( self ): print ( f \"Strength: { self . Strength } \" ) print ( f \"Dexterity: { self . Dexterity } \" ) print ( f \"Constitution: { self . Constitution } \" ) print ( f \"Intelligence: { self . Intelligence } \" ) print ( f \"Wisdom: { self . Wisdom } \" ) print ( f \"Charisma: { self . Charisma } \" ) Constructor Properties Methods partial class Character { private int StrengthAbility ; private int DexterityAbility ; private int ConstitutionAbility ; private int IntelligenceAbility ; private int WisdomAbility ; private int CharismaAbility ; private Race Race { get ; } public Character ( Race race ) { this . StrengthAbility = AbilityRoll (); this . DexterityAbility = AbilityRoll (); this . ConstitutionAbility = AbilityRoll (); this . IntelligenceAbility = AbilityRoll (); this . WisdomAbility = AbilityRoll (); this . CharismaAbility = AbilityRoll (); this . Race = race ; } public Character () : this ( Race . HUMAN ) { } } partial class Character { public int Strength { get => StrengthAbility + GetModifier ( StrengthAbility ) + GetRaceModifier ( Abilities . STRENGTH ) } public int Dexterity { get => DexterityAbility + GetModifier ( DexterityAbility ) + GetRaceModifier ( Abilities . DEXTERITY ) } public int Constitution { get => ConstitutionAbility + GetModifier ( ConstitutionAbility ) + GetRaceModifier ( Abilities . CONSTITUTION ) } public int Intelligence { get => IntelligenceAbility + GetModifier ( IntelligenceAbility ) + GetRaceModifier ( Abilities . INTELLIGENCE ) } public int Wisdom { get => WisdomAbility + GetModifier ( WisdomAbility ) + GetRaceModifier ( Abilities . WISDOM ) } public int Charisma { get => CharismaAbility + GetModifier ( CharismaAbility ) + GetRaceModifier ( Abilities . CHARISMA ) } } partial class Character { public void Report () { Console . Write ( $\"Strength: {Strength,2}\" ); Console . Write ( $\"Dexterity: {Dexterity,2}\" ); Console . Write ( $\"Constitution: {Constitution,2}\" ); Console . Write ( $\"Intelligence: {Intelligence,2}\" ); Console . Write ( $\"Wisdom: {Wisdom,2}\" ); Console . Write ( $\"Charisma: {Charisma,2}\" ); } static int Roll ( int ceiling ) { Random rng = new Random (); return rng . Next ( 1 , ceiling ); } static int AbilityRoll () { List < int > rolls = new List < int > { Roll ( 6 ), Roll ( 6 ), Roll ( 6 ), Roll ( 6 ) }; rolls . Remove ( rolls . Min ()); return rolls . Sum (); } public static int GetModifier ( int ability ) { return ( int ) System . Math . Floor ((( double ) ability - 10 ) / 2 ); } } RPG character generator Player class Subclasses Race class Player (): def __init__ ( self , name : str , race : Race , hp : int , mp : int ): self . _name = name self . _race = race self . _hp = hp self . _mp = mp @property def getName ( self ): return self . _name @property def getRace ( self ): return self . _race @property def getHp ( self ): return self . _hp @property def getMp ( self ): return self . _mp def attack ( self ): return \"Have at thee!\" class Warrior ( Player ): def __init__ ( self , name : str , race : Race ): super () . __init__ ( name , race , 200 , 0 ) def attack ( self ): return \"I will destroy with my sword, foul demon!\" class Priest ( Player ): def __init__ ( self , name : str , race : Race ): super () . __init__ ( name , race , 100 , 200 ) def attack ( self ): return \"Taste the wrath of the Two True Gods!\" class Mage ( Player ): def __init__ ( self , name : str , race : Race ): super () . __init__ ( name , race , 150 , 150 ) def attack ( self ): return \"You are overmatched by my esoteric artifices!\" import enum class Race ( enum . Enum ): HUMAN = enum . auto (), ELF = enum . auto (), DWARF = enum . auto () Player class Subclasses Race #include <string> class Player { protected : std :: string _name { \"Johnny Bravo\" }; Race _race { Race :: HUMAN }; int _hp { 100 }; int _mp { 100 }; public : Player ( std :: string n , Race r , int hp , int mp ) : _name { n }, _race { r }, _hp ( hp ), _mp ( mp ) {} virtual std :: string attack () = 0 ; int getHp () { return _hp ; } int getMp () { return _mp ; } std :: string getRace () { switch ( _race ) { case 0 : return \"human\" ; break ; case 1 : return \"elf\" ; break ; case 2 : return \"dwarf\" ; break ; default : return \"none\" ; break ; } } std :: string getName () { return _name ; } void setHp ( int n ) { _hp = n ; } void setMp ( int n ) { _mp = n ; } void setName ( std :: string s ) { _name = s ; } void setRace ( Race r ) { _race = r ;} }; class Warrior : public Player { public : Warrior ( std :: string n , Race r ) : Player ( n , r , 200 , 0 ) {} std :: string attack () { return \"I will destroy you with my sword, foul demon!\" ;} }; class Priest : public Player { public : Priest ( std :: string n , Race r ) : Player ( n , r , 100 , 200 ) {} std :: string attack () { return \"Taste the wrath of the Two True Gods!\" ;} }; class Mage : public Player { public : Mage ( std :: string n , Race r ) : Player ( n , r , 150 , 150 ) {} std :: string attack () { return \"You are overmatched by my esoteric artifices!\" ;} }; enum Race { HUMAN , ELF , DWARF }; Starships This project provides a scenario for implementing OOP and TDD principles in a variety of languages and implementations. Simple classes with intuitive properties and fields include Officer and Starship , which also has a field containing a variant of the StarshipClass enum. Fleet serves as a container for Starships. An Officer is paired with a Starship to form a StarshipDeployment . CaptainSelector , which is passed to StarshipDeployment by dependency injection, evaluates whether the Officer provided has what it takes to ply the inky black. This boils down to a check on the Officer's Grade property, which is simple to test in testing frameworks where a mocked Officer object can be set up with unsatisfactory Grade values. - StarshipDeployment also takes a StarshipValidator object by dependency injection, which it uses to perform checks on a given Starship. These checks provide opportunities to mock Starship and Officer objects in unit testing. - IsCaptained() checks if the Starship has a Captain assigned - ValidateRegistry() makes sure the Starship's registry number begins with NCC or NX - Evaluate() runs all the other methods in the class and returns True only if all checks pass. This provides the opportunity to test a mocked validator for invocation of the Evaluate() method. Officer Starship StarshipClass Starship from enum import Enum class StarshipClass ( Enum ): NX = 'NX' GALAXY = 'Galaxy' CONSTITUTION = 'Constitution' SOVEREIGN = 'Sovereign' DEFIANT = 'Defiant' INTREPID = 'Intrepid' MIRANDA = 'Miranda' class Starship : def __init__ ( self , name = None , starshipclass : StarshipClass = StarshipClass . NX , registry = None , crew = 0 , ): self . name = name self . registry = registry self . _crew = crew self . crew_on_leave = 0 self . _starshipclass = starshipclass @property def crew ( self ): return self . _crew @crew . setter def crew ( self , crew : int ): if crew < 0 : raise Exception else : self . _crew = crew @property def starshipclass ( self ): return self . _starshipclass @starshipclass . setter def starshipclass ( self , starshipclass : StarshipClass ): if starshipclass not in StarshipClass : raise Exception else : self . _starshipclass = starshipclass StarshipClass Officer CaptainSelector StarshipValidator StarshipDeployment public enum StarshipClass { NX , GALAXY , CONSTITUTION , SOVEREIGN , DEFIANT , INTREPID , MIRANDA } public interface IOfficer { string FirstName { get ; set ; } string LastName { get ; set ; } DateTime BirthDate { get ; set ; } char Grade { get ; set ; } string Name { get ; } } public class Officer : IOfficer { public string FirstName { get ; set ; } public string LastName { get ; set ; } public DateTime BirthDate { get ; set ; } public string Name { get { return $\"{FirstName} {LastName}\" ; } } public char Grade { get ; set ; } } public class CaptainSelector { public IOfficer Officer { get ; set ; } public CaptainSelector ( IOfficer officer ) { Officer = officer ; } public bool Evaluate () { return Officer . Grade == 'A' ? true : false ; } } public class StarshipValidator : IStarshipValidator { public IStarship Starship { get ; set ; } public bool IsCaptained () { return Starship . Captain != null ? true : false ; } public bool ValidateRegistry () { return Starship . Registry . StartsWith ( \"NCC\" ) || Starship . Registry . StartsWith ( \"NX\" ) ? true : false ; } public bool Evaluate () { return ValidateRegistry () && IsCaptained (); } } public class StarshipDeployment { public IStarshipValidator StarshipValidator { get ; set ; } public StarshipDeployment ( IStarshipValidator validator ) { StarshipValidator = validator ?? throw new ArgumentNullException ( nameof ( validator )); } public bool ValidateDestination ( string destination ) { return destination . Length > 1 ? true : false ; } public StarshipMission Deploy ( Starship starship , string destination ) { bool destinationValidated = ValidateDestination ( destination ); bool starshipValidated = StarshipValidator . Evaluate (); return destinationValidated && starshipValidated ? new StarshipMission { Starship = starship as Starship , Destination = destination } : throw new ArgumentException (); } } \ud83d\udcd8 Glossary C \"A programming language is low level when its programs require attention to the irrelevant.\" -Alan Perlis Despite C's reputation as a low-level programming language, in fact it merely emulates the ancient PDP-11, which is the only machine for which its abstract machine can be described as \"close to the metal\". In the age of parallel processes, C's serial nature... Sources: C is not a low-level programming language Enumeration In C# the term enumeration refers to the process of successively returning individual values. In Python, the term iteration is used to refer to the same thing, and iterable refers to an object that can be iterated, or parsed out into sub-elements. In Python, any object that exposes the __iter__() and __next__() dunder methods are iterable. In C#, the IEnumerable interface implements enumeration. Both languages feature a keyword that allows a subclass to access its direct parent. Whereas in Python the terms superclass and subclass are used, in C# the terms base class and derived class are preferred. Garbage collector A garbage collector is a feature of some programming language runtimes that periodically pauses execution to remove data that is no longer used. Such languages are considered unsuitable for use in database applications because of the unpredictable latency this garbage collection creates, despite the added memory safety. Loop unswitching One of the core optimizations that a C compiler performs; transforms a loop containing a conditional into a conditional with a loop in both parts, which changes flow control Register rename engine Component of modern high-end cores which is one of the largest consumers of die area and power Scalar Replacement Of Aggregates (SROA) One of the core optimizations that a C compiler performs; attempts to replace struct s and arrays with fixed lengths with individual variables, which allows the compiler to treat accesses as independent and elide operations entirely if it can prove the results are never visible, which also deletes padding sometimes. Segmented architecture Pointers might be segment IDs and an offset","title":"Overview"},{"location":"Coding/#overview","text":"","title":"Overview"},{"location":"Coding/#text","text":"","title":"Text"},{"location":"Coding/#hello-world","text":"Hello, world! Parameterized Interactive CLI framework \"Hello, world!\" Console . WriteLine ( \"Hello World!\" ); import sys def main (): name : str = sys . argv [ 1 ] print ( f \"Hello, { name } !\" ) if __name__ == \"__main__\" : main () using System.CommandLine ; using System.CommandLine.Invocation ; static int Main ( string [] args ) { var cmd = new RootCommand { new Argument < string >( \"name\" ), //, \"Your name\"), new Option < string? >( new [] { \"--greeting\" , \"-g\" }, \"The greeting to use\" ), }; cmd . Handler = CommandHandler . Create < string , string? >( HandleGreeting ); return cmd . Invoke ( args ); } static void HandleGreeting ( string? greeting , string name ) { Console . WriteLine ( $\"{greeting}, {name}\" ); } def main (): name : str = input ( \"What is your name? \" ) print ( f \"Hello, { name } !\" ) if __name__ == \"__main__\" : main () argparse import argparse def main (): parser = argparse . ArgumentParser ( description = \"Say hello\" ) parser . add_argument ( dest = \"name\" , metavar = \"name\" , default = \"World\" , help = \"Name to greet\" ) args = parser . parse_args () print ( f \"Hello, { args . name } !\" ) if __name__ == \"__main__\" : main () clap","title":"Hello, world!"},{"location":"Coding/#numbers","text":"Weight on Mars Double array def main (): weight = input ( \"Enter weight in kilograms: \\n \" ) try : mars_weight = ( int ( weight ) / 9.81 ) * 3.711 except ValueError : mars_weight = 0.0 print ( \"Weight on Mars: {} kg\" . format ( mars_weight )) if __name__ == '__main__' : main () List comprehension map() [ 2 * el for el in primes ] primes = [ 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 ] double = lambda x : 2 * x list ( map ( double , primes )) Parse a date string TryParse try/catch 1 2 3 4 5 6 7 8 9 10 11 12 namespace Program { class Program { static void Main () { string rawDate = \"07/04/1776\" ; DateTime . TryParse ( rawDate , out parsedDate ); Console . WriteLine ( parsedDate . ToLongDateString ()); // => \"July 4, 1776\" } } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 namespace Program { class Program { static void Main () { string rawDate = \"07/04/1776\" ; try { DateTime parsedDate = DateTime . Parse ( rawDate ); } catch ( FormatException ) { Console . WriteLine ( \"Unparsable!\" ) } } } }","title":"Numbers"},{"location":"Coding/#file-operations","text":"Create Read Copy Move using ( StreamWriter writer = File . CreateText ( \"test.txt\" )) { writer . WriteLine ( \"Hello, world!\" ); } with open ( 'text' , 'w' ) as f : f . write ( 'Hello, world!' ) ReadAllText ReadAllLines using System.IO ; string raven = File . ReadAllText ( \"raven\" ); using System.IO ; string [] raven = File . ReadAllLines ( \"raven\" ); Using StreamReader objects ReadToEnd Loop using System.IO ; using ( StreamReader reader = File . OpenText ( \"raven\" )) { reader . ReadToEnd (); } using System.IO ; using ( StreamReader reader = File . OpenText ( \"raven\" )) { string s ; while (( s = sr . ReadLine ()) != null ) { Console . WriteLine ( s ); } } with open ( 'raven' ) as f : f . readlines () using System.IO ; File . Copy ( ' raven ' , ' raven . bak ' , true ) import shutil shutil . copyfile ( 'raven' , 'raven.bak' ) using System.IO ; File . Move ( ' raven ' , ' raven . bak ' );","title":"File operations"},{"location":"Coding/#text-output","text":"fn main () { let args : Vec < String > = std :: env :: args (). collect (); let filename = & args [ 1 ]; let contents = std :: fs :: read_to_string ( filename ) . expect ( \"Couldn't read file\" ); println! ( \"{}\" , contents ); }","title":"Text output"},{"location":"Coding/#data-file-formats","text":"CSV JSON import csv with open ( \"greeks.csv\" ) as f : r = csv . reader ( f ) headers = next ( r ) data = [ row for row in r ] using System ; using System.IO ; using CsvHelper ; struct Greek { public string name { get ; set ; } public string city { get ; set ; } public string dob { get ; set ; } } class Program { static void Main ( string [] args ) { using ( StreamReader reader = new StreamReader ( \"greeks.csv\" )) { CsvReader csvreader = new CsvReader ( reader , System . Globalization . CultureInfo . InvariantCulture ); var data = csvreader . GetRecords < Greek >(); foreach ( Greek item in data ) { Console . WriteLine ( $\"{item.name,-15} {item.city,-15} {item.dob,-15}\" ); } } } } using ( var stream = await storageFile . OpenAsync ( FileAccessMode . Read )) { using ( var dataReader = new DataReader ( stream )) { await dataReader . LoadAsync (( uint ) stream . Size ); var json = dataReader . ReadString (( uint ) stream . Size ); customerList = JsonConvert . DeserializeObject < List < Customer >>( json ); } }","title":"Data file formats"},{"location":"Coding/#random-numbers","text":"Integer Real number Random r = new System . Random (); int result = r . Next ( 1 , 6 ); import random random . randrange ( 1 , 6 ) Random r = new System . Random (); int result = r . NextDouble (); import random random . random ()","title":"Random numbers"},{"location":"Coding/#string-formatting","text":"Socrates Athens 470 BC Plato Athens 428 BC Aristotle Stagira 384 BC Euclid Alexandria 325 BC Pythagoras Samos 570 BC In C#, multidimensional arrays cannot be traversed with the foreach loops which appear to flatten its structure. for ( int i = 0 ; i <= greeks . GetUpperBound ( 0 ); i ++) { Console . WriteLine ( \"{0,-10} {1,-10} {2,10}\" , greeks [ i , 0 ], greeks [ i , 1 ], greeks [ i , 2 ]); } for r in greeks : print ( \" {0:10} {1:10} {2:>10} \" . format ( r [ 0 ], r [ 1 ], r [ 2 ])) Currency formatting Console . WriteLine ( $\"{123456.789:C }\" ); // $123,456.79 Console . WriteLine ( 123456.789d . ToString ( \"C\" )); // $123,456.79 f-string locale module f \"$ { 123456.789 : ,.2f } \" # $123,456.79 Formatting a number in currency requires use of the locale module, and for the locale environment variables to be set. import locale locale . setlocale ( locale . LC_ALL , 'en_US.UTF-8' ) locale . currency ( 123456.789 ) # $123456.79","title":"String formatting"},{"location":"Coding/#network","text":"Berkeley sockets have formed the basis of modern network communication since their introduction in the early 1980s. The use of the term \"socket\" to refer to an endpoint for communication began as early as 1971 with ARPANET. As a concept it belongs in the Session layer of the OSI model. The original Berkeley sockets API, written in C, has been maintained in implementations of other languages Python's socket module Rust's net module The API: socket() creates a new socket in the operating system, identified by an integer. It returns a file descriptor bind() associates a socket with an address structure : IP address and port number listen() blocks for incoming connections accept() creates a new TCP connection from the remote client","title":"Network"},{"location":"Coding/#echo-server","text":"An echo server simply reflects text sent to it over a TCP connection Both Python's socket object and Rust's TcpListener object expose a bind() method, although in Rust host and port are combined in a string, whereas in Python they are passed as a (str,int) tuple. Both implementations expose an accept() method that returns a tuple, but in Python the tuple returned contains the socket (named \"conn\") object and a nested tuple that contains the address of the client. In Rust, the equivalent to the connection object seems to be a TcpStream object ... This object contains binary data which must be put into a buffer. In Rust the buffer is passed as a mutable reference to the read() method (i.e. read(&mut buffer) ). The buffer size is determined by the size of the initialized buffer. In Python the binary information is assigned to a variable using the recv() method, which does take an integer argument specifying the buffer size. echo \"Hello, world!\" | netcat localhost 8080 use std :: net :: TcpListener ; use std :: io :: Read ; fn main () { let server = Server :: new ( \"127.0.0.1:8000\" . to_string ()); server . run (); } pub struct Server { addr : String , } impl Server { pub fn new ( addr : String ) -> Self { Self { addr } } pub fn run ( self ) { println! ( \"Listening on {}\" , self . addr ); let listener = TcpListener :: bind ( & self . addr ). unwrap (); loop { match listener . accept () { Ok (( mut stream , _ )) => { let mut buffer = [ 0 ; 1024 ]; match stream . read ( & mut buffer ) { Ok ( _ ) => { println! ( \"Received request: {}\" , String :: from_utf8_lossy ( & buffer )); }, Err ( e ) => println! ( \"Failed to read from connection: {}\" , e ), } } Err ( e ) => println! ( \"Failed to establish a connection: {}\" , e ), } } } } import socket class Echo_Server : def __init__ ( self , address : str ): self . addr = address def run ( self ): HOST = self . addr [: self . addr . find ( \":\" )] PORT = int ( self . addr [ self . addr . find ( \":\" ) + 1 :]) with socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) as s : s . bind (( HOST , PORT )) s . listen () conn , addr = s . accept () with conn : print ( f \"Received connection from { addr } \" ) while True : data = conn . recv ( 1024 ) if not data : break print ( data . decode ( \"utf-8\" )) # conn.sendall(data) # Echo server def main (): server = Echo_Server ( \"127.0.0.1:8000\" ) server . run () if __name__ == '__main__' : main ()","title":"Echo server"},{"location":"Coding/#terminal","text":"","title":"Terminal"},{"location":"Coding/#input-validation-loop","text":"Such a loop will continuously prompt for valid input, in this case an integer. TryParse try/catch while ( true ): { int option ; int . TryParse ( Console . ReadLine (), out option ); if ( option != null ) { // ... } else { break ;} } while ( true ): { try { int option = Int32 . Parse ( Console . ReadLine ()); } catch ( FormatException ) { // Input was not an integer } catch ( OverflowException ) { // Number was too big } } while True : try : option = int ( input ()) except ValueError : # Integer was not able to be parsed","title":"Input validation loop"},{"location":"Coding/#guessing-game","text":"","title":"Guessing game"},{"location":"Coding/#oxford-comma","text":"using System.CommandLine ; using System.CommandLine.Invocation ; using System.Linq ; static int Main ( string [] args ) { var cmd = new RootCommand { new Argument < string []>( \"names\" ) }; cmd . Handler = CommandHandler . Create < string []>( Handler ); return cmd . Invoke ( args ); } static void Handler ( string [] names ) { Console . WriteLine ( $\"{String.Join(\" , \", names.Take(names.Length -1))}, and {names.Last<string>()}\" ); } import argparse def get_args (): p = argparse . ArgumentParser ( description = \"Listing args with Oxford comma\" ) p . add_argument ( \"words\" , nargs = \"+\" , help = \"Words to concatenate using Oxford comma\" ) return p . parse_args () def oxford_commafy ( words ): l = len ( words ) if l > 2 : words [ - 1 ] = f \"and { words [ - 1 ] } \" print ( \", \" . join ( words )) elif l == 2 : print ( f \" { words [ 0 ] } and { words [ 1 ] } \" ) else : print ( words [ 0 ]) def main (): args = get_args () . words oxford_commafy ( args ) if __name__ == \"__main__\" : main ()","title":"Oxford comma"},{"location":"Coding/#color-output","text":"Console . Color = ConsoleColor . Red ; Console . WriteLine ( \"Red!\" ) Console . ResetColor (); print ( f \" { colorama . Fore . RED } Red! { colorama . Style . RESET_ALL } \" ) Console . Color = ConsoleColor . Green ; Console . WriteLine ( \"Green!\" ) Console . ResetColor (); print ( f \" { colorama . Fore . GREEN } Green! { colorama . Style . RESET_ALL } \" ) Console . Color = ConsoleColor . Yellow ; Console . WriteLine ( \"Yellow!\" ) Console . ResetColor (); print ( f \" { colorama . Fore . YELLOW } Yellow! { colorama . Style . RESET_ALL } \" ) Console . Color = ConsoleColor . Blue ; Console . WriteLine ( \"Blue!\" ) Console . ResetColor (); print ( f \" { colorama . Fore . BLUE } Blue! { colorama . Style . RESET_ALL } \" ) Console . Color = ConsoleColor . Magenta ; Console . WriteLine ( \"Magenta!\" ) Console . ResetColor (); print ( f \" { colorama . Fore . MAGENTA } Magenta! { colorama . Style . RESET_ALL } \" )","title":"Color output"},{"location":"Coding/#calculator","text":"(argparse) import argparse def get_args (): parser = argparse . ArgumentParser () parser . add_argument ( \"operand1\" , type = float ) parser . add_argument ( \"operand2\" , type = float ) op = parser . add_mutually_exclusive_group () op . add_argument ( \"-a\" , \"--add\" , dest = \"add\" , action = \"store_true\" ) op . add_argument ( \"-s\" , \"--subtract\" , action = \"store_true\" ) op . add_argument ( \"-d\" , \"--divide\" , action = \"store_true\" ) op . add_argument ( \"-m\" , \"--multiply\" , action = \"store_true\" ) return parser . parse_args () def main (): args = get_args () if args . add : print ( \"Adding\" ) print ( args . operand1 , \" + \" , args . operand2 , \" = \" , args . operand1 + args . operand2 ) elif args . subtract : print ( \"Subtracting\" ) print ( args . operand1 , \" - \" , args . operand2 , \" = \" , args . operand1 - args . operand2 ) elif args . divide : print ( \"Dividing\" ) print ( args . operand1 , \" / \" , args . operand2 , \" = \" , args . operand1 / args . operand2 ) elif args . multiply : print ( \"Multiplying\" ) print ( args . operand1 , \" * \" , args . operand2 , \" = \" , args . operand1 * args . operand2 ) else : print ( \"Unknown operation!\" ) if __name__ == \"__main__\" : main ()","title":"Calculator"},{"location":"Coding/#input-validation","text":"string input ; int inputParsed ; while ( true ) { input = System . Console . ReadLine (); try { inputParsed = int . Parse ( input ); break ; } catch { System . Console . WriteLine ( \"Please input a number...\" ); } } System . Console . WriteLine ( $\"Number provided: {inputParsed}\" );","title":"Input validation"},{"location":"Coding/#subcommands","text":"./app command subcommand argument static int Main ( string [] args ) { var rootCommand = new RootCommand ( \"command\" ); var command = new Command ( \"subcommand\" ) { new Argument < string >( \"argument\" ); }; command . Handler = new CommandHandler . Create < string >( argumentHandler ); rootCommand . Add ( command ); rootCommand . Invoke ( args ); } private static void argumentHandler ( string argument ) { /* ... */ }","title":"Subcommands"},{"location":"Coding/#to-do-app","text":"Notably, the dstask Go application is a very sophisticated evolution of a terminal-based to-do app featuring subcommands, YAML-formatted tasks, and Git integration.","title":"To-do app"},{"location":"Coding/#tdd","text":"Test fixture","title":"TDD"},{"location":"Coding/#oop","text":"","title":"OOP"},{"location":"Coding/#dnd-character","text":"Generating a Dungeons 'n Dragons character provides the opportunity to exercise a variety of OOP techniques: public and private fields and properties and methods using simple arithmetic. A Dungeons 'n Dragons character has six character attributes that can be randomly assigned. This process, called an ability roll , is calculated by rolling four six-sided dice (d6) and summing the highest three values, discarding the lowest. The raw ability score is then modified according to a table to produce a final ability score. In the implementations below, all ability scores are dynamically calculated using getter functions that sum the raw ability score (stored as a private field) and modifier. Both the ability roll and modifier lookup are implemented as public static functions. Constructor Properties Methods class Character : def __init__ ( self , race : Race = Race . HUMAN ): self . _strength_ability = self . ability_roll () self . _dexterity_ability = self . ability_roll () self . _constitution_ability = self . ability_roll () self . _intelligence_ability = self . ability_roll () self . _wisdom_ability = self . ability_roll () self . _charisma_ability = self . ability_roll () self . _race = race class Character : @property def Strength ( self ): return self . _strength_ability + self . get_modifier ( self . _strength_ability ) @property def Dexterity ( self ): return self . _dexterity_ability + self . get_modifier ( self . _dexterity_ability ) @property def Constitution ( self ): return self . _constitution_ability + self . get_modifier ( self . _constitution_ability ) @property def Intelligence ( self ): return self . _intelligence_ability + self . get_modifier ( self . _intelligence_ability ) @property def Wisdom ( self ): return self . _wisdom_ability + self . get_modifier ( self . _wisdom_ability ) @property def Charisma ( self ): return self . _charisma_ability + self . get_modifier ( self . _charisma_ability ) @staticmethod def Roll ( range : int = 6 ): return random . randrange ( range ) + 1 @staticmethod def get_modifier ( score : int ): return math . floor (( score - 10 ) / 2 ) @classmethod def ability_roll ( cls ): rolls = [ cls . Roll (), cls . Roll (), cls . Roll (), cls . Roll ()] rolls . remove ( min ( rolls )) return sum ( rolls ) def report ( self ): print ( f \"Strength: { self . Strength } \" ) print ( f \"Dexterity: { self . Dexterity } \" ) print ( f \"Constitution: { self . Constitution } \" ) print ( f \"Intelligence: { self . Intelligence } \" ) print ( f \"Wisdom: { self . Wisdom } \" ) print ( f \"Charisma: { self . Charisma } \" ) Constructor Properties Methods partial class Character { private int StrengthAbility ; private int DexterityAbility ; private int ConstitutionAbility ; private int IntelligenceAbility ; private int WisdomAbility ; private int CharismaAbility ; private Race Race { get ; } public Character ( Race race ) { this . StrengthAbility = AbilityRoll (); this . DexterityAbility = AbilityRoll (); this . ConstitutionAbility = AbilityRoll (); this . IntelligenceAbility = AbilityRoll (); this . WisdomAbility = AbilityRoll (); this . CharismaAbility = AbilityRoll (); this . Race = race ; } public Character () : this ( Race . HUMAN ) { } } partial class Character { public int Strength { get => StrengthAbility + GetModifier ( StrengthAbility ) + GetRaceModifier ( Abilities . STRENGTH ) } public int Dexterity { get => DexterityAbility + GetModifier ( DexterityAbility ) + GetRaceModifier ( Abilities . DEXTERITY ) } public int Constitution { get => ConstitutionAbility + GetModifier ( ConstitutionAbility ) + GetRaceModifier ( Abilities . CONSTITUTION ) } public int Intelligence { get => IntelligenceAbility + GetModifier ( IntelligenceAbility ) + GetRaceModifier ( Abilities . INTELLIGENCE ) } public int Wisdom { get => WisdomAbility + GetModifier ( WisdomAbility ) + GetRaceModifier ( Abilities . WISDOM ) } public int Charisma { get => CharismaAbility + GetModifier ( CharismaAbility ) + GetRaceModifier ( Abilities . CHARISMA ) } } partial class Character { public void Report () { Console . Write ( $\"Strength: {Strength,2}\" ); Console . Write ( $\"Dexterity: {Dexterity,2}\" ); Console . Write ( $\"Constitution: {Constitution,2}\" ); Console . Write ( $\"Intelligence: {Intelligence,2}\" ); Console . Write ( $\"Wisdom: {Wisdom,2}\" ); Console . Write ( $\"Charisma: {Charisma,2}\" ); } static int Roll ( int ceiling ) { Random rng = new Random (); return rng . Next ( 1 , ceiling ); } static int AbilityRoll () { List < int > rolls = new List < int > { Roll ( 6 ), Roll ( 6 ), Roll ( 6 ), Roll ( 6 ) }; rolls . Remove ( rolls . Min ()); return rolls . Sum (); } public static int GetModifier ( int ability ) { return ( int ) System . Math . Floor ((( double ) ability - 10 ) / 2 ); } }","title":"DnD character"},{"location":"Coding/#rpg-character-generator","text":"Player class Subclasses Race class Player (): def __init__ ( self , name : str , race : Race , hp : int , mp : int ): self . _name = name self . _race = race self . _hp = hp self . _mp = mp @property def getName ( self ): return self . _name @property def getRace ( self ): return self . _race @property def getHp ( self ): return self . _hp @property def getMp ( self ): return self . _mp def attack ( self ): return \"Have at thee!\" class Warrior ( Player ): def __init__ ( self , name : str , race : Race ): super () . __init__ ( name , race , 200 , 0 ) def attack ( self ): return \"I will destroy with my sword, foul demon!\" class Priest ( Player ): def __init__ ( self , name : str , race : Race ): super () . __init__ ( name , race , 100 , 200 ) def attack ( self ): return \"Taste the wrath of the Two True Gods!\" class Mage ( Player ): def __init__ ( self , name : str , race : Race ): super () . __init__ ( name , race , 150 , 150 ) def attack ( self ): return \"You are overmatched by my esoteric artifices!\" import enum class Race ( enum . Enum ): HUMAN = enum . auto (), ELF = enum . auto (), DWARF = enum . auto () Player class Subclasses Race #include <string> class Player { protected : std :: string _name { \"Johnny Bravo\" }; Race _race { Race :: HUMAN }; int _hp { 100 }; int _mp { 100 }; public : Player ( std :: string n , Race r , int hp , int mp ) : _name { n }, _race { r }, _hp ( hp ), _mp ( mp ) {} virtual std :: string attack () = 0 ; int getHp () { return _hp ; } int getMp () { return _mp ; } std :: string getRace () { switch ( _race ) { case 0 : return \"human\" ; break ; case 1 : return \"elf\" ; break ; case 2 : return \"dwarf\" ; break ; default : return \"none\" ; break ; } } std :: string getName () { return _name ; } void setHp ( int n ) { _hp = n ; } void setMp ( int n ) { _mp = n ; } void setName ( std :: string s ) { _name = s ; } void setRace ( Race r ) { _race = r ;} }; class Warrior : public Player { public : Warrior ( std :: string n , Race r ) : Player ( n , r , 200 , 0 ) {} std :: string attack () { return \"I will destroy you with my sword, foul demon!\" ;} }; class Priest : public Player { public : Priest ( std :: string n , Race r ) : Player ( n , r , 100 , 200 ) {} std :: string attack () { return \"Taste the wrath of the Two True Gods!\" ;} }; class Mage : public Player { public : Mage ( std :: string n , Race r ) : Player ( n , r , 150 , 150 ) {} std :: string attack () { return \"You are overmatched by my esoteric artifices!\" ;} }; enum Race { HUMAN , ELF , DWARF };","title":"RPG character generator"},{"location":"Coding/#starships","text":"This project provides a scenario for implementing OOP and TDD principles in a variety of languages and implementations. Simple classes with intuitive properties and fields include Officer and Starship , which also has a field containing a variant of the StarshipClass enum. Fleet serves as a container for Starships. An Officer is paired with a Starship to form a StarshipDeployment . CaptainSelector , which is passed to StarshipDeployment by dependency injection, evaluates whether the Officer provided has what it takes to ply the inky black. This boils down to a check on the Officer's Grade property, which is simple to test in testing frameworks where a mocked Officer object can be set up with unsatisfactory Grade values. - StarshipDeployment also takes a StarshipValidator object by dependency injection, which it uses to perform checks on a given Starship. These checks provide opportunities to mock Starship and Officer objects in unit testing. - IsCaptained() checks if the Starship has a Captain assigned - ValidateRegistry() makes sure the Starship's registry number begins with NCC or NX - Evaluate() runs all the other methods in the class and returns True only if all checks pass. This provides the opportunity to test a mocked validator for invocation of the Evaluate() method. Officer Starship StarshipClass Starship from enum import Enum class StarshipClass ( Enum ): NX = 'NX' GALAXY = 'Galaxy' CONSTITUTION = 'Constitution' SOVEREIGN = 'Sovereign' DEFIANT = 'Defiant' INTREPID = 'Intrepid' MIRANDA = 'Miranda' class Starship : def __init__ ( self , name = None , starshipclass : StarshipClass = StarshipClass . NX , registry = None , crew = 0 , ): self . name = name self . registry = registry self . _crew = crew self . crew_on_leave = 0 self . _starshipclass = starshipclass @property def crew ( self ): return self . _crew @crew . setter def crew ( self , crew : int ): if crew < 0 : raise Exception else : self . _crew = crew @property def starshipclass ( self ): return self . _starshipclass @starshipclass . setter def starshipclass ( self , starshipclass : StarshipClass ): if starshipclass not in StarshipClass : raise Exception else : self . _starshipclass = starshipclass StarshipClass Officer CaptainSelector StarshipValidator StarshipDeployment public enum StarshipClass { NX , GALAXY , CONSTITUTION , SOVEREIGN , DEFIANT , INTREPID , MIRANDA } public interface IOfficer { string FirstName { get ; set ; } string LastName { get ; set ; } DateTime BirthDate { get ; set ; } char Grade { get ; set ; } string Name { get ; } } public class Officer : IOfficer { public string FirstName { get ; set ; } public string LastName { get ; set ; } public DateTime BirthDate { get ; set ; } public string Name { get { return $\"{FirstName} {LastName}\" ; } } public char Grade { get ; set ; } } public class CaptainSelector { public IOfficer Officer { get ; set ; } public CaptainSelector ( IOfficer officer ) { Officer = officer ; } public bool Evaluate () { return Officer . Grade == 'A' ? true : false ; } } public class StarshipValidator : IStarshipValidator { public IStarship Starship { get ; set ; } public bool IsCaptained () { return Starship . Captain != null ? true : false ; } public bool ValidateRegistry () { return Starship . Registry . StartsWith ( \"NCC\" ) || Starship . Registry . StartsWith ( \"NX\" ) ? true : false ; } public bool Evaluate () { return ValidateRegistry () && IsCaptained (); } } public class StarshipDeployment { public IStarshipValidator StarshipValidator { get ; set ; } public StarshipDeployment ( IStarshipValidator validator ) { StarshipValidator = validator ?? throw new ArgumentNullException ( nameof ( validator )); } public bool ValidateDestination ( string destination ) { return destination . Length > 1 ? true : false ; } public StarshipMission Deploy ( Starship starship , string destination ) { bool destinationValidated = ValidateDestination ( destination ); bool starshipValidated = StarshipValidator . Evaluate (); return destinationValidated && starshipValidated ? new StarshipMission { Starship = starship as Starship , Destination = destination } : throw new ArgumentException (); } }","title":"Starships"},{"location":"Coding/#glossary","text":"C \"A programming language is low level when its programs require attention to the irrelevant.\" -Alan Perlis Despite C's reputation as a low-level programming language, in fact it merely emulates the ancient PDP-11, which is the only machine for which its abstract machine can be described as \"close to the metal\". In the age of parallel processes, C's serial nature... Sources: C is not a low-level programming language Enumeration In C# the term enumeration refers to the process of successively returning individual values. In Python, the term iteration is used to refer to the same thing, and iterable refers to an object that can be iterated, or parsed out into sub-elements. In Python, any object that exposes the __iter__() and __next__() dunder methods are iterable. In C#, the IEnumerable interface implements enumeration. Both languages feature a keyword that allows a subclass to access its direct parent. Whereas in Python the terms superclass and subclass are used, in C# the terms base class and derived class are preferred. Garbage collector A garbage collector is a feature of some programming language runtimes that periodically pauses execution to remove data that is no longer used. Such languages are considered unsuitable for use in database applications because of the unpredictable latency this garbage collection creates, despite the added memory safety. Loop unswitching One of the core optimizations that a C compiler performs; transforms a loop containing a conditional into a conditional with a loop in both parts, which changes flow control Register rename engine Component of modern high-end cores which is one of the largest consumers of die area and power Scalar Replacement Of Aggregates (SROA) One of the core optimizations that a C compiler performs; attempts to replace struct s and arrays with fixed lengths with individual variables, which allows the compiler to treat accesses as independent and elide operations entirely if it can prove the results are never visible, which also deletes padding sometimes. Segmented architecture Pointers might be segment IDs and an offset","title":"\ud83d\udcd8 Glossary"},{"location":"Coding/6502/","text":"The 6502 processor is an 8-bit CPU that was used in many computers and consoles, including the NES and SNES. Specifications Address bus: 16 bit Emulation Component C++ Memory struct Memory The first 256B of memory is referred to as the ($0000-$00FF). Memory range Description 0x0000-0x00FF Zero Page 0x0100-0x01FF Stack Registers # Register Size Description PC Program Counter 16 bit pointer to the next instruction SP Stack pointer 8 bit holds low 8 bits of the next free location on the stack A Accumulator 8 bit used for all arithmetic operations (except for incrementation and decrementation) X X register 8 bit available for holding counter or offset values for memory access. It also has the special function of copying or changing the value of SP. Y Y register 8 bits available for holding counter or offset values for memory access. Opcodes All opcodes are 1 byte in size, and their operands are 0, 1, or 2 bytes Opcode Code Description CMP ( src ) LDA Load from ZP to A STA $85 Store accumulator in memory ( src ) LDA LDA #$33 ; Load 69 into A STA Exapunks Instruction Description LINK Move GRAB COPY","title":"6502"},{"location":"Coding/6502/#specifications","text":"Address bus: 16 bit","title":"Specifications"},{"location":"Coding/6502/#emulation","text":"Component C++ Memory struct","title":"Emulation"},{"location":"Coding/6502/#memory","text":"The first 256B of memory is referred to as the ($0000-$00FF). Memory range Description 0x0000-0x00FF Zero Page 0x0100-0x01FF Stack","title":"Memory"},{"location":"Coding/6502/#registers","text":"# Register Size Description PC Program Counter 16 bit pointer to the next instruction SP Stack pointer 8 bit holds low 8 bits of the next free location on the stack A Accumulator 8 bit used for all arithmetic operations (except for incrementation and decrementation) X X register 8 bit available for holding counter or offset values for memory access. It also has the special function of copying or changing the value of SP. Y Y register 8 bits available for holding counter or offset values for memory access.","title":"Registers"},{"location":"Coding/6502/#opcodes","text":"All opcodes are 1 byte in size, and their operands are 0, 1, or 2 bytes Opcode Code Description CMP ( src ) LDA Load from ZP to A STA $85 Store accumulator in memory ( src )","title":"Opcodes"},{"location":"Coding/6502/#lda","text":"LDA #$33 ; Load 69 into A","title":"LDA"},{"location":"Coding/6502/#sta","text":"","title":"STA"},{"location":"Coding/6502/#exapunks","text":"Instruction Description LINK Move GRAB COPY","title":"Exapunks"},{"location":"Coding/C%23/","text":"C# To-do Sort out Events section, in particular the example cited Develop C# implementation of CSV parser. This appears to be harder than it should be, apparently because the object returned by the CsvReader.GetRecords<T>() method is an IEnumerable which is one of the confusing points of the UWP course. YouTube tutorials do not seem helpful.. Variables Parsing Data types can be used as static classes, exposing a TryParse method. Int32 DateTime parsedInt = Int32 . TryParse ( rawInt ); parsedDate = DateTime . TryParse ( rawDate ); String Specify a verbatim literal string by prepending @ , which disables escape characters and forces interpretation of backslashes literally: string filePath = @\"C:\\televisions\\sony\\bravia.txt\" ; Specify a formatted string by prepending a $ int n ; string s = $\"{n} is a number\" ; Standard numeric format strings are used to format common numeric types. They take the form of a character (i.e. C for currency, N for number) followed by a number. They can be passed as arguments to the ToString method of the literal or in the placeholder of a formatted string after : . Console . WriteLine ( $\"{123.456789:C }\" ); // $123.46 Console . WriteLine ( 123.456789d . ToString ( \"C\" )); // $123.46 A precision specifier can define the number of fractional digits after the decimal separator. Console . WriteLine ( $\"{123.456789:C3 }\" ); // $123.457 Empty space can be added to either side of the value to create evenly spaced output by placing a number after a comma (positive for right-alignment, negative for left-alignment): Console . WriteLine ( $\"{123.456789, 15}\" ); Casting Because real numbers are stored as double s by default, in order to assign to a float variable you must append f to the literal: float num = 3.14f ; A similar logic pertains for integers to be declared as doubles : double num = 3d ; decimal data type literals, which have 28-29 significant digits, can be declared with the m suffix decimal num = 123.4567890123456789 m char literals can be encoded in Unicode: char umlaut = '\\ u00F6 ' ; Variables can be explicitly cast to some other data types by placing the new data type in parentheses before the value: int pi = ( int ) System . Math . PI ; This casting won't work with string , which can be cast by using the Convert type or parsed using the data type's Parse method. The differense is that using Convert will return a 0 if the value is null while Parse will throw an exception. w = \"5\" ; int wConverted = System . Convert . ToInt32 ( w ); int wParsed = int . Parse ( w ); Collection Arrays Arrays are declared differently from built-in arrays in C++. C# C++ int [] primes = { 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 }; int primes [ 10 ] { 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 } An empty array must still have its size declared int [] primes = new int [ 10 ]; An unnamed array: new [] { 1 , 2 , 3 }; Arrays can be traversed with a foreach loop, but the elements can not be changed.: foreach ( var i in container ) { // ... } Arrays can be copied with the Clone() and Copy() methods: see ArrayCloning . Arrays can be reversed in place with the Reverse method. int [] primes = { 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 }; Array . Reverse ( primes ); LINQ Language Integrated Query (LINQ) refers to a C# library that facilitates querying of collections. These are exposed as extension methods : methods that are available on already existing queryable types This means extension methods are exposedon existing collection types like Array and List because they are derived from IEnumerable<T> , and thus need no modification to serve as a LINQ data source. Linq methods are available in two semantically identical syntaxes: query syntax and method syntax (also lambda syntax). Query syntax is meant to be more intuitive for developers familiar with SQL. Method syntax allows method chaining. Query syntax Method syntax Python int [] numbers = { 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 }; var evens = from n in nums where n % 2 == 0 orderby n descending select n ; int [] numbers = { 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 }; var evens = nums . Where ( n => n % 2 == 0 ). OrderByDescending ( n => n ); numbers = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] evens = [ n for n in numbers if n % 2 == 0 ] list ( reversed ( evens )) Notably, unlike loop structures in C#, LINQ methods can work on unordered collections like Dictionaries. ObservableCollection The ObservableCollection class is used to define collections that provide notifications to data bindings when items are added or removed. As such, it is used in GUI programming... DateTime DateTimeOffset is preferred over DateTime because it includes an offset value that indicates the timezone. Parsing DateTime is a class that exposes several static methods of parsing raw values. All of them have overloads that accept CultureInfo objects (implementing IFormatProvider ) which can affect parsing of ambiguous dates. Parse() will attempt to parse a string and raise a FormatException if unable to do so. ParseExact() requires an exact string template and requires a CultureInfo object as well. Parse() Specifying culture ParseExact() string rawDate = \"07/04/1776\" ; try { DateTime parsedDate = DateTime . Parse ( rawDate ); } catch ( FormatException ) { Console . WriteLine ( \"Unparsable!\" ) } Console . WriteLine ( parsedDate . ToLongDateString ()); // => \"July 4, 1776\" string rawDate = \"07/04/1776\" ; try { DateTime parsedDate = DateTime . Parse ( rawDate , CultureInfo . GetCultureInfo ( \"en-GB\" )); } catch ( FormatException ) { Console . WriteLine ( \"Unparsable!\" ) } Console . WriteLine ( parsedDate . ToLongDateString ()); // => \"April 7, 1776\" string rawDate = \"07/04/1776\" ; try { DateTime parsedDate = DateTime . ParseExact ( rawDate , \"M/d/yyyy\" , CultureInfo . InvariantCulture ); } catch ( FormatException ) { Console . WriteLine ( \"Unparsable!\" ) } Console . WriteLine ( parsedDate . ToLongDateString ()); // => \"July 4, 1776\" TryParse() returns no value, but takes an out parameter. It does not throw an exception if the date is unparsable, but rather outputs the default date January 1, 1 AD. The overload that accepts a Culture object also requires a DateTimeStyles object. TryParse() Specifying culture string rawDate = \"07/04/1776\" ; DateTime parsedDate ; DateTime . TryParse ( rawDate , out parsedDate ); Console . WriteLine ( parsedDate . ToLongDateString ()); // => \"July 4, 1776\" string rawDate = \"07/04/1776\" ; DateTime parsedDate ; DateTime . TryParse ( rawDate , CultureInfo . GetCultureInfo ( \"en-GB\" ), DateTimeStyles . None , out parsedDate ); Console . WriteLine ( parsedDate . ToLongDateString ()); // => \"April 7, 1776 ParseExact() TryParseExact() Timezones TimeZoneInfo includes static methods that can access system timezones. DateTime does not include timezone information, so it must be specified at runtime. TimeZoneInfo sidneyTimeZone = TimeZoneInfo . FindSystemTimeZoneById ( \"E. Australia Standard Time\" ); var sydneyTime = TimeZoneInfo . ConvertTime ( DateTime . Now , sydneyTimeZone ); Enumerating all system timezones. foreach ( var timeZone in TimeZoneInfo . GetSystemTimeZones ()) { Console . WriteLine ( timeZone . GetUtcOffset ()); } Methods Lambda A lambda expression can have two forms, both of which use the lambda declaration operator => Expression lambda Statement lambda ( input - parameters ) => expression ( input - parameters ) => { statements } Anonymous event handlers can be reformulated as lambdas to reduce code complexity. SubmitButton . Click += delegate ( object sender , EventArgs e ) { MessageBox . Show ( \"Button Clicked\" ); } // Using a (statement) lambda: SubmitButton . Click += ( s , e ) => MessageBox . Show ( \"Button Clicked\" ); ref ref allows variables that are normally passed by value to be passed by reference. Pass by value Pass by reference Returning a value Integers are normally passed by value, so number will not change static void Main () { int number = 0 ; plusOne ( number ); } static void plusOne ( int n ) { n ++; } Now number will increment by one because it is being passed by reference. static void Main () { int number = 0 ; plusOne ( ref number ); } static void plusOne ( ref int n ) { n ++; } Alternatively, number can be reassigned a variable if the method is refactored to return the new value. static void Main () { int number = 0 ; number = plusOne ( number ); } static int plusOne ( ref int n ) { n ++; return n ; } out out allows a method to assign a value to a variable that has no value yet. It can be used to return multiple values. static void Main () { double n = 5 ; double nSquared ; square ( n , out nSquared ); Console . WriteLine ( $\"{n} ^ 2 = {nSquared}\" ); } static void square ( double x , out double y ) { y = System . Math . Pow ( x , 2 ); } out is prominently used in the TryParse method. params params allows you to process a variable number of similarly-typed arguments in the method signature. This collection of arguments is abstracted as an array. static void method ( int [] args ) { foreach ( int el in args ) { Console . WriteLine ( el ); } } If the arguments to be accepted are themselves arrays, then you must define an array of arrays ( ex. ). This technique may not have worked in previous versions of C#. static void method ( int [][] args ) { foreach ( var array in args ){ foreach ( int el in array ) { Console . WriteLine ( el ); } } } Delegates Delegates are a functional programming feature in C# that facilitate loose coupling. They allow a function to be abstracted so that updated logic can be implemented without incurring technical debt. Delegates take the form of a method signature using the delegate keyword. One or more methods implementing the delegate can be formulated which do not reference the delegate in any way, shape, or form, except for the fact that their method signature matches that specified by the delegate. Where the method is to be used, instead of calling the method directly, the delegate is instantiated like an object, but the name of the specific method that implements the delegate is passed as a parameter. The instantiated delegate can then be called, which passes the parameters to the method. This results in looser coupling because when changing implementation, only the parameter specifying the improved method needs to be adjusted, and the delegate ensures that the same pattern of parameters is enforced at compile-time. Delegates can be used for messaging in .NET and especially to tie events to event handlers, but they are no longer used as much as Func<T,TResult> and Action<T> . Initial implementation Improved implementation public delegate void InformationNeeded ( int n , string s ); static void Main () { InformationNeeded form = new InformationNeeded ( SimpleReport ) // ... form ( 2 , \"kiwi\" ); form ( 3 , \"jackfruit\" ); } void SimpleReport ( int m , string t ) { Console . WriteLine ( $\"int: {m}, string: {t}\" ); } public delegate void InformationNeeded ( int n , string s ); static void Main () { InformationNeeded form = new InformationNeeded ( BetterReport ) // ... form ( 2 , \"kiwi\" ); form ( 3 , \"jackfruit\" ); } void BetterReport ( int m , string t ) { Console . WriteLine ( $\"There are {m} items of type {t}\" ); } Events Events signal the occurrence of an action or notification. They are raised or fired (invoked) by the publisher and received by the event handler or subscriber . They represent a syntactic sugar over the delegate structure, which is used in the background as the pipeline to connect publisher and handler. The simplest way to define an event, using the builtin EventHandler type, is as follows: public event EventHandler Occurrence ; In actuality, EventHandler is itself a wrapper around a delegate, and any delegate can be wrapped by the event by the delegate's name as the event's data type: public delegate void InformationNeeded ( int n , string s ); class Form { public event InformationNeeded FormEvent ; } But because the event structure requires an object reference, the simplest implementation for raising an event is more involved. This is because the Main entry-point for C# programs is static, and not an object instance. The event must be defined within a class that is then instantiated. The event is implemented in an event handler that is a method within the same class that defines the event. After first checking if the event is null (abbreviated syntax using the null-conditional member access operator is equivalent) the event object is called. Here, the event handler is called by the constructor itself. namespace SimpleEvent { class Program { static void Main ( string [] args ) { TriggeringEvent eventTrigger = new TriggeringEvent (); } public class TriggeringEvent { public event EventHandler Event ; public TriggeringEvent () { OnEvent ( this , EventArgs . Empty ); } protected virtual void OnEvent ( object s , EventArgs e ) { var newEvent = Event as EventHandler ; if ( newEvent != null ) { newEvent ( this , EventArgs . Empty ); } // Null-conditional operator available since C# 6: // newEvent?.Invoke(this, EventArgs.Empty); } } } } If the method signature of the event handler is made public , then the event can be raised externally and called like any other method, and a slightly simpler example can be constructed. namespace SimpleEvent { class Program { static void Main ( string [] args ) { TriggeringEvent eventTrigger = new TriggeringEvent (); eventTrigger . OnEvent ( eventTrigger , EventArgs . Empty ); } public class TriggeringEvent { public event EventHandler Event ; public virtual void OnEvent ( object s , EventArgs e ) { Console . WriteLine ( \"OnEvent\" ); var newEvent = Event as EventHandler ; newEvent ?. Invoke ( this , EventArgs . Empty ); } } } } Conventionally, however, the event handler is not made public, but defined using the protected virtual void method signature. Event wiring refers to the process of adding subscribers to an event. In implementation, this involves adding the subscribers to the invocation list of the delegate that is used to tie the event to event handler. Event += EventSubscriber ; In actuality, this syntax uses delegate inference , where the compiler automatically determines the correct delegate to use. The fuller syntax avoiding the use of this feature would be Event += new EventHandler ( EventSubscriber ); The event is then fired by calling it, but this can only occur from within the type in which it is defined. So it has to be fired from within another of that type's methods. Anonymous methods and lambdas can also be used after the += operator: Event += ( s , e ) => Console . WriteLine ( \"Subscribing to event!\" ); In this example , adapted from a Pluralsight course, the OnMissionAccomplished and OnMissionStatusReport event handlers send two different types of events, respectively: MissionStatusReport and MissionAccomplished . Even though both of these events are EventHandler types, they are actually events. Async The async modifier is used to construct asynchronous code. By convention, asynchronous methods are named with \"Async\" to distinguish them. The await keyword marks the variable containing the result. public int Addition () { var a = SlowMethodOne (); var b = SlowMethodTwo (); return a + b ; } public async Task < int > AdditionAsync () { var a = SlowMethodOneAsync (); var b = SlowMethodTwoAsync (); return await a + await b ; } Return types used for async include: - Task - Task<T> - Void should generally be avoided with the exception of event handlers Async does not create new threads by default, so it is only suitable for UI and IO-bound methods, not CPU-bound methods. Here async is used to return an enumerable collection of Customer objects from the file IO system . public class CustomerDataProvider { private static readonly string _customersFileName = \"customers.json\" ; private static readonly StorageFolder _localFolder = ApplicationData . Current . LocalFolder ; public async Task < IEnumerable < Customer >> LoadCustomersAsync () { var storageFile = await _localFolder . TryGetItemAsync ( _customersFileName ) as StorageFile ; List < Customer > customerList = null ; // ... } } Here threads are used to handle JSON files and data on application load in the data provider for a GUI application LoadCustomersAsync SaveCustomersAsync public async Task < IEnumerable < Customer >> LoadCustomersAsync () { } public async Task SaveCustomersAsync ( IEnumerable < Customer > customers ) { } Exceptions Exceptions expose Message and StackTrace attributes that can be inspected for further information (ref. ExceptionHandling ) Member access operators are syntactic sugars that allow operations to be performed without exception handling. Operator Name Description => Lambda declaration operator ?. Null-conditional member access operator Applies the operation to its operand only if it evaluates to non-null, otherwise it returns null ?[] Null-conditional element access operator Applies the operation to its operand only if it evaluates to non-null, otherwise it returns null ?? Null-coalescing operator Returns value of left-hand operand if non-null, otherwise returns result of right-hand operand. ??= Null-coalescing assignment operator Assigns the value of the right-hand operand to the left-hand operand, only if the left-hand operand evaluates to null . Classes Access modifiers Classes can be declared with various access modifiers that affect the compiler's behavior. These are intended to prevent what would be runtime errors by turning them into compile-time errors, improving code quality. - static prevents instantiation - abstract indicates the class is to be completed in a derived class. Every method marked as abstract has to be implemented in the derived class, and the class has to be marked with abstract as well. - sealed prevents inheritance - partial allows the same class to be defined across multiple files Constructor If not defined, the compiler will provide a default constructor. A constructor can be overloaded by using the this keyword in the constructor's signature after a colon, as if invoking the second constructor: using System ; class Car { public string brand { get ; set ; } public Car () : this ( \"Ford\" ) { } public Car ( string brand ) { this . brand = brand ; } } class Program { static void Main ( string [] args ) { Car ford = new Car (); Console . WriteLine ( ford . brand ); // => Ford } } Properties A property protects the data of a private variable (\"field\") by implementing getter and setter accessor functions. These allow data validation or other logic to be performed when the variable is changed. The private variable being protected is called the backing store . By convention, properties have identifiers in title case. The identifier for the backing field of a property is conventionally the same as the property, except lowercase or prepended with an underscore. In the set accessor, the keyword value is used for the argument passed in. private string name ; // field public string Name // property { get { return name ; } set { this . name = value ; } } A common shorthand was introduced in C# 3 called automatically implemented properties , where the p public string Name { get ; set ; } The set accessor uses an implicit parameter value , whose value is the type of the property. class Person { private string _name ; // the name field public string Name // the Name property { get => _name ; set => _name = value ; } } Data validation for setter accessor: public class Date { private int _month = 7 ; // Backing store public int Month { get => _month ; set { if (( value > 0 ) && ( value < 13 )) { _month = value ; } } } } A property can be made read-only by simply removing the setter. An access modifier can also be applied to only one or the other of the accessors to enforce encapsulation. This can make the property read-only externally while still allowing the class's own logic to change the property's value: private set { if (( value > 0 ) && ( value < 13 )) { _month = value ; } } Similarly, fields can be modified with the readonly access modifier. This will prevent the variable from being changed in external code as well as in any internal methods. Readonly fields can only be set by the constructor or variable initializers. Static classes Classes marked with static are not instantiated. An example is the System.Console class, which is never instantiated even though its methods are available for use. This structure is called a singleton and is useful as a container for assorted utilities. Methods marked with static are independent of the class instance itself, and as such do not have access to fields that are not const . Polymorphism Modifiers like abstract , virtual , and override allow derived classes to implement logic that builds upon that of a base class. virtual allows you to declare methods and properties in a base class which can be overriden in a derived class. virtual cannot be used with static , abstract , private , or override . abstract is similar, except that the class itself must also be marked as an abstract class, preventing instantiation of the base class. Instead of defining a base function, only the signature is declared. In both cases, override is used to mark the implementation in the derived class. Base class ( abstract ) Derived class Derived class abstract class Shape { public abstract double GetArea (); } class Shape { public virtual double GetArea () { return ; } } class Rectangle : Shape { public double Length { get ; set ; } public double Width { get ; set ; } public Rectangle () { Length = 2 ; Width = 3 ; } public override double GetArea () { return Length * Width ; } } class Circle : Shape { public double Radius { get ; set ; } public Circle () { Radius = 3 ; } public override double GetArea () { return System . Math . PI * System . Math . Pow ( Radius , 2 ); } } Interfaces Interfaces can be used to break up dependencies and implement the dependency inversion principle . This principle holds that components should be dependent on abstractions, and not on implementations. ( src ) Interfaces contain property and method definitions that must be implemented in derived classes, and as such are similar in concept to abstract classes. Like abstract classes, an interface may not be instantiated. Unlike abstract classes, the override keyword is not used on classes that implement interfaces, and access modifiers are not acceptable for interface members. Also unlike abstract classes, smplementation of interface members is mandatory. And although a derived class can only inherit from a single base class, there is no limit on the number of interfaces that a derived class can inherit from. Interface identifiers conventionally with the capital I . Interface Implementation interface IAnimal { void AnimalSound (); } class Pig : IAnimal { public void AnimalSound () { Console . WriteLine ( \"Oink\" ); } } Notably, a commonly encountered interface is IEnumerable because both Lists and Arrays implement it. So methods that iterate over either Lists or Arrays typically use IEnumerable to accept either data type. Attributes Attributes appear to resemble Python decorators because like decorators appear on the line preceding a function or class definition, but they appear to be used for something else. Attributes in C# are used to adjust the function of code in a variety of ways. ObsoleteAttribute will produce a compiler warning or error (preventing compilation entirely) when deprecated code is being used. Warning Error [Obsolete(\"Don't use this class anymore, instead use ...\")] class Cow { } static void Main () { Cow betsy = new Cow (); } [Obsolete(\"Don't use this class anymore\", true)] class Cow { } static void Main () { Cow betsy = new Cow (); } A family of attributes exist to assist debugging. DebuggerStepThrough DebuggerDisplay DebuggerStepThrough can decorate certain methods to be stepped through or skipped while debugging. This is useful for situations where only some properties of a class have to be debugged. This allows more controlled debugging than using the \"Step over properties and operators\" setting in Debugging Options. ( src ) using System.Diagnostics ; struct Cow { public string Name { [ DebuggerStepThrough ] get { return \"Bessy\" ; } } public int Weight { get { return 5 ; } } } static void Main () { Cow betsy = new Cow (); } DebuggerDisplayAttribute allows an object's state to be formatted to be more understandable in the debugger's watch window. ( src ) using System.Diagnostics ; [DebuggerDisplay(\"{Name} weighs {Weight} lbs\")] struct Cow { public string Name ; public int Weight ; } class Program { static void Main ( string [] args ) { Console . WriteLine ( \"Hello World!\" ); // This triggers instantiation of the attribute typeof ( Program ). GetCustomAttributes ( false ); Cow betsy = new Cow { Name = \"Betsy\" , Weight = 1000 }; Console . WriteLine ( $\"{betsy.Name} weighs {betsy.Weight} lbs\" ); } } The CallerMemberNameAttribute can be added to string parameters in functions that are meant to process the name of the function calling them. This avoids the verbosity of placing nameof() on every invocation. ( src ) using System.ComponentModel ; using System.Runtime.CompilerServices ; namespace WiredBrainCoffee.UWP.Models { public class Customer : INotifyPropertyChanged { private string firstName ; public string FirstName { get => firstName ; set { firstName = value ; OnPropertyChanged ( nameof ( FirstName )); } } public string LastName { get ; set ; } public bool IsCoffeeDrinker { get ; set ; } public event PropertyChangedEventHandler PropertyChanged ; private void OnPropertyChanged ([ CallerMemberName ] string propertyName = null ) { PropertyChanged ?. Invoke ( this , new PropertyChangedEventArgs ( propertyName )); } } } In XAML, the ContentPropertyAttribute attribute is used to define whether or not a control accepts a default Content property field using Windows.UI.Xaml ; using Windows.UI.Xaml.Controls ; using Windows.UI.Xaml.Markup ; [ContentProperty(Name = nameof(Customer))] public sealed partial class CustomerDetailControl : UserControl { public CustomerDetailControl () { this . InitializeComponent (); } } Files Streams A Stream is an abstraction of a backing store , or sequence of bytes, which can be a file, an input/output device, a websocket, or an inter-process communication pipe. The Stream class itself is an abstract base class that can't be instantiated. FileStream is the concrete class that uses files as its backing store. Streams can support seeking, although network streams do not support seeking. This can be checked by calling the stream's boolean CanSeek property. Stream implements IDisposable , which means it can be disposed indirectly by being placed in a using block. A bit bucket is a stream with no backing store and is implemented as Stream.Null . Testing Tests are usually organized in a separate project that is linked to the project containing the system under test (SUT). Visual Studio has a built-in test-runner, but the dotnet CLI utility also allows the entire test suite to be executed from the command-line. dotnet test .NET supports several test frameworks. xUnit In xUnit , tests are organized into public classes, and test cases are composed by individual methods on this class, decorated with the Fact attribute. Test assertions are made with static Assert method calls. ( src ) public class TestCases { [Fact] public void TestCase () { Assert . Equal ( 2 + 2 , 4 ); } } Assertions that an exception must be thrown are generic method calls typed to the specific exception. public class StarshipDeploymentShould { [Fact] public void ThrowOnNullValidator () { var sut = new StarshipDeployment ( null ); Assert . Throws < ArgumentNullException >( sut ); } } Test fixtures can be formed on properties of the main test class. They must be initialized with the test class's constructor. Xunit Classes under test public class DeskBookerRequestProcessorTests { public DeskBookerRequestProcessor processor { get ; set ; } public DeskBookerRequestProcessorTests () { processor = new DeskBookerRequestProcessor (); } [Fact] public void ShouldReturnDeskBookerResultWithRequestValues () { var request = new DeskBookerRequest { FirstName = \"Thomas\" , LastName = \"Huber\" , Email = \"thomas@huber.com\" , Date = new DateTime ( 2020 , 1 , 28 ) }; var result = processor . BookDesk ( request ); Assert . NotNull ( result ); Assert . Equal ( request . FirstName , result . FirstName ); Assert . Equal ( request . LastName , result . LastName ); Assert . Equal ( request . Email , result . Email ); Assert . Equal ( request . Date , result . Date ); } [Fact] public void ShouldThrowExceptionIfRequestIsNull () { var exception = Assert . Throws < ArgumentNullException >(() => processor . BookDesk ( null )); Assert . Equal ( \"request\" , exception . ParamName ); } } namespace DeskBooker.Core.Processor { public class DeskBookingResult { public string FirstName { get ; set ; } public string LastName { get ; set ; } public DateTime Date { get ; set ; } } public class DeskBookingRequest { public string FirstName { get ; set ; } public string LastName { get ; set ; } public DateTime Date { get ; set ; } } public class DeskBookingRequestProcessor { public DeskBookingResult BookDesk ( DeskBookingRequest request ) { return new DeskBookingResult { FirstName = request . FirstName , LastName = request . LastName , Date = request . Date , }; } } } In this example, PersonProcessor is a public class whose constructor takes an ISqlDataAccess data provider by means of dependency injection. The LoadData method is setup, and the mocked object is instantiated with the Create method call. The mock will inject the mock data provider, which returns a List<PersonModel> . using ( var mock = AutoMock . GetLoose ()) { mock . Mock < ISqliteDataAccess >() . Setup ( x => x . LoadData < PersonModel >( \"SELECT * FROM Person\" )) . Returns ( GetSamplePeople ()); var sut = mock . Create < PersonProcessor >(); var expected = GetSamplePeople (); var actual = sut . LoadPeople (); Assert . True ( actual != null ); Assert . Equal ( actual . Count , expected . Count ); } The Theory attribute decorates a parameterized test and commonly appears in conjunction with InlineData attributes that contain the parameter values. using System ; using Xunit ; using System.Linq ; namespace MathTests { public class MathWorks { [Theory] [InlineData(2,2)] [InlineData(3,3,3)] [InlineData(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)] public void Addition ( params int [] ops ) { int loopsum = 0 ; foreach ( int i in ops ) { loopsum += i ; } int linqsum = ops . Sum (); Assert . Equal ( loopsum , linqsum ); } } } The xUnit test-runner can be modified using a JSON file named xunit.runner.json. This file must be copied to the output directory by selecting \"Copy if newer\" in the file's properties. This example will display the method names only, rather than the fully-qualified dotted name with namespace and class. { \"methodDisplay\" : \"method\" } Moq Moq (\"mock-you\") is an open-source mocking library available as a NuGet package. Mock objects are generics that take the abstract base class or interface used by the mocked object (see provider pattern ). Naturally, this means the concrete objects they are replacing must also be implementing those interfaces. There are two mock modes, strict and loose . By default, mock objects are loose, which means they will return default type values and not throw any exceptions to methods that have not been setup. Loose (default) Strict var mock = new Mock < IMockTarget >(); var mock = new Mock < IMockTarget >( MockBehavior . Strict ); Mock properties require setup. mock . Setup ( x => x . Property ). Returns ( \"Hello, world!\" ); Methods of mock objects also require setup using an identical syntax. Concrete arguments can be provided, but preferable is using argument matching . In argument matching, It.IsAny<T> is used like a type declaration to fill the place of any concrete variable used as an argument. ( src ) Argument matching Concrete mock . Setup ( x => x . IsValid ( It . IsAny < string >())). Returns ( true ); mock . Setup ( x => x . IsValid ( \"Hello, world!\" )). Returns ( true ); The mock object exposes an Object property that can be used to test assertions against properties of the mocked object. Assert . Equal ( mock . Object . Property , value ) A mock object's Verify is used to verify that a mocked method was called by the system under test. Verification is specific to the parameters of the mocked method call, and argument matching is available just as it is for setting up mocked methods. Here, the mocked validator, which is passed in to the SUT by dependency injection, must make a call to the validator's Evaluate() method. If the call is removed, the test will fail (\"Expected invocation on the mock at least once, but was never performed...\"). An overload of the Verify method also allows a custom error message to be specified. Another overload can ensure that the mocked method was not called, by passing Times.None after the lambda. The Times struct exposes other members like AtLeastOnce and Between that can specify any imaginable number or range of invocations. Test Custom error message SUT public class StarshipDeploymentShould { [Theory] [InlineData(\"Betelgeuse\")] public void EvaluateStarship ( string destination ) { var mockValidator = new Mock < IStarshipValidator >(); mockValidator . Setup ( x => x . Evaluate ()). Returns ( true ); var mockStarship = new Mock < IStarship >(); var sut = new StarshipDeployment ( mockValidator . Object as IStarshipValidator ); sut . Deploy ( mockStarship . Object as Starship , destination ); mockValidator . Verify ( x => x . Evaluate ()); } } public class StarshipDeploymentShould { [Theory] [InlineData(\"Betelgeuse\")] public void EvaluateStarship ( string destination ) { var mockValidator = new Mock < IStarshipValidator >(); mockValidator . Setup ( x => x . Evaluate ()). Returns ( true ); var mockStarship = new Mock < IStarship >(); var sut = new StarshipDeployment ( mockValidator . Object as IStarshipValidator ); sut . Deploy ( mockStarship . Object as Starship , destination ); mockValidator . Verify ( x => x . Evaluate (), \"Starships should be validated\" ); } } public class StarshipDeployment { public IStarshipValidator StarshipValidator { get ; set ; } public StarshipDeployment ( IStarshipValidator validator ) { StarshipValidator = validator ?? throw new ArgumentNullException ( nameof ( validator )); } public bool ValidateDestination ( string destination ) { return destination . Length > 1 ? true : false ; } public StarshipMission Deploy ( Starship starship , string destination ) { bool destinationValidated = ValidateDestination ( destination ); bool starshipValidated = StarshipValidator . Evaluate (); return destinationValidated && starshipValidated ? new StarshipMission { Starship = starship as Starship , Destination = destination } : throw new ArgumentException (); } } A mocked method can also be setup to throw an exception with the Throw<Exception>() method, a generic method that takes an Exception type. Application design Test-driven development and the requirement to be able to mock data providers has a strong influence on application architecture. Instead of tightly coupling models with a particular data provider (such as an hardcoding, an in-memory database, or parsing a file), the recommended pattern is dependency injection . A data provider that implements an interface is passed as an argument to the controller or viewmodel upon entry. For example, a DataProvider class is used to provide a list of integers on application load implements public interface IDataProvider { IEnumerable < int > LoadAsync (); } public class DataProvider : IDataProvider { async public List < int > LoadAsync () { return await List < int > { 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 , 29 , 31 }; } } A mocked data provider also implementing that interface can then be used in testing. .NET The .NET ecosystem has 3 runtimes , all of which implement the .NET Standard Library and rest on common build tools , languages, and runtime components .NET Framework released in 2002, making it the oldest runtime, and runs only on Windows. Two major components: Common Language Runtime (CLR) runs managed code and performs garbage collection .NET Framework Class Library (also called the Base Class Library ) is composed of many classes, interfaces, and value types .NET Core is cross-platform, open-source, and optimized for performance. Its application host is dotnet.exe Core Common Language Runtime (CoreCLR) is more lightweight than that of .NET Framework, but implements Just-In Time compilation .NET Core Class Library is smaller than (and actually a subset of) that of .NET Framework Mono for Xamarin is used for mobile platforms like IOS, Android, and OS X .NET Standard is a specification of which APIs are available across all these runtimes. It evolved from Portable Class Libraries (PCL) and will eventually replace them. .NET's package manager is NuGet . An assembly can be compiled to EXE or DLL. dotnet Install dotnet-format dotnet tool install -g dotnet-format Install the ASP.NET scaffolding engine dotnet tool install -g dotnet-aspnet-codegenerator Create a new xUnit project named tests dotnet new xunit -n tests Add a project file to a solution dotnet sln add ./Project/Project.csproj Add a project reference to a project dotnet add reference ./path/to/Project.csproj Install a NuGet package and add a PackageReference in the project file Moq System.CommandLine dotnet add package Moq dotnet add package System.CommandLine Run the dotnet try web server that supports .NET Interactive-style markdown: ```cs --source-file ./Program.cs --project ./project.csproj ``` Project files Project files are XML files that describe various metadata to the dotnet compiler. The root node is Project which has two subnodes that collect various information about the project: PropertyGroup can contain various elements that affect project settings: RootNamespace specifies the namespace that contains the Main() method for console applications TargetFramework specifies the targeted CLR framework: net5.0 , netcoreapp3.1 , etc LangVersion C# version: 9.0 , etc Nullable Enable nullable reference types ItemGroup contains references to NuGet packages ( PackageReference ) and other projects ( ProjectReference ). LangVersion Nullable ItemGroup <PropertyGroup> <LangVersion> preview </LangVersion> </PropertyGroup> <PropertyGroup> <Nullable> enable </Nullable> </PropertyGroup> <ItemGroup> <ProjectReference Include= \"/path/to/OtherProject.csproj\" /> <PackageReference Include= \"xunit\" Version= \"2.4.0\" /> </ItemGroup> Adding a reference to another project is also easily accomplished from the command-line. dotnet add project /path/to/OtherProject.csproj Packages NuGet is the official package manager for .NET. NuGet packages required for any project were stored in a XML packages.config file. But projects that use PackageReference may store that information in /obj/project.assets.json. System.CommandLine Prior to System.CommandLine, it had been up to the developer to build a custom solution resolving command-line arguments as an array of strings. Although .NET includes several earlier attempts at solving this problem, none had emerged as a default solution. Similar to Python's argparse , the CommandLine library allows you to construct a RootCommand object that accepts definitions of argument and options. Here, an argument is required: C# Python using System ; using System.CommandLine ; using System.CommandLine.Invocation ; namespace CommandLine { class Program { static int Main ( string [] args ) { var cmd = new RootCommand { new Argument < string >( \"name\" ) }; cmd . Handler = CommandHandler . Create < string >( HandleGreeting ); return cmd . Invoke ( args ); } static void HandleGreeting ( string name = \"world\" ) { Console . WriteLine ( $\"Hello, {name}\" ); } } } import argparse def get_args (): parser = argparse . ArgumentParser ( description = \"Say hello\" ) parser . add_argument ( dest = \"name\" , metavar = \"name\" , default = \"World\" , help = \"Name to greet\" ) return parser . parse_args () def main (): args = get_args () print ( f \"Hello, { args . name } !\" ) if __name__ == \"__main__\" : main () Here, the greeting can be specified with an optional parameter C# Python using System ; using System.CommandLine ; using System.CommandLine.Invocation ; namespace CommandLine { class Program { static int Main ( string [] args ) { var cmd = new RootCommand { new Argument < string >( \"name\" ), //, \"Your name\"), new Option < string? >( new [] { \"--greeting\" , \"-g\" }, \"The greeting to use\" ), }; cmd . Handler = CommandHandler . Create < string , string? >( HandleGreeting ); return cmd . Invoke ( args ); } static void HandleGreeting ( string? greeting , string name ) { Console . WriteLine ( $\"{greeting}, {name}\" ); } } } import argparse def get_args (): parser = argparse . ArgumentParser ( description = \"Say hello\" ) parser . add_argument ( dest = \"name\" , metavar = \"name\" , default = \"World\" , help = \"Name to greet\" ) parser . add_argument ( \"--greeting\" , \"-g\" , dest = \"greeting\" , default = \"Hello\" , help = \"Greeting to use\" ) return parser . parse_args () def main (): args = get_args () print ( f \" { args . greeting } , { args . name } !\" ) if __name__ == \"__main__\" : main () Documentation C# supports documentation comments that can be exported to an XML file, which can then be imported into a static site generator (especially DocFX). Visual Studio can be set to export these comments upon build. SDKs DynamoDB To develop a .NET application using DynamoDB, add the AWSSDK.DynamoDBv2 NuGet package. The AWS Explorer, part of the AWS Toolkit for Visual Studio extension, is also useful for setting up a new table. A user with programmatic access, including an Access Key and Secret Key, is necessary to use the toolkit. ( src ) Both .NET Core and .NET Framework are supported as target frameworks, but .NET Core uses exclusively asynchronous operations. A service client object is formed by instantiating AmazonDynamoDBClient . The exposed method PutItemAsync is used to save an item to a table as a PutItemRequest object. The item itself is provided as a Dictionary in the Item key, but the Dictionary's values are AttributeValue objects, formed with a magic key that determines the data type of the value. String Integer Boolean List new AttributeValue { S = \"Hello, world!\" } new AttributeValue { N = \"3\" } new AttributeValue { BOOL = true } new AttributeValue { L = new List < AttributeValue > { new AttributeValue { S = \"Socrates\" }, new AttributeValue { S = \"Plato\" }, new AttributeValue { S = \"Aristotle\" }, }} using Amazon.DynamoDBv2 ; namespace DynamoDBDemo { public class LowLevelSample { public static async Task ExecuteAsync () { using ( IAmazonDynamoDB ddbClient = new AmazonDynamoDBClient () { await ddbClient . PutItemAsync ( new PutItemRequest { TableName = \"Users\" , Item = new Dictionary < string , AttributeValue > { { \"Id\" , new AttributeValue { S = \"john@doe.com\" } }, { \"String\" , new AttributeValue { /* ... */ } } } }) }) } } } Concurrency Asynchronous programming Consuming APIs: HttpClient Multithreading The Task Parallel Library offers a high-level way to set up multiple threads. A Task represents an asynchronous operation. Task.Run() queues the work passed as the action to run on a different thread in the thread pool. Task.Run<T>() represents an asynchronous operation that returns a specific value type. Task . Run ( () => { // ... }); Objects in other threads will be inaccessible without using an object like Dispatcher in WPF Task . Run ( () => { Dispatcher . Invoke (() => { // ... }); }); To avoid blocking, we can make it asynchronous private async void Search_Click ( object sender , RoutedEventArgs e ) { await Task . Run () => { // ... Dispatcher . Invoke (() => { // ... } } } \ud83d\udcd8 Glossary Assembly A collection of types and resources that are built to work together and form a logical unit of functionality and which form the building blocks of .NET applications. Module A portable executable file (DLL or EXE) consisting of one or more classes and interfaces. Although multiple modules can theoretically compose a single assembly, in practice an assembly and module can be considered one and the same for most .NET applications. Provider pattern A favored development model in .NET, and a form of dependency injection where a class is passed as an argument to another class that uses it for some purpose. The key is that the provider must derive from an abstract base class or an interface to support mocks in unit testing.","title":"C&#35;"},{"location":"Coding/C%23/#c","text":"","title":"C&#35;"},{"location":"Coding/C%23/#to-do","text":"Sort out Events section, in particular the example cited Develop C# implementation of CSV parser. This appears to be harder than it should be, apparently because the object returned by the CsvReader.GetRecords<T>() method is an IEnumerable which is one of the confusing points of the UWP course. YouTube tutorials do not seem helpful..","title":"To-do"},{"location":"Coding/C%23/#variables","text":"","title":"Variables"},{"location":"Coding/C%23/#parsing","text":"Data types can be used as static classes, exposing a TryParse method. Int32 DateTime parsedInt = Int32 . TryParse ( rawInt ); parsedDate = DateTime . TryParse ( rawDate );","title":"Parsing"},{"location":"Coding/C%23/#string","text":"Specify a verbatim literal string by prepending @ , which disables escape characters and forces interpretation of backslashes literally: string filePath = @\"C:\\televisions\\sony\\bravia.txt\" ; Specify a formatted string by prepending a $ int n ; string s = $\"{n} is a number\" ; Standard numeric format strings are used to format common numeric types. They take the form of a character (i.e. C for currency, N for number) followed by a number. They can be passed as arguments to the ToString method of the literal or in the placeholder of a formatted string after : . Console . WriteLine ( $\"{123.456789:C }\" ); // $123.46 Console . WriteLine ( 123.456789d . ToString ( \"C\" )); // $123.46 A precision specifier can define the number of fractional digits after the decimal separator. Console . WriteLine ( $\"{123.456789:C3 }\" ); // $123.457 Empty space can be added to either side of the value to create evenly spaced output by placing a number after a comma (positive for right-alignment, negative for left-alignment): Console . WriteLine ( $\"{123.456789, 15}\" );","title":"String"},{"location":"Coding/C%23/#casting","text":"Because real numbers are stored as double s by default, in order to assign to a float variable you must append f to the literal: float num = 3.14f ; A similar logic pertains for integers to be declared as doubles : double num = 3d ; decimal data type literals, which have 28-29 significant digits, can be declared with the m suffix decimal num = 123.4567890123456789 m char literals can be encoded in Unicode: char umlaut = '\\ u00F6 ' ; Variables can be explicitly cast to some other data types by placing the new data type in parentheses before the value: int pi = ( int ) System . Math . PI ; This casting won't work with string , which can be cast by using the Convert type or parsed using the data type's Parse method. The differense is that using Convert will return a 0 if the value is null while Parse will throw an exception. w = \"5\" ; int wConverted = System . Convert . ToInt32 ( w ); int wParsed = int . Parse ( w );","title":"Casting"},{"location":"Coding/C%23/#collection","text":"","title":"Collection"},{"location":"Coding/C%23/#arrays","text":"Arrays are declared differently from built-in arrays in C++. C# C++ int [] primes = { 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 }; int primes [ 10 ] { 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 } An empty array must still have its size declared int [] primes = new int [ 10 ]; An unnamed array: new [] { 1 , 2 , 3 }; Arrays can be traversed with a foreach loop, but the elements can not be changed.: foreach ( var i in container ) { // ... } Arrays can be copied with the Clone() and Copy() methods: see ArrayCloning . Arrays can be reversed in place with the Reverse method. int [] primes = { 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 }; Array . Reverse ( primes );","title":"Arrays"},{"location":"Coding/C%23/#linq","text":"Language Integrated Query (LINQ) refers to a C# library that facilitates querying of collections. These are exposed as extension methods : methods that are available on already existing queryable types This means extension methods are exposedon existing collection types like Array and List because they are derived from IEnumerable<T> , and thus need no modification to serve as a LINQ data source. Linq methods are available in two semantically identical syntaxes: query syntax and method syntax (also lambda syntax). Query syntax is meant to be more intuitive for developers familiar with SQL. Method syntax allows method chaining. Query syntax Method syntax Python int [] numbers = { 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 }; var evens = from n in nums where n % 2 == 0 orderby n descending select n ; int [] numbers = { 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 }; var evens = nums . Where ( n => n % 2 == 0 ). OrderByDescending ( n => n ); numbers = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] evens = [ n for n in numbers if n % 2 == 0 ] list ( reversed ( evens )) Notably, unlike loop structures in C#, LINQ methods can work on unordered collections like Dictionaries.","title":"LINQ"},{"location":"Coding/C%23/#observablecollection","text":"The ObservableCollection class is used to define collections that provide notifications to data bindings when items are added or removed. As such, it is used in GUI programming...","title":"ObservableCollection"},{"location":"Coding/C%23/#datetime","text":"DateTimeOffset is preferred over DateTime because it includes an offset value that indicates the timezone.","title":"DateTime"},{"location":"Coding/C%23/#parsing_1","text":"DateTime is a class that exposes several static methods of parsing raw values. All of them have overloads that accept CultureInfo objects (implementing IFormatProvider ) which can affect parsing of ambiguous dates. Parse() will attempt to parse a string and raise a FormatException if unable to do so. ParseExact() requires an exact string template and requires a CultureInfo object as well. Parse() Specifying culture ParseExact() string rawDate = \"07/04/1776\" ; try { DateTime parsedDate = DateTime . Parse ( rawDate ); } catch ( FormatException ) { Console . WriteLine ( \"Unparsable!\" ) } Console . WriteLine ( parsedDate . ToLongDateString ()); // => \"July 4, 1776\" string rawDate = \"07/04/1776\" ; try { DateTime parsedDate = DateTime . Parse ( rawDate , CultureInfo . GetCultureInfo ( \"en-GB\" )); } catch ( FormatException ) { Console . WriteLine ( \"Unparsable!\" ) } Console . WriteLine ( parsedDate . ToLongDateString ()); // => \"April 7, 1776\" string rawDate = \"07/04/1776\" ; try { DateTime parsedDate = DateTime . ParseExact ( rawDate , \"M/d/yyyy\" , CultureInfo . InvariantCulture ); } catch ( FormatException ) { Console . WriteLine ( \"Unparsable!\" ) } Console . WriteLine ( parsedDate . ToLongDateString ()); // => \"July 4, 1776\" TryParse() returns no value, but takes an out parameter. It does not throw an exception if the date is unparsable, but rather outputs the default date January 1, 1 AD. The overload that accepts a Culture object also requires a DateTimeStyles object. TryParse() Specifying culture string rawDate = \"07/04/1776\" ; DateTime parsedDate ; DateTime . TryParse ( rawDate , out parsedDate ); Console . WriteLine ( parsedDate . ToLongDateString ()); // => \"July 4, 1776\" string rawDate = \"07/04/1776\" ; DateTime parsedDate ; DateTime . TryParse ( rawDate , CultureInfo . GetCultureInfo ( \"en-GB\" ), DateTimeStyles . None , out parsedDate ); Console . WriteLine ( parsedDate . ToLongDateString ()); // => \"April 7, 1776 ParseExact() TryParseExact()","title":"Parsing"},{"location":"Coding/C%23/#timezones","text":"TimeZoneInfo includes static methods that can access system timezones. DateTime does not include timezone information, so it must be specified at runtime. TimeZoneInfo sidneyTimeZone = TimeZoneInfo . FindSystemTimeZoneById ( \"E. Australia Standard Time\" ); var sydneyTime = TimeZoneInfo . ConvertTime ( DateTime . Now , sydneyTimeZone ); Enumerating all system timezones. foreach ( var timeZone in TimeZoneInfo . GetSystemTimeZones ()) { Console . WriteLine ( timeZone . GetUtcOffset ()); }","title":"Timezones"},{"location":"Coding/C%23/#methods","text":"","title":"Methods"},{"location":"Coding/C%23/#lambda","text":"A lambda expression can have two forms, both of which use the lambda declaration operator => Expression lambda Statement lambda ( input - parameters ) => expression ( input - parameters ) => { statements } Anonymous event handlers can be reformulated as lambdas to reduce code complexity. SubmitButton . Click += delegate ( object sender , EventArgs e ) { MessageBox . Show ( \"Button Clicked\" ); } // Using a (statement) lambda: SubmitButton . Click += ( s , e ) => MessageBox . Show ( \"Button Clicked\" );","title":"Lambda"},{"location":"Coding/C%23/#ref","text":"ref allows variables that are normally passed by value to be passed by reference. Pass by value Pass by reference Returning a value Integers are normally passed by value, so number will not change static void Main () { int number = 0 ; plusOne ( number ); } static void plusOne ( int n ) { n ++; } Now number will increment by one because it is being passed by reference. static void Main () { int number = 0 ; plusOne ( ref number ); } static void plusOne ( ref int n ) { n ++; } Alternatively, number can be reassigned a variable if the method is refactored to return the new value. static void Main () { int number = 0 ; number = plusOne ( number ); } static int plusOne ( ref int n ) { n ++; return n ; }","title":"ref"},{"location":"Coding/C%23/#out","text":"out allows a method to assign a value to a variable that has no value yet. It can be used to return multiple values. static void Main () { double n = 5 ; double nSquared ; square ( n , out nSquared ); Console . WriteLine ( $\"{n} ^ 2 = {nSquared}\" ); } static void square ( double x , out double y ) { y = System . Math . Pow ( x , 2 ); } out is prominently used in the TryParse method.","title":"out"},{"location":"Coding/C%23/#params","text":"params allows you to process a variable number of similarly-typed arguments in the method signature. This collection of arguments is abstracted as an array. static void method ( int [] args ) { foreach ( int el in args ) { Console . WriteLine ( el ); } } If the arguments to be accepted are themselves arrays, then you must define an array of arrays ( ex. ). This technique may not have worked in previous versions of C#. static void method ( int [][] args ) { foreach ( var array in args ){ foreach ( int el in array ) { Console . WriteLine ( el ); } } }","title":"params"},{"location":"Coding/C%23/#delegates","text":"Delegates are a functional programming feature in C# that facilitate loose coupling. They allow a function to be abstracted so that updated logic can be implemented without incurring technical debt. Delegates take the form of a method signature using the delegate keyword. One or more methods implementing the delegate can be formulated which do not reference the delegate in any way, shape, or form, except for the fact that their method signature matches that specified by the delegate. Where the method is to be used, instead of calling the method directly, the delegate is instantiated like an object, but the name of the specific method that implements the delegate is passed as a parameter. The instantiated delegate can then be called, which passes the parameters to the method. This results in looser coupling because when changing implementation, only the parameter specifying the improved method needs to be adjusted, and the delegate ensures that the same pattern of parameters is enforced at compile-time. Delegates can be used for messaging in .NET and especially to tie events to event handlers, but they are no longer used as much as Func<T,TResult> and Action<T> . Initial implementation Improved implementation public delegate void InformationNeeded ( int n , string s ); static void Main () { InformationNeeded form = new InformationNeeded ( SimpleReport ) // ... form ( 2 , \"kiwi\" ); form ( 3 , \"jackfruit\" ); } void SimpleReport ( int m , string t ) { Console . WriteLine ( $\"int: {m}, string: {t}\" ); } public delegate void InformationNeeded ( int n , string s ); static void Main () { InformationNeeded form = new InformationNeeded ( BetterReport ) // ... form ( 2 , \"kiwi\" ); form ( 3 , \"jackfruit\" ); } void BetterReport ( int m , string t ) { Console . WriteLine ( $\"There are {m} items of type {t}\" ); }","title":"Delegates"},{"location":"Coding/C%23/#events","text":"Events signal the occurrence of an action or notification. They are raised or fired (invoked) by the publisher and received by the event handler or subscriber . They represent a syntactic sugar over the delegate structure, which is used in the background as the pipeline to connect publisher and handler. The simplest way to define an event, using the builtin EventHandler type, is as follows: public event EventHandler Occurrence ; In actuality, EventHandler is itself a wrapper around a delegate, and any delegate can be wrapped by the event by the delegate's name as the event's data type: public delegate void InformationNeeded ( int n , string s ); class Form { public event InformationNeeded FormEvent ; } But because the event structure requires an object reference, the simplest implementation for raising an event is more involved. This is because the Main entry-point for C# programs is static, and not an object instance. The event must be defined within a class that is then instantiated. The event is implemented in an event handler that is a method within the same class that defines the event. After first checking if the event is null (abbreviated syntax using the null-conditional member access operator is equivalent) the event object is called. Here, the event handler is called by the constructor itself. namespace SimpleEvent { class Program { static void Main ( string [] args ) { TriggeringEvent eventTrigger = new TriggeringEvent (); } public class TriggeringEvent { public event EventHandler Event ; public TriggeringEvent () { OnEvent ( this , EventArgs . Empty ); } protected virtual void OnEvent ( object s , EventArgs e ) { var newEvent = Event as EventHandler ; if ( newEvent != null ) { newEvent ( this , EventArgs . Empty ); } // Null-conditional operator available since C# 6: // newEvent?.Invoke(this, EventArgs.Empty); } } } } If the method signature of the event handler is made public , then the event can be raised externally and called like any other method, and a slightly simpler example can be constructed. namespace SimpleEvent { class Program { static void Main ( string [] args ) { TriggeringEvent eventTrigger = new TriggeringEvent (); eventTrigger . OnEvent ( eventTrigger , EventArgs . Empty ); } public class TriggeringEvent { public event EventHandler Event ; public virtual void OnEvent ( object s , EventArgs e ) { Console . WriteLine ( \"OnEvent\" ); var newEvent = Event as EventHandler ; newEvent ?. Invoke ( this , EventArgs . Empty ); } } } } Conventionally, however, the event handler is not made public, but defined using the protected virtual void method signature. Event wiring refers to the process of adding subscribers to an event. In implementation, this involves adding the subscribers to the invocation list of the delegate that is used to tie the event to event handler. Event += EventSubscriber ; In actuality, this syntax uses delegate inference , where the compiler automatically determines the correct delegate to use. The fuller syntax avoiding the use of this feature would be Event += new EventHandler ( EventSubscriber ); The event is then fired by calling it, but this can only occur from within the type in which it is defined. So it has to be fired from within another of that type's methods. Anonymous methods and lambdas can also be used after the += operator: Event += ( s , e ) => Console . WriteLine ( \"Subscribing to event!\" ); In this example , adapted from a Pluralsight course, the OnMissionAccomplished and OnMissionStatusReport event handlers send two different types of events, respectively: MissionStatusReport and MissionAccomplished . Even though both of these events are EventHandler types, they are actually events.","title":"Events"},{"location":"Coding/C%23/#async","text":"The async modifier is used to construct asynchronous code. By convention, asynchronous methods are named with \"Async\" to distinguish them. The await keyword marks the variable containing the result. public int Addition () { var a = SlowMethodOne (); var b = SlowMethodTwo (); return a + b ; } public async Task < int > AdditionAsync () { var a = SlowMethodOneAsync (); var b = SlowMethodTwoAsync (); return await a + await b ; } Return types used for async include: - Task - Task<T> - Void should generally be avoided with the exception of event handlers Async does not create new threads by default, so it is only suitable for UI and IO-bound methods, not CPU-bound methods. Here async is used to return an enumerable collection of Customer objects from the file IO system . public class CustomerDataProvider { private static readonly string _customersFileName = \"customers.json\" ; private static readonly StorageFolder _localFolder = ApplicationData . Current . LocalFolder ; public async Task < IEnumerable < Customer >> LoadCustomersAsync () { var storageFile = await _localFolder . TryGetItemAsync ( _customersFileName ) as StorageFile ; List < Customer > customerList = null ; // ... } } Here threads are used to handle JSON files and data on application load in the data provider for a GUI application LoadCustomersAsync SaveCustomersAsync public async Task < IEnumerable < Customer >> LoadCustomersAsync () { } public async Task SaveCustomersAsync ( IEnumerable < Customer > customers ) { }","title":"Async"},{"location":"Coding/C%23/#exceptions","text":"Exceptions expose Message and StackTrace attributes that can be inspected for further information (ref. ExceptionHandling ) Member access operators are syntactic sugars that allow operations to be performed without exception handling. Operator Name Description => Lambda declaration operator ?. Null-conditional member access operator Applies the operation to its operand only if it evaluates to non-null, otherwise it returns null ?[] Null-conditional element access operator Applies the operation to its operand only if it evaluates to non-null, otherwise it returns null ?? Null-coalescing operator Returns value of left-hand operand if non-null, otherwise returns result of right-hand operand. ??= Null-coalescing assignment operator Assigns the value of the right-hand operand to the left-hand operand, only if the left-hand operand evaluates to null .","title":"Exceptions"},{"location":"Coding/C%23/#classes","text":"","title":"Classes"},{"location":"Coding/C%23/#access-modifiers","text":"Classes can be declared with various access modifiers that affect the compiler's behavior. These are intended to prevent what would be runtime errors by turning them into compile-time errors, improving code quality. - static prevents instantiation - abstract indicates the class is to be completed in a derived class. Every method marked as abstract has to be implemented in the derived class, and the class has to be marked with abstract as well. - sealed prevents inheritance - partial allows the same class to be defined across multiple files","title":"Access modifiers"},{"location":"Coding/C%23/#constructor","text":"If not defined, the compiler will provide a default constructor. A constructor can be overloaded by using the this keyword in the constructor's signature after a colon, as if invoking the second constructor: using System ; class Car { public string brand { get ; set ; } public Car () : this ( \"Ford\" ) { } public Car ( string brand ) { this . brand = brand ; } } class Program { static void Main ( string [] args ) { Car ford = new Car (); Console . WriteLine ( ford . brand ); // => Ford } }","title":"Constructor"},{"location":"Coding/C%23/#properties","text":"A property protects the data of a private variable (\"field\") by implementing getter and setter accessor functions. These allow data validation or other logic to be performed when the variable is changed. The private variable being protected is called the backing store . By convention, properties have identifiers in title case. The identifier for the backing field of a property is conventionally the same as the property, except lowercase or prepended with an underscore. In the set accessor, the keyword value is used for the argument passed in. private string name ; // field public string Name // property { get { return name ; } set { this . name = value ; } } A common shorthand was introduced in C# 3 called automatically implemented properties , where the p public string Name { get ; set ; } The set accessor uses an implicit parameter value , whose value is the type of the property. class Person { private string _name ; // the name field public string Name // the Name property { get => _name ; set => _name = value ; } } Data validation for setter accessor: public class Date { private int _month = 7 ; // Backing store public int Month { get => _month ; set { if (( value > 0 ) && ( value < 13 )) { _month = value ; } } } } A property can be made read-only by simply removing the setter. An access modifier can also be applied to only one or the other of the accessors to enforce encapsulation. This can make the property read-only externally while still allowing the class's own logic to change the property's value: private set { if (( value > 0 ) && ( value < 13 )) { _month = value ; } } Similarly, fields can be modified with the readonly access modifier. This will prevent the variable from being changed in external code as well as in any internal methods. Readonly fields can only be set by the constructor or variable initializers.","title":"Properties"},{"location":"Coding/C%23/#static-classes","text":"Classes marked with static are not instantiated. An example is the System.Console class, which is never instantiated even though its methods are available for use. This structure is called a singleton and is useful as a container for assorted utilities. Methods marked with static are independent of the class instance itself, and as such do not have access to fields that are not const .","title":"Static classes"},{"location":"Coding/C%23/#polymorphism","text":"Modifiers like abstract , virtual , and override allow derived classes to implement logic that builds upon that of a base class. virtual allows you to declare methods and properties in a base class which can be overriden in a derived class. virtual cannot be used with static , abstract , private , or override . abstract is similar, except that the class itself must also be marked as an abstract class, preventing instantiation of the base class. Instead of defining a base function, only the signature is declared. In both cases, override is used to mark the implementation in the derived class. Base class ( abstract ) Derived class Derived class abstract class Shape { public abstract double GetArea (); } class Shape { public virtual double GetArea () { return ; } } class Rectangle : Shape { public double Length { get ; set ; } public double Width { get ; set ; } public Rectangle () { Length = 2 ; Width = 3 ; } public override double GetArea () { return Length * Width ; } } class Circle : Shape { public double Radius { get ; set ; } public Circle () { Radius = 3 ; } public override double GetArea () { return System . Math . PI * System . Math . Pow ( Radius , 2 ); } }","title":"Polymorphism"},{"location":"Coding/C%23/#interfaces","text":"Interfaces can be used to break up dependencies and implement the dependency inversion principle . This principle holds that components should be dependent on abstractions, and not on implementations. ( src ) Interfaces contain property and method definitions that must be implemented in derived classes, and as such are similar in concept to abstract classes. Like abstract classes, an interface may not be instantiated. Unlike abstract classes, the override keyword is not used on classes that implement interfaces, and access modifiers are not acceptable for interface members. Also unlike abstract classes, smplementation of interface members is mandatory. And although a derived class can only inherit from a single base class, there is no limit on the number of interfaces that a derived class can inherit from. Interface identifiers conventionally with the capital I . Interface Implementation interface IAnimal { void AnimalSound (); } class Pig : IAnimal { public void AnimalSound () { Console . WriteLine ( \"Oink\" ); } } Notably, a commonly encountered interface is IEnumerable because both Lists and Arrays implement it. So methods that iterate over either Lists or Arrays typically use IEnumerable to accept either data type.","title":"Interfaces"},{"location":"Coding/C%23/#attributes","text":"Attributes appear to resemble Python decorators because like decorators appear on the line preceding a function or class definition, but they appear to be used for something else. Attributes in C# are used to adjust the function of code in a variety of ways. ObsoleteAttribute will produce a compiler warning or error (preventing compilation entirely) when deprecated code is being used. Warning Error [Obsolete(\"Don't use this class anymore, instead use ...\")] class Cow { } static void Main () { Cow betsy = new Cow (); } [Obsolete(\"Don't use this class anymore\", true)] class Cow { } static void Main () { Cow betsy = new Cow (); } A family of attributes exist to assist debugging. DebuggerStepThrough DebuggerDisplay DebuggerStepThrough can decorate certain methods to be stepped through or skipped while debugging. This is useful for situations where only some properties of a class have to be debugged. This allows more controlled debugging than using the \"Step over properties and operators\" setting in Debugging Options. ( src ) using System.Diagnostics ; struct Cow { public string Name { [ DebuggerStepThrough ] get { return \"Bessy\" ; } } public int Weight { get { return 5 ; } } } static void Main () { Cow betsy = new Cow (); } DebuggerDisplayAttribute allows an object's state to be formatted to be more understandable in the debugger's watch window. ( src ) using System.Diagnostics ; [DebuggerDisplay(\"{Name} weighs {Weight} lbs\")] struct Cow { public string Name ; public int Weight ; } class Program { static void Main ( string [] args ) { Console . WriteLine ( \"Hello World!\" ); // This triggers instantiation of the attribute typeof ( Program ). GetCustomAttributes ( false ); Cow betsy = new Cow { Name = \"Betsy\" , Weight = 1000 }; Console . WriteLine ( $\"{betsy.Name} weighs {betsy.Weight} lbs\" ); } } The CallerMemberNameAttribute can be added to string parameters in functions that are meant to process the name of the function calling them. This avoids the verbosity of placing nameof() on every invocation. ( src ) using System.ComponentModel ; using System.Runtime.CompilerServices ; namespace WiredBrainCoffee.UWP.Models { public class Customer : INotifyPropertyChanged { private string firstName ; public string FirstName { get => firstName ; set { firstName = value ; OnPropertyChanged ( nameof ( FirstName )); } } public string LastName { get ; set ; } public bool IsCoffeeDrinker { get ; set ; } public event PropertyChangedEventHandler PropertyChanged ; private void OnPropertyChanged ([ CallerMemberName ] string propertyName = null ) { PropertyChanged ?. Invoke ( this , new PropertyChangedEventArgs ( propertyName )); } } } In XAML, the ContentPropertyAttribute attribute is used to define whether or not a control accepts a default Content property field using Windows.UI.Xaml ; using Windows.UI.Xaml.Controls ; using Windows.UI.Xaml.Markup ; [ContentProperty(Name = nameof(Customer))] public sealed partial class CustomerDetailControl : UserControl { public CustomerDetailControl () { this . InitializeComponent (); } }","title":"Attributes"},{"location":"Coding/C%23/#files","text":"","title":"Files"},{"location":"Coding/C%23/#streams","text":"A Stream is an abstraction of a backing store , or sequence of bytes, which can be a file, an input/output device, a websocket, or an inter-process communication pipe. The Stream class itself is an abstract base class that can't be instantiated. FileStream is the concrete class that uses files as its backing store. Streams can support seeking, although network streams do not support seeking. This can be checked by calling the stream's boolean CanSeek property. Stream implements IDisposable , which means it can be disposed indirectly by being placed in a using block. A bit bucket is a stream with no backing store and is implemented as Stream.Null .","title":"Streams"},{"location":"Coding/C%23/#testing","text":"Tests are usually organized in a separate project that is linked to the project containing the system under test (SUT). Visual Studio has a built-in test-runner, but the dotnet CLI utility also allows the entire test suite to be executed from the command-line. dotnet test .NET supports several test frameworks.","title":"Testing"},{"location":"Coding/C%23/#xunit","text":"In xUnit , tests are organized into public classes, and test cases are composed by individual methods on this class, decorated with the Fact attribute. Test assertions are made with static Assert method calls. ( src ) public class TestCases { [Fact] public void TestCase () { Assert . Equal ( 2 + 2 , 4 ); } } Assertions that an exception must be thrown are generic method calls typed to the specific exception. public class StarshipDeploymentShould { [Fact] public void ThrowOnNullValidator () { var sut = new StarshipDeployment ( null ); Assert . Throws < ArgumentNullException >( sut ); } } Test fixtures can be formed on properties of the main test class. They must be initialized with the test class's constructor. Xunit Classes under test public class DeskBookerRequestProcessorTests { public DeskBookerRequestProcessor processor { get ; set ; } public DeskBookerRequestProcessorTests () { processor = new DeskBookerRequestProcessor (); } [Fact] public void ShouldReturnDeskBookerResultWithRequestValues () { var request = new DeskBookerRequest { FirstName = \"Thomas\" , LastName = \"Huber\" , Email = \"thomas@huber.com\" , Date = new DateTime ( 2020 , 1 , 28 ) }; var result = processor . BookDesk ( request ); Assert . NotNull ( result ); Assert . Equal ( request . FirstName , result . FirstName ); Assert . Equal ( request . LastName , result . LastName ); Assert . Equal ( request . Email , result . Email ); Assert . Equal ( request . Date , result . Date ); } [Fact] public void ShouldThrowExceptionIfRequestIsNull () { var exception = Assert . Throws < ArgumentNullException >(() => processor . BookDesk ( null )); Assert . Equal ( \"request\" , exception . ParamName ); } } namespace DeskBooker.Core.Processor { public class DeskBookingResult { public string FirstName { get ; set ; } public string LastName { get ; set ; } public DateTime Date { get ; set ; } } public class DeskBookingRequest { public string FirstName { get ; set ; } public string LastName { get ; set ; } public DateTime Date { get ; set ; } } public class DeskBookingRequestProcessor { public DeskBookingResult BookDesk ( DeskBookingRequest request ) { return new DeskBookingResult { FirstName = request . FirstName , LastName = request . LastName , Date = request . Date , }; } } } In this example, PersonProcessor is a public class whose constructor takes an ISqlDataAccess data provider by means of dependency injection. The LoadData method is setup, and the mocked object is instantiated with the Create method call. The mock will inject the mock data provider, which returns a List<PersonModel> . using ( var mock = AutoMock . GetLoose ()) { mock . Mock < ISqliteDataAccess >() . Setup ( x => x . LoadData < PersonModel >( \"SELECT * FROM Person\" )) . Returns ( GetSamplePeople ()); var sut = mock . Create < PersonProcessor >(); var expected = GetSamplePeople (); var actual = sut . LoadPeople (); Assert . True ( actual != null ); Assert . Equal ( actual . Count , expected . Count ); } The Theory attribute decorates a parameterized test and commonly appears in conjunction with InlineData attributes that contain the parameter values. using System ; using Xunit ; using System.Linq ; namespace MathTests { public class MathWorks { [Theory] [InlineData(2,2)] [InlineData(3,3,3)] [InlineData(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)] public void Addition ( params int [] ops ) { int loopsum = 0 ; foreach ( int i in ops ) { loopsum += i ; } int linqsum = ops . Sum (); Assert . Equal ( loopsum , linqsum ); } } } The xUnit test-runner can be modified using a JSON file named xunit.runner.json. This file must be copied to the output directory by selecting \"Copy if newer\" in the file's properties. This example will display the method names only, rather than the fully-qualified dotted name with namespace and class. { \"methodDisplay\" : \"method\" }","title":"xUnit"},{"location":"Coding/C%23/#moq","text":"Moq (\"mock-you\") is an open-source mocking library available as a NuGet package. Mock objects are generics that take the abstract base class or interface used by the mocked object (see provider pattern ). Naturally, this means the concrete objects they are replacing must also be implementing those interfaces. There are two mock modes, strict and loose . By default, mock objects are loose, which means they will return default type values and not throw any exceptions to methods that have not been setup. Loose (default) Strict var mock = new Mock < IMockTarget >(); var mock = new Mock < IMockTarget >( MockBehavior . Strict ); Mock properties require setup. mock . Setup ( x => x . Property ). Returns ( \"Hello, world!\" ); Methods of mock objects also require setup using an identical syntax. Concrete arguments can be provided, but preferable is using argument matching . In argument matching, It.IsAny<T> is used like a type declaration to fill the place of any concrete variable used as an argument. ( src ) Argument matching Concrete mock . Setup ( x => x . IsValid ( It . IsAny < string >())). Returns ( true ); mock . Setup ( x => x . IsValid ( \"Hello, world!\" )). Returns ( true ); The mock object exposes an Object property that can be used to test assertions against properties of the mocked object. Assert . Equal ( mock . Object . Property , value ) A mock object's Verify is used to verify that a mocked method was called by the system under test. Verification is specific to the parameters of the mocked method call, and argument matching is available just as it is for setting up mocked methods. Here, the mocked validator, which is passed in to the SUT by dependency injection, must make a call to the validator's Evaluate() method. If the call is removed, the test will fail (\"Expected invocation on the mock at least once, but was never performed...\"). An overload of the Verify method also allows a custom error message to be specified. Another overload can ensure that the mocked method was not called, by passing Times.None after the lambda. The Times struct exposes other members like AtLeastOnce and Between that can specify any imaginable number or range of invocations. Test Custom error message SUT public class StarshipDeploymentShould { [Theory] [InlineData(\"Betelgeuse\")] public void EvaluateStarship ( string destination ) { var mockValidator = new Mock < IStarshipValidator >(); mockValidator . Setup ( x => x . Evaluate ()). Returns ( true ); var mockStarship = new Mock < IStarship >(); var sut = new StarshipDeployment ( mockValidator . Object as IStarshipValidator ); sut . Deploy ( mockStarship . Object as Starship , destination ); mockValidator . Verify ( x => x . Evaluate ()); } } public class StarshipDeploymentShould { [Theory] [InlineData(\"Betelgeuse\")] public void EvaluateStarship ( string destination ) { var mockValidator = new Mock < IStarshipValidator >(); mockValidator . Setup ( x => x . Evaluate ()). Returns ( true ); var mockStarship = new Mock < IStarship >(); var sut = new StarshipDeployment ( mockValidator . Object as IStarshipValidator ); sut . Deploy ( mockStarship . Object as Starship , destination ); mockValidator . Verify ( x => x . Evaluate (), \"Starships should be validated\" ); } } public class StarshipDeployment { public IStarshipValidator StarshipValidator { get ; set ; } public StarshipDeployment ( IStarshipValidator validator ) { StarshipValidator = validator ?? throw new ArgumentNullException ( nameof ( validator )); } public bool ValidateDestination ( string destination ) { return destination . Length > 1 ? true : false ; } public StarshipMission Deploy ( Starship starship , string destination ) { bool destinationValidated = ValidateDestination ( destination ); bool starshipValidated = StarshipValidator . Evaluate (); return destinationValidated && starshipValidated ? new StarshipMission { Starship = starship as Starship , Destination = destination } : throw new ArgumentException (); } } A mocked method can also be setup to throw an exception with the Throw<Exception>() method, a generic method that takes an Exception type.","title":"Moq"},{"location":"Coding/C%23/#application-design","text":"Test-driven development and the requirement to be able to mock data providers has a strong influence on application architecture. Instead of tightly coupling models with a particular data provider (such as an hardcoding, an in-memory database, or parsing a file), the recommended pattern is dependency injection . A data provider that implements an interface is passed as an argument to the controller or viewmodel upon entry. For example, a DataProvider class is used to provide a list of integers on application load implements public interface IDataProvider { IEnumerable < int > LoadAsync (); } public class DataProvider : IDataProvider { async public List < int > LoadAsync () { return await List < int > { 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 , 29 , 31 }; } } A mocked data provider also implementing that interface can then be used in testing.","title":"Application design"},{"location":"Coding/C%23/#net","text":"The .NET ecosystem has 3 runtimes , all of which implement the .NET Standard Library and rest on common build tools , languages, and runtime components .NET Framework released in 2002, making it the oldest runtime, and runs only on Windows. Two major components: Common Language Runtime (CLR) runs managed code and performs garbage collection .NET Framework Class Library (also called the Base Class Library ) is composed of many classes, interfaces, and value types .NET Core is cross-platform, open-source, and optimized for performance. Its application host is dotnet.exe Core Common Language Runtime (CoreCLR) is more lightweight than that of .NET Framework, but implements Just-In Time compilation .NET Core Class Library is smaller than (and actually a subset of) that of .NET Framework Mono for Xamarin is used for mobile platforms like IOS, Android, and OS X .NET Standard is a specification of which APIs are available across all these runtimes. It evolved from Portable Class Libraries (PCL) and will eventually replace them. .NET's package manager is NuGet . An assembly can be compiled to EXE or DLL.","title":".NET"},{"location":"Coding/C%23/#dotnet","text":"Install dotnet-format dotnet tool install -g dotnet-format Install the ASP.NET scaffolding engine dotnet tool install -g dotnet-aspnet-codegenerator Create a new xUnit project named tests dotnet new xunit -n tests Add a project file to a solution dotnet sln add ./Project/Project.csproj Add a project reference to a project dotnet add reference ./path/to/Project.csproj Install a NuGet package and add a PackageReference in the project file Moq System.CommandLine dotnet add package Moq dotnet add package System.CommandLine Run the dotnet try web server that supports .NET Interactive-style markdown: ```cs --source-file ./Program.cs --project ./project.csproj ```","title":"dotnet"},{"location":"Coding/C%23/#project-files","text":"Project files are XML files that describe various metadata to the dotnet compiler. The root node is Project which has two subnodes that collect various information about the project: PropertyGroup can contain various elements that affect project settings: RootNamespace specifies the namespace that contains the Main() method for console applications TargetFramework specifies the targeted CLR framework: net5.0 , netcoreapp3.1 , etc LangVersion C# version: 9.0 , etc Nullable Enable nullable reference types ItemGroup contains references to NuGet packages ( PackageReference ) and other projects ( ProjectReference ). LangVersion Nullable ItemGroup <PropertyGroup> <LangVersion> preview </LangVersion> </PropertyGroup> <PropertyGroup> <Nullable> enable </Nullable> </PropertyGroup> <ItemGroup> <ProjectReference Include= \"/path/to/OtherProject.csproj\" /> <PackageReference Include= \"xunit\" Version= \"2.4.0\" /> </ItemGroup> Adding a reference to another project is also easily accomplished from the command-line. dotnet add project /path/to/OtherProject.csproj","title":"Project files"},{"location":"Coding/C%23/#packages","text":"NuGet is the official package manager for .NET. NuGet packages required for any project were stored in a XML packages.config file. But projects that use PackageReference may store that information in /obj/project.assets.json.","title":"Packages"},{"location":"Coding/C%23/#systemcommandline","text":"Prior to System.CommandLine, it had been up to the developer to build a custom solution resolving command-line arguments as an array of strings. Although .NET includes several earlier attempts at solving this problem, none had emerged as a default solution. Similar to Python's argparse , the CommandLine library allows you to construct a RootCommand object that accepts definitions of argument and options. Here, an argument is required: C# Python using System ; using System.CommandLine ; using System.CommandLine.Invocation ; namespace CommandLine { class Program { static int Main ( string [] args ) { var cmd = new RootCommand { new Argument < string >( \"name\" ) }; cmd . Handler = CommandHandler . Create < string >( HandleGreeting ); return cmd . Invoke ( args ); } static void HandleGreeting ( string name = \"world\" ) { Console . WriteLine ( $\"Hello, {name}\" ); } } } import argparse def get_args (): parser = argparse . ArgumentParser ( description = \"Say hello\" ) parser . add_argument ( dest = \"name\" , metavar = \"name\" , default = \"World\" , help = \"Name to greet\" ) return parser . parse_args () def main (): args = get_args () print ( f \"Hello, { args . name } !\" ) if __name__ == \"__main__\" : main () Here, the greeting can be specified with an optional parameter C# Python using System ; using System.CommandLine ; using System.CommandLine.Invocation ; namespace CommandLine { class Program { static int Main ( string [] args ) { var cmd = new RootCommand { new Argument < string >( \"name\" ), //, \"Your name\"), new Option < string? >( new [] { \"--greeting\" , \"-g\" }, \"The greeting to use\" ), }; cmd . Handler = CommandHandler . Create < string , string? >( HandleGreeting ); return cmd . Invoke ( args ); } static void HandleGreeting ( string? greeting , string name ) { Console . WriteLine ( $\"{greeting}, {name}\" ); } } } import argparse def get_args (): parser = argparse . ArgumentParser ( description = \"Say hello\" ) parser . add_argument ( dest = \"name\" , metavar = \"name\" , default = \"World\" , help = \"Name to greet\" ) parser . add_argument ( \"--greeting\" , \"-g\" , dest = \"greeting\" , default = \"Hello\" , help = \"Greeting to use\" ) return parser . parse_args () def main (): args = get_args () print ( f \" { args . greeting } , { args . name } !\" ) if __name__ == \"__main__\" : main ()","title":"System.CommandLine  "},{"location":"Coding/C%23/#documentation","text":"C# supports documentation comments that can be exported to an XML file, which can then be imported into a static site generator (especially DocFX). Visual Studio can be set to export these comments upon build.","title":"Documentation"},{"location":"Coding/C%23/#sdks","text":"","title":"SDKs"},{"location":"Coding/C%23/#dynamodb","text":"To develop a .NET application using DynamoDB, add the AWSSDK.DynamoDBv2 NuGet package. The AWS Explorer, part of the AWS Toolkit for Visual Studio extension, is also useful for setting up a new table. A user with programmatic access, including an Access Key and Secret Key, is necessary to use the toolkit. ( src ) Both .NET Core and .NET Framework are supported as target frameworks, but .NET Core uses exclusively asynchronous operations. A service client object is formed by instantiating AmazonDynamoDBClient . The exposed method PutItemAsync is used to save an item to a table as a PutItemRequest object. The item itself is provided as a Dictionary in the Item key, but the Dictionary's values are AttributeValue objects, formed with a magic key that determines the data type of the value. String Integer Boolean List new AttributeValue { S = \"Hello, world!\" } new AttributeValue { N = \"3\" } new AttributeValue { BOOL = true } new AttributeValue { L = new List < AttributeValue > { new AttributeValue { S = \"Socrates\" }, new AttributeValue { S = \"Plato\" }, new AttributeValue { S = \"Aristotle\" }, }} using Amazon.DynamoDBv2 ; namespace DynamoDBDemo { public class LowLevelSample { public static async Task ExecuteAsync () { using ( IAmazonDynamoDB ddbClient = new AmazonDynamoDBClient () { await ddbClient . PutItemAsync ( new PutItemRequest { TableName = \"Users\" , Item = new Dictionary < string , AttributeValue > { { \"Id\" , new AttributeValue { S = \"john@doe.com\" } }, { \"String\" , new AttributeValue { /* ... */ } } } }) }) } } }","title":"DynamoDB"},{"location":"Coding/C%23/#concurrency","text":"","title":"Concurrency"},{"location":"Coding/C%23/#asynchronous-programming","text":"Consuming APIs: HttpClient","title":"Asynchronous programming"},{"location":"Coding/C%23/#multithreading","text":"The Task Parallel Library offers a high-level way to set up multiple threads. A Task represents an asynchronous operation. Task.Run() queues the work passed as the action to run on a different thread in the thread pool. Task.Run<T>() represents an asynchronous operation that returns a specific value type. Task . Run ( () => { // ... }); Objects in other threads will be inaccessible without using an object like Dispatcher in WPF Task . Run ( () => { Dispatcher . Invoke (() => { // ... }); }); To avoid blocking, we can make it asynchronous private async void Search_Click ( object sender , RoutedEventArgs e ) { await Task . Run () => { // ... Dispatcher . Invoke (() => { // ... } } }","title":"Multithreading"},{"location":"Coding/C%23/#glossary","text":"Assembly A collection of types and resources that are built to work together and form a logical unit of functionality and which form the building blocks of .NET applications. Module A portable executable file (DLL or EXE) consisting of one or more classes and interfaces. Although multiple modules can theoretically compose a single assembly, in practice an assembly and module can be considered one and the same for most .NET applications. Provider pattern A favored development model in .NET, and a form of dependency injection where a class is passed as an argument to another class that uses it for some purpose. The key is that the provider must derive from an abstract base class or an interface to support mocks in unit testing.","title":"\ud83d\udcd8 Glossary"},{"location":"Coding/C%2B%2B/","text":"Preprocessing Preprocessing is the phase of executing preprocessing directives in a source file, which are then removed from the resulting translation unit , which combines the pure C or C++ code of a source file with all its included header files. The translation unit is then compiled into an object file , and it is the linker that then forms linkages between object files to produce an executable module. #define #include The #define directive specifies a macro which can define a text replacement to occur in code before it is compiled. Macros are considered a holdover from C, and other constructs like variable templates and function templates are considered better suited in C++. In practice, #define statements are most commonly used to handle header files. Here, any instance of \" PI \" in the source code will be replaced by the string of digits \"3.14159265\", but it will not be replaced if it forms part of an identifier or appears in a string literal or comment. #define PI 3.14159265 If a substitution string isn't specified then any instance of the identifier will simply be removed. #define break Macros may not span multiple lines without escaped line breaks, but during preprocessing these are removed and the substitution is made inline. #define PI 3.14\\ 159265 Function-like macros are possible because the preprocessor can recognize a function call in the macro identifier and replace its arguments intelligently. Here any invocation of the MAX() function call will have its arguments incorporated into the substitution statement. #define MAX(A, B) A >= B ? A : B All the lines following #ifndef ( #if !defined ) will be kept in the file as long as the identifier \"MYHEADER_H\" has not already been defined. This common is called an #include guard . #ifndef MYHEADER_H #define MYHEADER_H // ... #endif Variables Since C++14 , you can separate digits in a long integer with a single quote to make it more readable. int num { 12'345 }; // 12,345 Hexadecimal literals are prefixed with \"0x\" and octals with \"0\": int hex { 0xabcdef }; int oct { 0567 }; constexpr is used in some situations I can't figure out yet. static constexpr u32 MAX_MEM = 1024 * 64 ; size_t is a type alias defined in the Standard Library (in the cstddef header). It is an alias for an unsigned integer type. Initialization A braced initializer refers to placing the initial value of a variable in braces. This is a novel style of initialization introduced in C++11 . Its main advantage is that it will raise a compile-time error if the compiler has to perform a narrowing conversion of the value to match the declared type. int apples { 15 }; Older but equally valid ways of initializing variables: int oranges = 12 ; int kiwis ( 13 ); // \"functional notation\" Zero initialization refers to initializing a variable with empty braces. It works for any fundamental type, and numeric types initialize to zero. int grapes {}; // 0 Sequences, like this array class can be efficiently zero-initialized; the braces can contain any number of values up to the declared size of the array (remaining values will be zero-initialized). #include <array> array < int , 5 > myIntArray {}; Multi-dimensional arrays can be initialized with nested initializers: int myNums [ 2 ][ 3 ] { { 1 , 2 , 3 }, { 4 , 5 , 6 } }; Pointers Smart pointers (also called managed pointers ) are pointers that manage their own memory. They were introduced in C++11, and there is no longer a reason to use the earlier raw pointers . In older versions of C++, memory leaks were common because the programmer had to remember to release memory allocated dynamically from the heap/free store using the delete keyword. These older pointers are now called raw pointers The most commonly used smart pointer is unique_ptr : others include shared_ptr and weak_ptr . Only a single unique_ptr<T> can point to any memory address, unless ownership is transfered with move() . Since C++14, it is recommended to create unique pointers using makeunique<T>() . auto pdbl = make_unique < dbouble > (); When variables are declared with an asterisk * appended to the datatype or prepended to the identifier, the variable becomes a pointer to that type. Pointer identifiers usually begin with \"p\", a convention known as Hungarian notation . The size of pointers corresponds to the address space of available memory (4 bytes for 32-bit architectures, and 8 bytes for 64-bit). // The following statements are equivalent. long * pnum {}; long * pnum { nullptr }; void * is known as \"pointer to void type\", meaning variables defined as such are pointers to data of an unspecified type, making it similar to var in C#. The address-of operator & obtains the address of a variable. The address-of operator typically also occurs with the indirection operator or dereference operator (also * ) to access the data pointed to by a pointer. Using a dereferenced pointer is the same as using the variable to which it points. long num { 12345L }; long * pnum { & num }; long newnum { * pnum + 1 }; When a pointer points to an object with methods, like a vector<T> container , the indirect member selection operator ( -> ) can be used to access the methods. // The following statements are equivalent. auto * pdata { new std :: vector < int > {}}; std :: vector < int > data ; auto * pdata = & data ; // The following statements are equivalent. ( * pdata ). push_back ( 66 ); pdata -> push_back ( 66 ); Pointers to classes can be recast with the following syntax: Animal * ptr = & kitty ; (( Cat * ) ptr ) -> chaseMouse (); // newer, safer syntax ( reinterpret_cast < Cat > ( ptr )) -> chaseMouse (); Containers Containers are a type of data structure used to contain elements for various purposes. They are deeply tied to algorithms through iterators . Two array-like data structures defined in the Standard Library that are more typically used are array<T,N> and vector<T> Arrays An array is a variable that represents a contiguous sequence of memory locations, each storing an item of data of the same data type, each of which are called elements . Arrays must be declared with a constant integer expression that is fixed at compile time. Built-in arrays in C++ are inherited from C. int primes [ 10 ] { 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 } Sequence containers The two most common sequence containers are array<T,N> and vector<T> . All sequence containers expose several of a family of related member functions: Member function vector array list forward_list deque front() \u2705 \u2705 \u2705 \u2705 \u2705 back() \u2705 \u2705 \u2705 \u274c \u2705 push_front() \u274c \u274c \u2705 \u2705 \u2705 pop_front() \u274c \u274c \u2705 \u2705 \u2705 push_back() \u2705 \u274c \u2705 \u274c \u2705 pop_back() \u2705 \u274c \u2705 \u274c \u2705 insert() \u2705 \u274c \u2705 \u2705 \u2705 erase() \u2705 \u274c \u2705 \u2705 \u2705 array<T,N> (also \" array class \") is a fixed sequence defined with two template parameters to create an array of N elements of type T . Here it is zero initialized with an empty braced initializer . #include <array> array < int , 5 > myIntArray {}; Other methods: - fill() : Set every element of the array to the same value - size() : Return the number of elements as a type size_t - at() : Access an element at a given index but testing for a valid range. Safer than using the built-in index method. Vectors are sequential containers with typed elements like the array class , but are not limited to fixed sizes. The push_back() method is similar to a Python List.append() . Other methods like front() , back() , and pop_back() can be used to manipulate the vector. #include <vector> vector < int > vals ; The insert() method takes two arguments, one is an iterator , here provided by yet another vector method - begin() , and the content to be inserted. This code will insert the string at index 2. #include <iostream> #include <vector> #include <string> using namespace std ; int main () { vector < string > family ; family . push_back ( \"Plato\" ); family . push_back ( \"Aristotle\" ); family . push_back ( \"Socrates\" ); family . push_back ( \"Pythagoras\" ); family . push_back ( \"Aristarchos\" ); family . insert ( family . begin () + 2 , \"Dgiapusccu\" ); family . pop_back (); for ( string i : family ) { cout << i << endl ; } return 0 ; } A forward_list<T> is an implementation of the singly-linked list and is rarely used. A list<T> is an implementation of the doubly-linked list and is rarely used. The double-ended queue (deque) exposes push_Front() and push_back() methods. Container adapters A stack<T> implements last-in first-out (LIFO) semantics. Stacks support push and pop methods. A queue<T> implements first-in first-out (FIFO) semantics. Associate containers Standard iterators There are three types of iterator supported by containers in the Standard Library: - random access iterator support the widest variety of operations: vector<T> , array<T,N> , and deque<T> - forward iterators do not support decrement operations (\"going backwards\"), and this describes the operation of a forward_list<T> - bidirectional iterators support both increment and decrement operations, but cannot jump more than one value, describing the operation of a list<T> stack<T> , queue<T> , and priority_queue<T> do not have iterators whatsoever. Containers in the Standard Library expose a begin() member function, which is the most commonly used iterator. std :: vector < char > letters { 'a' , 'b' , 'c' , 'd' , 'e' } auto iter { letters . begin ()}; // Specifying type explicitly std :: vector :: iterator iter { letters . begin ()}; At a deep level, iterators are pointers, so dereferencing them produces the element of the container being iterated. std :: cout << * iter << std :: endl ; // a The container can now be traversed by incrementing and (sometimes) decrementing the iterator. ++ iter ; std :: cout << * iter << std :: endl ; // b A string can be reversed using the rbegin() and rend() iterators: string name { \"Lorem ipsum...\" }; string reverse ( name . rbegin (), name . rend ()); Maps A syntactic sugar has been available since C++17: for ([ x , y ] : coords ) { std :: cout << x << y << endl ; } It is equivalent to: for ( std :: pair < int , int > el : coords ) { std :: cout << el . first << el . second << endl ; } The pair type has two public fields: first second ## Math #include <math.h> using namespace std ; int main () { int num { 2 }; cout << pow ( num , 2 ) << endl ; } Classes New data types in C++ are created as classes , which can be composites of member variables of other types and member functions , allowing complex and intuitive models to be created. The three primary principles of OOP are: Encapsulation : member variables and functions are packaged together Data hiding preserves the integrity of an object Inheritance allows one type to define another. Polymorphism (in C++ implemented by calling member functions using a pointer or reference) allow behavior of base classes to be exposed from objects of derived classes Member variables Classes can contain member variables that are public or private (\" access specifiers \"), but it is best practice to make variables private while implementing accessor functions (getters and setters): - Hiding data preserves the integrity of objects - Loose coupling facilitates future change in codebase - Extra code can be injected for logging or data validation - Debuggers can set breakpoints on these getters and setters class Box { private : double length { 1 , 0 }; double width { 1 , 0 }; double height { 1 , 0 }; public : double volume () { return length * width * height ; } }; Initialization A member initializer list can be used to initialize fields more efficiently than explicit assignment. Box :: Box ( double lv , double wv , double hv ) : length { lv }, width { wv }, height { hv } {} This technique must have an expression in braces, even if they are empty. // Compiler error Box :: Box ( double lv , double wv , double hv ) : length { lv }, width { wv }, height { hv } Notably, this technique doesn't appear to work in derived class constructors. class Animal { public : std :: string _name {}; Animal ( std :: string n ) : _name { n } {} } class Dog : public Animal { public : Dog ( std :: string n , std :: string b ) : Animal ( n ) { _breed = b ;}; } A class constructor is called whenever a new instance of the class is defined. It always has the same name as the class itself and has no return data type because it returns no data. class Box { private : // ... public : Box ( double l , double w , double h ) { length = l ; width = w ; height = h ; } }; If a constructor isn't defined, the compiler will supply a default default constructor when an object is instantiated without initial values. To define a default constructor: Box () = default ; A destructor is a special member of a class executed to deal with cleanup upon use of the delete operator. A class can have only one destructor, and if one isn't defined then the compiler provides a default destructor that does nothing. The name of the destructor for a class is always the class name prefixed with a tilde, and similar to a constructor it cannot have a return type or parameters. ~ Box () = default ; Box ::~ Box () = default ; // when defined outside the class Base class destructors should always be declared as virtual . Access When variables of class types are instantiated with the const keyword, they are called const objects , and none of their member variables can be altered (member variables of const objects become immutable). The compiler will throw an error when attempting to invoke methods of const objects unless they are identified as const member functions by using the const keyword in the signature after the identifier (\"attribute\"?): class Box { double volume () const { /* ... */ } double getLength () const { return length ; } double getWidth () const { return width ; } double getHeight () const { return height ; } } public , private , and protected are access specifiers that determine how a member variables and functions can be accessed from the outside. When inheriting from a base class, an access specifier can also be used to determine how accessible that base class's members are within the derived class. class Dog : public Animal { // ... } - When the base class specifier is public , inherited members are unchanged - When the base class specifier is protected , inherited public and protected members become protected - When the base class specifier is private, all inherited members become private. A friend is a function to which a class grants access to its private internals. They may be useful in rare situations where a single function needs access to the internals of different kinds of objects. Inheritance When creating subclasses, you must remember: - Private variables must be placed in the protected access specifier so that they are accessible to child classes. - The base class access specifier must allow access to the base class's private variables ( public or protected ) - Parent class must have a default constructor class Animal { protected : // not `private:`! std :: string _name ; public : Animal () = default ; } class Dog : public Animal { /* ... */ } // Polymorphism Polymorphism in C++ refers to the practice of invoking a base class's member function rather than the derived class. Because the compiler performs early binding by default, a pointer typed to a base class but initialized to a derived class will invoke the base class's member function. Late binding can be used to force the pointer to use the derived class's member function even when the type of the pointer is the base class. This is done by using the virtual keyword on the base class's member function. Classes with virtual functions are called abstract classes and may not be instantiated. Abstract classes that are made of only virtual functions are called interfaces . class Base { public : virtual void doStuff () { /* ... */ } } class Derived : public Base { public : void doStuff () { /* ... */ } } int main () { Derived derived {}; Base * pointer = & derived ; // a pointer to an abstract class **may** be used pointer -> doStuff (); } Derived classes must then override this virtual function with the override keyword. class Derived : public Base { override doStuff () // ... } Enumerations Enumerations can be specified with enum . Without specifying a value, each element of the enum is given a successively greater integer value starting with 0, like the indexes of an array (an ordinal value ). enum Choices { A , B , C , D } These elements can be specified with or without the scope resolution operator :: cout << A ; // 0 cout << Choices :: A ; // 0 Templates Templates are used to have the compiler generate code automatically for a given data type. This is to avoid highly repetitive overloaded function definitions which only differ in parameter lists. The template and typename keywords define a template. The placeholder \"T\" represents the data type that will be replaced by a specific type by the compiler. template < typename T > T larger ( T a , T b ) { return a > b ? a : b ; } More than one data type can be used for the parameters, but in that case the return type must be explicitly specified: template < typename T1 , typename T2 > bool larger ( T1 a , T2 b ) { return a > b ; } Control flow The choices in a switch statement are called cases . You can only switch on constant expressions that can be evaluated at compile-time, typically literals but excluding strings. Each case must be followed by a break statement to prevent fallthrough , except for the default case . switch ( choice ) { case 1 : // ... break ; case 2 : // ... break ; default : // ... } Since C++11, the range-based for-loop is available, which works very similar to a Python for-in loop: for ( string num : nums ) { cout << num << endl ; } Functions Function prototypes , defining the function header (return data type, function name, and parameter list), describe a function sufficiently for the compiler to be able to compile calls to it and are required before using a function if the function declaration doesn't precede all the locations where it's called. #include <iostream> using namespace std ; // Without this prototype, there is a compile-time error. void printSomething (); int main () { printSomething (); return 0 ; } void printSomething () { cout << \"something...\" << endl ; } Passing by reference allows variables to be changed in-place and works by using the reference to the argument. Passing by value is the default parameter passing scheme , which works by actually copying the argument. int func ( int a ) { // Pass by value } int func ( int & a ) { // Pass by reference } Recursion Recursion requires a base case and at least one recursive case . The call stack is a stack data structure that figures prominently in recursive computing.## Home Project Description JamoftheMonthProject.cpp CLI application that calculates how much the user owes based on selected subscription tier and units purchased TicTacToe.cpp RPGCharGen.cpp Multiple classes using inheritance, virtual member functions, enums Task Description Reverse a string ...## Iterators An iterator is a classical and widespread design pattern that allows a wide variety of container-like objects to be traversed by exposing a uniform interface. However, loops based on iterators should only be used if access to the iterator is needed for advanced processing in the loop body. A range-based for loop is the recommended way to iterate over all elements of a container. Standard iterators begin() end() rbegin() Memory Memory leaks can be detected using valgrind . Namespaces A namespace is a block that attaches an extra name to every entity name that is declared or defined within it. The qualified name of each entity is the namespace name followed by the scope resolution operator :: followed by the basic entity name. Namespaces can be used to partition large codebases into logical groupings to avoid name clashes. If a namespace isn't defined, the global namespace , where entities have no namespace name attached, applies by default. You can define a namespace using the namespace keyword. namespace foo { // ... } Namespaces can be nested.. namespace outer { namespace inner { void foo () { // ... } } } outer :: inner :: foo () Namespace aliases can be formed: namespace outin = outer :: inner ; outin :: foo () The using keyword allows you to reference any name from a namespace without qualifying it. using namespace std ; It can also be used specify a type alias , where an alternative name is used to refer to an existing data type. using BigOnes = unsigned long long ; typedef unsigned long long BigOnes ; // Older, less intuitive syntax Operators Each operator is associated with a particular named function. Operators can be overloaded by implementing that function. bool Rectangle :: operator == ( const Rectangle & other ) const { return _length == other . _length && _width == other . _width ; } Header files Topic Header file array <array> deque <deque> exception <exception> map <map> Mathematical functions <math.h> queue <queue> stack <stack> vector <vector> Smart pointers <memory> Applications gtkmm gtkmm (historically \"GTK--\") is a C++ wrapper for an underlying GTK code base written in C. Compared to Qt , another GUI library, gtkmm uses more modern and native C++ features. In Ubuntu , installing the development environment is done with the gnome-devel metapackage: sudo apt install gnome-devel NES emulator Courses C++ Standard Template Library in Practice /# Topic Video Projects 01.01 The Course Overview 01.02 Templates Introduction to the STL TemplateSTL.cpp 01.03 General Concepts ExceptionSTL.cpp 01.04 Utilities - Common Utilities StringSTL.cpp 01.05 Utilities - Regex RegexSTL.cpp 01.06 Project - Bitcoin Exchange Program BTCX.cpp 01.07 Project - Coding 01.08 Project - Custom Writer Function 01.09 Review 02.01 Understanding Containers 02.02 Vectors 02.03 Standard Array 02.04 Lists 02.05 Stacks and Queues 02.06 Maps and Multimaps - Overview 02.07 Maps - Coding 02.08 Multimaps - Coding 02.09 Sets and Multisets 02.10 Project 02.11 Review 03.01 Iterators 03.02 Input Iterators 03.03 Output Iterators 03.04 Forward Iterators 03.05 Bidirectional Iterators 03.06 Random Access Iterators 03.07 Auxiliary Iterator Functions 03.08 Iterator Adaptors 03.09 Writing Generic Functions for Iterators 03.10 User - Defined Iterators 03.11 Project 03.12 Review 04.01 Introduction to Algorithms 04.02 Sequence Algorithms - for_each 04.03 Sequence Algorithms - equals 04.04 Copying 04.05 Moving 04.06 Removing 04.07 Sorting and Gathering - std::sort 04.08 Sorting and Gathering - std::partial_sort algorithm 04.09 Sorting and Gathering - std::partition 04.10 Sorting and Gathering - std::partition_copy 04.11 Searching and Finding - std::find 04.12 Sorting and Gathering - std::find_first_of, std::adjacent_find 04.13 Sorting and Gathering - std::search 04.14 Sorting and Gathering - std::binary_search 04.15 Counting 05.01 Replacing and Transforming - std::replace 05.02 Replacing and Transforming - std::replace_copy 05.03 Replacing and Transforming - equals 05.04 Swapping 05.05 Rotating 05.06 Randomizing 05.07 Permutations 05.08 Sampling 05.09 Min 05.10 Max 05.11 Clamp 05.12 Fill and Generate 05.13 Numeric Algorithms - std::accumulate 05.14 Numeric Algorithms - std::partial_sum and std::adjacent_difference 05.15 Numeric Algorithms - std::gcd, and std::lcm 05.16 Numeric Algorithms - std::inner_product and std::iota 05.17 Review 06.01 Basic Architecture of the I/O Stream Library 06.02 Console I/O - Interact with a User 06.03 Console I/O - Read Input 06.04 File I/O 06.05 String Streams 06.06 Manipulators and Formatters 06.07 Stream States 06.08 Low Level I/O 06.09 Overloading Stream Operators 06.10 Project - Overview 06.11 Project - Classes and structures 06.12 Project - Implementation 06.13 Review 07.01 Unique Pointers 07.02 Shared Pointers 07.03 Allocators 07.04 Defining an Allocator 07.05 Uninitialized Memory 07.06 Review 08.01 Introduction to Threading 08.02 Creating Threads 08.03 Locks 08.04 Shared Locks 08.05 Atomic Values 08.06 Async 08.07 Condition Variables 08.08 Project 08.09 Review 09.01 Concepts 09.02 Modules 09.03 Coroutines 09.04 Course Review Complete C++ Developer Course /# Topic Video Projects 1.1 Section Overview 1.2 Getting Started on Windows with Visual Studio Integrated Development Environment (IDE) 1.3 Getting Started on macOS or Linux with CodeBlocks IDE 1.4 Getting Started with macOS Catalina or Higher with Visual Studio Code 1.5 Finding Answers to Your Questions 2.1 Section Overview 2.2 Saying \"Hello\" to C++ 2.3 Variables and Data Types - Part 1 2.4 Variables and Data Types - Part Two 2.5 Variables and Data Types - Part Three 2.6 Comments 2.7 Arithmetic Operators 2.8 Relational Operators 2.9 Logical Operators 2.10 Symbolic Constants and Naming Conventions 2.11 User Input 2.12 Project - Average of Three 2.13 Project - MadLibs Clone 2.14 Section Wrap-Up 3.1 Section Overview 3.2 Introduction to Control Statements 3.3 Selection Control Statements 3.4 Repetition Control Statements 3.5 The Break and Continue Statements 3.6 Random Numbers 3.7 Project - Jam of the Month Club 3.8 Project - Odds and Evens 3.9 Project - Guess the Number 3.10 Section Wrap-Up 4.1 Section Overview 4.2 Built-in Arrays 4.3 The Array Class 4.4 The Vector Class 4.5 Multi-Dimensional Arrays 4.6 Project - Array Data 4.7 Project - Vector Data 4.8 Project - Parallel Arrays/Vectors 4.9 Section Wrap-Up 5.1 Section Overview 5.2 Function Prototypes and Definitions FunctionFun.cpp 5.3 Function Return Types and Parameters 5.4 Parameter Passing: Pass-by-Value and Pass-by-Reference PassingSchemes.cpp 5.5 Variable Scope and Lifetime ScopeFun 5.6 Function Overloading 5.7 The <cmath> Library 5.8 Recursion 5.9 Project - Return the Product of Three Parameters 5.10 Project - Return the Sum of Built-in Array Elements 5.11 Project - Return the Sum of Array Object Elements 5.12 Project - Retrieve the Sum of Array Object Elements by Reference 5.13 Project - Tic-Tac-Toe (ADVANCED) 5.14 Section Wrap-Up 6.1 Section Overview 6.2 Basics of Object Oriented Programming (OOP) 6.3 Encapsulation: Data Members and Member Functions 6.4 Separate Compilation 6.5 Constructors and Destructors 6.6 A Rectangle Class Rectangle.cpp 6.7 A Book Class 6.8 Project - A Bank Account Class 6.9 Project - A Pizza Class 6.10 Project - A Circle Class 6.11 Section Wrap-Up 7.1 Section Overview 7.2 Exceptions and the Exception Hierarchy 7.3 Logic Errors 7.4 Runtime Errors and Throwing Exceptions 7.5 Rethrowing Exceptions 7.6 Custom Exceptions 7.7 Basic Testing and Debugging 7.8 Project - Throwing and Handling an Out_of_Range Exception 7.9 Project - Creating and Using Your Own Exception 7.10 Section Wrap-Up 8.1 Section Overview 8.2 Introduction to Pointers 8.3 Dynamic Memory - Part 1 8.4 Dynamic Memory (- Part 2 8.5 Const Correctness 8.6 Project - Dynamically Creating Rectangles 8.7 Project - Dynamically Creating Circles 8.8 Section Wrap-Up 9.1 Section Overview 9.2 Sequential File Output 9.3 Sequential File Input 9.4 More File Input/ Output (I/O) 9.5 Project - Reading Data from File and Printing Statistics 9.6 Project - Dynamically Creating Rectangles from File 9.7 Project - Shopping Item File 9.8 Section Wrap-Up 10.1 Section Overview 10.2 Inheritance - Part 1 10.3 Inheritance - Part 2) 10.4 Polymorphism and Late Binding 10.5 Enumerated Types This video explains enumerated types. EnumFun.cpp 10.6 Project - Derived Cat Class Cat.cpp 10.7 Project \u2013 Role Playing Game (RPG) Player Character Creation RPGCharGen 10.8 Section Wrap-Up 11.1 Section Overview 11.2 Templates - Standard Template Library (STL) TemplateFun.cpp 11.3 queue<T> stack<T> deque<T> Standard Template Library (STL) - Part 1) DequeFun.cpp, StackFun.cpp, QueueProject.cpp 11.4 map<T> Standard Template Library (STL) - Part 2 ContactsFun.cpp, AlgorithmFun.cpp 11.5 unique_ptr<T> Smart Pointers SmartPointerFun.cpp, Car.cpp 11.6 Friend functions Friend Functions and Friend Classes FriendFunctions.cpp 11.7 Operator Overloading OverloadingFun.cpp, Rectangle.h 11.8 map<T> Project - Dictionary of Terms DictionaryProject.cpp 11.9 Project - Aliens Aliens.cpp 11.10 Section Wrap-Up RPGCharGen cpp h #include <iostream> #include \"RPGCharGen.h\" using namespace std ; int main () { Warrior w { \"Doofus McGroober\" , Race :: HUMAN }; cout << \"Player name: \" << w . getName () << endl ; cout << \"Player HP: \" << w . getHp () << endl ; cout << \"Player MP: \" << w . getMp () << endl ; cout << \"Player race: \" << w . getRace () << endl ; w . attack (); Priest m { \"Brother Tolkien\" , Race :: ELF }; cout << \"Player name: \" << m . getName () << endl ; cout << \"Player HP: \" << m . getHp () << endl ; cout << \"Player MP: \" << m . getMp () << endl ; cout << \"Player race: \" << m . getRace () << endl ; m . attack (); Mage n { \"Smart Frodo\" , Race :: DWARF }; cout << \"Player name: \" << n . getName () << endl ; cout << \"Player HP: \" << n . getHp () << endl ; cout << \"Player MP: \" << n . getMp () << endl ; cout << \"Player race: \" << n . getRace () << endl ; n . attack (); return 0 ; } #if !defined(RPGCHARGEN_H) #define RPGCHARGEN_H #include <string> enum Race { HUMAN , ELF , DWARF }; class Player { protected : std :: string _name { \"Johnny Bravo\" }; Race _race { Race :: HUMAN }; int _hp { 100 }; int _mp { 100 }; public : Player ( std :: string n , Race r , int hp , int mp ) : _name { n }, _race { r }, _hp ( hp ), _mp ( mp ) {} virtual std :: string attack () = 0 ; int getHp () { return _hp ; } int getMp () { return _mp ; } std :: string getRace () { switch ( _race ) { case 0 : return \"human\" ; break ; case 1 : return \"elf\" ; break ; case 2 : return \"dwarf\" ; break ; default : return \"none\" ; break ; } } std :: string getName () { return _name ; } void setHp ( int n ) { _hp = n ; } void setMp ( int n ) { _mp = n ; } void setName ( std :: string s ) { _name = s ; } void setRace ( Race r ) { _race = r ;} }; class Warrior : public Player { public : Warrior ( std :: string n , Race r ) : Player ( n , r , 200 , 0 ) {} std :: string attack () { return \"I will destroy you with my sword, foul demon!\" ;} }; class Priest : public Player { public : Priest ( std :: string n , Race r ) : Player ( n , r , 100 , 200 ) {} std :: string attack () { return \"Taste the wrath of the Two True Gods!\" ;} }; class Mage : public Player { public : Mage ( std :: string n , Race r ) : Player ( n , r , 150 , 150 ) {} std :: string attack () { return \"You are overmatched by my esoteric artifices!\" ;} }; #endif // RPGCHARGEN_H","title":"C++"},{"location":"Coding/C%2B%2B/#preprocessing","text":"Preprocessing is the phase of executing preprocessing directives in a source file, which are then removed from the resulting translation unit , which combines the pure C or C++ code of a source file with all its included header files. The translation unit is then compiled into an object file , and it is the linker that then forms linkages between object files to produce an executable module. #define #include The #define directive specifies a macro which can define a text replacement to occur in code before it is compiled. Macros are considered a holdover from C, and other constructs like variable templates and function templates are considered better suited in C++. In practice, #define statements are most commonly used to handle header files. Here, any instance of \" PI \" in the source code will be replaced by the string of digits \"3.14159265\", but it will not be replaced if it forms part of an identifier or appears in a string literal or comment. #define PI 3.14159265 If a substitution string isn't specified then any instance of the identifier will simply be removed. #define break Macros may not span multiple lines without escaped line breaks, but during preprocessing these are removed and the substitution is made inline. #define PI 3.14\\ 159265 Function-like macros are possible because the preprocessor can recognize a function call in the macro identifier and replace its arguments intelligently. Here any invocation of the MAX() function call will have its arguments incorporated into the substitution statement. #define MAX(A, B) A >= B ? A : B All the lines following #ifndef ( #if !defined ) will be kept in the file as long as the identifier \"MYHEADER_H\" has not already been defined. This common is called an #include guard . #ifndef MYHEADER_H #define MYHEADER_H // ... #endif","title":"Preprocessing"},{"location":"Coding/C%2B%2B/#variables","text":"Since C++14 , you can separate digits in a long integer with a single quote to make it more readable. int num { 12'345 }; // 12,345 Hexadecimal literals are prefixed with \"0x\" and octals with \"0\": int hex { 0xabcdef }; int oct { 0567 }; constexpr is used in some situations I can't figure out yet. static constexpr u32 MAX_MEM = 1024 * 64 ; size_t is a type alias defined in the Standard Library (in the cstddef header). It is an alias for an unsigned integer type.","title":"Variables"},{"location":"Coding/C%2B%2B/#initialization","text":"A braced initializer refers to placing the initial value of a variable in braces. This is a novel style of initialization introduced in C++11 . Its main advantage is that it will raise a compile-time error if the compiler has to perform a narrowing conversion of the value to match the declared type. int apples { 15 }; Older but equally valid ways of initializing variables: int oranges = 12 ; int kiwis ( 13 ); // \"functional notation\" Zero initialization refers to initializing a variable with empty braces. It works for any fundamental type, and numeric types initialize to zero. int grapes {}; // 0 Sequences, like this array class can be efficiently zero-initialized; the braces can contain any number of values up to the declared size of the array (remaining values will be zero-initialized). #include <array> array < int , 5 > myIntArray {}; Multi-dimensional arrays can be initialized with nested initializers: int myNums [ 2 ][ 3 ] { { 1 , 2 , 3 }, { 4 , 5 , 6 } };","title":"Initialization"},{"location":"Coding/C%2B%2B/#pointers","text":"Smart pointers (also called managed pointers ) are pointers that manage their own memory. They were introduced in C++11, and there is no longer a reason to use the earlier raw pointers . In older versions of C++, memory leaks were common because the programmer had to remember to release memory allocated dynamically from the heap/free store using the delete keyword. These older pointers are now called raw pointers The most commonly used smart pointer is unique_ptr : others include shared_ptr and weak_ptr . Only a single unique_ptr<T> can point to any memory address, unless ownership is transfered with move() . Since C++14, it is recommended to create unique pointers using makeunique<T>() . auto pdbl = make_unique < dbouble > (); When variables are declared with an asterisk * appended to the datatype or prepended to the identifier, the variable becomes a pointer to that type. Pointer identifiers usually begin with \"p\", a convention known as Hungarian notation . The size of pointers corresponds to the address space of available memory (4 bytes for 32-bit architectures, and 8 bytes for 64-bit). // The following statements are equivalent. long * pnum {}; long * pnum { nullptr }; void * is known as \"pointer to void type\", meaning variables defined as such are pointers to data of an unspecified type, making it similar to var in C#. The address-of operator & obtains the address of a variable. The address-of operator typically also occurs with the indirection operator or dereference operator (also * ) to access the data pointed to by a pointer. Using a dereferenced pointer is the same as using the variable to which it points. long num { 12345L }; long * pnum { & num }; long newnum { * pnum + 1 }; When a pointer points to an object with methods, like a vector<T> container , the indirect member selection operator ( -> ) can be used to access the methods. // The following statements are equivalent. auto * pdata { new std :: vector < int > {}}; std :: vector < int > data ; auto * pdata = & data ; // The following statements are equivalent. ( * pdata ). push_back ( 66 ); pdata -> push_back ( 66 ); Pointers to classes can be recast with the following syntax: Animal * ptr = & kitty ; (( Cat * ) ptr ) -> chaseMouse (); // newer, safer syntax ( reinterpret_cast < Cat > ( ptr )) -> chaseMouse ();","title":"Pointers"},{"location":"Coding/C%2B%2B/#containers","text":"Containers are a type of data structure used to contain elements for various purposes. They are deeply tied to algorithms through iterators . Two array-like data structures defined in the Standard Library that are more typically used are array<T,N> and vector<T>","title":"Containers"},{"location":"Coding/C%2B%2B/#arrays","text":"An array is a variable that represents a contiguous sequence of memory locations, each storing an item of data of the same data type, each of which are called elements . Arrays must be declared with a constant integer expression that is fixed at compile time. Built-in arrays in C++ are inherited from C. int primes [ 10 ] { 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 }","title":"Arrays"},{"location":"Coding/C%2B%2B/#sequence-containers","text":"The two most common sequence containers are array<T,N> and vector<T> . All sequence containers expose several of a family of related member functions: Member function vector array list forward_list deque front() \u2705 \u2705 \u2705 \u2705 \u2705 back() \u2705 \u2705 \u2705 \u274c \u2705 push_front() \u274c \u274c \u2705 \u2705 \u2705 pop_front() \u274c \u274c \u2705 \u2705 \u2705 push_back() \u2705 \u274c \u2705 \u274c \u2705 pop_back() \u2705 \u274c \u2705 \u274c \u2705 insert() \u2705 \u274c \u2705 \u2705 \u2705 erase() \u2705 \u274c \u2705 \u2705 \u2705 array<T,N> (also \" array class \") is a fixed sequence defined with two template parameters to create an array of N elements of type T . Here it is zero initialized with an empty braced initializer . #include <array> array < int , 5 > myIntArray {}; Other methods: - fill() : Set every element of the array to the same value - size() : Return the number of elements as a type size_t - at() : Access an element at a given index but testing for a valid range. Safer than using the built-in index method. Vectors are sequential containers with typed elements like the array class , but are not limited to fixed sizes. The push_back() method is similar to a Python List.append() . Other methods like front() , back() , and pop_back() can be used to manipulate the vector. #include <vector> vector < int > vals ; The insert() method takes two arguments, one is an iterator , here provided by yet another vector method - begin() , and the content to be inserted. This code will insert the string at index 2. #include <iostream> #include <vector> #include <string> using namespace std ; int main () { vector < string > family ; family . push_back ( \"Plato\" ); family . push_back ( \"Aristotle\" ); family . push_back ( \"Socrates\" ); family . push_back ( \"Pythagoras\" ); family . push_back ( \"Aristarchos\" ); family . insert ( family . begin () + 2 , \"Dgiapusccu\" ); family . pop_back (); for ( string i : family ) { cout << i << endl ; } return 0 ; } A forward_list<T> is an implementation of the singly-linked list and is rarely used. A list<T> is an implementation of the doubly-linked list and is rarely used. The double-ended queue (deque) exposes push_Front() and push_back() methods.","title":"Sequence containers"},{"location":"Coding/C%2B%2B/#container-adapters","text":"A stack<T> implements last-in first-out (LIFO) semantics. Stacks support push and pop methods. A queue<T> implements first-in first-out (FIFO) semantics.","title":"Container adapters"},{"location":"Coding/C%2B%2B/#associate-containers","text":"","title":"Associate containers"},{"location":"Coding/C%2B%2B/#standard-iterators","text":"There are three types of iterator supported by containers in the Standard Library: - random access iterator support the widest variety of operations: vector<T> , array<T,N> , and deque<T> - forward iterators do not support decrement operations (\"going backwards\"), and this describes the operation of a forward_list<T> - bidirectional iterators support both increment and decrement operations, but cannot jump more than one value, describing the operation of a list<T> stack<T> , queue<T> , and priority_queue<T> do not have iterators whatsoever. Containers in the Standard Library expose a begin() member function, which is the most commonly used iterator. std :: vector < char > letters { 'a' , 'b' , 'c' , 'd' , 'e' } auto iter { letters . begin ()}; // Specifying type explicitly std :: vector :: iterator iter { letters . begin ()}; At a deep level, iterators are pointers, so dereferencing them produces the element of the container being iterated. std :: cout << * iter << std :: endl ; // a The container can now be traversed by incrementing and (sometimes) decrementing the iterator. ++ iter ; std :: cout << * iter << std :: endl ; // b A string can be reversed using the rbegin() and rend() iterators: string name { \"Lorem ipsum...\" }; string reverse ( name . rbegin (), name . rend ());","title":"Standard iterators"},{"location":"Coding/C%2B%2B/#maps","text":"A syntactic sugar has been available since C++17: for ([ x , y ] : coords ) { std :: cout << x << y << endl ; } It is equivalent to: for ( std :: pair < int , int > el : coords ) { std :: cout << el . first << el . second << endl ; } The pair type has two public fields: first second ## Math #include <math.h> using namespace std ; int main () { int num { 2 }; cout << pow ( num , 2 ) << endl ; }","title":"Maps"},{"location":"Coding/C%2B%2B/#classes","text":"New data types in C++ are created as classes , which can be composites of member variables of other types and member functions , allowing complex and intuitive models to be created. The three primary principles of OOP are: Encapsulation : member variables and functions are packaged together Data hiding preserves the integrity of an object Inheritance allows one type to define another. Polymorphism (in C++ implemented by calling member functions using a pointer or reference) allow behavior of base classes to be exposed from objects of derived classes","title":"Classes"},{"location":"Coding/C%2B%2B/#member-variables","text":"Classes can contain member variables that are public or private (\" access specifiers \"), but it is best practice to make variables private while implementing accessor functions (getters and setters): - Hiding data preserves the integrity of objects - Loose coupling facilitates future change in codebase - Extra code can be injected for logging or data validation - Debuggers can set breakpoints on these getters and setters class Box { private : double length { 1 , 0 }; double width { 1 , 0 }; double height { 1 , 0 }; public : double volume () { return length * width * height ; } };","title":"Member variables"},{"location":"Coding/C%2B%2B/#initialization_1","text":"A member initializer list can be used to initialize fields more efficiently than explicit assignment. Box :: Box ( double lv , double wv , double hv ) : length { lv }, width { wv }, height { hv } {} This technique must have an expression in braces, even if they are empty. // Compiler error Box :: Box ( double lv , double wv , double hv ) : length { lv }, width { wv }, height { hv } Notably, this technique doesn't appear to work in derived class constructors. class Animal { public : std :: string _name {}; Animal ( std :: string n ) : _name { n } {} } class Dog : public Animal { public : Dog ( std :: string n , std :: string b ) : Animal ( n ) { _breed = b ;}; } A class constructor is called whenever a new instance of the class is defined. It always has the same name as the class itself and has no return data type because it returns no data. class Box { private : // ... public : Box ( double l , double w , double h ) { length = l ; width = w ; height = h ; } }; If a constructor isn't defined, the compiler will supply a default default constructor when an object is instantiated without initial values. To define a default constructor: Box () = default ; A destructor is a special member of a class executed to deal with cleanup upon use of the delete operator. A class can have only one destructor, and if one isn't defined then the compiler provides a default destructor that does nothing. The name of the destructor for a class is always the class name prefixed with a tilde, and similar to a constructor it cannot have a return type or parameters. ~ Box () = default ; Box ::~ Box () = default ; // when defined outside the class Base class destructors should always be declared as virtual .","title":"Initialization"},{"location":"Coding/C%2B%2B/#access","text":"When variables of class types are instantiated with the const keyword, they are called const objects , and none of their member variables can be altered (member variables of const objects become immutable). The compiler will throw an error when attempting to invoke methods of const objects unless they are identified as const member functions by using the const keyword in the signature after the identifier (\"attribute\"?): class Box { double volume () const { /* ... */ } double getLength () const { return length ; } double getWidth () const { return width ; } double getHeight () const { return height ; } } public , private , and protected are access specifiers that determine how a member variables and functions can be accessed from the outside. When inheriting from a base class, an access specifier can also be used to determine how accessible that base class's members are within the derived class. class Dog : public Animal { // ... } - When the base class specifier is public , inherited members are unchanged - When the base class specifier is protected , inherited public and protected members become protected - When the base class specifier is private, all inherited members become private. A friend is a function to which a class grants access to its private internals. They may be useful in rare situations where a single function needs access to the internals of different kinds of objects.","title":"Access"},{"location":"Coding/C%2B%2B/#inheritance","text":"When creating subclasses, you must remember: - Private variables must be placed in the protected access specifier so that they are accessible to child classes. - The base class access specifier must allow access to the base class's private variables ( public or protected ) - Parent class must have a default constructor class Animal { protected : // not `private:`! std :: string _name ; public : Animal () = default ; } class Dog : public Animal { /* ... */ } //","title":"Inheritance"},{"location":"Coding/C%2B%2B/#polymorphism","text":"Polymorphism in C++ refers to the practice of invoking a base class's member function rather than the derived class. Because the compiler performs early binding by default, a pointer typed to a base class but initialized to a derived class will invoke the base class's member function. Late binding can be used to force the pointer to use the derived class's member function even when the type of the pointer is the base class. This is done by using the virtual keyword on the base class's member function. Classes with virtual functions are called abstract classes and may not be instantiated. Abstract classes that are made of only virtual functions are called interfaces . class Base { public : virtual void doStuff () { /* ... */ } } class Derived : public Base { public : void doStuff () { /* ... */ } } int main () { Derived derived {}; Base * pointer = & derived ; // a pointer to an abstract class **may** be used pointer -> doStuff (); } Derived classes must then override this virtual function with the override keyword. class Derived : public Base { override doStuff () // ... }","title":"Polymorphism"},{"location":"Coding/C%2B%2B/#enumerations","text":"Enumerations can be specified with enum . Without specifying a value, each element of the enum is given a successively greater integer value starting with 0, like the indexes of an array (an ordinal value ). enum Choices { A , B , C , D } These elements can be specified with or without the scope resolution operator :: cout << A ; // 0 cout << Choices :: A ; // 0","title":"Enumerations"},{"location":"Coding/C%2B%2B/#templates","text":"Templates are used to have the compiler generate code automatically for a given data type. This is to avoid highly repetitive overloaded function definitions which only differ in parameter lists. The template and typename keywords define a template. The placeholder \"T\" represents the data type that will be replaced by a specific type by the compiler. template < typename T > T larger ( T a , T b ) { return a > b ? a : b ; } More than one data type can be used for the parameters, but in that case the return type must be explicitly specified: template < typename T1 , typename T2 > bool larger ( T1 a , T2 b ) { return a > b ; }","title":"Templates"},{"location":"Coding/C%2B%2B/#control-flow","text":"The choices in a switch statement are called cases . You can only switch on constant expressions that can be evaluated at compile-time, typically literals but excluding strings. Each case must be followed by a break statement to prevent fallthrough , except for the default case . switch ( choice ) { case 1 : // ... break ; case 2 : // ... break ; default : // ... } Since C++11, the range-based for-loop is available, which works very similar to a Python for-in loop: for ( string num : nums ) { cout << num << endl ; }","title":"Control flow"},{"location":"Coding/C%2B%2B/#functions","text":"Function prototypes , defining the function header (return data type, function name, and parameter list), describe a function sufficiently for the compiler to be able to compile calls to it and are required before using a function if the function declaration doesn't precede all the locations where it's called. #include <iostream> using namespace std ; // Without this prototype, there is a compile-time error. void printSomething (); int main () { printSomething (); return 0 ; } void printSomething () { cout << \"something...\" << endl ; } Passing by reference allows variables to be changed in-place and works by using the reference to the argument. Passing by value is the default parameter passing scheme , which works by actually copying the argument. int func ( int a ) { // Pass by value } int func ( int & a ) { // Pass by reference }","title":"Functions"},{"location":"Coding/C%2B%2B/#recursion","text":"Recursion requires a base case and at least one recursive case . The call stack is a stack data structure that figures prominently in recursive computing.## Home Project Description JamoftheMonthProject.cpp CLI application that calculates how much the user owes based on selected subscription tier and units purchased TicTacToe.cpp RPGCharGen.cpp Multiple classes using inheritance, virtual member functions, enums Task Description Reverse a string ...## Iterators An iterator is a classical and widespread design pattern that allows a wide variety of container-like objects to be traversed by exposing a uniform interface. However, loops based on iterators should only be used if access to the iterator is needed for advanced processing in the loop body. A range-based for loop is the recommended way to iterate over all elements of a container. Standard iterators begin() end() rbegin()","title":"Recursion"},{"location":"Coding/C%2B%2B/#memory","text":"Memory leaks can be detected using valgrind .","title":"Memory"},{"location":"Coding/C%2B%2B/#namespaces","text":"A namespace is a block that attaches an extra name to every entity name that is declared or defined within it. The qualified name of each entity is the namespace name followed by the scope resolution operator :: followed by the basic entity name. Namespaces can be used to partition large codebases into logical groupings to avoid name clashes. If a namespace isn't defined, the global namespace , where entities have no namespace name attached, applies by default. You can define a namespace using the namespace keyword. namespace foo { // ... } Namespaces can be nested.. namespace outer { namespace inner { void foo () { // ... } } } outer :: inner :: foo () Namespace aliases can be formed: namespace outin = outer :: inner ; outin :: foo () The using keyword allows you to reference any name from a namespace without qualifying it. using namespace std ; It can also be used specify a type alias , where an alternative name is used to refer to an existing data type. using BigOnes = unsigned long long ; typedef unsigned long long BigOnes ; // Older, less intuitive syntax","title":"Namespaces"},{"location":"Coding/C%2B%2B/#operators","text":"Each operator is associated with a particular named function. Operators can be overloaded by implementing that function. bool Rectangle :: operator == ( const Rectangle & other ) const { return _length == other . _length && _width == other . _width ; }","title":"Operators"},{"location":"Coding/C%2B%2B/#header-files","text":"Topic Header file array <array> deque <deque> exception <exception> map <map> Mathematical functions <math.h> queue <queue> stack <stack> vector <vector> Smart pointers <memory>","title":"Header files"},{"location":"Coding/C%2B%2B/#applications","text":"","title":"Applications"},{"location":"Coding/C%2B%2B/#gtkmm","text":"gtkmm (historically \"GTK--\") is a C++ wrapper for an underlying GTK code base written in C. Compared to Qt , another GUI library, gtkmm uses more modern and native C++ features. In Ubuntu , installing the development environment is done with the gnome-devel metapackage: sudo apt install gnome-devel","title":"gtkmm"},{"location":"Coding/C%2B%2B/#nes-emulator","text":"","title":"NES emulator"},{"location":"Coding/C%2B%2B/#courses","text":"","title":"Courses"},{"location":"Coding/C%2B%2B/#c-standard-template-library-in-practice","text":"/# Topic Video Projects 01.01 The Course Overview 01.02 Templates Introduction to the STL TemplateSTL.cpp 01.03 General Concepts ExceptionSTL.cpp 01.04 Utilities - Common Utilities StringSTL.cpp 01.05 Utilities - Regex RegexSTL.cpp 01.06 Project - Bitcoin Exchange Program BTCX.cpp 01.07 Project - Coding 01.08 Project - Custom Writer Function 01.09 Review 02.01 Understanding Containers 02.02 Vectors 02.03 Standard Array 02.04 Lists 02.05 Stacks and Queues 02.06 Maps and Multimaps - Overview 02.07 Maps - Coding 02.08 Multimaps - Coding 02.09 Sets and Multisets 02.10 Project 02.11 Review 03.01 Iterators 03.02 Input Iterators 03.03 Output Iterators 03.04 Forward Iterators 03.05 Bidirectional Iterators 03.06 Random Access Iterators 03.07 Auxiliary Iterator Functions 03.08 Iterator Adaptors 03.09 Writing Generic Functions for Iterators 03.10 User - Defined Iterators 03.11 Project 03.12 Review 04.01 Introduction to Algorithms 04.02 Sequence Algorithms - for_each 04.03 Sequence Algorithms - equals 04.04 Copying 04.05 Moving 04.06 Removing 04.07 Sorting and Gathering - std::sort 04.08 Sorting and Gathering - std::partial_sort algorithm 04.09 Sorting and Gathering - std::partition 04.10 Sorting and Gathering - std::partition_copy 04.11 Searching and Finding - std::find 04.12 Sorting and Gathering - std::find_first_of, std::adjacent_find 04.13 Sorting and Gathering - std::search 04.14 Sorting and Gathering - std::binary_search 04.15 Counting 05.01 Replacing and Transforming - std::replace 05.02 Replacing and Transforming - std::replace_copy 05.03 Replacing and Transforming - equals 05.04 Swapping 05.05 Rotating 05.06 Randomizing 05.07 Permutations 05.08 Sampling 05.09 Min 05.10 Max 05.11 Clamp 05.12 Fill and Generate 05.13 Numeric Algorithms - std::accumulate 05.14 Numeric Algorithms - std::partial_sum and std::adjacent_difference 05.15 Numeric Algorithms - std::gcd, and std::lcm 05.16 Numeric Algorithms - std::inner_product and std::iota 05.17 Review 06.01 Basic Architecture of the I/O Stream Library 06.02 Console I/O - Interact with a User 06.03 Console I/O - Read Input 06.04 File I/O 06.05 String Streams 06.06 Manipulators and Formatters 06.07 Stream States 06.08 Low Level I/O 06.09 Overloading Stream Operators 06.10 Project - Overview 06.11 Project - Classes and structures 06.12 Project - Implementation 06.13 Review 07.01 Unique Pointers 07.02 Shared Pointers 07.03 Allocators 07.04 Defining an Allocator 07.05 Uninitialized Memory 07.06 Review 08.01 Introduction to Threading 08.02 Creating Threads 08.03 Locks 08.04 Shared Locks 08.05 Atomic Values 08.06 Async 08.07 Condition Variables 08.08 Project 08.09 Review 09.01 Concepts 09.02 Modules 09.03 Coroutines 09.04 Course Review","title":"C++ Standard Template Library in Practice"},{"location":"Coding/C%2B%2B/#complete-c-developer-course","text":"/# Topic Video Projects 1.1 Section Overview 1.2 Getting Started on Windows with Visual Studio Integrated Development Environment (IDE) 1.3 Getting Started on macOS or Linux with CodeBlocks IDE 1.4 Getting Started with macOS Catalina or Higher with Visual Studio Code 1.5 Finding Answers to Your Questions 2.1 Section Overview 2.2 Saying \"Hello\" to C++ 2.3 Variables and Data Types - Part 1 2.4 Variables and Data Types - Part Two 2.5 Variables and Data Types - Part Three 2.6 Comments 2.7 Arithmetic Operators 2.8 Relational Operators 2.9 Logical Operators 2.10 Symbolic Constants and Naming Conventions 2.11 User Input 2.12 Project - Average of Three 2.13 Project - MadLibs Clone 2.14 Section Wrap-Up 3.1 Section Overview 3.2 Introduction to Control Statements 3.3 Selection Control Statements 3.4 Repetition Control Statements 3.5 The Break and Continue Statements 3.6 Random Numbers 3.7 Project - Jam of the Month Club 3.8 Project - Odds and Evens 3.9 Project - Guess the Number 3.10 Section Wrap-Up 4.1 Section Overview 4.2 Built-in Arrays 4.3 The Array Class 4.4 The Vector Class 4.5 Multi-Dimensional Arrays 4.6 Project - Array Data 4.7 Project - Vector Data 4.8 Project - Parallel Arrays/Vectors 4.9 Section Wrap-Up 5.1 Section Overview 5.2 Function Prototypes and Definitions FunctionFun.cpp 5.3 Function Return Types and Parameters 5.4 Parameter Passing: Pass-by-Value and Pass-by-Reference PassingSchemes.cpp 5.5 Variable Scope and Lifetime ScopeFun 5.6 Function Overloading 5.7 The <cmath> Library 5.8 Recursion 5.9 Project - Return the Product of Three Parameters 5.10 Project - Return the Sum of Built-in Array Elements 5.11 Project - Return the Sum of Array Object Elements 5.12 Project - Retrieve the Sum of Array Object Elements by Reference 5.13 Project - Tic-Tac-Toe (ADVANCED) 5.14 Section Wrap-Up 6.1 Section Overview 6.2 Basics of Object Oriented Programming (OOP) 6.3 Encapsulation: Data Members and Member Functions 6.4 Separate Compilation 6.5 Constructors and Destructors 6.6 A Rectangle Class Rectangle.cpp 6.7 A Book Class 6.8 Project - A Bank Account Class 6.9 Project - A Pizza Class 6.10 Project - A Circle Class 6.11 Section Wrap-Up 7.1 Section Overview 7.2 Exceptions and the Exception Hierarchy 7.3 Logic Errors 7.4 Runtime Errors and Throwing Exceptions 7.5 Rethrowing Exceptions 7.6 Custom Exceptions 7.7 Basic Testing and Debugging 7.8 Project - Throwing and Handling an Out_of_Range Exception 7.9 Project - Creating and Using Your Own Exception 7.10 Section Wrap-Up 8.1 Section Overview 8.2 Introduction to Pointers 8.3 Dynamic Memory - Part 1 8.4 Dynamic Memory (- Part 2 8.5 Const Correctness 8.6 Project - Dynamically Creating Rectangles 8.7 Project - Dynamically Creating Circles 8.8 Section Wrap-Up 9.1 Section Overview 9.2 Sequential File Output 9.3 Sequential File Input 9.4 More File Input/ Output (I/O) 9.5 Project - Reading Data from File and Printing Statistics 9.6 Project - Dynamically Creating Rectangles from File 9.7 Project - Shopping Item File 9.8 Section Wrap-Up 10.1 Section Overview 10.2 Inheritance - Part 1 10.3 Inheritance - Part 2) 10.4 Polymorphism and Late Binding 10.5 Enumerated Types This video explains enumerated types. EnumFun.cpp 10.6 Project - Derived Cat Class Cat.cpp 10.7 Project \u2013 Role Playing Game (RPG) Player Character Creation RPGCharGen 10.8 Section Wrap-Up 11.1 Section Overview 11.2 Templates - Standard Template Library (STL) TemplateFun.cpp 11.3 queue<T> stack<T> deque<T> Standard Template Library (STL) - Part 1) DequeFun.cpp, StackFun.cpp, QueueProject.cpp 11.4 map<T> Standard Template Library (STL) - Part 2 ContactsFun.cpp, AlgorithmFun.cpp 11.5 unique_ptr<T> Smart Pointers SmartPointerFun.cpp, Car.cpp 11.6 Friend functions Friend Functions and Friend Classes FriendFunctions.cpp 11.7 Operator Overloading OverloadingFun.cpp, Rectangle.h 11.8 map<T> Project - Dictionary of Terms DictionaryProject.cpp 11.9 Project - Aliens Aliens.cpp 11.10 Section Wrap-Up","title":"Complete C++ Developer Course"},{"location":"Coding/C%2B%2B/#rpgchargen","text":"cpp h #include <iostream> #include \"RPGCharGen.h\" using namespace std ; int main () { Warrior w { \"Doofus McGroober\" , Race :: HUMAN }; cout << \"Player name: \" << w . getName () << endl ; cout << \"Player HP: \" << w . getHp () << endl ; cout << \"Player MP: \" << w . getMp () << endl ; cout << \"Player race: \" << w . getRace () << endl ; w . attack (); Priest m { \"Brother Tolkien\" , Race :: ELF }; cout << \"Player name: \" << m . getName () << endl ; cout << \"Player HP: \" << m . getHp () << endl ; cout << \"Player MP: \" << m . getMp () << endl ; cout << \"Player race: \" << m . getRace () << endl ; m . attack (); Mage n { \"Smart Frodo\" , Race :: DWARF }; cout << \"Player name: \" << n . getName () << endl ; cout << \"Player HP: \" << n . getHp () << endl ; cout << \"Player MP: \" << n . getMp () << endl ; cout << \"Player race: \" << n . getRace () << endl ; n . attack (); return 0 ; } #if !defined(RPGCHARGEN_H) #define RPGCHARGEN_H #include <string> enum Race { HUMAN , ELF , DWARF }; class Player { protected : std :: string _name { \"Johnny Bravo\" }; Race _race { Race :: HUMAN }; int _hp { 100 }; int _mp { 100 }; public : Player ( std :: string n , Race r , int hp , int mp ) : _name { n }, _race { r }, _hp ( hp ), _mp ( mp ) {} virtual std :: string attack () = 0 ; int getHp () { return _hp ; } int getMp () { return _mp ; } std :: string getRace () { switch ( _race ) { case 0 : return \"human\" ; break ; case 1 : return \"elf\" ; break ; case 2 : return \"dwarf\" ; break ; default : return \"none\" ; break ; } } std :: string getName () { return _name ; } void setHp ( int n ) { _hp = n ; } void setMp ( int n ) { _mp = n ; } void setName ( std :: string s ) { _name = s ; } void setRace ( Race r ) { _race = r ;} }; class Warrior : public Player { public : Warrior ( std :: string n , Race r ) : Player ( n , r , 200 , 0 ) {} std :: string attack () { return \"I will destroy you with my sword, foul demon!\" ;} }; class Priest : public Player { public : Priest ( std :: string n , Race r ) : Player ( n , r , 100 , 200 ) {} std :: string attack () { return \"Taste the wrath of the Two True Gods!\" ;} }; class Mage : public Player { public : Mage ( std :: string n , Race r ) : Player ( n , r , 150 , 150 ) {} std :: string attack () { return \"You are overmatched by my esoteric artifices!\" ;} }; #endif // RPGCHARGEN_H","title":"RPGCharGen"},{"location":"Coding/C/","text":"C Linux Compiling pkg-config Compiling a GTK program written in C gcc -Wall -g helloworld.c -o helloworld \\ $( pkg-config --cflags gtk+-3.0 ) \\ // ( 1 ) $( pkg-config --libs gtk+-3.0 ) // ( 2 ) pkg-config returns directory names, which gcc will use tto ensure all header files are available. Here, pkg-config appends options to eh command-line used by the linker including library directory path extensions and a list of libraries needed for linking to the executable. Syscalls The open() , close() , read() , and write() syscalls form the heart of low-level file I/O in Linux. Some system calls accept flag arguments , specified using symbolic constants. Some of these are single-bit values which are combined into a single value using the bitwise OR operator | . close close ( fd ); #include <fcntl.h> #include <stdlib.h> #define BSIZE 16384 void main () { int fin , fout ; char buf [ BSIZE ]; int count ; if (( fin = open ( \"foo\" , O_RDONLY )) < 0 ) { perror ( \"foo\" ); exit ( 1 ); } if (( fout = open ( \"bar\" , O_WRONLY | O_CREAT , 0644 )) < 0 ) { perror ( \"bar\" ); exit ( 2 ); } while (( count = read ( fin , buf , BSIZE )) > 0 ) write ( fout , buf , count ); close ( fin ); close ( fout ); } exec There are seven variations of exec() , all of which are used to replace the current process with the contents of another thread. These variations are distinguished by how they pass arguments ( l ist or v ector), whether or not they create a new environment ( e ), and whether they require a full pathname or must search on the path environment variable ( p ). For example, execvpe specifies an executable on the path, creates a new environment, and passes arguments in a vector. #include <stdio.h> #include <unistd.h> #include <stdlib.h> main () { char line [ 100 ]; while ( printf ( \"> \" ), gets ( line ) != NULL ) { if ( fork () == 0 ) { execlp ( line , line , ( char * ) 0 ); printf ( \"%s: not found \\n \" , line ); exit ( 1 ); } else wait ( 0 ); } } exit exit() ends the program, returning the integer provided in parentheses as the exit status of the process. fork fork() is used to create a new process and is typically associated with exec() and wait() . This simple example shows how the value returned by the fork() call differs between the parent and child processes. #include <stdio.h> void main () { if ( fork ()) // i.e. anything but 0 printf ( \"I am the parent \\n \" ); else printf ( \"I am the child \\n \" ); } #include <sys/types.h> #include <stdio.h> #include <unistd.h> int main () { pid_t pid ; pid = fork (); if ( pid < 0 ) { fprintf ( stderr , \"Fork Failed\" ); return 1 ; } else if ( pid == 0 ) { execlp ( \"/bin/ls\" , \"ls\" , NULL ); } else { wait ( NULL ); printf ( \"Child complete\" ); } return 0 ; } ftruncate #include <stdio.h> #include <stdlib.h> #include <fcntl.h> #include <sys/shm.h> #include <sys/stat.h> #include <string.h> #include <sys/mman.h> int main () { const int SIZE = 4096 ; // size of shared memory object (bytes) const char * name = \"OS\" ; // name of shared memory object const char * message_0 = \"Hello\" ; const char * message_1 = \"World!\" ; int fd ; // shared memory file descriptor char * ptr ; // pointer to shared memory object // create the shared memory object fd = shm_open ( name , O_CREAT | O_RDWR , 0666 ); // configure size of the shared memory object ftruncate ( fd , SIZE ); // memory map the shared memory object ptr = ( char * ) mmap ( 0 , SIZE , PROT_READ | PROT_WRITE , MAP_SHARED , fd , 0 ); // write to shared memory object sprintf ( ptr , \"%s\" , message_0 ); ptr += strlen ( message_0 ); sprintf ( ptr , \"%s\" , message_1 ); ptr += strlen ( message_1 ); return 0 ; } getpid #include <unistd.h> getpid (); #include <stdio.h> #include <stdlib.h> int main () { int status ; if ( fork ()) { printf ( \"parent waiting for child ... \\n \" ); wait ( & status ); if ( WIFEXITED ( status )) printf ( \"child ended normally, exit status = %d \\n \" , WEXITSTATUS ( status )); if ( WIFSIGNALED ( status )) printf ( \"child terminated by signal %d \\n \" , WTERMSIG ( status )); } else { printf ( \"child running -- PID is %d \\n \" , getpid ()); sleep ( 50 ); exit ( 3 ); } } getrandom Introduced in Linux 3.17 to allow userspace applications to request random numbers in the case of no access to random devices (i.e. containers). By default it will use the /dev/urandom pool, which normally does not block. A flag can be provided to use the /dev/random pool instead. lseek Repositions the file read/write offset, allowing random access to an open file descriptor. #include <unistd.h> lseek ( fd , offset , // (1) whence // (2) ); Byte offset, positive or negative (usually negative when with respect to end-of file using SEEK_END flag). Accepts one of several flags that determine where the offset is relative to: SEEK_SET (beginning of file), SEEK_CUR (current position), or SEEK_END (end of file). - -8 <-- malloc memcpy Used for copying stack-allocated data. memcpy ( dst , // size , // flags // ); mmap Maps a file into memory, allowing it to be accessed as if were an array. mmap ( addr , // (1) length , // (2) prot , // (3) flags , // (4) fd , // (5) offset // (6) ); // (7) Commonly set to NULL , allowing the kernel to choose the address. Length of the mapping Typically a combination of PROT_READ and/or PROT_WRITE Determines whether the mapped region is shared with other processes: MAP_SHARED or MAP_PRIVATE File descriptor from open() Multiple of page size, and often set to 0 to map the entire file Return value is the address to which the file has been mapped (similar to malloc() ) IPC producer IPC consumer #include <stdio.h> #include <stdlib.h> #include <fcntl.h> #include <sys/shm.h> #include <sys/stat.h> #include <string.h> #include <sys/mman.h> int main () { const int SIZE = 4096 ; // size of shared memory object (bytes) const char * name = \"OS\" ; // name of shared memory object const char * message_0 = \"Hello\" ; const char * message_1 = \"World!\" ; int fd ; // shared memory file descriptor char * ptr ; // pointer to shared memory object // create the shared memory object fd = shm_open ( name , O_CREAT | O_RDWR , 0666 ); // configure size of the shared memory object ftruncate ( fd , SIZE ); // memory map the shared memory object ptr = ( char * ) mmap ( 0 , SIZE , PROT_READ | PROT_WRITE , MAP_SHARED , fd , 0 ); // write to shared memory object sprintf ( ptr , \"%s\" , message_0 ); ptr += strlen ( message_0 ); sprintf ( ptr , \"%s\" , message_1 ); ptr += strlen ( message_1 ); return 0 ; } #include <stdio.h> #include <stdlib.h> #include <fcntl.h> #include <sys/shm.h> #include <sys/stat.h> #include <sys/mman.h> int main () { const int SIZE = 4096 ; const char * name = \"OS\" ; int fd ; char * ptr ; fd = shm_open ( name , O_RDONLY , 0666 ); ptr = ( char * ) mmap ( 0 , SIZE , PROT_READ | PROT_WRITE , MAP_SHARED , fd , 0 ); printf ( \"%s\" , ( char * ) ptr ); shm_unlink ( name ); return 0 ; } msync open A call to open() creates a new open file descriptor fd = open ( \"foo\" , O_RDWR | // (1) O_TRUNC | // (2) O_APPEND // (3) ); Access mode flag specifying reading and writing: O_RDWR , O_RDONLY , or O_WRONLY . Open-time flag required for writing. However, calling ftruncate() is recommended over use of this flag in open() , which is retained for backwards compatibility. Operating mode flag that makes all write operations write data at the end of the file, extending it, regardless of the current file position. #include <fcntl.h> #include <stdlib.h> #define BSIZE 16384 void main () { int fin , fout ; char buf [ BSIZE ]; int count ; if (( fin = open ( \"foo\" , O_RDONLY )) < 0 ) { perror ( \"foo\" ); exit ( 1 ); } if (( fout = open ( \"bar\" , O_WRONLY | O_CREAT , 0644 )) < 0 ) { perror ( \"bar\" ); exit ( 2 ); } while (( count = read ( fin , buf , BSIZE )) > 0 ) write ( fout , buf , count ); close ( fin ); close ( fout ); } read Like write() , calls to read() require require a pointer to the buffer as well as the count of bytes which must not exceed the buffer's actual size. read() will return the number of bytes actually read and 0 on end-of-file. read ( fd , buffer , count ); #include <fcntl.h> #include <stdlib.h> #define BSIZE 16384 void main () { int fin , fout ; char buf [ BSIZE ]; int count ; if (( fin = open ( \"foo\" , O_RDONLY )) < 0 ) { perror ( \"foo\" ); exit ( 1 ); } if (( fout = open ( \"bar\" , O_WRONLY | O_CREAT , 0644 )) < 0 ) { perror ( \"bar\" ); exit ( 2 ); } while (( count = read ( fin , buf , BSIZE )) > 0 ) write ( fout , buf , count ); close ( fin ); close ( fout ); } shm_open #include <stdio.h> #include <stdlib.h> #include <fcntl.h> #include <sys/shm.h> #include <sys/stat.h> #include <string.h> #include <sys/mman.h> int main () { const int SIZE = 4096 ; // size of shared memory object (bytes) const char * name = \"OS\" ; // name of shared memory object const char * message_0 = \"Hello\" ; const char * message_1 = \"World!\" ; int fd ; // shared memory file descriptor char * ptr ; // pointer to shared memory object // create the shared memory object fd = shm_open ( name , O_CREAT | O_RDWR , 0666 ); // configure size of the shared memory object ftruncate ( fd , SIZE ); // memory map the shared memory object ptr = ( char * ) mmap ( 0 , SIZE , PROT_READ | PROT_WRITE , MAP_SHARED , fd , 0 ); // write to shared memory object sprintf ( ptr , \"%s\" , message_0 ); ptr += strlen ( message_0 ); sprintf ( ptr , \"%s\" , message_1 ); ptr += strlen ( message_1 ); return 0 ; } shm_unlink Removes the shared-memory segment after the consumer has accessed it. #include <stdio.h> #include <stdlib.h> #include <fcntl.h> #include <sys/shm.h> #include <sys/stat.h> #include <sys/mman.h> int main () { const int SIZE = 4096 ; const char * name = \"OS\" ; int fd ; char * ptr ; fd = shm_open ( name , O_RDONLY , 0666 ); ptr = ( char * ) mmap ( 0 , SIZE , PROT_READ | PROT_WRITE , MAP_SHARED , fd , 0 ); printf ( \"%s\" , ( char * ) ptr ); shm_unlink ( name ); return 0 ; } wait wait() blocks until one of the process's children terminates and returns an integer. int status ; wait ( & status ); // (1) Passing 0 or NULL will discard the child's exit status. The exit status is separted into upper and lower bytes. If the process was killed by a signal the top bit of the lower byte is called the \"Core Dumped\" flag. There are several macros used to analyze the exit status. WIFEXITED true if child exited normally WEXITSTATUS WIFSIGNALED true if child terminated by signal WTERMSIG signal number #include <stdio.h> #include <stdlib.h> int main () { int status ; if ( fork ()) { printf ( \"parent waiting for child ... \\n \" ); wait ( & status ); if ( WIFEXITED ( status )) printf ( \"child ended normally, exit status = %d \\n \" , WEXITSTATUS ( status )); if ( WIFSIGNALED ( status )) printf ( \"child terminated by signal %d \\n \" , WTERMSIG ( status )); } else { printf ( \"child running -- PID is %d \\n \" , getpid ()); sleep ( 50 ); exit ( 3 ); } } #include <sys/types.h> #include <stdio.h> #include <unistd.h> int main () { pid_t pid ; pid = fork (); if ( pid < 0 ) { fprintf ( stderr , \"Fork Failed\" ); return 1 ; } else if ( pid == 0 ) { execlp ( \"/bin/ls\" , \"ls\" , NULL ); } else { wait ( NULL ); printf ( \"Child complete\" ); } return 0 ; } write Like read() , write() takes an integer file descriptor, a pointer to the buffer, as well as a count of bytes which must not exceed the buffer's size. write ( fd , buffer , count ); #include <fcntl.h> #include <stdlib.h> #define BSIZE 16384 void main () { int fin , fout ; char buf [ BSIZE ]; int count ; if (( fin = open ( \"foo\" , O_RDONLY )) < 0 ) { perror ( \"foo\" ); exit ( 1 ); } if (( fout = open ( \"bar\" , O_WRONLY | O_CREAT , 0644 )) < 0 ) { perror ( \"bar\" ); exit ( 2 ); } while (( count = read ( fin , buf , BSIZE )) > 0 ) write ( fout , buf , count ); close ( fin ); close ( fout ); } Pthreads Pthreads provides an API for multithreaded programming in C. In Pthreads, new threads are spawned explicitly using pthread_create passing the name of a function which the thread will run. Notably, this function must have precisely the following signature: void * func ( void * arg ) Also, when compiling a program using Pthreads with gcc the -lpthread option must be set. pthread_attr_init #include <pthread.h> #include <stdio.h> #include <stdlib.h> int sum ; void * runner ( void * param ); int main ( int argc , char * argv []) { pthread_t tid ; pthread_attr_t attr ; pthread_attr_init ( & attr ); pthread_create ( & tid , & attr , runner , argv [ 1 ]); pthread_join ( tid , NULL ); printf ( \"sum = %d \\n \" , sum ); } void * runner ( void * param ) { int i , upper = atoi ( param ); sum = 0 ; for ( i = 1 ; i <= upper ; i ++ ) sum += i ; pthread_exit ( 0 ); } pthread_create #include <pthread.h> #include <stdio.h> #include <stdlib.h> int sum ; void * runner ( void * param ); int main ( int argc , char * argv []) { pthread_t tid ; pthread_attr_t attr ; pthread_attr_init ( & attr ); pthread_create ( & tid , & attr , runner , argv [ 1 ]); pthread_join ( tid , NULL ); printf ( \"sum = %d \\n \" , sum ); } void * runner ( void * param ) { int i , upper = atoi ( param ); sum = 0 ; for ( i = 1 ; i <= upper ; i ++ ) sum += i ; pthread_exit ( 0 ); } pthread_exit #include <pthread.h> #include <stdio.h> #include <stdlib.h> int sum ; void * runner ( void * param ); int main ( int argc , char * argv []) { pthread_t tid ; pthread_attr_t attr ; pthread_attr_init ( & attr ); pthread_create ( & tid , & attr , runner , argv [ 1 ]); pthread_join ( tid , NULL ); printf ( \"sum = %d \\n \" , sum ); } void * runner ( void * param ) { int i , upper = atoi ( param ); sum = 0 ; for ( i = 1 ; i <= upper ; i ++ ) sum += i ; pthread_exit ( 0 ); } pthread_join #include <pthread.h> #include <stdio.h> #include <stdlib.h> int sum ; void * runner ( void * param ); int main ( int argc , char * argv []) { pthread_t tid ; pthread_attr_t attr ; pthread_attr_init ( & attr ); pthread_create ( & tid , & attr , runner , argv [ 1 ]); pthread_join ( tid , NULL ); printf ( \"sum = %d \\n \" , sum ); } void * runner ( void * param ) { int i , upper = atoi ( param ); sum = 0 ; for ( i = 1 ; i <= upper ; i ++ ) sum += i ; pthread_exit ( 0 ); } Structs task_struct Represents the process control block Tasks GTK Hello, World! #include <gtk/gtk.h> int main ( int argc , char * argv []) { GtkWidget * window ; gtk_init ( & argc , & argv ); window = gtk_window_new ( GTK_WINDOW_TOPLEVEL ); gtk_window_set_title ( GTK_WINDOW ( window ), \"Hello, World!\" ); gtk_container_set_border_width ( GTK_CONTAINER ( window ), 10 ); gtk_widget_set_size_request ( window , 300 , 300 ); g_signal_connect ( G_OBJECT ( window ), \"destroy\" , G_CALLBACK ( gtk_main_quit ), NULL ); // (1) GtkWidget * label = gtk_label_new ( \"Hello, World!\" ); gtk_container_add ( GTK_CONTAINER ( window ), label ); gtk_widget_show_all ( window ); gtk_main (); return 0 ; } Without connecting the signals, the process will not terminate after clicking the close button, although the window will close.","title":"C"},{"location":"Coding/C/#c","text":"","title":"C"},{"location":"Coding/C/#linux","text":"","title":"Linux"},{"location":"Coding/C/#compiling","text":"","title":"Compiling"},{"location":"Coding/C/#pkg-config","text":"Compiling a GTK program written in C gcc -Wall -g helloworld.c -o helloworld \\ $( pkg-config --cflags gtk+-3.0 ) \\ // ( 1 ) $( pkg-config --libs gtk+-3.0 ) // ( 2 ) pkg-config returns directory names, which gcc will use tto ensure all header files are available. Here, pkg-config appends options to eh command-line used by the linker including library directory path extensions and a list of libraries needed for linking to the executable.","title":"pkg-config"},{"location":"Coding/C/#syscalls","text":"The open() , close() , read() , and write() syscalls form the heart of low-level file I/O in Linux. Some system calls accept flag arguments , specified using symbolic constants. Some of these are single-bit values which are combined into a single value using the bitwise OR operator | .","title":"Syscalls"},{"location":"Coding/C/#close","text":"close ( fd ); #include <fcntl.h> #include <stdlib.h> #define BSIZE 16384 void main () { int fin , fout ; char buf [ BSIZE ]; int count ; if (( fin = open ( \"foo\" , O_RDONLY )) < 0 ) { perror ( \"foo\" ); exit ( 1 ); } if (( fout = open ( \"bar\" , O_WRONLY | O_CREAT , 0644 )) < 0 ) { perror ( \"bar\" ); exit ( 2 ); } while (( count = read ( fin , buf , BSIZE )) > 0 ) write ( fout , buf , count ); close ( fin ); close ( fout ); }","title":"close"},{"location":"Coding/C/#exec","text":"There are seven variations of exec() , all of which are used to replace the current process with the contents of another thread. These variations are distinguished by how they pass arguments ( l ist or v ector), whether or not they create a new environment ( e ), and whether they require a full pathname or must search on the path environment variable ( p ). For example, execvpe specifies an executable on the path, creates a new environment, and passes arguments in a vector. #include <stdio.h> #include <unistd.h> #include <stdlib.h> main () { char line [ 100 ]; while ( printf ( \"> \" ), gets ( line ) != NULL ) { if ( fork () == 0 ) { execlp ( line , line , ( char * ) 0 ); printf ( \"%s: not found \\n \" , line ); exit ( 1 ); } else wait ( 0 ); } }","title":"exec"},{"location":"Coding/C/#exit","text":"exit() ends the program, returning the integer provided in parentheses as the exit status of the process.","title":"exit"},{"location":"Coding/C/#fork","text":"fork() is used to create a new process and is typically associated with exec() and wait() . This simple example shows how the value returned by the fork() call differs between the parent and child processes. #include <stdio.h> void main () { if ( fork ()) // i.e. anything but 0 printf ( \"I am the parent \\n \" ); else printf ( \"I am the child \\n \" ); } #include <sys/types.h> #include <stdio.h> #include <unistd.h> int main () { pid_t pid ; pid = fork (); if ( pid < 0 ) { fprintf ( stderr , \"Fork Failed\" ); return 1 ; } else if ( pid == 0 ) { execlp ( \"/bin/ls\" , \"ls\" , NULL ); } else { wait ( NULL ); printf ( \"Child complete\" ); } return 0 ; }","title":"fork"},{"location":"Coding/C/#ftruncate","text":"#include <stdio.h> #include <stdlib.h> #include <fcntl.h> #include <sys/shm.h> #include <sys/stat.h> #include <string.h> #include <sys/mman.h> int main () { const int SIZE = 4096 ; // size of shared memory object (bytes) const char * name = \"OS\" ; // name of shared memory object const char * message_0 = \"Hello\" ; const char * message_1 = \"World!\" ; int fd ; // shared memory file descriptor char * ptr ; // pointer to shared memory object // create the shared memory object fd = shm_open ( name , O_CREAT | O_RDWR , 0666 ); // configure size of the shared memory object ftruncate ( fd , SIZE ); // memory map the shared memory object ptr = ( char * ) mmap ( 0 , SIZE , PROT_READ | PROT_WRITE , MAP_SHARED , fd , 0 ); // write to shared memory object sprintf ( ptr , \"%s\" , message_0 ); ptr += strlen ( message_0 ); sprintf ( ptr , \"%s\" , message_1 ); ptr += strlen ( message_1 ); return 0 ; }","title":"ftruncate"},{"location":"Coding/C/#getpid","text":"#include <unistd.h> getpid (); #include <stdio.h> #include <stdlib.h> int main () { int status ; if ( fork ()) { printf ( \"parent waiting for child ... \\n \" ); wait ( & status ); if ( WIFEXITED ( status )) printf ( \"child ended normally, exit status = %d \\n \" , WEXITSTATUS ( status )); if ( WIFSIGNALED ( status )) printf ( \"child terminated by signal %d \\n \" , WTERMSIG ( status )); } else { printf ( \"child running -- PID is %d \\n \" , getpid ()); sleep ( 50 ); exit ( 3 ); } }","title":"getpid"},{"location":"Coding/C/#getrandom","text":"Introduced in Linux 3.17 to allow userspace applications to request random numbers in the case of no access to random devices (i.e. containers). By default it will use the /dev/urandom pool, which normally does not block. A flag can be provided to use the /dev/random pool instead.","title":"getrandom"},{"location":"Coding/C/#lseek","text":"Repositions the file read/write offset, allowing random access to an open file descriptor. #include <unistd.h> lseek ( fd , offset , // (1) whence // (2) ); Byte offset, positive or negative (usually negative when with respect to end-of file using SEEK_END flag). Accepts one of several flags that determine where the offset is relative to: SEEK_SET (beginning of file), SEEK_CUR (current position), or SEEK_END (end of file). - -8 <--","title":"lseek"},{"location":"Coding/C/#malloc","text":"","title":"malloc"},{"location":"Coding/C/#memcpy","text":"Used for copying stack-allocated data. memcpy ( dst , // size , // flags // );","title":"memcpy"},{"location":"Coding/C/#mmap","text":"Maps a file into memory, allowing it to be accessed as if were an array. mmap ( addr , // (1) length , // (2) prot , // (3) flags , // (4) fd , // (5) offset // (6) ); // (7) Commonly set to NULL , allowing the kernel to choose the address. Length of the mapping Typically a combination of PROT_READ and/or PROT_WRITE Determines whether the mapped region is shared with other processes: MAP_SHARED or MAP_PRIVATE File descriptor from open() Multiple of page size, and often set to 0 to map the entire file Return value is the address to which the file has been mapped (similar to malloc() ) IPC producer IPC consumer #include <stdio.h> #include <stdlib.h> #include <fcntl.h> #include <sys/shm.h> #include <sys/stat.h> #include <string.h> #include <sys/mman.h> int main () { const int SIZE = 4096 ; // size of shared memory object (bytes) const char * name = \"OS\" ; // name of shared memory object const char * message_0 = \"Hello\" ; const char * message_1 = \"World!\" ; int fd ; // shared memory file descriptor char * ptr ; // pointer to shared memory object // create the shared memory object fd = shm_open ( name , O_CREAT | O_RDWR , 0666 ); // configure size of the shared memory object ftruncate ( fd , SIZE ); // memory map the shared memory object ptr = ( char * ) mmap ( 0 , SIZE , PROT_READ | PROT_WRITE , MAP_SHARED , fd , 0 ); // write to shared memory object sprintf ( ptr , \"%s\" , message_0 ); ptr += strlen ( message_0 ); sprintf ( ptr , \"%s\" , message_1 ); ptr += strlen ( message_1 ); return 0 ; } #include <stdio.h> #include <stdlib.h> #include <fcntl.h> #include <sys/shm.h> #include <sys/stat.h> #include <sys/mman.h> int main () { const int SIZE = 4096 ; const char * name = \"OS\" ; int fd ; char * ptr ; fd = shm_open ( name , O_RDONLY , 0666 ); ptr = ( char * ) mmap ( 0 , SIZE , PROT_READ | PROT_WRITE , MAP_SHARED , fd , 0 ); printf ( \"%s\" , ( char * ) ptr ); shm_unlink ( name ); return 0 ; }","title":"mmap"},{"location":"Coding/C/#msync","text":"","title":"msync"},{"location":"Coding/C/#open","text":"A call to open() creates a new open file descriptor fd = open ( \"foo\" , O_RDWR | // (1) O_TRUNC | // (2) O_APPEND // (3) ); Access mode flag specifying reading and writing: O_RDWR , O_RDONLY , or O_WRONLY . Open-time flag required for writing. However, calling ftruncate() is recommended over use of this flag in open() , which is retained for backwards compatibility. Operating mode flag that makes all write operations write data at the end of the file, extending it, regardless of the current file position. #include <fcntl.h> #include <stdlib.h> #define BSIZE 16384 void main () { int fin , fout ; char buf [ BSIZE ]; int count ; if (( fin = open ( \"foo\" , O_RDONLY )) < 0 ) { perror ( \"foo\" ); exit ( 1 ); } if (( fout = open ( \"bar\" , O_WRONLY | O_CREAT , 0644 )) < 0 ) { perror ( \"bar\" ); exit ( 2 ); } while (( count = read ( fin , buf , BSIZE )) > 0 ) write ( fout , buf , count ); close ( fin ); close ( fout ); }","title":"open"},{"location":"Coding/C/#read","text":"Like write() , calls to read() require require a pointer to the buffer as well as the count of bytes which must not exceed the buffer's actual size. read() will return the number of bytes actually read and 0 on end-of-file. read ( fd , buffer , count ); #include <fcntl.h> #include <stdlib.h> #define BSIZE 16384 void main () { int fin , fout ; char buf [ BSIZE ]; int count ; if (( fin = open ( \"foo\" , O_RDONLY )) < 0 ) { perror ( \"foo\" ); exit ( 1 ); } if (( fout = open ( \"bar\" , O_WRONLY | O_CREAT , 0644 )) < 0 ) { perror ( \"bar\" ); exit ( 2 ); } while (( count = read ( fin , buf , BSIZE )) > 0 ) write ( fout , buf , count ); close ( fin ); close ( fout ); }","title":"read"},{"location":"Coding/C/#shm_open","text":"#include <stdio.h> #include <stdlib.h> #include <fcntl.h> #include <sys/shm.h> #include <sys/stat.h> #include <string.h> #include <sys/mman.h> int main () { const int SIZE = 4096 ; // size of shared memory object (bytes) const char * name = \"OS\" ; // name of shared memory object const char * message_0 = \"Hello\" ; const char * message_1 = \"World!\" ; int fd ; // shared memory file descriptor char * ptr ; // pointer to shared memory object // create the shared memory object fd = shm_open ( name , O_CREAT | O_RDWR , 0666 ); // configure size of the shared memory object ftruncate ( fd , SIZE ); // memory map the shared memory object ptr = ( char * ) mmap ( 0 , SIZE , PROT_READ | PROT_WRITE , MAP_SHARED , fd , 0 ); // write to shared memory object sprintf ( ptr , \"%s\" , message_0 ); ptr += strlen ( message_0 ); sprintf ( ptr , \"%s\" , message_1 ); ptr += strlen ( message_1 ); return 0 ; }","title":"shm_open"},{"location":"Coding/C/#shm_unlink","text":"Removes the shared-memory segment after the consumer has accessed it. #include <stdio.h> #include <stdlib.h> #include <fcntl.h> #include <sys/shm.h> #include <sys/stat.h> #include <sys/mman.h> int main () { const int SIZE = 4096 ; const char * name = \"OS\" ; int fd ; char * ptr ; fd = shm_open ( name , O_RDONLY , 0666 ); ptr = ( char * ) mmap ( 0 , SIZE , PROT_READ | PROT_WRITE , MAP_SHARED , fd , 0 ); printf ( \"%s\" , ( char * ) ptr ); shm_unlink ( name ); return 0 ; }","title":"shm_unlink"},{"location":"Coding/C/#wait","text":"wait() blocks until one of the process's children terminates and returns an integer. int status ; wait ( & status ); // (1) Passing 0 or NULL will discard the child's exit status. The exit status is separted into upper and lower bytes. If the process was killed by a signal the top bit of the lower byte is called the \"Core Dumped\" flag. There are several macros used to analyze the exit status. WIFEXITED true if child exited normally WEXITSTATUS WIFSIGNALED true if child terminated by signal WTERMSIG signal number #include <stdio.h> #include <stdlib.h> int main () { int status ; if ( fork ()) { printf ( \"parent waiting for child ... \\n \" ); wait ( & status ); if ( WIFEXITED ( status )) printf ( \"child ended normally, exit status = %d \\n \" , WEXITSTATUS ( status )); if ( WIFSIGNALED ( status )) printf ( \"child terminated by signal %d \\n \" , WTERMSIG ( status )); } else { printf ( \"child running -- PID is %d \\n \" , getpid ()); sleep ( 50 ); exit ( 3 ); } } #include <sys/types.h> #include <stdio.h> #include <unistd.h> int main () { pid_t pid ; pid = fork (); if ( pid < 0 ) { fprintf ( stderr , \"Fork Failed\" ); return 1 ; } else if ( pid == 0 ) { execlp ( \"/bin/ls\" , \"ls\" , NULL ); } else { wait ( NULL ); printf ( \"Child complete\" ); } return 0 ; }","title":"wait"},{"location":"Coding/C/#write","text":"Like read() , write() takes an integer file descriptor, a pointer to the buffer, as well as a count of bytes which must not exceed the buffer's size. write ( fd , buffer , count ); #include <fcntl.h> #include <stdlib.h> #define BSIZE 16384 void main () { int fin , fout ; char buf [ BSIZE ]; int count ; if (( fin = open ( \"foo\" , O_RDONLY )) < 0 ) { perror ( \"foo\" ); exit ( 1 ); } if (( fout = open ( \"bar\" , O_WRONLY | O_CREAT , 0644 )) < 0 ) { perror ( \"bar\" ); exit ( 2 ); } while (( count = read ( fin , buf , BSIZE )) > 0 ) write ( fout , buf , count ); close ( fin ); close ( fout ); }","title":"write"},{"location":"Coding/C/#pthreads","text":"Pthreads provides an API for multithreaded programming in C. In Pthreads, new threads are spawned explicitly using pthread_create passing the name of a function which the thread will run. Notably, this function must have precisely the following signature: void * func ( void * arg ) Also, when compiling a program using Pthreads with gcc the -lpthread option must be set.","title":"Pthreads"},{"location":"Coding/C/#pthread_attr_init","text":"#include <pthread.h> #include <stdio.h> #include <stdlib.h> int sum ; void * runner ( void * param ); int main ( int argc , char * argv []) { pthread_t tid ; pthread_attr_t attr ; pthread_attr_init ( & attr ); pthread_create ( & tid , & attr , runner , argv [ 1 ]); pthread_join ( tid , NULL ); printf ( \"sum = %d \\n \" , sum ); } void * runner ( void * param ) { int i , upper = atoi ( param ); sum = 0 ; for ( i = 1 ; i <= upper ; i ++ ) sum += i ; pthread_exit ( 0 ); }","title":"pthread_attr_init"},{"location":"Coding/C/#pthread_create","text":"#include <pthread.h> #include <stdio.h> #include <stdlib.h> int sum ; void * runner ( void * param ); int main ( int argc , char * argv []) { pthread_t tid ; pthread_attr_t attr ; pthread_attr_init ( & attr ); pthread_create ( & tid , & attr , runner , argv [ 1 ]); pthread_join ( tid , NULL ); printf ( \"sum = %d \\n \" , sum ); } void * runner ( void * param ) { int i , upper = atoi ( param ); sum = 0 ; for ( i = 1 ; i <= upper ; i ++ ) sum += i ; pthread_exit ( 0 ); }","title":"pthread_create"},{"location":"Coding/C/#pthread_exit","text":"#include <pthread.h> #include <stdio.h> #include <stdlib.h> int sum ; void * runner ( void * param ); int main ( int argc , char * argv []) { pthread_t tid ; pthread_attr_t attr ; pthread_attr_init ( & attr ); pthread_create ( & tid , & attr , runner , argv [ 1 ]); pthread_join ( tid , NULL ); printf ( \"sum = %d \\n \" , sum ); } void * runner ( void * param ) { int i , upper = atoi ( param ); sum = 0 ; for ( i = 1 ; i <= upper ; i ++ ) sum += i ; pthread_exit ( 0 ); }","title":"pthread_exit"},{"location":"Coding/C/#pthread_join","text":"#include <pthread.h> #include <stdio.h> #include <stdlib.h> int sum ; void * runner ( void * param ); int main ( int argc , char * argv []) { pthread_t tid ; pthread_attr_t attr ; pthread_attr_init ( & attr ); pthread_create ( & tid , & attr , runner , argv [ 1 ]); pthread_join ( tid , NULL ); printf ( \"sum = %d \\n \" , sum ); } void * runner ( void * param ) { int i , upper = atoi ( param ); sum = 0 ; for ( i = 1 ; i <= upper ; i ++ ) sum += i ; pthread_exit ( 0 ); }","title":"pthread_join"},{"location":"Coding/C/#structs","text":"","title":"Structs"},{"location":"Coding/C/#task_struct","text":"Represents the process control block","title":"task_struct"},{"location":"Coding/C/#tasks","text":"","title":"Tasks"},{"location":"Coding/C/#gtk","text":"","title":"GTK"},{"location":"Coding/C/#hello-world","text":"#include <gtk/gtk.h> int main ( int argc , char * argv []) { GtkWidget * window ; gtk_init ( & argc , & argv ); window = gtk_window_new ( GTK_WINDOW_TOPLEVEL ); gtk_window_set_title ( GTK_WINDOW ( window ), \"Hello, World!\" ); gtk_container_set_border_width ( GTK_CONTAINER ( window ), 10 ); gtk_widget_set_size_request ( window , 300 , 300 ); g_signal_connect ( G_OBJECT ( window ), \"destroy\" , G_CALLBACK ( gtk_main_quit ), NULL ); // (1) GtkWidget * label = gtk_label_new ( \"Hello, World!\" ); gtk_container_add ( GTK_CONTAINER ( window ), label ); gtk_widget_show_all ( window ); gtk_main (); return 0 ; } Without connecting the signals, the process will not terminate after clicking the close button, although the window will close.","title":"Hello, World!"},{"location":"Coding/CS/","text":"MSCS Links Program description Course descriptions Core requirements COP 6611 Operating Systems Course description (2021-2022) Syllabus Book: Silberschatz, et al. Operating Systems Concepts . 10th ed. EEL 6764 Principles of computer architecture Course description (2021-2022) Syllabus Book: Hennessy, et al. Computer Architecture: A Quantitative Approach . COT 6405 Theory of Algorithms Course description (2021-2022) Syllabus Book: Cormen, et al. Introduction to Algorithms . 3rd ed. Operating Systems System calls provide an interface to operating system services, but are to be distinguished from API s (like libc in Linux) that expose functions intended for use by application programmers and abstract and hide the implementation detail of system calls. The runtime environment (RTE) includes all software needed to run applications written in a particular language, such as compilers, interpreters, libraries, and loaders. The RTE constitutes a system-call interface that links API functional calls to OS syscalls. At the level of CPU architecture, the ABI defines how binary code interfaces for a given OS on a given architecture. Three methods passing parameters to system calls: Parameters passed to registers individually Passing to a single register the address a block or table in memory containing parameters (if there are more parameters than registers) Push parameters onto a stack, which is popped by the OS Syscalls can be grouped into several functional categories: Process control: creating, controlling, and ending processes File management: creating, deleting, opening, reading, writing, repositioning files and getting and setting file attributes Device management: granting access to system resources, including physical and virtual devices Information maintenance: dumping memory for debugging, the single step CPU mode which executes a trap after every instruction, as well as trivial calls to retrieve date and time Communications: both message-passing and shared-memory models of IPC as well as network stack Protection: file and disk permissions System utilities facilitate user interfaces to system calls with CLI and GUI tools. Source files are compiled into relocatable object files that are combined into a single binary executable file by the linker , which is then loaded into memory by the loader . Associated with this process is relocation which assigns final memory addresses to the program parts, adjusting code and data accordingly. Most systems allow dynamically linked libraries which are linked and loaded when needed, reducing the size of the executable. Object and executable files have standard formats that determine the layout of the header, instructions, and variables which must be at certain locations in specified structures so that the OS can open the file: Linux: ELF Windows: Portable Executable (PE) macOS: Mach-O Operating systems can be categorized by design methodology: Monolithic kernels have the greatest performance because all the functionality of the kernel runs in a single address space loaded from a single binary file. However these kernels are tightly coupled systems and are harder to design and maintain. Layered operating systems have the advantage of simplicity, since once lower layers are debugged they can be relied on to support higher-level functions. However, performance is poor due to the overhead of traversing multiple layers. Microkernels remove all nonessential functions and move them into userspace. Extending the OS is easier, and security is good, but performance is worse than monolithic kernels. Modularity , as implemented in Linux device drivers which are implemented as LKMs, allows a kernel to dynamically load dependencies when needed. In practice, all OSes in use combine different structures as expedient, forming hybrids of these ideal types. Debugging is the activity of finding and fixing errors in a system. OSes typically write error information to logs and can also capture core dumps , a capture of the memory of a process. Performance of a system can be monitored using counters and tracing : Counters on Linux like ps, top, vmstat, netstat, and iostat can provide information on a granular, per-process level or across the system. They rely on the virtual filesystems mounted by the Linux kernel at /proc. Tracing tools like strace, gdb, perf, and tcpdump are used to investigate an individual event in the course of debugging. A promising new approach to debugging is using eBPF , which can also capture tracing information using an in-kernel virtual machine that executes eBPF instructions. BPF Compiler Collection (BCC) is a set of Python scripts that embed C code and provide a frontend to eBPF . Processes A process is a program in execution and comprises the fundamental unit of work and main concern of a computer system. The memory layout of a process can be separated into several sections: Text: executable code Data: global variables Heap: memory dynamically allocated during runtime Stack: parameters, return addresses, and local variables used during function invocation Each time a function is called, an activation record containing function parameters, local variables, and the return address is pushed onto the stack. When control is returned from the function, the activation record is popped from the stack. The stack (which begins at the top of the program's allocated memory range) and heap sections will grow toward each other but must never overlap. The memory used by these sections can be displayed with the size command. Each process is represented in the operating system by a process control block (PCB) (represented by task_struct in Linux). On systems that support threads, the PCB is expanded to include information for each thread. The CPU scheduler selects processes for execution, a process called dispatching , or moves them to and from various queues: the ready queue where a process awaits dispatch or the wait queue where a process awaits the completion of I/O. When the CPU core switches to another process, the state of one process is saved and that of another is restored, a process called context-switching . A parent process may create new processes called children (see fork ), identified by an integer number known as a PID . In a normal operating system the processes form a process tree stemming from a single root, the initial process which has a PID of 1 (on modern Linux systems, systemd ). Processes can terminate themselves (see exit ) or be terminated due to a variety of circumstances. A parent process can wait for the termination of a child process with wait . A process that has terminated but whose parent has not yet called wait is a zombie process. All processes transition to this state briefly when they terminate. Some OSes like LInux allow orphan processes to continue existing after the parent process has terminated. In other systems, terminating all children when the parent is terminated is referred to as cascading termination . In IPC , processes exchange data using one of two methods: shared memory or message passing . Shared memory in Linux is implemented with memory-mapped files Concurrency A thread , comprising a thread identifier, a program counter (PC), a register set, and a stack, is the basic unit of CPU utilization. All threads from a single process share its code and data sections and other OS resources like open files and signals. Multithreaded programming allows more efficient use of multicore and multi-CPU systems, since each core is only capable of processing a single thread at a time. Such systems are pareallel systems because they can perform more than one task at a time. Concurrent systems are ones that support more than one task by allowing all tasks to make progress. Single-core systems could achieve concurrency through the use of CPU schedulers, but not parallelism. Contemporary OSes support kernel threads which are managed directly by the OS. User threads in contrast are supported above the kernel and managed without kernel support. Pthreads is the threads extension of the POSIX specification and can be provided as either a kernel-level or user-level library. In a Pthreads program, separate threads begin execution in a specified function. Implicit threading refers to the increasingly popular strategy of transfering the burden of creating and managing threading from application developers to compilers and runtime libraries. Several implicit threading approaches have evolved: Thread pools maintain a finite number of threads which await work, preventing resource exhaustion by having an unbounded number of possible threads. Fork-join model is often characterized as explicit thread creation, but by creating parallel tasks using the library instead of threads during the fork phase directly it can be implicit. OpenMP is a C++ library that provides an API of compiler directives that mark out parallel regions , or blocks of code that may run in parallel, creating as many threads as there are processing cores available. Grand Central Dispatch (GCD) is an Apple runtime library, API, and language extensions that allow developers to identify sections of code to run in parallel. Thread Building Blocks (TBB) is a C++ template library In Linux, a struct task_struct , which contains pointers to other data structures where data are stored, exists for each task in the system.","title":"MSCS"},{"location":"Coding/CS/#mscs","text":"Links Program description Course descriptions","title":"MSCS"},{"location":"Coding/CS/#core-requirements","text":"COP 6611 Operating Systems Course description (2021-2022) Syllabus Book: Silberschatz, et al. Operating Systems Concepts . 10th ed. EEL 6764 Principles of computer architecture Course description (2021-2022) Syllabus Book: Hennessy, et al. Computer Architecture: A Quantitative Approach . COT 6405 Theory of Algorithms Course description (2021-2022) Syllabus Book: Cormen, et al. Introduction to Algorithms . 3rd ed.","title":"Core requirements"},{"location":"Coding/CS/#operating-systems","text":"System calls provide an interface to operating system services, but are to be distinguished from API s (like libc in Linux) that expose functions intended for use by application programmers and abstract and hide the implementation detail of system calls. The runtime environment (RTE) includes all software needed to run applications written in a particular language, such as compilers, interpreters, libraries, and loaders. The RTE constitutes a system-call interface that links API functional calls to OS syscalls. At the level of CPU architecture, the ABI defines how binary code interfaces for a given OS on a given architecture. Three methods passing parameters to system calls: Parameters passed to registers individually Passing to a single register the address a block or table in memory containing parameters (if there are more parameters than registers) Push parameters onto a stack, which is popped by the OS Syscalls can be grouped into several functional categories: Process control: creating, controlling, and ending processes File management: creating, deleting, opening, reading, writing, repositioning files and getting and setting file attributes Device management: granting access to system resources, including physical and virtual devices Information maintenance: dumping memory for debugging, the single step CPU mode which executes a trap after every instruction, as well as trivial calls to retrieve date and time Communications: both message-passing and shared-memory models of IPC as well as network stack Protection: file and disk permissions System utilities facilitate user interfaces to system calls with CLI and GUI tools. Source files are compiled into relocatable object files that are combined into a single binary executable file by the linker , which is then loaded into memory by the loader . Associated with this process is relocation which assigns final memory addresses to the program parts, adjusting code and data accordingly. Most systems allow dynamically linked libraries which are linked and loaded when needed, reducing the size of the executable. Object and executable files have standard formats that determine the layout of the header, instructions, and variables which must be at certain locations in specified structures so that the OS can open the file: Linux: ELF Windows: Portable Executable (PE) macOS: Mach-O Operating systems can be categorized by design methodology: Monolithic kernels have the greatest performance because all the functionality of the kernel runs in a single address space loaded from a single binary file. However these kernels are tightly coupled systems and are harder to design and maintain. Layered operating systems have the advantage of simplicity, since once lower layers are debugged they can be relied on to support higher-level functions. However, performance is poor due to the overhead of traversing multiple layers. Microkernels remove all nonessential functions and move them into userspace. Extending the OS is easier, and security is good, but performance is worse than monolithic kernels. Modularity , as implemented in Linux device drivers which are implemented as LKMs, allows a kernel to dynamically load dependencies when needed. In practice, all OSes in use combine different structures as expedient, forming hybrids of these ideal types. Debugging is the activity of finding and fixing errors in a system. OSes typically write error information to logs and can also capture core dumps , a capture of the memory of a process. Performance of a system can be monitored using counters and tracing : Counters on Linux like ps, top, vmstat, netstat, and iostat can provide information on a granular, per-process level or across the system. They rely on the virtual filesystems mounted by the Linux kernel at /proc. Tracing tools like strace, gdb, perf, and tcpdump are used to investigate an individual event in the course of debugging. A promising new approach to debugging is using eBPF , which can also capture tracing information using an in-kernel virtual machine that executes eBPF instructions. BPF Compiler Collection (BCC) is a set of Python scripts that embed C code and provide a frontend to eBPF .","title":"Operating Systems"},{"location":"Coding/CS/#processes","text":"A process is a program in execution and comprises the fundamental unit of work and main concern of a computer system. The memory layout of a process can be separated into several sections: Text: executable code Data: global variables Heap: memory dynamically allocated during runtime Stack: parameters, return addresses, and local variables used during function invocation Each time a function is called, an activation record containing function parameters, local variables, and the return address is pushed onto the stack. When control is returned from the function, the activation record is popped from the stack. The stack (which begins at the top of the program's allocated memory range) and heap sections will grow toward each other but must never overlap. The memory used by these sections can be displayed with the size command. Each process is represented in the operating system by a process control block (PCB) (represented by task_struct in Linux). On systems that support threads, the PCB is expanded to include information for each thread. The CPU scheduler selects processes for execution, a process called dispatching , or moves them to and from various queues: the ready queue where a process awaits dispatch or the wait queue where a process awaits the completion of I/O. When the CPU core switches to another process, the state of one process is saved and that of another is restored, a process called context-switching . A parent process may create new processes called children (see fork ), identified by an integer number known as a PID . In a normal operating system the processes form a process tree stemming from a single root, the initial process which has a PID of 1 (on modern Linux systems, systemd ). Processes can terminate themselves (see exit ) or be terminated due to a variety of circumstances. A parent process can wait for the termination of a child process with wait . A process that has terminated but whose parent has not yet called wait is a zombie process. All processes transition to this state briefly when they terminate. Some OSes like LInux allow orphan processes to continue existing after the parent process has terminated. In other systems, terminating all children when the parent is terminated is referred to as cascading termination . In IPC , processes exchange data using one of two methods: shared memory or message passing . Shared memory in Linux is implemented with memory-mapped files","title":"Processes"},{"location":"Coding/CS/#concurrency","text":"A thread , comprising a thread identifier, a program counter (PC), a register set, and a stack, is the basic unit of CPU utilization. All threads from a single process share its code and data sections and other OS resources like open files and signals. Multithreaded programming allows more efficient use of multicore and multi-CPU systems, since each core is only capable of processing a single thread at a time. Such systems are pareallel systems because they can perform more than one task at a time. Concurrent systems are ones that support more than one task by allowing all tasks to make progress. Single-core systems could achieve concurrency through the use of CPU schedulers, but not parallelism. Contemporary OSes support kernel threads which are managed directly by the OS. User threads in contrast are supported above the kernel and managed without kernel support. Pthreads is the threads extension of the POSIX specification and can be provided as either a kernel-level or user-level library. In a Pthreads program, separate threads begin execution in a specified function. Implicit threading refers to the increasingly popular strategy of transfering the burden of creating and managing threading from application developers to compilers and runtime libraries. Several implicit threading approaches have evolved: Thread pools maintain a finite number of threads which await work, preventing resource exhaustion by having an unbounded number of possible threads. Fork-join model is often characterized as explicit thread creation, but by creating parallel tasks using the library instead of threads during the fork phase directly it can be implicit. OpenMP is a C++ library that provides an API of compiler directives that mark out parallel regions , or blocks of code that may run in parallel, creating as many threads as there are processing cores available. Grand Central Dispatch (GCD) is an Apple runtime library, API, and language extensions that allow developers to identify sections of code to run in parallel. Thread Building Blocks (TBB) is a C++ template library In Linux, a struct task_struct , which contains pointers to other data structures where data are stored, exists for each task in the system.","title":"Concurrency"},{"location":"Coding/Courses/","text":"Courses The Complete C# Masterclass /# Video Topic Projects 01.01 Welcome and a brief introduction to the Course 01.02 Guide Lecture - How to install Visual Studio 01.03 Guide lecture - Creating a project in Visual Studio 01.04 Your first C# program 02.01 What is a variable and what is its relationship with the data types IntegerDataTypes 02.02 The \"numbers\" data type - Integers 02.03 The \"numbers with a decimal point data types - float, double, decimal FloatingPointDataTypes 02.04 The \"Yes or No\" data types - booleans Boolean 02.05 The \"single symbol\" dat atypes - characters Characters 02.06 The \"information as text\" data types - strings Strings 02.07 Collections of information from a specific data type - arrays Arrays 02.08 Some cool, useful tricks with strings StringTricks 02.09 Transforming any data type into a string - allows you to use string methods 02.10 The 3 different ways to build strings 02.11 The 3 different ways to convert one data type to another 03.01 Write vs WriteLine, when to use which? WriteandWriteLine 03.04 Accepting single character inputs from the Console - Read method ReadingCharacter 03.05 Accepting string inputs from the Console - ReadLine method ReadLine 03.06 Accepting inputs as keys from the Console - ReadKey ReadKey 03.07 Changing the color of the text and the background of the text in the Console ConsoleColors 03.08 Changing cursor settings in the Console - Size, Visibility, Position CursorSettings 03.09 Controlling the size of the Console window - WindowSize, BufferSize and more ConsoleSize 04.01 Arithmetic Operators 04.02 Assignment Operators 04.03 Comparison Operators 04.04 Logical Operators && , || 04.05 Ternary Operator 06.02 Practicing while loops MiniWhileGame 06.04 The for loops and their common uses ForLoops 06.05 Practicing for loops MenuWhile 06.06 foreach loop ForeachLoops 07.05 Methods with variable number of arguments params ListExamples 07.08 Methods with ref and out arguments ref , out RefAndOut 08.01 Introduction to one-dimensional arrays IntroductionToArrays 08.02 Outputting arrays string.Join() OutputtingArrays 08.03 Correctly cloning arrays Array.Clone() ArrayCloning 08.04 Reversing arrays Array.Reverse() 08.05 Bubble sorting algorithm BubbleSort 08.09 Lists List<T> 08.10 Practice working with List<T> s List<T> ListExamples 09.01 Introduction to multidimensional arrays 11.01 Introduction to exception handling try catch 11.02 Catching multiple exceptions 11.03 Inspecting caught exception string.Substring() MultipleExceptions 11.04 finally block finally TryCatchFinally 12.01 Introduction to object-oriented programming 12.02 Creating a basic class RPGCharGen 12.03 Fields and properties - the variables of a class RPGCharGen 12.04 Methods - the actions of a class RPGCharGen 12.05 Constructors - the builders of a class RPGCharGen 12.06 Namespaces and files - structuring your project RPGCharGen 13.02 Controlling the accessors of a property - read, write, and read-write properties RPGCharGen 13.03 Implementing validation in properties RPGCharGen 13.04 Validation and exceptions throw RPGCharGen 13.05 Properties and fields - when to use which RPGCharGen 14.01 this this RPGCharGen 14.03 Overloading constructors RPGCharGen 14.04 Chaining constructors RPGCharGen 15.01 public and private access modifiers public , private RPGCharGen 15.02 internal and protected access modifiers internal , protected RPGCharGen 16.01 Static fields and properties RPGCharGen 16.02 const and readonly 16.03 Static methods 16.04 Static classes 16.05 enum 17.01 Introduction to Inheritance Learn C# By Building Applications /# Video Topic Projects 02.12 Static vs. non-static ArrayCloning int [] primes = { 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 , 29 , 31 , 37 , 41 , 43 , }; //int[] primesClone = (int[])primes.Clone(); int [] primesClone = new int [ primes . Length ]; Array . Copy ( primes , primesClone , primes . Length ); primesClone [ primesClone . Length - 1 ] = 47 ; foreach ( var i in primesClone ) { Console . WriteLine ( i ); } Arrays int [] numbers = new int [ 10 ]; for ( int i = 0 ; i < numbers . Length ; i ++) { System . Console . WriteLine ( numbers [ i ]); } string [] fruits = { \"apple\" , \"banana\" , \"jackfruit\" , \"kiwi\" , \"mango\" }; for ( int i = 0 ; i < fruits . Length ; i ++) { System . Console . WriteLine ( fruits [ i ]); } Boolean int firstNumber = 4 ; int secondNumber = 6 ; bool isSmaller = firstNumber < secondNumber ; bool isTheCookieJarFull = false ; bool isTheCookieJarEmpty = ! isTheCookieJarFull ; Characters System . Console . InputEncoding = System . Text . Encoding . UTF8 ; System . Console . OutputEncoding = System . Text . Encoding . UTF8 ; char x = 'x' ; System . Console . WriteLine ( x ); char plus = '\\ u002b ' ; System . Console . WriteLine ( plus ); char umlaut = '\\ u00F6 ' ; System . Console . WriteLine ( umlaut ); ConsoleColors Console . ForegroundColor = ConsoleColor . Green ; Console . WriteLine ( \"Once upon a midnight dreary\" ); Console . ForegroundColor = ConsoleColor . Cyan ; Console . WriteLine ( \"While I pondered weak and weary\" ); Console . ResetColor (); Console . WriteLine ( \"Over many a quaint and curious volume of forgotten lore,\" ); ConsoleSize Console . WindowHeight = 20 ; Console . WindowWidth = 20 ; Console . SetWindowSize ( 30 , 30 ); Console . BufferHeight = 40 ; Console . BufferWidth = 40 ; Console . WindowLeft = 10 ; Console . WindowTop = 10 ; Console . SetWindowPosition ( 10 , 10 ); CursorSettings Console . Title = \"Terminal\" ; Console . CursorVisible = true ; Console . CursorSize = 50 ; Console . SetCursorPosition ( 20 , 10 ); ExceptionHandling Console . WriteLine ( \"'q' to quit\" ); List < int > primes = new List < int >(); while ( true ) { string input = Console . ReadLine (); try { primes . Add ( Int32 . Parse ( input )); } catch ( FormatException ex ) { if ( input . ToLower () == \"q\" ) { break ; } else { Console . WriteLine ( ex . Message ); //string stacktrace = ex.StackTrace; //string filename = stacktrace.Substring(stacktrace.IndexOf(':') - 1); //Console.WriteLine(filename); } } } Console . WriteLine ( String . Join ( \" \" , primes )); FloatingPointDataTypes float floatNum = 13.14321365431f ; string output = $\"Floating point numbers have a maximum precision of 8 digits: {floatNum}\" ; System . Console . WriteLine ( output ); float radius = 3.5f ; double area = System . Math . PI * System . Math . Pow ( radius , 2d ); System . Console . WriteLine ( $\"A circle with a radius of {radius} has an are of {area}.\" ); float floatMax = float . MaxValue ; float floatMin = float . MinValue ; System . Console . WriteLine ( $\"float ranges from {floatMin} to {floatMax}\" ); double doubleMax = double . MaxValue ; double doubleMin = double . MinValue ; System . Console . WriteLine ( $\"double ranges from {doubleMin} to {doubleMax}\" ); decimal decimalMax = decimal . MaxValue ; decimal decimalMin = decimal . MinValue ; System . Console . WriteLine ( $\"decimal ranges from {decimalMin} to {decimalMax}\" ); ForeachLoops int [] primes = { 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 }; foreach ( var item in primes ) { Console . WriteLine ( item ); } ForLoops int [] primes = { 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 }; for ( int i = 0 ; i < primes . Length ; i ++) { primes [ i ] = 2 * primes [ i ]; Console . WriteLine ( primes [ i ]); } IntegerDataTypes int intMax = int . MaxValue ; int intMin = int . MinValue ; string output = $\"int ranges from {intMin} to {intMax}\" ; Console . WriteLine ( output ); uint uintMin = uint . MinValue ; uint uintMax = uint . MaxValue ; output = $\"uint ranges from {uintMin} to {uintMax}\" ; Console . WriteLine ( output ); byte byteMin = byte . MinValue ; byte byteMax = byte . MaxValue ; output = $\"byte ranges from {byteMin} to {byteMax}\" ; Console . WriteLine ( output ); long longMin = long . MinValue ; long longMax = long . MaxValue ; output = $\"long ranges from {longMin} to {longMax}\" ; Console . WriteLine ( output ); ulong ulongMin = ulong . MinValue ; ulong ulongMax = ulong . MaxValue ; output = $\"ulong ranges from {ulongMin} to {ulongMax}\" ; Console . WriteLine ( output ); ListExamples static void Main ( string [] args ) { int [] arr1 = { 0 , 1 , 2 , 3 , 4 }; int [] arr2 = { 5 , 6 , 7 , 8 , 9 }; List < int > list = new List < int >(); list = splat ( arr1 , arr2 ); foreach ( var el in list ) { Console . WriteLine ( el ); } } static List < int > splat ( params int [][] arg ) { List < int > output = new List < int >(); foreach ( int [] arr in arg ) { output . AddRange ( arr ); } return output ; } MiniWhileGame static void Main ( string [] args ) { int mage = 30 ; int warrior = 40 ; while ( mage > 0 && warrior > 0 ) { warrior -= mage_attack (); mage -= warrior_attack (); } Console . WriteLine ( $\"Conflict ends with Mage having {mage} points and Warrior having {warrior}!\" ); } private static int warrior_attack () { var r = new System . Random (); int result = r . Next ( 1 , 3 ); Console . WriteLine ( $\"Warrior attacks Mage, dealing {result} damage!\" ); return result ; } private static int mage_attack () { var r = new System . Random (); int result = r . Next ( 7 , 9 ); Console . WriteLine ( $\"Mage attacks Warrior, dealing {result} damage!\" ); return result ; } MenuWhile static void Main ( string [] args ) { string [] fruits = { \"Apple\" , \"Banana\" , \"Date\" , \"Grape\" , \"Jackfruit\" , \"Kiwi\" , \"Lime\" , \"Orange\" , \"Peach\" , \"Strawberry\" }; string [] menu = { \"Add New Item\" , \"Edit Item\" , \"Remove Item\" , \"View All Items\" , \"Exit\" }; int choice ; do { choice = Menu ( menu ); switch ( choice ) { case 1 : for ( int i = 0 ; i < fruits . Length ; i ++) { if ( fruits [ i ] == null ) { Console . Write ( \"Please add a new fruit: \" ); fruits [ i ] = Console . ReadLine (); break ; } } break ; case 2 : int chosenFruit = 0 ; do { Console . Write ( $\"Edit fruit number (1 to {fruits.Length}): \" ); try { chosenFruit = Convert . ToInt32 ( Console . ReadLine ()); } catch { InvalidInput (); } } while ( chosenFruit == 0 ); Console . Write ( $\"Enter new value for the {fruits[chosenFruit - 1]}: \" ); fruits [ chosenFruit - 1 ] = Console . ReadLine (); break ; case 3 : break ; case 4 : Console . WriteLine ( \"Current fruits: \" ); for ( int i = 0 ; i < fruits . Length ; i ++) { if ( fruits [ i ] != null ) { Console . WriteLine ( fruits [ i ]); } } break ; case 5 : break ; default : InvalidInput (); break ; } } while ( true ); } private static int Menu ( string [] menu ) { Console . WriteLine ( '\\n' ); for ( var i = 0 ; i < menu . Length ; i ++) { Console . WriteLine ( \"{0}. {1}\" , i + 1 , menu [ i ]); } Console . Write ( \"Your choice: \" ); string input = Console . ReadLine (); try { int output = Int32 . Parse ( input ); return output ; } catch ( FormatException ) { InvalidInput (); return - 1 ; } catch ( OverflowException ) { InvalidInput (); return - 1 ; } } private static void InvalidInput () { Console . ForegroundColor = ConsoleColor . Red ; Console . WriteLine ( \"Invalid input!\" ); Console . ResetColor (); } OutputtingArrays int [] primes = { 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 , 29 , 31 , 37 , 41 , 43 , 47 }; Console . WriteLine ( $\"{primes.Length} total primes\" ); List < int > lessThanTwenty = new List < int >(); List < int > moreThanTwenty = new List < int >(); foreach ( int i in primes ) { if ( i < 20 ) { lessThanTwenty . Add ( i ); } else { moreThanTwenty . Add ( i ); } } Console . WriteLine ( $\"Primes < 20:\\n {string.Join(\" , \", lessThanTwenty)}\" ); Console . WriteLine ( $\"Primes >= 20:\\n {string.Join(\" , \", moreThanTwenty)}\" ); ReadingCharacter Console . Write ( \"How old are you? \" ); int age ; int . TryParse ( Console . ReadLine (), out age ) ; Console . WriteLine ( $\"You are {age} years old (allegedly).\" ); ReadKey Console . WriteLine ( $\"\\n\\nKey pressed: {key.Key}\" ); Console . WriteLine ( $\"Key as char: {key.KeyChar}\" ); Console . WriteLine ( $\"Modifiers: {key.Modifiers}\" ); ReadLine Console . Write ( \"Input the drive letter: \" ); string driveLetter = Console . ReadLine (); Console . Write ( \"Input the folder path: \" ); string folderPath = Console . ReadLine (); Console . Write ( \"Input the file name: \" ); string fileName = Console . ReadLine (); Console . WriteLine ( $\"{driveLetter}:\\\\{folderPath}\\\\{fileName}.exe\" ); RefAndOut static void Main ( string [] args ) { int number = 0 ; Console . WriteLine ( number ); IncreaseByOne ( ref number ); Console . WriteLine ( number ); } static void IncreaseByOne ( ref int n ) { n ++; } static void Main () { double n = 5 ; double nSquared ; square ( n , out nSquared ); Console . WriteLine ( $\"{n} ^ 2 = {nSquared}\" ); } static void square ( double x , out double y ) { y = System . Math . Pow ( x , 2 ); } RPGCharGen Strings string username = \"admin\" ; System . Console . WriteLine ( username [ 0 ]); // impossible, strings are immutable username [ 0 ] = 'A' ; StringTricks string fruitJuice = \"Strawberry juice\" ; string separator = new string ( '-' , fruitJuice . Length ); System . Console . WriteLine ( fruitJuice ); System . Console . WriteLine ( separator ); System . Console . WriteLine ( fruitJuice . Contains ( \"j\" )); System . Console . WriteLine ( fruitJuice . IndexOf ( \"r\" )); System . Console . WriteLine ( fruitJuice . LastIndexOf ( \"r\" )); System . Console . WriteLine ( fruitJuice . ToUpper (). Contains ( \"J\" )); System . Console . WriteLine ( fruitJuice . ToUpper (). IndexOf ( \"RR\" )); System . Console . WriteLine ( fruitJuice . ToUpper (). LastIndexOf ( \"RR\" )); TryCatchFinally StreamWriter sw = null ; try { sw = File . CreateText ( Directory . GetCurrentDirectory () + @\"/test.txt\" ); int number = int . Parse ( Console . ReadLine ()); int dividend = 5 / number ; sw . Write ( number ); } catch ( FormatException ex ) { Console . WriteLine ( ex . Message ); } catch ( DivideByZeroException ex ) { Console . WriteLine ( ex . Message ); } finally { sw . Close (); } WriteAndWriteLine string heading = \"Protein Intake Week: 1\" ; string underline = new string ( '=' , heading . Length ); double num1 = 80.885570 ; double num2 = 94.564645 ; double num3 = 78.678931 ; double num4 = 88.66654 ; double num5 = 88.6466 ; double num6 = 76.777 ; double num7 = 91.85759 ; double sum = num1 + num2 + num3 + num4 + num5 + num6 + num7 ; double [] array = { num1 , num2 , num3 , num4 , num5 , num6 , num7 }; System . Console . WriteLine ( \"|{0}|\" , heading ); System . Console . WriteLine ( \"|{0}|\" , underline ); foreach ( double i in array ) { System . Console . WriteLine ( $\"|{i, 22:N2}|\" ); } System . Console . WriteLine ( \"|{0}|\" , underline ); System . Console . WriteLine ( \"|Total: {0, 15:N2}|\" , sum ); Test-Driven Development in C# Create a Red Unit Test Mocking with Moq and xUnit","title":"Courses"},{"location":"Coding/Courses/#courses","text":"","title":"Courses"},{"location":"Coding/Courses/#the-complete-c-masterclass","text":"/# Video Topic Projects 01.01 Welcome and a brief introduction to the Course 01.02 Guide Lecture - How to install Visual Studio 01.03 Guide lecture - Creating a project in Visual Studio 01.04 Your first C# program 02.01 What is a variable and what is its relationship with the data types IntegerDataTypes 02.02 The \"numbers\" data type - Integers 02.03 The \"numbers with a decimal point data types - float, double, decimal FloatingPointDataTypes 02.04 The \"Yes or No\" data types - booleans Boolean 02.05 The \"single symbol\" dat atypes - characters Characters 02.06 The \"information as text\" data types - strings Strings 02.07 Collections of information from a specific data type - arrays Arrays 02.08 Some cool, useful tricks with strings StringTricks 02.09 Transforming any data type into a string - allows you to use string methods 02.10 The 3 different ways to build strings 02.11 The 3 different ways to convert one data type to another 03.01 Write vs WriteLine, when to use which? WriteandWriteLine 03.04 Accepting single character inputs from the Console - Read method ReadingCharacter 03.05 Accepting string inputs from the Console - ReadLine method ReadLine 03.06 Accepting inputs as keys from the Console - ReadKey ReadKey 03.07 Changing the color of the text and the background of the text in the Console ConsoleColors 03.08 Changing cursor settings in the Console - Size, Visibility, Position CursorSettings 03.09 Controlling the size of the Console window - WindowSize, BufferSize and more ConsoleSize 04.01 Arithmetic Operators 04.02 Assignment Operators 04.03 Comparison Operators 04.04 Logical Operators && , || 04.05 Ternary Operator 06.02 Practicing while loops MiniWhileGame 06.04 The for loops and their common uses ForLoops 06.05 Practicing for loops MenuWhile 06.06 foreach loop ForeachLoops 07.05 Methods with variable number of arguments params ListExamples 07.08 Methods with ref and out arguments ref , out RefAndOut 08.01 Introduction to one-dimensional arrays IntroductionToArrays 08.02 Outputting arrays string.Join() OutputtingArrays 08.03 Correctly cloning arrays Array.Clone() ArrayCloning 08.04 Reversing arrays Array.Reverse() 08.05 Bubble sorting algorithm BubbleSort 08.09 Lists List<T> 08.10 Practice working with List<T> s List<T> ListExamples 09.01 Introduction to multidimensional arrays 11.01 Introduction to exception handling try catch 11.02 Catching multiple exceptions 11.03 Inspecting caught exception string.Substring() MultipleExceptions 11.04 finally block finally TryCatchFinally 12.01 Introduction to object-oriented programming 12.02 Creating a basic class RPGCharGen 12.03 Fields and properties - the variables of a class RPGCharGen 12.04 Methods - the actions of a class RPGCharGen 12.05 Constructors - the builders of a class RPGCharGen 12.06 Namespaces and files - structuring your project RPGCharGen 13.02 Controlling the accessors of a property - read, write, and read-write properties RPGCharGen 13.03 Implementing validation in properties RPGCharGen 13.04 Validation and exceptions throw RPGCharGen 13.05 Properties and fields - when to use which RPGCharGen 14.01 this this RPGCharGen 14.03 Overloading constructors RPGCharGen 14.04 Chaining constructors RPGCharGen 15.01 public and private access modifiers public , private RPGCharGen 15.02 internal and protected access modifiers internal , protected RPGCharGen 16.01 Static fields and properties RPGCharGen 16.02 const and readonly 16.03 Static methods 16.04 Static classes 16.05 enum 17.01 Introduction to Inheritance","title":"The Complete C# Masterclass"},{"location":"Coding/Courses/#learn-c-by-building-applications","text":"/# Video Topic Projects 02.12 Static vs. non-static","title":"Learn C# By Building Applications"},{"location":"Coding/Courses/#arraycloning","text":"int [] primes = { 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 , 29 , 31 , 37 , 41 , 43 , }; //int[] primesClone = (int[])primes.Clone(); int [] primesClone = new int [ primes . Length ]; Array . Copy ( primes , primesClone , primes . Length ); primesClone [ primesClone . Length - 1 ] = 47 ; foreach ( var i in primesClone ) { Console . WriteLine ( i ); }","title":"ArrayCloning"},{"location":"Coding/Courses/#arrays","text":"int [] numbers = new int [ 10 ]; for ( int i = 0 ; i < numbers . Length ; i ++) { System . Console . WriteLine ( numbers [ i ]); } string [] fruits = { \"apple\" , \"banana\" , \"jackfruit\" , \"kiwi\" , \"mango\" }; for ( int i = 0 ; i < fruits . Length ; i ++) { System . Console . WriteLine ( fruits [ i ]); }","title":"Arrays"},{"location":"Coding/Courses/#boolean","text":"int firstNumber = 4 ; int secondNumber = 6 ; bool isSmaller = firstNumber < secondNumber ; bool isTheCookieJarFull = false ; bool isTheCookieJarEmpty = ! isTheCookieJarFull ;","title":"Boolean"},{"location":"Coding/Courses/#characters","text":"System . Console . InputEncoding = System . Text . Encoding . UTF8 ; System . Console . OutputEncoding = System . Text . Encoding . UTF8 ; char x = 'x' ; System . Console . WriteLine ( x ); char plus = '\\ u002b ' ; System . Console . WriteLine ( plus ); char umlaut = '\\ u00F6 ' ; System . Console . WriteLine ( umlaut );","title":"Characters"},{"location":"Coding/Courses/#consolecolors","text":"Console . ForegroundColor = ConsoleColor . Green ; Console . WriteLine ( \"Once upon a midnight dreary\" ); Console . ForegroundColor = ConsoleColor . Cyan ; Console . WriteLine ( \"While I pondered weak and weary\" ); Console . ResetColor (); Console . WriteLine ( \"Over many a quaint and curious volume of forgotten lore,\" );","title":"ConsoleColors"},{"location":"Coding/Courses/#consolesize","text":"Console . WindowHeight = 20 ; Console . WindowWidth = 20 ; Console . SetWindowSize ( 30 , 30 ); Console . BufferHeight = 40 ; Console . BufferWidth = 40 ; Console . WindowLeft = 10 ; Console . WindowTop = 10 ; Console . SetWindowPosition ( 10 , 10 );","title":"ConsoleSize"},{"location":"Coding/Courses/#cursorsettings","text":"Console . Title = \"Terminal\" ; Console . CursorVisible = true ; Console . CursorSize = 50 ; Console . SetCursorPosition ( 20 , 10 );","title":"CursorSettings"},{"location":"Coding/Courses/#exceptionhandling","text":"Console . WriteLine ( \"'q' to quit\" ); List < int > primes = new List < int >(); while ( true ) { string input = Console . ReadLine (); try { primes . Add ( Int32 . Parse ( input )); } catch ( FormatException ex ) { if ( input . ToLower () == \"q\" ) { break ; } else { Console . WriteLine ( ex . Message ); //string stacktrace = ex.StackTrace; //string filename = stacktrace.Substring(stacktrace.IndexOf(':') - 1); //Console.WriteLine(filename); } } } Console . WriteLine ( String . Join ( \" \" , primes ));","title":"ExceptionHandling"},{"location":"Coding/Courses/#floatingpointdatatypes","text":"float floatNum = 13.14321365431f ; string output = $\"Floating point numbers have a maximum precision of 8 digits: {floatNum}\" ; System . Console . WriteLine ( output ); float radius = 3.5f ; double area = System . Math . PI * System . Math . Pow ( radius , 2d ); System . Console . WriteLine ( $\"A circle with a radius of {radius} has an are of {area}.\" ); float floatMax = float . MaxValue ; float floatMin = float . MinValue ; System . Console . WriteLine ( $\"float ranges from {floatMin} to {floatMax}\" ); double doubleMax = double . MaxValue ; double doubleMin = double . MinValue ; System . Console . WriteLine ( $\"double ranges from {doubleMin} to {doubleMax}\" ); decimal decimalMax = decimal . MaxValue ; decimal decimalMin = decimal . MinValue ; System . Console . WriteLine ( $\"decimal ranges from {decimalMin} to {decimalMax}\" );","title":"FloatingPointDataTypes"},{"location":"Coding/Courses/#foreachloops","text":"int [] primes = { 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 }; foreach ( var item in primes ) { Console . WriteLine ( item ); }","title":"ForeachLoops"},{"location":"Coding/Courses/#forloops","text":"int [] primes = { 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 }; for ( int i = 0 ; i < primes . Length ; i ++) { primes [ i ] = 2 * primes [ i ]; Console . WriteLine ( primes [ i ]); }","title":"ForLoops"},{"location":"Coding/Courses/#integerdatatypes","text":"int intMax = int . MaxValue ; int intMin = int . MinValue ; string output = $\"int ranges from {intMin} to {intMax}\" ; Console . WriteLine ( output ); uint uintMin = uint . MinValue ; uint uintMax = uint . MaxValue ; output = $\"uint ranges from {uintMin} to {uintMax}\" ; Console . WriteLine ( output ); byte byteMin = byte . MinValue ; byte byteMax = byte . MaxValue ; output = $\"byte ranges from {byteMin} to {byteMax}\" ; Console . WriteLine ( output ); long longMin = long . MinValue ; long longMax = long . MaxValue ; output = $\"long ranges from {longMin} to {longMax}\" ; Console . WriteLine ( output ); ulong ulongMin = ulong . MinValue ; ulong ulongMax = ulong . MaxValue ; output = $\"ulong ranges from {ulongMin} to {ulongMax}\" ; Console . WriteLine ( output );","title":"IntegerDataTypes"},{"location":"Coding/Courses/#listexamples","text":"static void Main ( string [] args ) { int [] arr1 = { 0 , 1 , 2 , 3 , 4 }; int [] arr2 = { 5 , 6 , 7 , 8 , 9 }; List < int > list = new List < int >(); list = splat ( arr1 , arr2 ); foreach ( var el in list ) { Console . WriteLine ( el ); } } static List < int > splat ( params int [][] arg ) { List < int > output = new List < int >(); foreach ( int [] arr in arg ) { output . AddRange ( arr ); } return output ; }","title":"ListExamples"},{"location":"Coding/Courses/#miniwhilegame","text":"static void Main ( string [] args ) { int mage = 30 ; int warrior = 40 ; while ( mage > 0 && warrior > 0 ) { warrior -= mage_attack (); mage -= warrior_attack (); } Console . WriteLine ( $\"Conflict ends with Mage having {mage} points and Warrior having {warrior}!\" ); } private static int warrior_attack () { var r = new System . Random (); int result = r . Next ( 1 , 3 ); Console . WriteLine ( $\"Warrior attacks Mage, dealing {result} damage!\" ); return result ; } private static int mage_attack () { var r = new System . Random (); int result = r . Next ( 7 , 9 ); Console . WriteLine ( $\"Mage attacks Warrior, dealing {result} damage!\" ); return result ; }","title":"MiniWhileGame"},{"location":"Coding/Courses/#menuwhile","text":"static void Main ( string [] args ) { string [] fruits = { \"Apple\" , \"Banana\" , \"Date\" , \"Grape\" , \"Jackfruit\" , \"Kiwi\" , \"Lime\" , \"Orange\" , \"Peach\" , \"Strawberry\" }; string [] menu = { \"Add New Item\" , \"Edit Item\" , \"Remove Item\" , \"View All Items\" , \"Exit\" }; int choice ; do { choice = Menu ( menu ); switch ( choice ) { case 1 : for ( int i = 0 ; i < fruits . Length ; i ++) { if ( fruits [ i ] == null ) { Console . Write ( \"Please add a new fruit: \" ); fruits [ i ] = Console . ReadLine (); break ; } } break ; case 2 : int chosenFruit = 0 ; do { Console . Write ( $\"Edit fruit number (1 to {fruits.Length}): \" ); try { chosenFruit = Convert . ToInt32 ( Console . ReadLine ()); } catch { InvalidInput (); } } while ( chosenFruit == 0 ); Console . Write ( $\"Enter new value for the {fruits[chosenFruit - 1]}: \" ); fruits [ chosenFruit - 1 ] = Console . ReadLine (); break ; case 3 : break ; case 4 : Console . WriteLine ( \"Current fruits: \" ); for ( int i = 0 ; i < fruits . Length ; i ++) { if ( fruits [ i ] != null ) { Console . WriteLine ( fruits [ i ]); } } break ; case 5 : break ; default : InvalidInput (); break ; } } while ( true ); } private static int Menu ( string [] menu ) { Console . WriteLine ( '\\n' ); for ( var i = 0 ; i < menu . Length ; i ++) { Console . WriteLine ( \"{0}. {1}\" , i + 1 , menu [ i ]); } Console . Write ( \"Your choice: \" ); string input = Console . ReadLine (); try { int output = Int32 . Parse ( input ); return output ; } catch ( FormatException ) { InvalidInput (); return - 1 ; } catch ( OverflowException ) { InvalidInput (); return - 1 ; } } private static void InvalidInput () { Console . ForegroundColor = ConsoleColor . Red ; Console . WriteLine ( \"Invalid input!\" ); Console . ResetColor (); }","title":"MenuWhile"},{"location":"Coding/Courses/#outputtingarrays","text":"int [] primes = { 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 , 29 , 31 , 37 , 41 , 43 , 47 }; Console . WriteLine ( $\"{primes.Length} total primes\" ); List < int > lessThanTwenty = new List < int >(); List < int > moreThanTwenty = new List < int >(); foreach ( int i in primes ) { if ( i < 20 ) { lessThanTwenty . Add ( i ); } else { moreThanTwenty . Add ( i ); } } Console . WriteLine ( $\"Primes < 20:\\n {string.Join(\" , \", lessThanTwenty)}\" ); Console . WriteLine ( $\"Primes >= 20:\\n {string.Join(\" , \", moreThanTwenty)}\" );","title":"OutputtingArrays"},{"location":"Coding/Courses/#readingcharacter","text":"Console . Write ( \"How old are you? \" ); int age ; int . TryParse ( Console . ReadLine (), out age ) ; Console . WriteLine ( $\"You are {age} years old (allegedly).\" );","title":"ReadingCharacter"},{"location":"Coding/Courses/#readkey","text":"Console . WriteLine ( $\"\\n\\nKey pressed: {key.Key}\" ); Console . WriteLine ( $\"Key as char: {key.KeyChar}\" ); Console . WriteLine ( $\"Modifiers: {key.Modifiers}\" );","title":"ReadKey"},{"location":"Coding/Courses/#readline","text":"Console . Write ( \"Input the drive letter: \" ); string driveLetter = Console . ReadLine (); Console . Write ( \"Input the folder path: \" ); string folderPath = Console . ReadLine (); Console . Write ( \"Input the file name: \" ); string fileName = Console . ReadLine (); Console . WriteLine ( $\"{driveLetter}:\\\\{folderPath}\\\\{fileName}.exe\" );","title":"ReadLine"},{"location":"Coding/Courses/#refandout","text":"static void Main ( string [] args ) { int number = 0 ; Console . WriteLine ( number ); IncreaseByOne ( ref number ); Console . WriteLine ( number ); } static void IncreaseByOne ( ref int n ) { n ++; } static void Main () { double n = 5 ; double nSquared ; square ( n , out nSquared ); Console . WriteLine ( $\"{n} ^ 2 = {nSquared}\" ); } static void square ( double x , out double y ) { y = System . Math . Pow ( x , 2 ); }","title":"RefAndOut"},{"location":"Coding/Courses/#rpgchargen","text":"","title":"RPGCharGen"},{"location":"Coding/Courses/#strings","text":"string username = \"admin\" ; System . Console . WriteLine ( username [ 0 ]); // impossible, strings are immutable username [ 0 ] = 'A' ;","title":"Strings"},{"location":"Coding/Courses/#stringtricks","text":"string fruitJuice = \"Strawberry juice\" ; string separator = new string ( '-' , fruitJuice . Length ); System . Console . WriteLine ( fruitJuice ); System . Console . WriteLine ( separator ); System . Console . WriteLine ( fruitJuice . Contains ( \"j\" )); System . Console . WriteLine ( fruitJuice . IndexOf ( \"r\" )); System . Console . WriteLine ( fruitJuice . LastIndexOf ( \"r\" )); System . Console . WriteLine ( fruitJuice . ToUpper (). Contains ( \"J\" )); System . Console . WriteLine ( fruitJuice . ToUpper (). IndexOf ( \"RR\" )); System . Console . WriteLine ( fruitJuice . ToUpper (). LastIndexOf ( \"RR\" ));","title":"StringTricks"},{"location":"Coding/Courses/#trycatchfinally","text":"StreamWriter sw = null ; try { sw = File . CreateText ( Directory . GetCurrentDirectory () + @\"/test.txt\" ); int number = int . Parse ( Console . ReadLine ()); int dividend = 5 / number ; sw . Write ( number ); } catch ( FormatException ex ) { Console . WriteLine ( ex . Message ); } catch ( DivideByZeroException ex ) { Console . WriteLine ( ex . Message ); } finally { sw . Close (); }","title":"TryCatchFinally"},{"location":"Coding/Courses/#writeandwriteline","text":"string heading = \"Protein Intake Week: 1\" ; string underline = new string ( '=' , heading . Length ); double num1 = 80.885570 ; double num2 = 94.564645 ; double num3 = 78.678931 ; double num4 = 88.66654 ; double num5 = 88.6466 ; double num6 = 76.777 ; double num7 = 91.85759 ; double sum = num1 + num2 + num3 + num4 + num5 + num6 + num7 ; double [] array = { num1 , num2 , num3 , num4 , num5 , num6 , num7 }; System . Console . WriteLine ( \"|{0}|\" , heading ); System . Console . WriteLine ( \"|{0}|\" , underline ); foreach ( double i in array ) { System . Console . WriteLine ( $\"|{i, 22:N2}|\" ); } System . Console . WriteLine ( \"|{0}|\" , underline ); System . Console . WriteLine ( \"|Total: {0, 15:N2}|\" , sum );","title":"WriteAndWriteLine"},{"location":"Coding/Courses/#test-driven-development-in-c","text":"","title":"Test-Driven Development in C#"},{"location":"Coding/Courses/#create-a-red-unit-test","text":"","title":"Create a Red Unit Test"},{"location":"Coding/Courses/#mocking-with-moq-and-xunit","text":"","title":"Mocking with Moq and xUnit"},{"location":"Coding/Debugging/","text":"Debugging Debuggers pdb python -m pdb script.py import sys def main (): name : str = sys . argv [ 1 ] print ( f \"Hello, { name } !\" ) if __name__ == \"__main__\" : main () Use debugging to intercept the value of the variable before the final string is displayed. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 import math import sys def main (): number = get_number () factorial = get_factorial ( number ) print ( f \" { number } ! = { factorial } \" ) def get_factorial ( number : int ): factorial = math . prod ( list ( range ( number + 1 ))) return factorial def get_number (): number = 0 if len ( sys . argv ) == 2 : try : number = int ( sys . argv [ 1 ]) return number except IndexError : pass except ValueError : pass while True : try : number = int ( input ( \"Enter a number: \" )) break except ValueError : print ( \"Argument must be an integer\" ) return number if __name__ == \"__main__\" : main () When running pdb with no breakpoints, top-level statements begin to be executed. The debugger proceeds from top to bottom, following the interpreter, from import statements to function definitions, until it finally reaches the entrypoint. If the debugger is directed to step over the main function, the program will execute normally. The entirety of the source code can be printed to the interactive pdb shell by using ll or longlist . A similar command is l [ist] which lists 11 lines around the current line. Repeated execution progressively print more of the source. The c [ontinue] command results in the execution being resumed without interruption until the next breakpoint. If no breakpoints are defined, the program is executed as it would be normally. Invoking pdb without any breakpoints and proceeding with n [ext] results in top-level code being executed. Stepping into function calls with s [tep] is similar to adding a breakpoint at the function definition with b [reak] , at least for a single function call. In these examples, the factorial is being incorrectly calculated, such that it will always produce a 0. This can be inspected in the debugger by using the p command, which evaluates the following arguments as an expression . Alternatively, any Python statement can be preceded with the ! command, with which you can change variable values. Notably, any whitespace after ! will cause an IndentationError p factorial !factorial = 120 The interact command allows you to enter a Python REPL, which allows statements to be interactively entered. However these do not have an effect on the program state once the shell is terminated. (Pdb) p factorial 0 (Pdb) interact >>> factorial = 120 >>> ^D now exiting InteractiveConsole... (Pdb) p factorial 0 gdb Invoke on executable \"program\", compiled with debugging symbols. gdb program Display the first 10 lines of source code from main.rs list main.rs:0 Display the source code of the stack_only function list stack_only Run code after placing a breakpoint on functions \"stack_only\" and \"stack_and_heap\" b stack_only b stack_and_heap r Display stack frames bt 2 Inspect local variables and arguments info locals info args Step forward n Display data at given memory location as a digit x /d 0x55555559bba0 Enter TUI mode Ctrl-X A . This mode does not work well with print statements. GDB also contains a Python runtime, so you can run commands inline using the python command. \ud83d\udcd8 Glossary step into If the current line contains a function call, move execution context into it to continue stepwise execution of code statements within that function. If not, identical to step over step over Execute the current line and pause on the next.","title":"Debugging"},{"location":"Coding/Debugging/#debugging","text":"","title":"Debugging"},{"location":"Coding/Debugging/#debuggers","text":"","title":"Debuggers"},{"location":"Coding/Debugging/#pdb","text":"python -m pdb script.py import sys def main (): name : str = sys . argv [ 1 ] print ( f \"Hello, { name } !\" ) if __name__ == \"__main__\" : main () Use debugging to intercept the value of the variable before the final string is displayed. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 import math import sys def main (): number = get_number () factorial = get_factorial ( number ) print ( f \" { number } ! = { factorial } \" ) def get_factorial ( number : int ): factorial = math . prod ( list ( range ( number + 1 ))) return factorial def get_number (): number = 0 if len ( sys . argv ) == 2 : try : number = int ( sys . argv [ 1 ]) return number except IndexError : pass except ValueError : pass while True : try : number = int ( input ( \"Enter a number: \" )) break except ValueError : print ( \"Argument must be an integer\" ) return number if __name__ == \"__main__\" : main () When running pdb with no breakpoints, top-level statements begin to be executed. The debugger proceeds from top to bottom, following the interpreter, from import statements to function definitions, until it finally reaches the entrypoint. If the debugger is directed to step over the main function, the program will execute normally. The entirety of the source code can be printed to the interactive pdb shell by using ll or longlist . A similar command is l [ist] which lists 11 lines around the current line. Repeated execution progressively print more of the source. The c [ontinue] command results in the execution being resumed without interruption until the next breakpoint. If no breakpoints are defined, the program is executed as it would be normally. Invoking pdb without any breakpoints and proceeding with n [ext] results in top-level code being executed. Stepping into function calls with s [tep] is similar to adding a breakpoint at the function definition with b [reak] , at least for a single function call. In these examples, the factorial is being incorrectly calculated, such that it will always produce a 0. This can be inspected in the debugger by using the p command, which evaluates the following arguments as an expression . Alternatively, any Python statement can be preceded with the ! command, with which you can change variable values. Notably, any whitespace after ! will cause an IndentationError p factorial !factorial = 120 The interact command allows you to enter a Python REPL, which allows statements to be interactively entered. However these do not have an effect on the program state once the shell is terminated. (Pdb) p factorial 0 (Pdb) interact >>> factorial = 120 >>> ^D now exiting InteractiveConsole... (Pdb) p factorial 0","title":"pdb "},{"location":"Coding/Debugging/#gdb","text":"Invoke on executable \"program\", compiled with debugging symbols. gdb program Display the first 10 lines of source code from main.rs list main.rs:0 Display the source code of the stack_only function list stack_only Run code after placing a breakpoint on functions \"stack_only\" and \"stack_and_heap\" b stack_only b stack_and_heap r Display stack frames bt 2 Inspect local variables and arguments info locals info args Step forward n Display data at given memory location as a digit x /d 0x55555559bba0 Enter TUI mode Ctrl-X A . This mode does not work well with print statements. GDB also contains a Python runtime, so you can run commands inline using the python command.","title":"gdb"},{"location":"Coding/Debugging/#glossary","text":"step into If the current line contains a function call, move execution context into it to continue stepwise execution of code statements within that function. If not, identical to step over step over Execute the current line and pause on the next.","title":"\ud83d\udcd8 Glossary"},{"location":"Coding/DnD/","text":"\ud83d\udc32\ufe0f Dungeons & Dragons Ability scores Six ability scores define the capabilities of every character and monster in the game. Each of these are obtained by rolling 4d6, then discarding the die with the lowest value. Strength Dexterity Constitution Intelligence Wisdom Charisma Alternatively, 27 points can be spent according to the following table to arbitrarily determine ability scores. Ability score Cost 8 0 9 1 10 2 11 3 12 4 13 5 14 7 15 9 Finally, the following set of scores can be used if either of the two methods above are unacceptable: 15, 14, 13, 12, 10, and 8. Ability modifiers Ability modifiers apply to each score according to its value. These modifiers are equivalent to finding the floor of half of the score's difference with ten (i.e. (13-10) // 2 = 1, but (7-10) // 2 = -2) Ability score Modifier Ability Score Modifier Ability Score Modifier 1 -5 12-13 +1 22-23 +6 2-3 -4 14-15 +2 24-25 +7 4-5 -3 16-17 +3 26-27 +8 6-7 -2 18-19 +4 28-29 +9 8-9 -1 20-21 +5 30 +10 10-11 0 Class A class describes a character's vocation, talents, and combat style. Class features are capabilities unique to each class Proficiencies in armor, weapons, saving throws, and other items define a character's talents","title":"\ud83d\udc32&#xFE0F;  Dungeons & Dragons"},{"location":"Coding/DnD/#dungeons-dragons","text":"","title":"\ud83d\udc32&#xFE0F;  Dungeons &amp; Dragons"},{"location":"Coding/DnD/#ability-scores","text":"Six ability scores define the capabilities of every character and monster in the game. Each of these are obtained by rolling 4d6, then discarding the die with the lowest value. Strength Dexterity Constitution Intelligence Wisdom Charisma Alternatively, 27 points can be spent according to the following table to arbitrarily determine ability scores. Ability score Cost 8 0 9 1 10 2 11 3 12 4 13 5 14 7 15 9 Finally, the following set of scores can be used if either of the two methods above are unacceptable: 15, 14, 13, 12, 10, and 8.","title":"Ability scores"},{"location":"Coding/DnD/#ability-modifiers","text":"Ability modifiers apply to each score according to its value. These modifiers are equivalent to finding the floor of half of the score's difference with ten (i.e. (13-10) // 2 = 1, but (7-10) // 2 = -2) Ability score Modifier Ability Score Modifier Ability Score Modifier 1 -5 12-13 +1 22-23 +6 2-3 -4 14-15 +2 24-25 +7 4-5 -3 16-17 +3 26-27 +8 6-7 -2 18-19 +4 28-29 +9 8-9 -1 20-21 +5 30 +10 10-11 0","title":"Ability modifiers"},{"location":"Coding/DnD/#class","text":"A class describes a character's vocation, talents, and combat style. Class features are capabilities unique to each class Proficiencies in armor, weapons, saving throws, and other items define a character's talents","title":"Class"},{"location":"Coding/Dogfood/","text":"\ud83d\udc36 Dogfood data A problem I've encountered is that when it comes to anything to do with software, learning is doing. But there isn't really much play data easily available. Some tutorials go to the trouble of providing files for download, and I'm sure anyone reading this already has a favorite API or JSON repo where they go to get stuff. But the situation is not ideal, and I've never encountered a body of data in any learning materials I've encountered that compelled me to come back to it time and again. So I made it raven These .csv files have proven useful to me as sample datasets to use while learning the Unix filters grep , sed , and awk . I hope whoever finds them finds them just as valuable. \ud83e\udde0 Greek philosophers Very small dataset featuring all your favorite Ancients: Name City Date of birth Socrates Athens 470 BC Plato Athens 428 BC Aristotle Stagira 384 BC Euclid Alexandria 325 BC Pythagoras Samos 570 BC \ud83d\udea2 Ships of the line A somewhat larger, more complicated dataset featuring some of history's battliest battleships: Name Country Displacement Length Beam Commissioned date Yamato Japan 65027 256 38.9 16 December 1941 USS Enterprise United States of America 19800 251.4 33.4 12 May 1938 Bismarck Germany 41700 251 36 24 August 1940 HMS Dreadnought United Kingdom 18120 160.6 25 2 December 1906 USS Iowa United States of America 46000 270.43 32.97 22 February 1943 HMS Vanguard United Kingdom 45200 248.2 32.9 12 May 1946 \ud83d\ude80 Starships Ships famous, to varying degrees, for plying the inky black. If these interest you, check out my Starships repo too. Starships Officers Name Class Registry Crew USS Enterprise Constitution NCC-1701 203 USS Constitution Constitution NCC-1700 204 USS Defiant Defiant NX-74205 50 USS Voyager Intrepid NCC-74656 141 USS Enterprise Galaxy NCC-1701-D 6000 USS Reliant Miranda NCC-1864 35 Name Date of birth Kirk, James 2233-03-22 Picard, Jean-Luc 2305-07-13 \ud83e\uddd4 Mathematicians An expanded assortment of smartypants from a different era. The third and largest dataset, with two date fields and one full of semicolon-delimited subfields ripe for parsing: Name Surname Date of birth Date of death Concepts Carl Gauss 30 April 1777 23 February 1855 Gaussian elimination; Gauss\u2013Jordan elimination; Gauss\u2013Seidel method; Gauss's cyclotomic formula; Gauss's lemma; Gaussian binomial coefficient; Gauss transformation; Gauss\u2013Bodenmiller theorem; Gauss\u2013Bolyai\u2013Lobachevsky space; Gauss\u2013Bonnet theorem; Generalized Gauss\u2013Bonnet theorem; Braid theory; Gauss\u2013Codazzi equations; Gauss\u2013Manin connection; Newton line; Gauss's area formula; Gauss's lemma; Gauss map; Gaussian curvature; Gauss circle problem; Gauss\u2013Kuzmin\u2013Wirsing constant; Gauss's constant; Gauss's digamma theorem; Gauss's generalization of Wilson's theorem; Gauss's lemma; Gauss map; Gaussian moat; Gauss class number problem; Gauss's multiplication formula George Berkeley 12 March 1685 14 January 1753 Gottfried Leibniz 1 July 1646 14 November 1716 Calculus; Monads; Best of all possible worlds; Pre-established harmony; Identity of indiscernibles; Matrix (mathematics); Leibniz integral rule; Principle of sufficient reason; Notation for differentiation; Product rule; Vis viva; Boolean algebra; Salva veritate; Stepped reckoner; Symbolic logic; Semiotics; Analysis situs; Law of Continuity; Transcendental law of homogeneity; Ars combinatoria; Calculus ratiocinator; Leibniz's notation; Characteristica universalis; Problem of why there is anything at all; Pluralistic idealism; Metaphysical dynamism; Relationism; Apperception; A priori/a posteriori distinction Immanuel Kant 22 April 1724 12 February 1804 Kantianism; Kantian ethics; Neo-Kantianism Isaac Newton 25 December 1642 20 March 1726 \"Gauss\u2013Newton algorithm; Newton\u2013Cotes formulas; Newton\u2013Okounkov body; Newton\u2013Pepys problem; Newton fractal; Newton's identities; Newton's inequalities; Newton's method; Newton's method in optimization; Newton's notation; Newton polygon; Newton polynomial; Newton series; Newton's theorem about ovals; Truncated Newton method; bucket argument; Newton's cannonball; Universal gravitational constant; Newton's cradle; Newton disc; Newton\u2013Cartan theory; Newton\u2013Euler equations; Newton's law of cooling; Newton's laws of motion; Newton's law of universal gravitation; Newton's metal; Newton's reflector; Newton's rings; Rotating spheres; Newton scale; Newton's sphere theorem John Locke 29 August 1632 28 October 1704 Labor theory of property; Social contract; State of nature Joseph Fourier 21 March 1768 16 May 1830 Fourier series; Fourier transform; Fourier's law of conduction; Fourier-Motzkin elimination Joseph-Louis Lagrange 25 January 1736 10 April 1813 Lagrangian analysis; Lagrangian coordinates; Lagrangian derivative; Lagrangian drifter; Lagrangian foliation; Lagrangian Grassmannian; Lagrangian intersection Floer homology; Lagrangian mechanics; Lagrangian (field theory); Lagrangian system; Lagrangian mixing; Lagrangian point; Lagrangian relaxation; Lagrangian submanifold; Lagrangian subspace; Nonlocal Lagrangian; Proca lagrangian; Special Lagrangian submanifold; Euler\u2013Lagrange equation; Green\u2013Lagrange strain; Lagrange bracket; Lagrange\u2013d'Alembert principle; Lagrange error bound; Lagrange form; Lagrange interpolation; Lagrange invariant; Lagrange inversion theorem; Lagrange multiplier; Lagrange number; Lagrange point colonization; Lagrange polynomial; Lagrange property; Lagrange reversion theorem; Lagrange resolvent; Lagrange spectrum; Lagrange stream function; Lagrange's approximation theorem; Lagrange's formula (disambiguation); Lagrange's identity (disambiguation); Lagrange's theorem (group theory); Lagrange's theorem (number theory); Lagrange's four-square theorem; Lagrange's trigonometric identities Pierre-Simon Laplace 23 March 1749 5 March 1827 Bayesian inference; Bayesian probability; Laplace's equation; Laplacian; Laplace transform; Inverse Laplace transform; Laplace distribution; Laplace's demon; Laplace expansion; Young\u2013Laplace equation; Laplace number; Laplace limit; Laplace invariant; Laplace principle; Laplace's principle of insufficient reason; Laplace's method; Laplace expansion; Laplace force; Laplace filter; Laplace functional; Laplacian matrix; Laplace motion; Laplace plane; Laplace pressure; Laplace resonance; Laplace's spherical harmonics; Laplace smoothing; Laplace expansion; Laplace expansion; Laplace-Bayes estimator; Laplace\u2013Stieltjes transform; Laplace\u2013Runge\u2013Lenz vector; Nebular hypothesis Ren\u00e9 Descartes 31 March 1596 11 February 1650 Cartesian circle; Cartesian coordinate system; Cartesian diagram; Cartesian diver; Cartesian morphism; Cartesian plane; Cartesian product; Cartesian product of graphs; Cartesian theater; Cartesian tree; Descartes' rule of signs; Descartes' theorem (4 tangent circles); Descartes' theorem (on total angular defect); Folium of Descartes","title":"\ud83d\udc36 Dogfood data"},{"location":"Coding/Dogfood/#dogfood-data","text":"A problem I've encountered is that when it comes to anything to do with software, learning is doing. But there isn't really much play data easily available. Some tutorials go to the trouble of providing files for download, and I'm sure anyone reading this already has a favorite API or JSON repo where they go to get stuff. But the situation is not ideal, and I've never encountered a body of data in any learning materials I've encountered that compelled me to come back to it time and again. So I made it raven These .csv files have proven useful to me as sample datasets to use while learning the Unix filters grep , sed , and awk . I hope whoever finds them finds them just as valuable.","title":"\ud83d\udc36 Dogfood data"},{"location":"Coding/Dogfood/#greek-philosophers","text":"Very small dataset featuring all your favorite Ancients: Name City Date of birth Socrates Athens 470 BC Plato Athens 428 BC Aristotle Stagira 384 BC Euclid Alexandria 325 BC Pythagoras Samos 570 BC","title":"\ud83e\udde0 Greek philosophers"},{"location":"Coding/Dogfood/#ships-of-the-line","text":"A somewhat larger, more complicated dataset featuring some of history's battliest battleships: Name Country Displacement Length Beam Commissioned date Yamato Japan 65027 256 38.9 16 December 1941 USS Enterprise United States of America 19800 251.4 33.4 12 May 1938 Bismarck Germany 41700 251 36 24 August 1940 HMS Dreadnought United Kingdom 18120 160.6 25 2 December 1906 USS Iowa United States of America 46000 270.43 32.97 22 February 1943 HMS Vanguard United Kingdom 45200 248.2 32.9 12 May 1946","title":"\ud83d\udea2 Ships of the line"},{"location":"Coding/Dogfood/#starships","text":"Ships famous, to varying degrees, for plying the inky black. If these interest you, check out my Starships repo too. Starships Officers Name Class Registry Crew USS Enterprise Constitution NCC-1701 203 USS Constitution Constitution NCC-1700 204 USS Defiant Defiant NX-74205 50 USS Voyager Intrepid NCC-74656 141 USS Enterprise Galaxy NCC-1701-D 6000 USS Reliant Miranda NCC-1864 35 Name Date of birth Kirk, James 2233-03-22 Picard, Jean-Luc 2305-07-13","title":"\ud83d\ude80 Starships"},{"location":"Coding/Dogfood/#mathematicians","text":"An expanded assortment of smartypants from a different era. The third and largest dataset, with two date fields and one full of semicolon-delimited subfields ripe for parsing: Name Surname Date of birth Date of death Concepts Carl Gauss 30 April 1777 23 February 1855 Gaussian elimination; Gauss\u2013Jordan elimination; Gauss\u2013Seidel method; Gauss's cyclotomic formula; Gauss's lemma; Gaussian binomial coefficient; Gauss transformation; Gauss\u2013Bodenmiller theorem; Gauss\u2013Bolyai\u2013Lobachevsky space; Gauss\u2013Bonnet theorem; Generalized Gauss\u2013Bonnet theorem; Braid theory; Gauss\u2013Codazzi equations; Gauss\u2013Manin connection; Newton line; Gauss's area formula; Gauss's lemma; Gauss map; Gaussian curvature; Gauss circle problem; Gauss\u2013Kuzmin\u2013Wirsing constant; Gauss's constant; Gauss's digamma theorem; Gauss's generalization of Wilson's theorem; Gauss's lemma; Gauss map; Gaussian moat; Gauss class number problem; Gauss's multiplication formula George Berkeley 12 March 1685 14 January 1753 Gottfried Leibniz 1 July 1646 14 November 1716 Calculus; Monads; Best of all possible worlds; Pre-established harmony; Identity of indiscernibles; Matrix (mathematics); Leibniz integral rule; Principle of sufficient reason; Notation for differentiation; Product rule; Vis viva; Boolean algebra; Salva veritate; Stepped reckoner; Symbolic logic; Semiotics; Analysis situs; Law of Continuity; Transcendental law of homogeneity; Ars combinatoria; Calculus ratiocinator; Leibniz's notation; Characteristica universalis; Problem of why there is anything at all; Pluralistic idealism; Metaphysical dynamism; Relationism; Apperception; A priori/a posteriori distinction Immanuel Kant 22 April 1724 12 February 1804 Kantianism; Kantian ethics; Neo-Kantianism Isaac Newton 25 December 1642 20 March 1726 \"Gauss\u2013Newton algorithm; Newton\u2013Cotes formulas; Newton\u2013Okounkov body; Newton\u2013Pepys problem; Newton fractal; Newton's identities; Newton's inequalities; Newton's method; Newton's method in optimization; Newton's notation; Newton polygon; Newton polynomial; Newton series; Newton's theorem about ovals; Truncated Newton method; bucket argument; Newton's cannonball; Universal gravitational constant; Newton's cradle; Newton disc; Newton\u2013Cartan theory; Newton\u2013Euler equations; Newton's law of cooling; Newton's laws of motion; Newton's law of universal gravitation; Newton's metal; Newton's reflector; Newton's rings; Rotating spheres; Newton scale; Newton's sphere theorem John Locke 29 August 1632 28 October 1704 Labor theory of property; Social contract; State of nature Joseph Fourier 21 March 1768 16 May 1830 Fourier series; Fourier transform; Fourier's law of conduction; Fourier-Motzkin elimination Joseph-Louis Lagrange 25 January 1736 10 April 1813 Lagrangian analysis; Lagrangian coordinates; Lagrangian derivative; Lagrangian drifter; Lagrangian foliation; Lagrangian Grassmannian; Lagrangian intersection Floer homology; Lagrangian mechanics; Lagrangian (field theory); Lagrangian system; Lagrangian mixing; Lagrangian point; Lagrangian relaxation; Lagrangian submanifold; Lagrangian subspace; Nonlocal Lagrangian; Proca lagrangian; Special Lagrangian submanifold; Euler\u2013Lagrange equation; Green\u2013Lagrange strain; Lagrange bracket; Lagrange\u2013d'Alembert principle; Lagrange error bound; Lagrange form; Lagrange interpolation; Lagrange invariant; Lagrange inversion theorem; Lagrange multiplier; Lagrange number; Lagrange point colonization; Lagrange polynomial; Lagrange property; Lagrange reversion theorem; Lagrange resolvent; Lagrange spectrum; Lagrange stream function; Lagrange's approximation theorem; Lagrange's formula (disambiguation); Lagrange's identity (disambiguation); Lagrange's theorem (group theory); Lagrange's theorem (number theory); Lagrange's four-square theorem; Lagrange's trigonometric identities Pierre-Simon Laplace 23 March 1749 5 March 1827 Bayesian inference; Bayesian probability; Laplace's equation; Laplacian; Laplace transform; Inverse Laplace transform; Laplace distribution; Laplace's demon; Laplace expansion; Young\u2013Laplace equation; Laplace number; Laplace limit; Laplace invariant; Laplace principle; Laplace's principle of insufficient reason; Laplace's method; Laplace expansion; Laplace force; Laplace filter; Laplace functional; Laplacian matrix; Laplace motion; Laplace plane; Laplace pressure; Laplace resonance; Laplace's spherical harmonics; Laplace smoothing; Laplace expansion; Laplace expansion; Laplace-Bayes estimator; Laplace\u2013Stieltjes transform; Laplace\u2013Runge\u2013Lenz vector; Nebular hypothesis Ren\u00e9 Descartes 31 March 1596 11 February 1650 Cartesian circle; Cartesian coordinate system; Cartesian diagram; Cartesian diver; Cartesian morphism; Cartesian plane; Cartesian product; Cartesian product of graphs; Cartesian theater; Cartesian tree; Descartes' rule of signs; Descartes' theorem (4 tangent circles); Descartes' theorem (on total angular defect); Folium of Descartes","title":"\ud83e\uddd4 Mathematicians"},{"location":"Coding/GUI/","text":"\ud83d\udda5\ufe0f GUI frameworks comparison Although XAML widgets can declare their own headers, in Tkinter this is implemented as separate LabelFrame widgets, meant to contain controls. Tkinter widgets must be slaved in a manner which ultimately leads to the root object initialized by Tk() , conceptually similar to the Window root node in XAML. To refer to UI elements, XAML prefers the term control whereas tkinter and other frameworks prefer widget . Elements that can contain other elements and are used to visually organize the application are known as layout panels . WinUI control GTK tkinter Button Button Button Checkbox CheckButton Checkbutton TextBox Entry Entry ListView TreeView StackPanel Box DatePicker Calendar DateEntry Widgets Label In both GTK and Tkinter frameworks, a window size must be set or else it will collapse to the boundaries of the contained label. PyGTK tkinter import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk import sys class AppWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , name = \"world\" , ** kwargs ): super () . __init__ ( * args , ** kwargs ) label = Gtk . Label . new ( f \"Hello { name } \" ) self . add ( label ) self . set_size_request ( 200 , 200 ) class Application ( Gtk . Application ): def __init__ ( self , name , * args , ** kwargs ): self . name = name super () . __init__ ( * args , application_id = \"org.example.myapp\" , ** kwargs ) self . window = None def do_activate ( self ): if not self . window : self . window = AppWindow ( application = self , name = self . name , title = f \"Hello, { self . name } !\" ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = Application ( sys . argv [ - 1 ]) app . run () import tkinter from tkinter import ttk import sys class Window ( tkinter . Tk ): def __init__ ( self , name = \"World\" ): super () . __init__ () self . title ( f \"Hello, { name } !\" ) self . geometry ( \"200x200\" ) self . resizable ( False , False ) ttk . Label ( self , text = f \"Hello, { name } !\" ) . grid ( column = 0 , row = 0 ) if __name__ == '__main__' : try : win = Window ( name = sys . argv [ 1 ]) except IndexError : win = Window () win . mainloop () Date picker XAML tkinter <Window x:Class= \"EmployeeManager.WinUI.MainWindow\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:EmployeeManager.WinUI\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" > <DatePicker Header= \"Entry date\" /> </Window> import tkinter as tk from tkinter.ttk import LabelFrame from tkcalendar import DateEntry window = tk . Tk () frame = LabelFrame ( window , text = \"Entry date: \" ) frame . pack () DateEntry ( frame ) . pack () tk . mainloop () Textbox PyGTK import os , sys import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk class AppWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_border_width ( 10 ) prompt_str = \"What is the password for \" + os . getlogin () + \"?\" question = Gtk . Label ( label = prompt_str ) label = Gtk . Label ( label = \"Password:\" ) passwd = Gtk . Entry () passwd . set_visibility ( False ) # passwd.set_invisible_char(\"*\") hbox = Gtk . Box ( orientation = Gtk . Orientation . HORIZONTAL , spacing = 0 ) hbox . pack_start ( label , False , False , 5 ) hbox . pack_start ( passwd , False , False , 5 ) vbox = Gtk . Box ( orientation = Gtk . Orientation . VERTICAL , spacing = 0 ) vbox . pack_start ( question , False , False , 0 ) vbox . pack_start ( hbox , False , False , 0 ) self . add ( vbox ) class Application ( Gtk . Application ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , application_id = \"org.example.myapp\" , ** kwargs ) self . window = None def do_activate ( self ): if not self . window : self . window = AppWindow ( application = self , title = \"Password\" ) self . window . show_all () self . window . present () if __name__ == \"__main__\" : app = Application () app . run ( sys . argv ) ListView PyGTK import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . gen_treeview () scrolled_win = Gtk . ScrolledWindow . new ( None , None ) scrolled_win . set_policy ( Gtk . PolicyType . AUTOMATIC , Gtk . PolicyType . AUTOMATIC ) scrolled_win . add ( self . treeview ) self . add ( scrolled_win ) self . set_size_request ( 200 , 200 ) def get_liststore ( self ): store = Gtk . ListStore . new (( str ,)) store . append ([ \"Socrates\" ]) store . append ([ \"Plato\" ]) store . append ([ \"Aristotle\" ]) return store def gen_treeview ( self ): self . treeview = Gtk . TreeView . new () self . treeview . set_model ( self . get_liststore ()) self . treeview . append_column ( Gtk . TreeViewColumn ( \"Greeks\" , Gtk . CellRendererText . new (), text = 0 )) class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.myapp' ) def do_activate ( self ): self . window = ApplicationWindow ( application = self , title = \"Greeks\" ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = Application () app . run () Toggle Button #!/usr/bin/python3 import sys import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk class AppWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_border_width ( 10 ) vbox = Gtk . Box . new ( orientation = Gtk . Orientation . VERTICAL , spacing = 0 ) toggle1 = Gtk . ToggleButton . new_with_mnemonic ( \"_Deactivate the other one!\" ) toggle2 = Gtk . ToggleButton . new_with_mnemonic ( \"_No! Deactivate that one!\" ) toggle1 . connect ( \"toggled\" , self . on_button_toggled , toggle2 ) toggle2 . connect ( \"toggled\" , self . on_button_toggled , toggle1 ) vbox . pack_start ( toggle1 , True , True , 1 ) vbox . pack_start ( toggle2 , True , True , 1 ) self . add ( vbox ) def on_button_toggled ( self , toggle , other_toggle ): if ( Gtk . ToggleButton . get_active ( toggle )): other_toggle . set_sensitive ( False ) else : other_toggle . set_sensitive ( True ) class Application ( Gtk . Application ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , application_id = \"org.example.myapp\" , ** kwargs ) self . window = None def do_activate ( self ): if not self . window : self . window = AppWindow ( application = self , title = \"Toggle Buttons\" ) self . window . show_all () self . window . present () if __name__ == \"__main__\" : app = Application () app . run ( sys . argv ) Tasks Wired Brain Coffee tkinter import tkinter as tk from tkinter.ttk import Button from tkinter.ttk import Labelframe from tkinter.ttk import Label from tkinter.ttk import Entry from tkinter.ttk import Checkbutton # from tkinter.ttk import import tkcalendar LabelFrame = Labelframe window = tk . Tk () window . title ( \"Wired Brain Coffee\" ) main = Labelframe ( window ) main . pack () headline = Label ( main , text = \"Wired Brain Coffee\" ) headline . grid ( row = 0 , column = 0 , columnspan = 2 ) sidebar = LabelFrame ( main ) sidebar . grid ( row = 1 , column = 0 ) refresh = Button ( sidebar , text = \"Refresh\" ) refresh . pack () mainarea = LabelFrame ( main ) mainarea . grid ( row = 1 , column = 1 ) firstname_frame = Labelframe ( mainarea , text = \"First name\" ) firstname_frame . pack () firstname_textbox = Entry ( firstname_frame ) . pack () # firstname_textbox.pack(expand='yes', fill='both') dateentry_frame = Labelframe ( mainarea , text = \"Entry date\" ) dateentry_frame . pack () dateentry_picker = tkcalendar . DateEntry ( dateentry_frame ) dateentry_picker . pack () jobrole_frame = Labelframe ( mainarea , text = \"Job Role\" ) jobrole_frame . pack () jobrole_textbox = Entry ( jobrole_frame ) . pack () # jobrole_textbox.pack(expand='yes', fill='both') iscoffeedrinker = Checkbutton ( mainarea , text = \"Is coffee drinker?\" ) . pack () # iscoffeedrinker.pack() window . resizable = ( True , True ) window . mainloop () Raven lines PyGTK import sys import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk import itertools class AppWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_border_width ( 25 ) button = Gtk . Button . new_with_mnemonic ( \"_Raven\" ) button . connect ( \"clicked\" , self . on_button_clicked ) button . set_relief ( Gtk . ReliefStyle . NORMAL ) self . add ( button ) self . set_size_request ( 200 , 100 ) with open ( \"/home/jasper/notes/docs/Coding/Dogfood/raven.txt\" ) as f : self . raven = itertools . cycle ([ l . strip () for l in f . readlines ()]) def on_button_clicked ( self , button ): print ( next ( self . raven )) class Application ( Gtk . Application ): def __init__ ( self , argv , * args , ** kwargs ): super () . __init__ ( * args , application_id = \"org.example.myapp\" , ** kwargs ) self . name = argv [ - 1 ] self . window = None def do_activate ( self ): if not self . window : self . window = AppWindow ( application = self , title = f \"Hello { self . name } !\" ) self . window . show_all () self . window . present () if __name__ == \"__main__\" : app = Application ( sys . argv ) app . run ( sys . argv ) Login dialog box PyGTK import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_size_request ( 300 , 300 ) grid = Gtk . Grid . new () image = Gtk . Image . new_from_icon_name ( \"dialog-password\" , Gtk . IconSize . DIALOG ) grid . attach ( image , 0 , 0 , 1 , 1 ) grid . attach ( Gtk . Label ( label = \"Enter your credentials.\" ), 0 , 1 , 2 , 1 ) grid . attach ( Gtk . Label ( label = \"User name:\" ), 0 , 2 , 1 , 1 ) grid . attach ( Gtk . Entry (), 1 , 2 , 1 , 1 ) grid . attach ( Gtk . Label ( label = \"Password:\" ), 0 , 3 , 1 , 1 ) grid . attach ( Gtk . Entry ( visibility = False ), 1 , 3 , 1 , 1 ) class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = \"org.example.myapp\" ) def do_activate ( self ): self . window = ApplicationWindow ( application = self , title = \"Hello, World!\" ) self . window . show_all () self . window . present () if __name__ == \"__main__\" : app = Application () app . run () Timeshift clone Type Location Schedule Users","title":"\ud83d\udda5&#xFE0F; GUI frameworks comparison"},{"location":"Coding/GUI/#gui-frameworks-comparison","text":"Although XAML widgets can declare their own headers, in Tkinter this is implemented as separate LabelFrame widgets, meant to contain controls. Tkinter widgets must be slaved in a manner which ultimately leads to the root object initialized by Tk() , conceptually similar to the Window root node in XAML. To refer to UI elements, XAML prefers the term control whereas tkinter and other frameworks prefer widget . Elements that can contain other elements and are used to visually organize the application are known as layout panels . WinUI control GTK tkinter Button Button Button Checkbox CheckButton Checkbutton TextBox Entry Entry ListView TreeView StackPanel Box DatePicker Calendar DateEntry","title":"\ud83d\udda5&#xFE0F; GUI frameworks comparison"},{"location":"Coding/GUI/#widgets","text":"","title":"Widgets"},{"location":"Coding/GUI/#label","text":"In both GTK and Tkinter frameworks, a window size must be set or else it will collapse to the boundaries of the contained label. PyGTK tkinter import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk import sys class AppWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , name = \"world\" , ** kwargs ): super () . __init__ ( * args , ** kwargs ) label = Gtk . Label . new ( f \"Hello { name } \" ) self . add ( label ) self . set_size_request ( 200 , 200 ) class Application ( Gtk . Application ): def __init__ ( self , name , * args , ** kwargs ): self . name = name super () . __init__ ( * args , application_id = \"org.example.myapp\" , ** kwargs ) self . window = None def do_activate ( self ): if not self . window : self . window = AppWindow ( application = self , name = self . name , title = f \"Hello, { self . name } !\" ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = Application ( sys . argv [ - 1 ]) app . run () import tkinter from tkinter import ttk import sys class Window ( tkinter . Tk ): def __init__ ( self , name = \"World\" ): super () . __init__ () self . title ( f \"Hello, { name } !\" ) self . geometry ( \"200x200\" ) self . resizable ( False , False ) ttk . Label ( self , text = f \"Hello, { name } !\" ) . grid ( column = 0 , row = 0 ) if __name__ == '__main__' : try : win = Window ( name = sys . argv [ 1 ]) except IndexError : win = Window () win . mainloop ()","title":"Label"},{"location":"Coding/GUI/#date-picker","text":"XAML tkinter <Window x:Class= \"EmployeeManager.WinUI.MainWindow\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:EmployeeManager.WinUI\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" > <DatePicker Header= \"Entry date\" /> </Window> import tkinter as tk from tkinter.ttk import LabelFrame from tkcalendar import DateEntry window = tk . Tk () frame = LabelFrame ( window , text = \"Entry date: \" ) frame . pack () DateEntry ( frame ) . pack () tk . mainloop ()","title":"Date picker"},{"location":"Coding/GUI/#textbox","text":"PyGTK import os , sys import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk class AppWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_border_width ( 10 ) prompt_str = \"What is the password for \" + os . getlogin () + \"?\" question = Gtk . Label ( label = prompt_str ) label = Gtk . Label ( label = \"Password:\" ) passwd = Gtk . Entry () passwd . set_visibility ( False ) # passwd.set_invisible_char(\"*\") hbox = Gtk . Box ( orientation = Gtk . Orientation . HORIZONTAL , spacing = 0 ) hbox . pack_start ( label , False , False , 5 ) hbox . pack_start ( passwd , False , False , 5 ) vbox = Gtk . Box ( orientation = Gtk . Orientation . VERTICAL , spacing = 0 ) vbox . pack_start ( question , False , False , 0 ) vbox . pack_start ( hbox , False , False , 0 ) self . add ( vbox ) class Application ( Gtk . Application ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , application_id = \"org.example.myapp\" , ** kwargs ) self . window = None def do_activate ( self ): if not self . window : self . window = AppWindow ( application = self , title = \"Password\" ) self . window . show_all () self . window . present () if __name__ == \"__main__\" : app = Application () app . run ( sys . argv )","title":"Textbox"},{"location":"Coding/GUI/#listview","text":"PyGTK import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . gen_treeview () scrolled_win = Gtk . ScrolledWindow . new ( None , None ) scrolled_win . set_policy ( Gtk . PolicyType . AUTOMATIC , Gtk . PolicyType . AUTOMATIC ) scrolled_win . add ( self . treeview ) self . add ( scrolled_win ) self . set_size_request ( 200 , 200 ) def get_liststore ( self ): store = Gtk . ListStore . new (( str ,)) store . append ([ \"Socrates\" ]) store . append ([ \"Plato\" ]) store . append ([ \"Aristotle\" ]) return store def gen_treeview ( self ): self . treeview = Gtk . TreeView . new () self . treeview . set_model ( self . get_liststore ()) self . treeview . append_column ( Gtk . TreeViewColumn ( \"Greeks\" , Gtk . CellRendererText . new (), text = 0 )) class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.myapp' ) def do_activate ( self ): self . window = ApplicationWindow ( application = self , title = \"Greeks\" ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = Application () app . run ()","title":"ListView"},{"location":"Coding/GUI/#toggle-button","text":"#!/usr/bin/python3 import sys import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk class AppWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_border_width ( 10 ) vbox = Gtk . Box . new ( orientation = Gtk . Orientation . VERTICAL , spacing = 0 ) toggle1 = Gtk . ToggleButton . new_with_mnemonic ( \"_Deactivate the other one!\" ) toggle2 = Gtk . ToggleButton . new_with_mnemonic ( \"_No! Deactivate that one!\" ) toggle1 . connect ( \"toggled\" , self . on_button_toggled , toggle2 ) toggle2 . connect ( \"toggled\" , self . on_button_toggled , toggle1 ) vbox . pack_start ( toggle1 , True , True , 1 ) vbox . pack_start ( toggle2 , True , True , 1 ) self . add ( vbox ) def on_button_toggled ( self , toggle , other_toggle ): if ( Gtk . ToggleButton . get_active ( toggle )): other_toggle . set_sensitive ( False ) else : other_toggle . set_sensitive ( True ) class Application ( Gtk . Application ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , application_id = \"org.example.myapp\" , ** kwargs ) self . window = None def do_activate ( self ): if not self . window : self . window = AppWindow ( application = self , title = \"Toggle Buttons\" ) self . window . show_all () self . window . present () if __name__ == \"__main__\" : app = Application () app . run ( sys . argv )","title":"Toggle Button"},{"location":"Coding/GUI/#tasks","text":"","title":"Tasks"},{"location":"Coding/GUI/#wired-brain-coffee","text":"tkinter import tkinter as tk from tkinter.ttk import Button from tkinter.ttk import Labelframe from tkinter.ttk import Label from tkinter.ttk import Entry from tkinter.ttk import Checkbutton # from tkinter.ttk import import tkcalendar LabelFrame = Labelframe window = tk . Tk () window . title ( \"Wired Brain Coffee\" ) main = Labelframe ( window ) main . pack () headline = Label ( main , text = \"Wired Brain Coffee\" ) headline . grid ( row = 0 , column = 0 , columnspan = 2 ) sidebar = LabelFrame ( main ) sidebar . grid ( row = 1 , column = 0 ) refresh = Button ( sidebar , text = \"Refresh\" ) refresh . pack () mainarea = LabelFrame ( main ) mainarea . grid ( row = 1 , column = 1 ) firstname_frame = Labelframe ( mainarea , text = \"First name\" ) firstname_frame . pack () firstname_textbox = Entry ( firstname_frame ) . pack () # firstname_textbox.pack(expand='yes', fill='both') dateentry_frame = Labelframe ( mainarea , text = \"Entry date\" ) dateentry_frame . pack () dateentry_picker = tkcalendar . DateEntry ( dateentry_frame ) dateentry_picker . pack () jobrole_frame = Labelframe ( mainarea , text = \"Job Role\" ) jobrole_frame . pack () jobrole_textbox = Entry ( jobrole_frame ) . pack () # jobrole_textbox.pack(expand='yes', fill='both') iscoffeedrinker = Checkbutton ( mainarea , text = \"Is coffee drinker?\" ) . pack () # iscoffeedrinker.pack() window . resizable = ( True , True ) window . mainloop ()","title":"Wired Brain Coffee"},{"location":"Coding/GUI/#raven-lines","text":"PyGTK import sys import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk import itertools class AppWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_border_width ( 25 ) button = Gtk . Button . new_with_mnemonic ( \"_Raven\" ) button . connect ( \"clicked\" , self . on_button_clicked ) button . set_relief ( Gtk . ReliefStyle . NORMAL ) self . add ( button ) self . set_size_request ( 200 , 100 ) with open ( \"/home/jasper/notes/docs/Coding/Dogfood/raven.txt\" ) as f : self . raven = itertools . cycle ([ l . strip () for l in f . readlines ()]) def on_button_clicked ( self , button ): print ( next ( self . raven )) class Application ( Gtk . Application ): def __init__ ( self , argv , * args , ** kwargs ): super () . __init__ ( * args , application_id = \"org.example.myapp\" , ** kwargs ) self . name = argv [ - 1 ] self . window = None def do_activate ( self ): if not self . window : self . window = AppWindow ( application = self , title = f \"Hello { self . name } !\" ) self . window . show_all () self . window . present () if __name__ == \"__main__\" : app = Application ( sys . argv ) app . run ( sys . argv )","title":"Raven lines"},{"location":"Coding/GUI/#login-dialog-box","text":"PyGTK import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_size_request ( 300 , 300 ) grid = Gtk . Grid . new () image = Gtk . Image . new_from_icon_name ( \"dialog-password\" , Gtk . IconSize . DIALOG ) grid . attach ( image , 0 , 0 , 1 , 1 ) grid . attach ( Gtk . Label ( label = \"Enter your credentials.\" ), 0 , 1 , 2 , 1 ) grid . attach ( Gtk . Label ( label = \"User name:\" ), 0 , 2 , 1 , 1 ) grid . attach ( Gtk . Entry (), 1 , 2 , 1 , 1 ) grid . attach ( Gtk . Label ( label = \"Password:\" ), 0 , 3 , 1 , 1 ) grid . attach ( Gtk . Entry ( visibility = False ), 1 , 3 , 1 , 1 ) class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = \"org.example.myapp\" ) def do_activate ( self ): self . window = ApplicationWindow ( application = self , title = \"Hello, World!\" ) self . window . show_all () self . window . present () if __name__ == \"__main__\" : app = Application () app . run ()","title":"Login dialog box"},{"location":"Coding/GUI/#timeshift-clone","text":"Type Location Schedule Users","title":"Timeshift clone"},{"location":"Coding/Mermaid/","text":"Mermaid From here %%{init: { 'logLevel': 'debug', 'theme': 'base', 'gitGraph': {'rotateCommitLabel': false}} }%% gitGraph commit id: \"feat(api): ...\" commit id: \"a\" commit id: \"b\" commit id: \"fix(client): .extra long label..\" branch c2 commit id: \"feat(modules): ...\" commit id: \"test(client): ...\" checkout main commit id: \"fix(api): ...\" commit id: \"ci: ...\" branch b1 commit branch b2 commit","title":"Mermaid"},{"location":"Coding/Mermaid/#mermaid","text":"From here %%{init: { 'logLevel': 'debug', 'theme': 'base', 'gitGraph': {'rotateCommitLabel': false}} }%% gitGraph commit id: \"feat(api): ...\" commit id: \"a\" commit id: \"b\" commit id: \"fix(client): .extra long label..\" branch c2 commit id: \"feat(modules): ...\" commit id: \"test(client): ...\" checkout main commit id: \"fix(api): ...\" commit id: \"ci: ...\" branch b1 commit branch b2 commit","title":"Mermaid"},{"location":"Coding/Others/","text":"\ud83e\uddaa Perl Perl6 offers an interactive shell, but previous versions needed a specialized command to be run through the interpreter General syntax - Semicolons terminate lines - Whitespace is irrelevant, except inside strings - Enclosing function arguments in parentheses is optional Inline execution of code Compare similar syntax for sed (MP:17, YUG:614) perl6 -e 'code' Enable warning messages perl6 -w Request an implicit input-reading loop that stores records in $_ perl6 -n Request an implicit input-reading loop that stores records in $_ and automatically prints that variable after optional processing of its contents perl6 -p -l :automatically insert an output record separator at end of the output of print -0digits define the character that marks the end of an input record, using octal digits Shebang #!/usr/bin/env perl` Perl variables are of three types, associated with 3 corresponding sigils which begin the identifiers - Scalars: $ - Arrays: @ - Hashes (associative arrays): % my declares and initializes a variable. Variables can be typed by placing a type between my and the identifier. my $animal = \"camel\" my Str $animal = \"camel\" Predefined variables $. # current line number $_ # conventionally used as a default pattern space for searches; is already initialized but not defined; does **not** function like in bash $[ # current array base subscript (0 by default) $/ # input line separator (newline by default) @ARGV # arguments passed from the command-line; index 0 is the first **additional** argument passed subsequent to the name of the script Sources: Python to Perl6 - nutshell Scalar Operations $pbj = 'peanut butter'.' and '.'jelly' string concatenation was performed with . operator in Perl5 (YUG:617) $pbj = 'peanut butter'~' and '~'jelly' string concatenation now performed with ~ operator in Perl6 print '*' x 40 x operator repeats the string (YUG:617) Array my @array = (element, element, element) | my @array = element, element, elmeent initialize arrays by (optionally) enclosing elements in parentheses (not brackets) @array[n] retrieve element at (0-based) index {n} $matrix[0]->[0] arrow or infix operator can also be used to dereference array refrences $#array return number of the last subscript in the array (effectively length-1) @colors = ( 'red' , 'green' , 'yellow' , 'orange' ); ( $c [ 0 ], $c [ 1 ], $c [ 3 ], $c [ 5 ]) = @colors ; arrays can be declared by initializing constituent elements (array slicing) (PBX:85) Operations @list=(2..10); assign a range of numbers (arr-rng) (PBX:81) @letters=( 'A' .. 'Z' ); assign a range of letters (PBX:82) my @biglist = |@smalllist, |@littlelist array unpacking is done using the | prefix operator, allowing for concatenation Hash Hashes are key-value pairs my %fruits = (apple => red, ...) declare a hash %fruits{'apple'} values are obtained by referencing the key in curly braces (vice brackets) (p6) Functions defined return 1 if the variable passed as argument has a value and null if it does not die mesg print {mesg} and exit; used to implement error-handling join (YUG...) print print to STDOUT, but with no ending string specified prompt take input from STDIN read filehandle, variable, n read {n} bytes into {variable} from {filehandle} (deprecated in Perl6) (PBX:96) say print to STDOUT, but with a newline at the end (Perl6) undef undefine a defined variable, releasing the memory allocated for it Arrays pop @array remove and return last element of {array} push @array, elements append {elements} to {array} shift @array remove and return first element of {array} splice @array, index, n[, elements] remove {n} {elements} from {array}, starting at {index}; or, add {elements} to {array} at {index} unshift @array, elements prepend elements to an array (on the left side) Strings chomp remove last character from a scalar or last character from each word in an array if and only if that character is the input line separator chop remove last character from a scalar or last character from each word in an array; intended for use in removing newlines s/pattern/substitution/g ; substitution command works similar to sed tr/regex1/regex2/ ; replace instances of {regex1} with {regex2}; like replacing lowercase letters with uppercase split /delim/ Hashes keys(%hash) ; return an array of the keys of {hash} values(%hash) ; return an array of the values of {hash} Filehandles <> null filehandle (YUG:618) INFILE filehandle used when opening a file for input (YUG:632) OUTFILE filehandle used when opening a file for writing STDIN deprecated in Perl6 (PBX:96) Special literals These have been deprecated in Perl6 (PBX:51) __LINE__ (Perl5) | $?LINE (Perl6) current line number __FILE__ (Perl5) | $?FILE (Perl6) current filename __END__ logical end of the script (\\004 in Unix and \\032 in Windows) __DATA__ special filehandle __PACKAGE__ (Perl5) | $?PACKAGE (Perl6) current package; default package is main Control flow Loops for @array { action } | deprecated: foreach $var (@arr) { statements} foreach loops have been replaced by for in Perl6 for @array { .print } | deprecated: for @array { print } here the print function is working as a method on $_ , the default variable for @array1 Z @array2 zip up elements of two separate arrays (Source)[https://perl6advent.wordpress.com/2009/12/07/day-7-looping-for-fun-and-profit/] Documentation Implemented using markup language called \"pod\", which uses directives distinguishable by the = character =begin pod start documentation =end pod end documentation Glossary lvalue any value that can be \"assigned to\", and which represents a named region of storage (PBX:71) array slice when the elements of one array are assigned values from another (PBX:84) Object-oriented programming instance variables declared with has keyword: has $.name; class attributes are declard with my keyword, then a method is declared to allow it to be referenced Math Mathematical consonants have their own keywords in Perl6: pi , e , and tau (2*pi) Other topics rename function in Debian The Rename Command Visual Basic VB script doing manual backup On Error Resume Next Set objNetwork = WScript.CreateObject(\"WScript.Network\") objNetwork.RemoveNetworkDrive \"P:\" objNetwork.MapNetworkDrive \"P:\", \"\\\\islfs02.hlm.ad.moffitt.usf.edu\\research\\lab_proteomics\",, \"hlm\\proteomics_backup\",\"orbitrap\" Dim WSHShell Set WSHShell = WScript.CreateObject(\"WScript.Shell\") q = \"\"\"\" sp = \" \" from = q & \"c:\\xcalibur\\Data\\*.*\" & q dest = sp & q & \"p:\\backup\\QE_FOCUS\\data\" & q cmd = \"c:\\windows\\system32\\xcopy \" & from & dest & \" /D /H /E /C /K /R /Y /I\" WSHShell.Run cmd, 1, true from = q & \"c:\\xcalibur\\methods\\*.*\" & q dest = sp & q & \"p:\\backup\\QE_FOCUS\\methods\" & q cmd = \"c:\\windows\\system32\\xcopy \" & from & dest & \" /D /H /E /C /K /R /Y /I\" WSHShell.Run cmd, 1, true Set WSHShell = Nothing WScript.Quit(0)","title":"\ud83e\uddaa Perl"},{"location":"Coding/Others/#perl","text":"Perl6 offers an interactive shell, but previous versions needed a specialized command to be run through the interpreter General syntax - Semicolons terminate lines - Whitespace is irrelevant, except inside strings - Enclosing function arguments in parentheses is optional Inline execution of code Compare similar syntax for sed (MP:17, YUG:614) perl6 -e 'code' Enable warning messages perl6 -w Request an implicit input-reading loop that stores records in $_ perl6 -n Request an implicit input-reading loop that stores records in $_ and automatically prints that variable after optional processing of its contents perl6 -p -l :automatically insert an output record separator at end of the output of print -0digits define the character that marks the end of an input record, using octal digits Shebang #!/usr/bin/env perl` Perl variables are of three types, associated with 3 corresponding sigils which begin the identifiers - Scalars: $ - Arrays: @ - Hashes (associative arrays): % my declares and initializes a variable. Variables can be typed by placing a type between my and the identifier. my $animal = \"camel\" my Str $animal = \"camel\" Predefined variables $. # current line number $_ # conventionally used as a default pattern space for searches; is already initialized but not defined; does **not** function like in bash $[ # current array base subscript (0 by default) $/ # input line separator (newline by default) @ARGV # arguments passed from the command-line; index 0 is the first **additional** argument passed subsequent to the name of the script Sources: Python to Perl6 - nutshell","title":"\ud83e\uddaa Perl"},{"location":"Coding/Others/#scalar","text":"","title":"Scalar"},{"location":"Coding/Others/#operations","text":"$pbj = 'peanut butter'.' and '.'jelly' string concatenation was performed with . operator in Perl5 (YUG:617) $pbj = 'peanut butter'~' and '~'jelly' string concatenation now performed with ~ operator in Perl6 print '*' x 40 x operator repeats the string (YUG:617)","title":"Operations"},{"location":"Coding/Others/#array","text":"my @array = (element, element, element) | my @array = element, element, elmeent initialize arrays by (optionally) enclosing elements in parentheses (not brackets) @array[n] retrieve element at (0-based) index {n} $matrix[0]->[0] arrow or infix operator can also be used to dereference array refrences $#array return number of the last subscript in the array (effectively length-1) @colors = ( 'red' , 'green' , 'yellow' , 'orange' ); ( $c [ 0 ], $c [ 1 ], $c [ 3 ], $c [ 5 ]) = @colors ; arrays can be declared by initializing constituent elements (array slicing) (PBX:85)","title":"Array"},{"location":"Coding/Others/#operations_1","text":"@list=(2..10); assign a range of numbers (arr-rng) (PBX:81) @letters=( 'A' .. 'Z' ); assign a range of letters (PBX:82) my @biglist = |@smalllist, |@littlelist array unpacking is done using the | prefix operator, allowing for concatenation","title":"Operations"},{"location":"Coding/Others/#hash","text":"Hashes are key-value pairs my %fruits = (apple => red, ...) declare a hash %fruits{'apple'} values are obtained by referencing the key in curly braces (vice brackets) (p6)","title":"Hash"},{"location":"Coding/Others/#functions","text":"defined return 1 if the variable passed as argument has a value and null if it does not die mesg print {mesg} and exit; used to implement error-handling join (YUG...) print print to STDOUT, but with no ending string specified prompt take input from STDIN read filehandle, variable, n read {n} bytes into {variable} from {filehandle} (deprecated in Perl6) (PBX:96) say print to STDOUT, but with a newline at the end (Perl6) undef undefine a defined variable, releasing the memory allocated for it","title":"Functions"},{"location":"Coding/Others/#arrays","text":"pop @array remove and return last element of {array} push @array, elements append {elements} to {array} shift @array remove and return first element of {array} splice @array, index, n[, elements] remove {n} {elements} from {array}, starting at {index}; or, add {elements} to {array} at {index} unshift @array, elements prepend elements to an array (on the left side)","title":"Arrays"},{"location":"Coding/Others/#strings","text":"chomp remove last character from a scalar or last character from each word in an array if and only if that character is the input line separator chop remove last character from a scalar or last character from each word in an array; intended for use in removing newlines s/pattern/substitution/g ; substitution command works similar to sed tr/regex1/regex2/ ; replace instances of {regex1} with {regex2}; like replacing lowercase letters with uppercase split /delim/","title":"Strings"},{"location":"Coding/Others/#hashes","text":"keys(%hash) ; return an array of the keys of {hash} values(%hash) ; return an array of the values of {hash}","title":"Hashes"},{"location":"Coding/Others/#filehandles","text":"<> null filehandle (YUG:618) INFILE filehandle used when opening a file for input (YUG:632) OUTFILE filehandle used when opening a file for writing STDIN deprecated in Perl6 (PBX:96)","title":"Filehandles"},{"location":"Coding/Others/#special-literals","text":"These have been deprecated in Perl6 (PBX:51) __LINE__ (Perl5) | $?LINE (Perl6) current line number __FILE__ (Perl5) | $?FILE (Perl6) current filename __END__ logical end of the script (\\004 in Unix and \\032 in Windows) __DATA__ special filehandle __PACKAGE__ (Perl5) | $?PACKAGE (Perl6) current package; default package is main","title":"Special literals"},{"location":"Coding/Others/#control-flow","text":"","title":"Control flow"},{"location":"Coding/Others/#loops","text":"for @array { action } | deprecated: foreach $var (@arr) { statements} foreach loops have been replaced by for in Perl6 for @array { .print } | deprecated: for @array { print } here the print function is working as a method on $_ , the default variable for @array1 Z @array2 zip up elements of two separate arrays (Source)[https://perl6advent.wordpress.com/2009/12/07/day-7-looping-for-fun-and-profit/]","title":"Loops"},{"location":"Coding/Others/#documentation","text":"Implemented using markup language called \"pod\", which uses directives distinguishable by the = character =begin pod start documentation =end pod end documentation","title":"Documentation"},{"location":"Coding/Others/#glossary","text":"lvalue any value that can be \"assigned to\", and which represents a named region of storage (PBX:71) array slice when the elements of one array are assigned values from another (PBX:84)","title":"Glossary"},{"location":"Coding/Others/#object-oriented-programming","text":"instance variables declared with has keyword: has $.name; class attributes are declard with my keyword, then a method is declared to allow it to be referenced","title":"Object-oriented programming"},{"location":"Coding/Others/#math","text":"Mathematical consonants have their own keywords in Perl6: pi , e , and tau (2*pi)","title":"Math"},{"location":"Coding/Others/#other-topics","text":"","title":"Other topics"},{"location":"Coding/Others/#rename-function-in-debian","text":"The Rename Command","title":"rename function in Debian"},{"location":"Coding/Others/#visual-basic","text":"VB script doing manual backup On Error Resume Next Set objNetwork = WScript.CreateObject(\"WScript.Network\") objNetwork.RemoveNetworkDrive \"P:\" objNetwork.MapNetworkDrive \"P:\", \"\\\\islfs02.hlm.ad.moffitt.usf.edu\\research\\lab_proteomics\",, \"hlm\\proteomics_backup\",\"orbitrap\" Dim WSHShell Set WSHShell = WScript.CreateObject(\"WScript.Shell\") q = \"\"\"\" sp = \" \" from = q & \"c:\\xcalibur\\Data\\*.*\" & q dest = sp & q & \"p:\\backup\\QE_FOCUS\\data\" & q cmd = \"c:\\windows\\system32\\xcopy \" & from & dest & \" /D /H /E /C /K /R /Y /I\" WSHShell.Run cmd, 1, true from = q & \"c:\\xcalibur\\methods\\*.*\" & q dest = sp & q & \"p:\\backup\\QE_FOCUS\\methods\" & q cmd = \"c:\\windows\\system32\\xcopy \" & from & dest & \" /D /H /E /C /K /R /Y /I\" WSHShell.Run cmd, 1, true Set WSHShell = Nothing WScript.Quit(0)","title":"Visual Basic"},{"location":"Coding/Python/","text":"\ud83d\udc0d Python Decorators Sources: Primer on Python Decorators YouTube tutorial A decorator is any function that accepts a function and returns a function. Decorators are one of the main ways that Python implements functional programming principles. Functions are first-class objects and can be passed as parameters. import logging def hello_wrapper ( name , func ): func ( f 'Hello { name } ' ) hello_wrapper ( \"world\" , func = print ) # Hello world hello_wrapper ( \"logs\" , func = logging . warning ) # WARNING:root:Hello logs with open ( 'hello.txt' , 'w' ) as f : hello_wrapper ( 'everyone!' , func = f . write ) import random def anagram ( t ): l = [ c for c in t ] random . shuffle ( l ) print ( \"\" . join ( l )) hello_wrapper ( 'Japushku' , anagram ) # eHoulhluaskpJ The function has to be passed as a reference, actually calling it will cause the wrapper function to attempt to execute the value returned by the inner function. hello_wrapper ( \"world\" , func = print ()) # Error def outer (): print ( 'Hi from the outer function' ) def inner (): print ( 'Hello from the inner function' ) inner () We can use the __name__ attribute to access a passed function's name. def hello ( func ): print ( f 'Hello { func . __name__ } ' ) hello ( outer ) # Hello outer We can also return functions, which can then be invoked def hello ( func ): print ( f 'Hello { func . __name__ } ' ) return func hello ( outer )() ''' Hi from the outer function Hello from the inner function ''' new_outer = hello ( outer ) new_outer is outer # True def wrapper ( func ): print ( f 'Before { func . __name__ } ' ) func () print ( f 'After { func . __name__ } ' ) wrapper ( outer ) ''' Before outer Hi from the outer function Hello from the inner function After outer ''' The true decorator pattern appears here, where wrapper is called the decorator and outer has been decorated . def wrapper ( func ): def _wrapper (): print ( f 'Before { func . __name__ } ' ) func () print ( f 'After { func . __name__ } ' ) return _wrapper outer = wrapper ( outer ) But the usual syntax since Python 2.4 is to place the decorator on the line above the decorated function, preceded by @ : @wrapper def outer (): print ( 'Hi from the outer function' ) def inner (): print ( 'Hello from the inner function' ) inner () _wrapper here does not accept any positional arguments, so wrapping functions that take arguments will produce a TypeError @wrapper def say_hello ( name ): print ( f 'Hello { name } !' ) # error The solution is to incorporate *args, **kwargs into the definition of the inner function, as well as the invocation of the function passed in. def wrapper ( func ): def _wrapper ( * args , ** kwargs ): print ( f 'Before { func . __name__ } ' ) func ( * args , ** kwargs ) print ( f 'After { func . __name__ } ' ) return _wrapper Returned values are not captured yet: def wrapper ( func ): def _wrapper ( * args , ** kwargs ): print ( f 'Before { func . __name__ } ' ) value = func ( * args , ** kwargs ) print ( f 'After { func . __name__ } ' ) return value return _wrapper Inspecting the decorated function's __name__ attribute reveals that it is still named _wrapper say_hello . __name__ # '_wrapper' This is also true for other attributes, including docstring. functools.wraps is a decorator factory to reassign attributes to the wrapped function. This is considered superior to the functools.update_wrapper function which is also available. def wrapper ( func ): @functools . wraps ( func ) def _wrapper ( * args , ** kwargs ): print ( f 'Before { func . __name__ } ' ) value = func ( * args , ** kwargs ) print ( f 'After { func . __name__ } ' ) return value return _wrapper This forms an ideal starting template for the creation of custom decorators. Classes Properties In the Python documentation , attributes accessed with accessor functions are called managed attributes , which makes the term equivalent to properties in C#. Three methods can be defined using the @property decorator Constructor Getter Setter Deleter def __init__ ( self , price ): self . _price = price @property def price ( self ): return self . _price @price . setter def price ( self , new_price ): if new_price > 0 : self . _price = new_price else : raise ValueError @price . deleter def price ( self ): del self . _price Class methods The @classmethod decorator prevents the interpreter from passing in the instantiated object using self , rather the class itself is passed in as the cls argument. This means that the methods decorated as such must take not self as the first argument but cls @classmethod def classmethod ( cls ): pass The @staticmethod decorator prevents the interpreter from passing any additional arguments whatsoever. The resulting method has no access to the object itself nor the class and functions like a procedurally defined function. Formatting flake8 , black , and yapf (Google) are CLI tools used to automatically format Python code. Web frameworks Django A typical Django project contains multiple apps , which are Python packages containing their own models, views, templates, and urls. A model is the single definitive source of information about your data, and contains the essential fields and behaviors of the data you're storing. Migrations are necessary when Model classes are updated. And for projects sufficiently advanced, migration scripts must be developed for any such changes. Async Server Gateway Interface (ASGI) is the spiritual successor to, and superset of, WSGI . It implements the new Python standard for asynchronous web servers and applications, which resembles that of websockets . From WSGI to ASGI WSGI is coupled tightly with the synchronous request-response model familiar from HTTP 1.1. URL patterns (stored in the urlpatterns list defined in the project-level urls.py file) can be parameterized. Here, the template <int:x> specifies an integer to be assigned to the view parameter x . from app.views import my_view urlpatterns = [ path ( '/example/<int:x>' , my_view ) ] modelform_factory can be used to automatically produce a webform from a Model class. # views.py MeetingForm = modelform_factory ( Meeting , exclude = []) This can be placed into a template using the {{ form }} template tag. Note, a {% csrf_token %} template tag must also be present for a submit button to work. { % block content % } < h1 > Plan a new meeting </ h1 > < form method = \"POST\" > < table > {{ form }} </ table > { % csrf_token % } < button type = \"submit\" > Create </ button > </ form > { % endblock content % } When the modelform_factory class has been defined, it is instantiated within the view function . This object exposes several methods: - is_valid data validation is strongly recommended for any form input - save imports the validated form data into the database def new ( request ): if request . method == 'POST' : form = MeetingForm ( request . POST ) if form . is_valid (): form . save () return redirect ( \"home\" ) else : form = MeetingForm () return render ( request , \"meetings/new.html\" , { \"form\" : form }) Template Django templates are HTML files with additional markup to signify places where data can be dynamically inserted. The data used by the views file is called the template context . Templates must be placed within the /templates folder within the app, and it is considered best practice to place templates within a nested subdirectory within it, e.g. /templates/app . Django template tags are specified beween {% .. %} and allow for interpolation of data. < ul > { % for m in meetings % } { % endfor % } </ ul > URLs can be built by using the url template tag, specifying the name of a URL urlpatterns = [ path ( '' , home , name = 'home' ) ] < a href = \"{ % u rl 'home' %}\" > Home </ a > Models Models In Django, a Model class is mapped to a database table. Each object is a record in that table. Model objects expose several attributes and methods Get all objects meetings = Meeting . objects . all () Get count of objects in database count = Meeting . objects . count () Query for a specific object meeting = Meeting . objects . get ( pk = id ) get_object_or_404 may be better for most cases meeting = get_object_or_404 ( Meeting , pk = id ) Adding a new app python manage.py startapp app Then add to settings.py in project directory INSTALLED_APPS = [ # ... 'app' , ] There appears to be much flexibility in the arrangement of input controls in a form. So long as the Submit button is child to the form element, tasks are accepted in the To-Do app. Per Bulma documentation, the field class is intended as a container for label.label s, .control s, and optional p.help text. In contrast, control is a block container meant to enhance single form controls and can only contain input , select , button , or icon elements. form .field ( method= \"POST\" , action= \"/\" ) label .label Enter something to do .control | {{form.title}} | {% csrf_token %} button .button.is-primary ( type= \"submit\" ) Submit FastAPI Variables values can be taken from the route or from query parameters following a question mark. Routes Query parameters from fastapi import FastAPI starships = FastAPI () @starships . get ( \"/starships/ {registry} \" ) def get_starship ( name : str ): return { \"response\" : f \"Hello, { name } \" } from fastapi import FastAPI starships = FastAPI () @starships . get ( \"/\" ) def get_starship ( name : str = \"world\" ): return { \"response\" : f \"Hello, { name } \" } FastAPI is notable for being able to use type hints to construct data models, which are much lighter than the object relational models used by other frameworks. FastAPI Django from pydantic import BaseModel class Starship ( BaseModel ): name : str registry : str crew : int from django.db import models class Starship ( models . Model ): name = models . CharField ( max_length = 50 ) registry = models . CharField ( max_length = 15 ) crew = models . IntegerField () Dogfood data can be incorporated by using the keyword argument unpacking or \"double splat\" operator ( ** ) data = { \"name\" : \"USS Enterprise\" , \"registry\" : \"NCC-1701\" , \"crew\" : 203 } enterprise = Starship ( ** data ) POST method definitions then can use this newly defined class to validate posted data db = [] @app . post ( \"/starships\" ) async def create_starship ( starship : Starship ): db . append ( starship ) FastAPI supports Jinja templates to serve HTML templates import fastapi from fastapi.templating import Jinja2Templates # specifies the directory where templates are to be found templates = Jinja2Templates ( \"templates\" ) api = fastapi . APIRouter () @api . get ( '/' ) def index ( request : starlette . requests . Request ): return templates . TemplateResponse ( \"helloworld.html\" , { \"request\" : request }) By default, FastAPI also exposes web applications at /docs where you can test out all the exposed API methods. FastAPI integrates with ASGI servers like Uvicorn and Hypercorn, which can run a specific web application by name from the command-line or from within the script: Shell Python uvicorn main:starships --port 7000 import uvicorn uvicorn . run ( starships , port = 7000 ) Virtual environments pipenv pipenv --python 3 .6 venv Create a virtual environment named project python -m venv project virtualenv Create a virtual environment named project using a different version of Python virtualenv -p /usr/bin/python2 project Testing Pytest is a popular testing framework preferred to unittest by many Python developers because it follows Pythonic conventions more closely. In contrast to unittest's custom methods, pytest relies on the builtin assert statement. pytest unittest Class under test from phonebook import PhoneBook import pytest @pytest . fixture def phonebook (): phonebook = PhoneBook () yield phonebook phonebook . clear () def test_lookup_by_name ( phonebook ): phonebook . add ( \"Bob\" , \"1234\" ) assert \"1234\" == phonebook . lookup ( \"Bob\" ) def test_phonebook_contains_all_names ( phonebook ): phonebook . add ( \"Bob\" , \"1234\" ) assert \"Bob\" in phonebook . names () def test_missing_name_raises_error ( phonebook ): with pytest . raises ( KeyError ): phonebook . lookup ( \"Bob\" ) python -m pytest import unittest from phonebook import PhoneBook class PhoneBookTest ( unittest . TestCase ): def test_lookup_by_name ( self ): self . phonebook . add ( \"Bob\" , \"12345\" ) number = self . phonebook . lookup ( \"Bob\" ) self . assertEqual ( \"12345\" , number ) def test_missing_name ( self ): with self . assertRaises ( KeyError ): self . phonebook . lookup ( \"missing\" ) def test_empty_phonebook_is_consistent ( self ): self . assertTrue ( self . phonebook . is_consistent ()) def setUp ( self ) -> None : self . phonebook = PhoneBook () def tearDown ( self ) -> None : self . phonebook . clear () python -m unittest import os class PhoneBook : def __init__ ( self , cache_directory = os . getcwd ()): self . numbers = {} self . filename = os . path . join ( cache_directory , \"phonebook.txt\" ) self . cache = open ( self . filename , \"w\" ) def add ( self , name , number ): self . numbers [ name ] = number def lookup ( self , name ): return self . numbers [ name ] def is_consistent ( self ): return True def names ( self ): return set ( self . numbers . keys ()) def clear ( self ): self . cache . close () os . remove ( self . filename ) Doctest A doctest is a docstring containing what looks like interactive Python sessions. Python Docs \"\"\" Return the factorial of n, an exact integer >= 0. >>> [factorial(n) for n in range(6)] [1, 1, 2, 6, 24, 120] >>> factorial(30) 265252859812191058636308480000000 >>> factorial(-1) Traceback (most recent call last): ... ValueError: n must be >= 0 Factorials of floats are OK, but the float must be an exact integer: >>> factorial(30.1) Traceback (most recent call last): ... ValueError: n must be exact integer >>> factorial(30.0) 265252859812191058636308480000000 It must also not be ridiculously large: >>> factorial(1e100) Traceback (most recent call last): ... OverflowError: n too large \"\"\" This can then be run if __name__ == '__main__' : import doctest doctest . testmod () pytest PyTest relies on the built-in assert statement. Fixtures The @pytest.fixture decorator facilitiates the creation of test fixtures. The fixture function's name is used as argument to the test case, and the value returned can be used by the logic within. ( src ) Any clean-up logic can be invoked in this fixture as well by replacing return with yield . Pytest also provides its own tmpdir test fixture for temporary directories. ( src ) Before After tmpdir from phonebook import PhoneBook import pytest def test_lookup_by_name ( phonebook ): phonebook = PhoneBook () phonebook . add ( \"Bob\" , \"1234\" ) assert \"1234\" == phonebook . lookup ( \"Bob\" ) def test_phonebook_contains_all_names ( phonebook ): phonebook = PhoneBook () phonebook . add ( \"Bob\" , \"1234\" ) assert \"Bob\" in phonebook . names () def test_missing_name_raises_error ( phonebook ): phonebook = PhoneBook () with pytest . raises ( KeyError ): phonebook . lookup ( \"Bob\" ) from phonebook import PhoneBook import pytest @pytest . fixture def phonebook (): phonebook = PhoneBook () yield phonebook phonebook . clear () def test_lookup_by_name ( phonebook ): # phonebook = PhoneBook() phonebook . add ( \"Bob\" , \"1234\" ) assert \"1234\" == phonebook . lookup ( \"Bob\" ) def test_phonebook_contains_all_names ( phonebook ): # phonebook = PhoneBook() phonebook . add ( \"Bob\" , \"1234\" ) assert \"Bob\" in phonebook . names () def test_missing_name_raises_error ( phonebook ): # phonebook = PhoneBook() with pytest . raises ( KeyError ): phonebook . lookup ( \"Bob\" ) from phonebook import PhoneBook import pytest @pytest . fixture def phonebook ( tmpdir ): phonebook = PhoneBook ( tmpdir ) return phonebook def test_lookup_by_name ( phonebook ): # phonebook = PhoneBook() phonebook . add ( \"Bob\" , \"1234\" ) assert \"1234\" == phonebook . lookup ( \"Bob\" ) def test_phonebook_contains_all_names ( phonebook ): # phonebook = PhoneBook() phonebook . add ( \"Bob\" , \"1234\" ) assert \"Bob\" in phonebook . names () def test_missing_name_raises_error ( phonebook ): # phonebook = PhoneBook() with pytest . raises ( KeyError ): phonebook . lookup ( \"Bob\" ) unittest unittest is a testing framework built into Python's Standard Library that was based on JUnit. unittest came out in 2001, when JUnit was being ported and adapted to many languages. Collectively, these frameworks were referred to as the xUnit family . unittest's method names do not follow Python conventions because it predates the PEP-8 naming standard. unittest allows you to create test classes that inherit from TestCase . Assertions Assertions are implemented in individual methods of the TestCase subclass through unittest methods like assertEqual and assertRaises , etc. Notably, TestCase subclasses must not have an __init__() constructor method defined. def test_lookup_by_name ( self ): phonebook = PhoneBook () phonebook . add ( \"Bob\" , \"12345\" ) number = phonebook . lookup ( \"Bob\" ) self . assertEqual ( \"12345\" , number ) assertRaises must be placed in a context manager. Here, the test case will run the code within the with block and check to make sure it raises the specified exception: KeyError : ( src ) def test_missing_name ( self ): fleet = Fleet () with self . assertRaises ( KeyError ): fleet . lookup ( \"bla\" ) Fixtures setUp is run before every test method, allowing a test fixture to be created to avoid repetitive code. tearDown is called after every method, which allows these resources to be released, even if the test case raises an exception. However, if it is setUp that raises the exception, then neither the test case nor tearDown will run. ( src , src ) Before After import unittest from phonebook import PhoneBook class PhoneBookTest ( unittest . TestCase ): def test_lookup_by_name ( self ): phonebook = PhoneBook () phonebook . add ( \"Bob\" , \"12345\" ) number = phonebook . lookup ( \"Bob\" ) self . assertEqual ( \"12345\" , number ) def test_missing_name ( self ): phonebook = PhoneBook () with self . assertRaises ( KeyError ): phonebook . lookup ( \"missing\" ) @unittest . skip ( \"WIP\" ) def test_empty_phonebook_is_consistent ( self ): phonebook = PhoneBook () self . assertTrue ( phonebook . is_consistent ()) import unittest from phonebook import PhoneBook class PhoneBookTest ( unittest . TestCase ): def setUp ( self ) -> None : self . phonebook = PhoneBook () def tearDown ( self ) -> None : self . phonebook . clear () def test_lookup_by_name ( self ): # phonebook = PhoneBook() self . phonebook . add ( \"Bob\" , \"12345\" ) number = self . phonebook . lookup ( \"Bob\" ) self . assertEqual ( \"12345\" , number ) def test_missing_name ( self ): # phonebook = PhoneBook() with self . assertRaises ( KeyError ): self . phonebook . lookup ( \"missing\" ) @unittest . skip ( \"WIP\" ) def test_empty_phonebook_is_consistent ( self ): # phonebook = PhoneBook() self . assertTrue ( self . phonebook . is_consistent ()) The @unittest.skip decorator will tell the test runner to skip the decorated test case ( src ) @unittest . skip ( \"WIP\" ) def test_empty_phonebook_is_consistent ( self ): phonebook = PhoneBook () self . assertTrue ( phonebook . is_consistent ()) The command line entry point is made with a call to unittest.main() , which executes the tests. ( src ) import unittest from my_sum import sum class TestSum ( unittest . TestCase ): def test_list_int ( self ): \"\"\" Test that it can sum a list of integers \"\"\" data = [ 1 , 2 , 3 ] result = sum ( data ) self . assertEqual ( result , 6 ) if __name__ == '__main__' : unittest . main () Integration tests By convention, tests are put in their own directory as sibling to the main module ( in order to be able to import it ). Integration and unit tests should be organized separately. . \u251c\u2500\u2500 project \u2502 \u2514\u2500\u2500 __init__.py \u2514\u2500\u2500 tests \u251c\u2500\u2500 integration \u2514\u2500\u2500 unit Run all integration tests within specified directory. python -m unittest discover -s tests/integration Tasks Deserialize YAML JSON import yaml with open ( './starships.yaml' ) as f : starships = yaml . safe_load ( f ) import json with open ( './starships.json' ) as f : data = json . load ( f ) Serialize YAML JSON import yaml with open ( './starships.yaml' , 'w' ) as f : yaml . dump ( starships , f ) import json with open ( '/starships.json' , \"w\" ) as f : json . dump ( data , f ) Modules When learning unfamiliar packages and importing them in a demonstration script, care must be taken that the demonstration script does not have the same name as the package being studied. If so, attempting to import the package while in an interpreter within that directory will cause the interpreter to try importing the incomplete script and not the package. When running a Python interpreter within this directory, the files \"calc\" and \"main\" can be imported as modules by specifying their names with no file extension. . \u251c\u2500\u2500 calc.py \u2514\u2500\u2500 main.py import calc # No errors import main # No errors Specifying the full filename including extension does produce an error import calc.py # Error import main.py # Error argparse The ArgumentParser object exposes an attribute that contains the value passed in from the command-line. This attribute takes its identifier from the dest keyword argument when invoking the add_argument() method. Python Output import argparse def get_args (): parser = argparse . ArgumentParser () parser . add_argument ( dest = 'bar' ) return parser . parse_args () def main (): args = get_args () . bar The optional value assigned to description will be displayed when running the script with the options -h or --help parser = argparse . ArgumentParser ( description = helptext ) usage: argparse_practice.py [-h] [-f bar] optional arguments: -h, --help show this help message and exit -f bar, --foo bar A help string can be provided as a keyword argument to help . parser . add_argument ( \"foo\" , help = \"bar\" ) A data type can be specified as an argument to type : parser . add_argument ( \"foo\" , type = int ) An alternative name for the dest value on the command-line (but which does not affect the identifier of the attribute on which the value is exposed) can be specified by metavar . parser . add_argument ( \"foo\" , metavar = \"bar\" ) The examples above used positional parameters (i.e. an argument). A named parameter (an option or flag, i.e. -h , --help , etc) requires - at the beginning of the string and values from the command-line to be passed after = or Space . add_argument supports the required keyword argument for named parameters. Note that use of the option on the command-line at all requires an argument to it, even if the option is not required itself. parser . add_argument ( '-r' , '--radius' , type = int , required = True , help = 'radius' ) A flag option can be created by defining an action keyword parameter. ( src ) parser . add_argument ( '-o' , '--on' , help = 'A boolean flag' , action = 'store_true' ) add_mutually_exclusive_group() can be used to add a group of mutually exclusive arguments. In this case, add_argument() is invoked on the new object returned by this method and not directly on the ArgumentParser() object. g = ArgumentParser . add_mutually_exclusive_group () g . add_argument ( \"-v\" , \"--verbose\" , action = \"store_true\" ) g . add_argument ( \"-q\" , \"--quiet\" , \"-s\" , \"--silent\" , action = \"store_true\" , help = 'quiet/silent mode' ) User input can be restricted by providing a value for choices , which will accept any iterable value including lists, ranges, and strings: parser . add_argument ( \"foo\" , choices = [ \"bar\" , \"baz\" ]) parser . add_argument ( \"foo\" , choices = range ( 1 , 10 )) parser . add_argument ( \"foo\" , choices = 'Hello, world!' ) # equivalent to ['H','e', ...] Sources Python documentation asyncio The asyncio module offers an implementation of coroutines which allow tasks to control context switching to implement concurrency . The await keyword is a checkpoint that indicates where it is safe for the process to go to another coroutine, allowing total control over context switching import asyncio import time counter = 0 async def func1 (): global counter while True : counter += 1 counter -= 1 await asyncio . sleep ( 0 ) async def func2 (): global counter while True : counter += 1 counter -= 1 await asyncio . sleep ( 0 ) asyncio . gather ( func1 (), func2 ()) asyncio . get_event_loop () . run_forever () async def get_users (): users = await client . do_query ( 'select * from users' ) return users async def main (): task = asyncio . create_task ( get_users ()) # ... await task asyncio . run ( main ()) Allows the joining of multiple threads. async def get_users (): users = await client . do_query ( 'select * from users' ) return users async def main (): await asyncio . gather ( get_users (), get_users (), ) asyncio . run ( main ()) async def get_users (): users = await client . do_query ( 'select * from users' ) return users asyncio . run ( get_users ()) async def main (): users = await get_users () print ( users ) asyncio . run ( main ()) Sources: Demistifying Python's Async and Await keywords azure.cosmos import azure.cosmos from azure.cosmos.partition_key import PartitionKey database = cosmos_client . create_database ( 'RetailDemo' ) container = database . create_container ( id = 'WebsiteData' , partition_key = PartitionKey ( path = '/CartID' )) print ( 'Container WebsiteData created' ) bullet bullet.Check() implements a checkbox widget: cli = bullet . Check ( prompt = \"Choose from the following items: \" , choices = [ 'pepperoni' , 'sausage' , 'green peppers' ]) bullet.Bullet() implements a radio button widget: cli = bullet . Bullet ( prompt = \"Choose from the following items: \" , choices = [ 'red' , 'white' , 'blue' ]) The resulting object then exposes a launch() method. cli . launch () click Click modifies functions using decorators whch determine the command-line arguments elements that the decorated function can see. Hello World program. Click import click @click . command () def hello (): click . echo ( 'Hello World!' ) if __name__ = '__main__' : hello () Modified Hello World import click @click . command () @click . option ( '--count' , default = 1 , help = 'number of greetings' ) @click . argument ( 'name' ) def hello ( count , name ): for x in range ( count ): click . echo ( 'Hello %s !' % name ) if __name__ == '__main__' : hello () Developing the pdfcropper tool; passing --examref changes the numbers. import click @click . command () @click . option ( '--examref' , is_flag = True ) def hello ( examref ): top , right , bottom , left = 0 , 0 , 0 , 0 if examref : top , right , bottom , left = 1 , 2 , 3 , 4 click . echo ( f 'Your numbers are: top ( { top } ), right { right } , bottom { bottom } , left { left } ' ) if __name__ == '__main__' : hello () @click.group() decorators allow nested command groups to be created. There are two ways of adding commands to command groups : - Using the group as a decorator, whereby the name of the function decorated by @click.group() is then used to decorate commands: @click . group () def group1 () pass @group1 . command () def command1 (): pass - Using the add_command method @click . group () def group1 () pass @click . command () def command1 (): pass group1 . add_command ( command1 ) For example, to imitate the nested commands available in netsh : netsh interface ip @click . group () def interface (): pass @interface . command ( 'ip' ) @click . argument ( 'args' , nargs =- 1 ) # All arguments passed in as tuple \"args\" def interface_ip ( args ): pass Docstrings of groups and commands show up as progressive help messages when they are invoked from the command-line. @click . group () def cli (): pass @click . command () def initdb (): click . echo ( 'Initialized the database' ) @click . command () def dropdb (): click . echo ( 'Dropped the database' ) cli . add_command ( initdb ) cli . add_command ( dropdb ) if __name__ == '__main__' : cli () CommandCollection flattens the structure of grouped commands so that the commands in all the contained groups appear in a single tier. It also becomes the entry-point of the script. Example from GitHub : # Three command groups cli1, cli2, and cli3 declared: @click . group () def cli1 (): pass @click . group () def cli2 (): pass @click . group () def cli3 (): pass # Three commands each belonging to a separate group @cli1 . command () def server (): pass @cli2 . command () def console (): pass @cli3 . command () def routes (): pass # CommandCollection flattens the grouped commands such that all the commands are available at once: cli = click . CommandCollection ( sources = [ cli1 , cli2 , cli3 ]) if __name__ == '__main__' : cli () collections abc provides Mapping and MutableMapping ABCs to formalize the interfaces of dict and similar types ChainMap Lookups are performed on each mapping in order Counter Holds an integer count for each key; each new key adds to the count deque : Thread-safe double-ended queue that supports most list methods namedtuple Card = namedtuple ( 'Card' ,[ 'rank' , 'suit' ]) ` City = namedtuple ( 'City' , 'Name Country Population Coordinates' . split ( ' ' )] OrderedDict: Maintains keys in insertion order UserDict: Designed to be subclassed colorama Colorama provides a set of enums that resolve to terminal codes when concatenated with strings. colorama . Fore . GREEN colorama . Style . RESET_ALL csv with open ( 'file.csv' , newline = '' ): data = [ row for row in csv . reader ( f )] csv.DictReader with open ( 'greeks.csv' ) as f : reader = csv . DictReader ( f ) for row in reader : print ( row [ 'name' ], row [ 'city' ], row [ 'dob' ]) datetime datetime . date ( 2016 , 7 , 24 ) datetime . date . today () Difference between datetime objects is a timedelta Parse strings into datetime objects datetime . strptime ( datestring , formatstring ) # Various metacharacters are defined for `strptime` datetime . datetime . strptime ( '06/30/1992' , '%m/ %d /%Y' ) discord.py pip install discord.py client = discord.Client () Client objects expose a decorator that is used for event handlers, functions named after various events: - on_ready - on_member_join - on_error - on_message @client . event async def on_ready (): print ( f ' { client . user } has connected to Discord!' ) Another decorator is exposed for in-chat commands ( commands.Bot has to be instantiated first.) @bot . command ( name = 'roll_dice' , help = 'Simulates rolling dice.' ) async def roll ( ctx , number_of_dice : int , number_of_sides : int ): dice = [ str ( random . choice ( range ( 1 , number_of_sides + 1 ))) for _ in range ( number_of_dice ) ] await ctx . send ( ', ' . join ( dice )) client . run ( token ) bot = comands . Bot ( command_prefix = '!' ) dotenv pip install -U python-dotenv Load a .env file placed in the current working directory. load_dotenv () value = os . getenv ( 'key' ) functools For higher-order functions (functions that act on or return other functions)\\ Apply function of two arguments cumulatively to the items of iterable in order to reduce it to a single value functools . reduce ( function , iterable [, initializer ]) Calculate ((((1+2) +3) +4) +5) functools . reduce ( lambda x , y : x + y , [ 1 , 2 , 3 , 4 , 5 ]) functools.reduce(lambda a,b: a*b, range(1,6)) => 120 : factorial glob Produce a list of strings glob . glob ( '*.py' ) heapq Support heaps , data objects where each node is either greater than or equal to its parent ( max-heap ) or less than or equal to its parent ( min-heap ) Create a heap from {iterable} heapq . heapify ( iterable ) Remove and return the smallest element of {heap} heapq . heappop ( heap ) Replace the smallest element of {heap} with {element} heapq . heapreplace ( heap , element ) http Start an HTTP server for the current directory python http.server itertools cycle() works like next() , but it restarts from the beginning of the iterable that is passed as argument after the last element has been reached. with open ( 'raven' ) as f : raven = [ l for l in f ] itertools . cycle ( raven ) json Deserialize Serialize import json with open ( 'starships.json' ) as f : data = json . load ( f ) import json with open ( 'starships.json' , \"w\" ) as f : json . dump ( data , f ) logging import logging def main (): logging . basicConfig ( filename = '/tmp/learn-logging.log' , level = logging . ERROR , format = ' %(asctime)s %(levelname)s : %(message)s ' ) logging . info ( \"Once upon a midnight dreary,\" ) logging . warning ( 'While I pondered weak and weary,' ) logging . error ( 'Over many a quaint and curious volume of forgotten lore,' ) if __name__ == '__main__' : main () npyscreen Widget library and application framework built on top of ncurses . Documentation ] Three main types of object compose npyscreen applications: - Application objects manage forms and other classes - Form objects form the canvas upon which widgets are arrayed - Form general-purpose - FormMutt - Widget objects are individual controls - TitleText text entry - TitleSelectOne equivalent to radio buttons - TitleDateCombo allows picking of date on a small calendar npyscreen.wrapper_basic is the main entry point import npyscreen def myFunction ( * args ): pass if __name__ == '__main__' : npyscreen . wrapper_basic ( myFunction ) print \"Blink and you missed it!\" npyscreen.Form is equivalent to the Tk() object, which is typically instantiated as win in GUI frameworks. F = npyscreen . Form ( name = 'My Test Application' ) Several important methods are key: - create() The standard constructor calls this method, which does nothing by default and is meant to be overriden in subclasses. Widgets are defined here. npyscreen.FormMutt imitates a UI layout popularized by applications like mutt , irssi , and vim , with a title bar at the top, a command line at the bottom, and a status line directly above the command line. ACTION_CONTROLLER can be defined in the FormMutt subclass as the name of a subclass of ActionControllerSimple . Commands for the application can be defined as callbacks in the create() method. self . add_action ( ident , call_back , True ) Callbacks are called with the following arguments: call_back ( command_line , control_widget_proxy , live = True ) class ActionControllerSearch ( npyscreen . ActionControllerSimple ): def create ( self ): self . add_action ( '^/.*' , self . set_search , True ) def set_search ( self , command_line , widget_proxy , live ): self . parent . value . set_filter ( command_line [ 1 :]) self . parent . wMain . values = self . parent . value . get () self . parent . wMain . display () class FmSearchActive ( npyscreen . FormMuttActiveTraditional ): ACTION_CONTROLLER = ActionControllerSearch npyscreen.NPSAppManaged is the preferred superclass to support object-oriented implementation. class MyApplication ( npyscreen . NPSAppManaged ): pass Calling run() method of application object as main entry point. run() activates the default form, which should be given an id of MAIN if __name__ == '__main__' : TestApp = MyApplication () . run () print \"All objects, baby.\" Using a try / except block to allow for well-mannered exit in case of KeyboardInterrupt (Ctrl+C) GitHub try : App () . run () except KeyboardInterrupt : sys . exit ( 0 ) There are three methods for registering a Form object with a NPSAppManaged instance; - addForm() creates a new form and returns a weakref.proxy to it - addFormClass() register a class of Form rather than an instance - registerForm() It continually displays the Form named by its NEXT_ACTIVE_FORM attribute. Use the afterEditing method to allow exiting. class myEmployeeForm ( npyscreen . Form ): def afterEditing ( self ): self . parentApp . setNextForm ( None ) numpy numpy.ndarray - 2-dimensional array - items can be fetched using the syntax a[i, j] - arrays can be sliced with syntax a[m:n, k:l] - FP:35 numpy.arange(n) build a numpy.ndarray object with numbers 0 to n-1 (FP:52) numpy.loadtxt(filename) load numbers stored in a text file (FP:53) optparse Instantiate the parser object parser = optparse . OptionParser ( usage = __doc__ . strip ()) # add an option parser . add_option ( '--timeout' ) os Execute shell command given by string. The value returned is actually the exit code, not the output of the command to STDOUT. os . system ( 'ls -la' ) Store output in a variable os . popen ( 'ls -la' ) . read () Navigate filesystem os . getcwd () os . chdir ( path ) Test for existence of a file os . path . isfile ( file ) pandas summary: open-source Python library used for data science operation: runs over NumPy - good for storing lists-of-lists (CSV) print(df) - prints it out in an easy to read tabular format DataFrame is the main object in pandas - head() , tail() - prints out the first, last several rows (5 by default) - optional numerical argument defines number of rows - describe() - numerical analyses, including count, unique, mean, etc - sort_values('field',ascending=False) pathlib Create a new pathlib object; represents a file or directory pathlib . Path ( path ) Test for existence of a file pathlib . Path . is_file ( file ) Test for existence of a directory pathlib . Path . is_dir ( dir ) Find all .py files Returns a generator pathlib . Path . glob ( '*.py' ) Open a file. This makes a file object that is automatically closed, similar to open builtin: pathlib . Path . open () Display file extension pathlib . Path . suffix () Display file size pathlib . Path . stat () . st_size pyinstaller Source: RealPython tutorial Installing PyInstaller , even in a virtual environment, will install the pyinstaller executable to $HOME/.local/bin. On Windows, it is installed to another directory within LOCALAPPDATA . pip install pyinstaller PyInstaller creates primarily 3 items: - .spec file, named after the CLI script - build/ folder, which can be ignored - dist/ folder, containing the final artifact at dist/cli/cli or dist/cli/cli.exe Several options are available hidden-import name onefile pyinstaller script.py --onefile On Windows, if PyInstaller is run from a virtual environment without necessary modules installed, they may not be available for compilation into the artifact. This does not appear to be an issue with Linux. This problem appears to be specific to certain modules, like emoji . pythonnet Docs: ? ! Developers recommend Mono version 5.20.1 Issues 939 On Ubuntu, the eoan universe repository has to be added deb https://archive.ubuntu.com/ubuntu/ eoan universe deb https://archive.ubuntu.com/ubuntu/ eoan-updates universe But I can't figure out how to add the older version, because the recommended syntax produces the error \"Unable to correct problems, you have held broken packages\" sudo apt install mono-devel = 5 .18.0.240+dfsg-3 Maybe try the tarballs on Mono's website... Or maybe there's another repo I don't know about.. apt install clang libglib2.0-dev python3-dev pip install pycparser pythonnet pip install -U setuptools random Random choice with replacement random . choice ( iterable ) Shuffle elements of an iterable in-place [FP:42] random . shuffle ( iterable ) scrapy Best used to obtain one \"stream\" of data at a time, without trying to obtain data from different pages scrapy runspider spider.py -o file.json Display HTML source of the scraped page print ( response . txt ) Get {URL} fetch ( 'url' ) Select a CSS selector # Returns a `SelectorList` response . css ( 'p' ) # Retrieve full HTML elements response . css ( 'p' ) . extract () Retrieve only the text within the element response . css ( 'p::text' ) . extract () response . css ( 'p::text' ) . extract_first () response . css ( 'p::text' ) . extract ()[ 0 ] Get the href attribute value for an anchor tag response . css ( 'a' ) . attrib [ 'href' ] Launch Scrapy shell and scrape $URL scrapy shell $URL Make a default spider named {quotes} that will be restricted to {domain} scrapy genspider quotes domain scrapy runspider scrapy1 . py Run a spider, saving scraped data to a JSON file scrapy runspider spider . py - o items . json Method which contains most of the logic of the spider, especially after the yield keyword. For multiple items, a structural basis for iteration must be found and for each iteration, data is yielded Extract URL from link using standard CSS selection techniques Add the domain name to a relative link response . urljoin () Recursively call the parse method again on the next page yield scrapy . Request ( url = next_page_url , callback = self . parse ) Scrape detail pages - parse_details would be a spider method sibling to the main parse method - if a detail page has more information than the main, then the yield keyword should be in parse_details yield scrapy . Request ( url = { url }, callback = self . parse_details ) setuptools Setuptools is for uploading to PyPi. To create self-contained executable files, use pyinstaller . PROJECT \u251c\u2500\u2500 PROJECT # Additional code files will be placed in here \u2502 \u2514\u2500\u2500 init.py \u2514\u2500\u2500 setup.py # Containing a call to `setuptools.setup()` 1 directory, 2 files setup.py from setuptools import setup setup ( name = 'funniest' , version = '0.1' , description = 'The funniest joke in the world' , url = 'http://github.com/storborg/funniest' , author = 'Flying Circus' , author_email = 'flyingcircus@example.com' , license = 'MIT' , packages = [ 'funniest' ], zip_safe = False ) If the package has dependencies, they can be added by appending a install_requires keyword argument passing an array of the module names setup ( install_requires = [ 'markdown' , ], ) Reserve the name, upload package metadata, and create the pypi.python.org webpage python setup . py register Create a source distribution, producing a tarball inside the top-level directory python setup . py sdist Upload the source distribution python setup . py sdist upload Do all the above in a single step python setup . py register sdist upload socket The socket module is Python's standard interface for the transport layer. Sockets can be classified by family AF_INET Internet AF_UNIX for UNIX sockets and type : - SOCK_STREAM TCP - SOCK_DGRAM UDP These enum values are required upon initialization of a socket object: Ortega : 25 client_socket = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) Sources: Sockets tutorial TCP server TCP Client UDP server UDP client import socket with socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) as s : s . bind (( HOST , PORT )) import socket with socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) as s : s . connect (( HOST , PORT )) import socket with socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) as s : s . bind (( HOST , PORT )) import socket msg = \"Hello, world!\" with socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) as s : s . sendto ( msg . encode (), ( HOST , PORT )) Define port on which to listen for connections. serversocket . bind (( 'localhost' , 80 )) Connect to a remote socket in one direction client_socket . connect (( 'www.packtpub.com' , 80 )) Convert a domain name into IPv4 address socket . gethostbyname ( 'packtpub.com' ) # '83.166.169.231' Defaults to localhost with no arguments s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) s . bind (( socket . gethostname (), 1234 )) Get protocol name from port number socket . getservbyport ( 80 ) # 'http' Listen to a maximum of 10 connections serversocket . listen ( 10 ) Receive bytestream from server msg = s . recv ( 1024 ) print ( msg . decode ( 'utf-8' )) sqlite3 Create a Connect connection object and employee.db (binary) if it doesn't exist conn = sqlite . connect ( 'employee.db' ) Create a Connect.Cursor object c = conn . cursor () Perform SQL commands with Connect.Cursor.execute() . Create tablename with fields field of type type ( null , integer , real , text , blob ); never use Python's native string operations (f-strings, etc) to form commands, because this method is vulnerable to SQL injection. YouTube c . execute ( '''CREATE TABLE {tablename} ( {field} {type} , {field} {type} ...)) Save changes conn . commit () Close connection conn . close () subprocess subprocess modules allows you to spawn new processes, interact with file descriptors, and obtain exit codes. The recommended approach is to use the run() function as default, which runs a CLI command with options as a list of strings and returns a CompletedProcess instance.\\ Execute shell command Unlike os.system , subprocess.run() takes a list of arguments. subprocess . run ([ 'ls' , '-l,' . '], 0) Set capture_output to True to save output, stored as property stdout of the returned object. data = subprocess . run ([ 'ls,' - l ',' . '], 0, capture_output=True) The data is stored as a bytestring , which can be decoded to a normal string. data . stdout . decode ( 'utf-8' ) This return a CompletedProcess instance with the command's output stored under the stdout property subprocess . run ([ 'ls' , '-l' , '/dev/null' ], capture_output = True ) This will raise a CalledProcessError exception because of the non-zero exit code subprocess . run ( 'exit 1' , shell = True , check = True ) sys Return site-specific directory where Python files are installed sys . prefix # /usr/local/ by default tabulate termcolor Print text in a color code termcolor . cprint ( text , color ) threading Docs counter = 0 lock = threading . RLock () def func1 (): global counter while True : with lock : counter += 1 counter -= 1 def func2 (): global counter while True : with lock : counter += 1 counter -= 1 threading . Thrad ( target = func1 ) . start () threading . Thrad ( target = func2 ) . start () counter = 0 def func1 (): global counter while True : counter += 1 counter -= 1 def func2 (): global counter while True : counter += 1 counter -= 1 threading . Thrad ( target = func1 ) . start () threading . Thrad ( target = func2 ) . start () typing As tuples, their attributes are immutable class Starship ( NamedTuple ): name : str registry : str crew : int urllib Download an RFC file from rfc-editor.org Ortega rfc_raw = urllib . request . urlopen ( url ) . read () rfc = rfc_raw . decode () weakref Weak references are references to objects which return exceptions when that object has been garbage collected Create a weak reference to {object} # A weak reference created using `ref` must be dereferenced r = weakref . ref ( obj ) r () . method () r . method () # will not work # A weak reference created using `proxy` does not need to be dereferenced: weakref . proxy ( obj ) winrm Winrm allows you to connect Linux and Windows hosts over WinRM. adamtheautomator.com Begin a WinRM session. If no errors are thrown, the session has been successfully established session = winrm . Session ( ipaddress , auth = ( username , password )) yaml pip install pyyaml Deserialize Serialize import yaml with open ( './starships.yaml' ) as f : starships = yaml . safe_load ( f ) There is a load method but it requires specifying one of four possible values for the Loader kwarg. import yaml with open ( './starships.yaml' , 'w' ) as f : yaml . dump ( starships , f ) Resources Introduction to YAML xml books.xml <?xml version=\"1.0\"?> <catalog> <book id= \"bk101\" > <author> Gambardella, Matthew </author> <title> XML Developer's Guide </title> <genre> Computer </genre> <price> 44.95 </price> <publish_date> 2000-10-01 </publish_date> <description> An in-depth look at creating applications with XML. </description> </book> <book id= \"bk102\" > <author> Ralls, Kim </author> <title> Midnight Rain </title> <genre> Fantasy </genre> <price> 5.95 </price> <publish_date> 2000-12-16 </publish_date> <description> A former architect battles corporate zombies, an evil sorceress, and her own childhood to become queen of the world. </description> </book> <book id= \"bk103\" > <author> Corets, Eva </author> <title> Maeve Ascendant </title> <genre> Fantasy </genre> <price> 5.95 </price> <publish_date> 2000-11-17 </publish_date> <description> After the collapse of a nanotechnology society in England, the young survivors lay the foundation for a new society. </description> </book> <book id= \"bk104\" > <author> Corets, Eva </author> <title> Oberon's Legacy </title> <genre> Fantasy </genre> <price> 5.95 </price> <publish_date> 2001-03-10 </publish_date> <description> In post-apocalypse England, the mysterious agent known only as Oberon helps to create a new life for the inhabitants of London. Sequel to Maeve Ascendant. </description> </book> <book id= \"bk105\" > <author> Corets, Eva </author> <title> The Sundered Grail </title> <genre> Fantasy </genre> <price> 5.95 </price> <publish_date> 2001-09-10 </publish_date> <description> The two daughters of Maeve, half-sisters, battle one another for control of England. Sequel to Oberon's Legacy. </description> </book> <book id= \"bk106\" > <author> Randall, Cynthia </author> <title> Lover Birds </title> <genre> Romance </genre> <price> 4.95 </price> <publish_date> 2000-09-02 </publish_date> <description> When Carla meets Paul at an ornithology conference, tempers fly as feathers get ruffled. </description> </book> <book id= \"bk107\" > <author> Thurman, Paula </author> <title> Splish Splash </title> <genre> Romance </genre> <price> 4.95 </price> <publish_date> 2000-11-02 </publish_date> <description> A deep sea diver finds true love twenty thousand leagues beneath the sea. </description> </book> <book id= \"bk108\" > <author> Knorr, Stefan </author> <title> Creepy Crawlies </title> <genre> Horror </genre> <price> 4.95 </price> <publish_date> 2000-12-06 </publish_date> <description> An anthology of horror stories about roaches, centipedes, scorpions and other insects. </description> </book> <book id= \"bk109\" > <author> Kress, Peter </author> <title> Paradox Lost </title> <genre> Science Fiction </genre> <price> 6.95 </price> <publish_date> 2000-11-02 </publish_date> <description> After an inadvertant trip through a Heisenberg Uncertainty Device, James Salway discovers the problems of being quantum. </description> </book> <book id= \"bk110\" > <author> O'Brien, Tim </author> <title> Microsoft .NET: The Programming Bible </title> <genre> Computer </genre> <price> 36.95 </price> <publish_date> 2000-12-09 </publish_date> <description> Microsoft's .NET initiative is explored in detail in this deep programmer's reference. </description> </book> <book id= \"bk111\" > <author> O'Brien, Tim </author> <title> MSXML3: A Comprehensive Guide </title> <genre> Computer </genre> <price> 36.95 </price> <publish_date> 2000-12-01 </publish_date> <description> The Microsoft MSXML3 parser is covered in detail, with attention to XML DOM interfaces, XSLT processing, SAX and more. </description> </book> <book id= \"bk112\" > <author> Galos, Mike </author> <title> Visual Studio 7: A Comprehensive Guide </title> <genre> Computer </genre> <price> 49.95 </price> <publish_date> 2001-04-16 </publish_date> <description> Microsoft Visual Studio 7 is explored in depth, looking at how Visual Basic, Visual C++, C#, and ASP+ are integrated into a comprehensive development environment. </description> </book> </catalog> The etree submodule contains the ElementTree object which can open a string filename to deserialize XML data using parse() , which returns an ElementTree object, representing an XML document. A Python string can also be parsed with fromstring() , which actually returns an Element object. File String tree = xml . etree . ElementTree . parse ( 'books.xml' ) tree = xml . etree . ElementTree . fromstring ( books ) The getroot() method returns an Element object of the XML document's root node. root = tree . getroot () The parsed data can be displayed using the tostring() static method, providing an Element as argument. ElementTree . tostring ( root ) Children of an element can be filtered using findall() . This returns a list of Elements. books = root . findall ( 'book' ) Any Element object exposes an attrib property which returns a dictionary of attributes. [b.attrib for b in books] Attributes can be written to an Element using the set() method. root . set ( 'foo' , 'bar' ) Attributes can also be manipulated on the attrib property with normal Python dictionary operations. Setting Deleting root . attrib [ 'foo' ] = 'bar' del ( root . attrib [ 'hello' ]) Commit changes to disk. The argument can be a string representing the filename or a file object (in which case the file must be opened as a binary). Encoding can be specified (default is UTF-8 ) and a XML declaration can also be automatically generated. String File object tree . write ( 'books.xml' , encoding = 'UTF-16' , xml_declaration = True ) with open ( 'books.xml' , 'wb' ) as f : tree . write ( f ) Find elements by element name tree . findall ( 'book' ) Glossary Method resolution order Method resolution order (MRO) is the order of base classes that are searched when using super() . It is accessed with __mro__ , which returns a tuple of base classes in order of precedence, ending in object which is the root class of all classes. ( src ) Non-interactive debugging Non-interactive debugging is the most basic form of debugging, dependent on print or log statements placed within the body of code. Type slot A type slot is any of a number of fields within each magic method, including __new__() , __init__() , and __prepare__() (which returns a dictionary-like object that's used as the local namespace for all code from the class body)","title":"\ud83d\udc0d Python"},{"location":"Coding/Python/#python","text":"","title":"\ud83d\udc0d Python"},{"location":"Coding/Python/#decorators","text":"Sources: Primer on Python Decorators YouTube tutorial A decorator is any function that accepts a function and returns a function. Decorators are one of the main ways that Python implements functional programming principles. Functions are first-class objects and can be passed as parameters. import logging def hello_wrapper ( name , func ): func ( f 'Hello { name } ' ) hello_wrapper ( \"world\" , func = print ) # Hello world hello_wrapper ( \"logs\" , func = logging . warning ) # WARNING:root:Hello logs with open ( 'hello.txt' , 'w' ) as f : hello_wrapper ( 'everyone!' , func = f . write ) import random def anagram ( t ): l = [ c for c in t ] random . shuffle ( l ) print ( \"\" . join ( l )) hello_wrapper ( 'Japushku' , anagram ) # eHoulhluaskpJ The function has to be passed as a reference, actually calling it will cause the wrapper function to attempt to execute the value returned by the inner function. hello_wrapper ( \"world\" , func = print ()) # Error def outer (): print ( 'Hi from the outer function' ) def inner (): print ( 'Hello from the inner function' ) inner () We can use the __name__ attribute to access a passed function's name. def hello ( func ): print ( f 'Hello { func . __name__ } ' ) hello ( outer ) # Hello outer We can also return functions, which can then be invoked def hello ( func ): print ( f 'Hello { func . __name__ } ' ) return func hello ( outer )() ''' Hi from the outer function Hello from the inner function ''' new_outer = hello ( outer ) new_outer is outer # True def wrapper ( func ): print ( f 'Before { func . __name__ } ' ) func () print ( f 'After { func . __name__ } ' ) wrapper ( outer ) ''' Before outer Hi from the outer function Hello from the inner function After outer ''' The true decorator pattern appears here, where wrapper is called the decorator and outer has been decorated . def wrapper ( func ): def _wrapper (): print ( f 'Before { func . __name__ } ' ) func () print ( f 'After { func . __name__ } ' ) return _wrapper outer = wrapper ( outer ) But the usual syntax since Python 2.4 is to place the decorator on the line above the decorated function, preceded by @ : @wrapper def outer (): print ( 'Hi from the outer function' ) def inner (): print ( 'Hello from the inner function' ) inner () _wrapper here does not accept any positional arguments, so wrapping functions that take arguments will produce a TypeError @wrapper def say_hello ( name ): print ( f 'Hello { name } !' ) # error The solution is to incorporate *args, **kwargs into the definition of the inner function, as well as the invocation of the function passed in. def wrapper ( func ): def _wrapper ( * args , ** kwargs ): print ( f 'Before { func . __name__ } ' ) func ( * args , ** kwargs ) print ( f 'After { func . __name__ } ' ) return _wrapper Returned values are not captured yet: def wrapper ( func ): def _wrapper ( * args , ** kwargs ): print ( f 'Before { func . __name__ } ' ) value = func ( * args , ** kwargs ) print ( f 'After { func . __name__ } ' ) return value return _wrapper Inspecting the decorated function's __name__ attribute reveals that it is still named _wrapper say_hello . __name__ # '_wrapper' This is also true for other attributes, including docstring. functools.wraps is a decorator factory to reassign attributes to the wrapped function. This is considered superior to the functools.update_wrapper function which is also available. def wrapper ( func ): @functools . wraps ( func ) def _wrapper ( * args , ** kwargs ): print ( f 'Before { func . __name__ } ' ) value = func ( * args , ** kwargs ) print ( f 'After { func . __name__ } ' ) return value return _wrapper This forms an ideal starting template for the creation of custom decorators.","title":"Decorators"},{"location":"Coding/Python/#classes","text":"","title":"Classes"},{"location":"Coding/Python/#properties","text":"In the Python documentation , attributes accessed with accessor functions are called managed attributes , which makes the term equivalent to properties in C#. Three methods can be defined using the @property decorator Constructor Getter Setter Deleter def __init__ ( self , price ): self . _price = price @property def price ( self ): return self . _price @price . setter def price ( self , new_price ): if new_price > 0 : self . _price = new_price else : raise ValueError @price . deleter def price ( self ): del self . _price","title":"Properties"},{"location":"Coding/Python/#class-methods","text":"The @classmethod decorator prevents the interpreter from passing in the instantiated object using self , rather the class itself is passed in as the cls argument. This means that the methods decorated as such must take not self as the first argument but cls @classmethod def classmethod ( cls ): pass The @staticmethod decorator prevents the interpreter from passing any additional arguments whatsoever. The resulting method has no access to the object itself nor the class and functions like a procedurally defined function.","title":"Class methods"},{"location":"Coding/Python/#formatting","text":"flake8 , black , and yapf (Google) are CLI tools used to automatically format Python code.","title":"Formatting"},{"location":"Coding/Python/#web-frameworks","text":"","title":"Web frameworks"},{"location":"Coding/Python/#django","text":"A typical Django project contains multiple apps , which are Python packages containing their own models, views, templates, and urls. A model is the single definitive source of information about your data, and contains the essential fields and behaviors of the data you're storing. Migrations are necessary when Model classes are updated. And for projects sufficiently advanced, migration scripts must be developed for any such changes. Async Server Gateway Interface (ASGI) is the spiritual successor to, and superset of, WSGI . It implements the new Python standard for asynchronous web servers and applications, which resembles that of websockets . From WSGI to ASGI WSGI is coupled tightly with the synchronous request-response model familiar from HTTP 1.1. URL patterns (stored in the urlpatterns list defined in the project-level urls.py file) can be parameterized. Here, the template <int:x> specifies an integer to be assigned to the view parameter x . from app.views import my_view urlpatterns = [ path ( '/example/<int:x>' , my_view ) ] modelform_factory can be used to automatically produce a webform from a Model class. # views.py MeetingForm = modelform_factory ( Meeting , exclude = []) This can be placed into a template using the {{ form }} template tag. Note, a {% csrf_token %} template tag must also be present for a submit button to work. { % block content % } < h1 > Plan a new meeting </ h1 > < form method = \"POST\" > < table > {{ form }} </ table > { % csrf_token % } < button type = \"submit\" > Create </ button > </ form > { % endblock content % } When the modelform_factory class has been defined, it is instantiated within the view function . This object exposes several methods: - is_valid data validation is strongly recommended for any form input - save imports the validated form data into the database def new ( request ): if request . method == 'POST' : form = MeetingForm ( request . POST ) if form . is_valid (): form . save () return redirect ( \"home\" ) else : form = MeetingForm () return render ( request , \"meetings/new.html\" , { \"form\" : form })","title":"Django"},{"location":"Coding/Python/#template","text":"Django templates are HTML files with additional markup to signify places where data can be dynamically inserted. The data used by the views file is called the template context . Templates must be placed within the /templates folder within the app, and it is considered best practice to place templates within a nested subdirectory within it, e.g. /templates/app . Django template tags are specified beween {% .. %} and allow for interpolation of data. < ul > { % for m in meetings % } { % endfor % } </ ul > URLs can be built by using the url template tag, specifying the name of a URL urlpatterns = [ path ( '' , home , name = 'home' ) ] < a href = \"{ % u rl 'home' %}\" > Home </ a >","title":"Template"},{"location":"Coding/Python/#models","text":"Models In Django, a Model class is mapped to a database table. Each object is a record in that table. Model objects expose several attributes and methods Get all objects meetings = Meeting . objects . all () Get count of objects in database count = Meeting . objects . count () Query for a specific object meeting = Meeting . objects . get ( pk = id ) get_object_or_404 may be better for most cases meeting = get_object_or_404 ( Meeting , pk = id ) Adding a new app python manage.py startapp app Then add to settings.py in project directory INSTALLED_APPS = [ # ... 'app' , ] There appears to be much flexibility in the arrangement of input controls in a form. So long as the Submit button is child to the form element, tasks are accepted in the To-Do app. Per Bulma documentation, the field class is intended as a container for label.label s, .control s, and optional p.help text. In contrast, control is a block container meant to enhance single form controls and can only contain input , select , button , or icon elements. form .field ( method= \"POST\" , action= \"/\" ) label .label Enter something to do .control | {{form.title}} | {% csrf_token %} button .button.is-primary ( type= \"submit\" ) Submit","title":"Models"},{"location":"Coding/Python/#fastapi","text":"Variables values can be taken from the route or from query parameters following a question mark. Routes Query parameters from fastapi import FastAPI starships = FastAPI () @starships . get ( \"/starships/ {registry} \" ) def get_starship ( name : str ): return { \"response\" : f \"Hello, { name } \" } from fastapi import FastAPI starships = FastAPI () @starships . get ( \"/\" ) def get_starship ( name : str = \"world\" ): return { \"response\" : f \"Hello, { name } \" } FastAPI is notable for being able to use type hints to construct data models, which are much lighter than the object relational models used by other frameworks. FastAPI Django from pydantic import BaseModel class Starship ( BaseModel ): name : str registry : str crew : int from django.db import models class Starship ( models . Model ): name = models . CharField ( max_length = 50 ) registry = models . CharField ( max_length = 15 ) crew = models . IntegerField () Dogfood data can be incorporated by using the keyword argument unpacking or \"double splat\" operator ( ** ) data = { \"name\" : \"USS Enterprise\" , \"registry\" : \"NCC-1701\" , \"crew\" : 203 } enterprise = Starship ( ** data ) POST method definitions then can use this newly defined class to validate posted data db = [] @app . post ( \"/starships\" ) async def create_starship ( starship : Starship ): db . append ( starship ) FastAPI supports Jinja templates to serve HTML templates import fastapi from fastapi.templating import Jinja2Templates # specifies the directory where templates are to be found templates = Jinja2Templates ( \"templates\" ) api = fastapi . APIRouter () @api . get ( '/' ) def index ( request : starlette . requests . Request ): return templates . TemplateResponse ( \"helloworld.html\" , { \"request\" : request }) By default, FastAPI also exposes web applications at /docs where you can test out all the exposed API methods. FastAPI integrates with ASGI servers like Uvicorn and Hypercorn, which can run a specific web application by name from the command-line or from within the script: Shell Python uvicorn main:starships --port 7000 import uvicorn uvicorn . run ( starships , port = 7000 )","title":"FastAPI"},{"location":"Coding/Python/#virtual-environments","text":"","title":"Virtual environments"},{"location":"Coding/Python/#pipenv","text":"pipenv --python 3 .6","title":"pipenv"},{"location":"Coding/Python/#venv","text":"Create a virtual environment named project python -m venv project","title":"venv"},{"location":"Coding/Python/#virtualenv","text":"Create a virtual environment named project using a different version of Python virtualenv -p /usr/bin/python2 project","title":"virtualenv"},{"location":"Coding/Python/#testing","text":"Pytest is a popular testing framework preferred to unittest by many Python developers because it follows Pythonic conventions more closely. In contrast to unittest's custom methods, pytest relies on the builtin assert statement. pytest unittest Class under test from phonebook import PhoneBook import pytest @pytest . fixture def phonebook (): phonebook = PhoneBook () yield phonebook phonebook . clear () def test_lookup_by_name ( phonebook ): phonebook . add ( \"Bob\" , \"1234\" ) assert \"1234\" == phonebook . lookup ( \"Bob\" ) def test_phonebook_contains_all_names ( phonebook ): phonebook . add ( \"Bob\" , \"1234\" ) assert \"Bob\" in phonebook . names () def test_missing_name_raises_error ( phonebook ): with pytest . raises ( KeyError ): phonebook . lookup ( \"Bob\" ) python -m pytest import unittest from phonebook import PhoneBook class PhoneBookTest ( unittest . TestCase ): def test_lookup_by_name ( self ): self . phonebook . add ( \"Bob\" , \"12345\" ) number = self . phonebook . lookup ( \"Bob\" ) self . assertEqual ( \"12345\" , number ) def test_missing_name ( self ): with self . assertRaises ( KeyError ): self . phonebook . lookup ( \"missing\" ) def test_empty_phonebook_is_consistent ( self ): self . assertTrue ( self . phonebook . is_consistent ()) def setUp ( self ) -> None : self . phonebook = PhoneBook () def tearDown ( self ) -> None : self . phonebook . clear () python -m unittest import os class PhoneBook : def __init__ ( self , cache_directory = os . getcwd ()): self . numbers = {} self . filename = os . path . join ( cache_directory , \"phonebook.txt\" ) self . cache = open ( self . filename , \"w\" ) def add ( self , name , number ): self . numbers [ name ] = number def lookup ( self , name ): return self . numbers [ name ] def is_consistent ( self ): return True def names ( self ): return set ( self . numbers . keys ()) def clear ( self ): self . cache . close () os . remove ( self . filename )","title":"Testing"},{"location":"Coding/Python/#doctest","text":"A doctest is a docstring containing what looks like interactive Python sessions. Python Docs \"\"\" Return the factorial of n, an exact integer >= 0. >>> [factorial(n) for n in range(6)] [1, 1, 2, 6, 24, 120] >>> factorial(30) 265252859812191058636308480000000 >>> factorial(-1) Traceback (most recent call last): ... ValueError: n must be >= 0 Factorials of floats are OK, but the float must be an exact integer: >>> factorial(30.1) Traceback (most recent call last): ... ValueError: n must be exact integer >>> factorial(30.0) 265252859812191058636308480000000 It must also not be ridiculously large: >>> factorial(1e100) Traceback (most recent call last): ... OverflowError: n too large \"\"\" This can then be run if __name__ == '__main__' : import doctest doctest . testmod ()","title":"Doctest"},{"location":"Coding/Python/#pytest","text":"PyTest relies on the built-in assert statement.","title":"pytest"},{"location":"Coding/Python/#fixtures","text":"The @pytest.fixture decorator facilitiates the creation of test fixtures. The fixture function's name is used as argument to the test case, and the value returned can be used by the logic within. ( src ) Any clean-up logic can be invoked in this fixture as well by replacing return with yield . Pytest also provides its own tmpdir test fixture for temporary directories. ( src ) Before After tmpdir from phonebook import PhoneBook import pytest def test_lookup_by_name ( phonebook ): phonebook = PhoneBook () phonebook . add ( \"Bob\" , \"1234\" ) assert \"1234\" == phonebook . lookup ( \"Bob\" ) def test_phonebook_contains_all_names ( phonebook ): phonebook = PhoneBook () phonebook . add ( \"Bob\" , \"1234\" ) assert \"Bob\" in phonebook . names () def test_missing_name_raises_error ( phonebook ): phonebook = PhoneBook () with pytest . raises ( KeyError ): phonebook . lookup ( \"Bob\" ) from phonebook import PhoneBook import pytest @pytest . fixture def phonebook (): phonebook = PhoneBook () yield phonebook phonebook . clear () def test_lookup_by_name ( phonebook ): # phonebook = PhoneBook() phonebook . add ( \"Bob\" , \"1234\" ) assert \"1234\" == phonebook . lookup ( \"Bob\" ) def test_phonebook_contains_all_names ( phonebook ): # phonebook = PhoneBook() phonebook . add ( \"Bob\" , \"1234\" ) assert \"Bob\" in phonebook . names () def test_missing_name_raises_error ( phonebook ): # phonebook = PhoneBook() with pytest . raises ( KeyError ): phonebook . lookup ( \"Bob\" ) from phonebook import PhoneBook import pytest @pytest . fixture def phonebook ( tmpdir ): phonebook = PhoneBook ( tmpdir ) return phonebook def test_lookup_by_name ( phonebook ): # phonebook = PhoneBook() phonebook . add ( \"Bob\" , \"1234\" ) assert \"1234\" == phonebook . lookup ( \"Bob\" ) def test_phonebook_contains_all_names ( phonebook ): # phonebook = PhoneBook() phonebook . add ( \"Bob\" , \"1234\" ) assert \"Bob\" in phonebook . names () def test_missing_name_raises_error ( phonebook ): # phonebook = PhoneBook() with pytest . raises ( KeyError ): phonebook . lookup ( \"Bob\" )","title":"Fixtures"},{"location":"Coding/Python/#unittest","text":"unittest is a testing framework built into Python's Standard Library that was based on JUnit. unittest came out in 2001, when JUnit was being ported and adapted to many languages. Collectively, these frameworks were referred to as the xUnit family . unittest's method names do not follow Python conventions because it predates the PEP-8 naming standard. unittest allows you to create test classes that inherit from TestCase .","title":"unittest"},{"location":"Coding/Python/#assertions","text":"Assertions are implemented in individual methods of the TestCase subclass through unittest methods like assertEqual and assertRaises , etc. Notably, TestCase subclasses must not have an __init__() constructor method defined. def test_lookup_by_name ( self ): phonebook = PhoneBook () phonebook . add ( \"Bob\" , \"12345\" ) number = phonebook . lookup ( \"Bob\" ) self . assertEqual ( \"12345\" , number ) assertRaises must be placed in a context manager. Here, the test case will run the code within the with block and check to make sure it raises the specified exception: KeyError : ( src ) def test_missing_name ( self ): fleet = Fleet () with self . assertRaises ( KeyError ): fleet . lookup ( \"bla\" )","title":"Assertions"},{"location":"Coding/Python/#fixtures_1","text":"setUp is run before every test method, allowing a test fixture to be created to avoid repetitive code. tearDown is called after every method, which allows these resources to be released, even if the test case raises an exception. However, if it is setUp that raises the exception, then neither the test case nor tearDown will run. ( src , src ) Before After import unittest from phonebook import PhoneBook class PhoneBookTest ( unittest . TestCase ): def test_lookup_by_name ( self ): phonebook = PhoneBook () phonebook . add ( \"Bob\" , \"12345\" ) number = phonebook . lookup ( \"Bob\" ) self . assertEqual ( \"12345\" , number ) def test_missing_name ( self ): phonebook = PhoneBook () with self . assertRaises ( KeyError ): phonebook . lookup ( \"missing\" ) @unittest . skip ( \"WIP\" ) def test_empty_phonebook_is_consistent ( self ): phonebook = PhoneBook () self . assertTrue ( phonebook . is_consistent ()) import unittest from phonebook import PhoneBook class PhoneBookTest ( unittest . TestCase ): def setUp ( self ) -> None : self . phonebook = PhoneBook () def tearDown ( self ) -> None : self . phonebook . clear () def test_lookup_by_name ( self ): # phonebook = PhoneBook() self . phonebook . add ( \"Bob\" , \"12345\" ) number = self . phonebook . lookup ( \"Bob\" ) self . assertEqual ( \"12345\" , number ) def test_missing_name ( self ): # phonebook = PhoneBook() with self . assertRaises ( KeyError ): self . phonebook . lookup ( \"missing\" ) @unittest . skip ( \"WIP\" ) def test_empty_phonebook_is_consistent ( self ): # phonebook = PhoneBook() self . assertTrue ( self . phonebook . is_consistent ()) The @unittest.skip decorator will tell the test runner to skip the decorated test case ( src ) @unittest . skip ( \"WIP\" ) def test_empty_phonebook_is_consistent ( self ): phonebook = PhoneBook () self . assertTrue ( phonebook . is_consistent ()) The command line entry point is made with a call to unittest.main() , which executes the tests. ( src ) import unittest from my_sum import sum class TestSum ( unittest . TestCase ): def test_list_int ( self ): \"\"\" Test that it can sum a list of integers \"\"\" data = [ 1 , 2 , 3 ] result = sum ( data ) self . assertEqual ( result , 6 ) if __name__ == '__main__' : unittest . main ()","title":"Fixtures"},{"location":"Coding/Python/#integration-tests","text":"By convention, tests are put in their own directory as sibling to the main module ( in order to be able to import it ). Integration and unit tests should be organized separately. . \u251c\u2500\u2500 project \u2502 \u2514\u2500\u2500 __init__.py \u2514\u2500\u2500 tests \u251c\u2500\u2500 integration \u2514\u2500\u2500 unit Run all integration tests within specified directory. python -m unittest discover -s tests/integration","title":"Integration tests"},{"location":"Coding/Python/#tasks","text":"Deserialize YAML JSON import yaml with open ( './starships.yaml' ) as f : starships = yaml . safe_load ( f ) import json with open ( './starships.json' ) as f : data = json . load ( f ) Serialize YAML JSON import yaml with open ( './starships.yaml' , 'w' ) as f : yaml . dump ( starships , f ) import json with open ( '/starships.json' , \"w\" ) as f : json . dump ( data , f )","title":"Tasks"},{"location":"Coding/Python/#modules","text":"When learning unfamiliar packages and importing them in a demonstration script, care must be taken that the demonstration script does not have the same name as the package being studied. If so, attempting to import the package while in an interpreter within that directory will cause the interpreter to try importing the incomplete script and not the package. When running a Python interpreter within this directory, the files \"calc\" and \"main\" can be imported as modules by specifying their names with no file extension. . \u251c\u2500\u2500 calc.py \u2514\u2500\u2500 main.py import calc # No errors import main # No errors Specifying the full filename including extension does produce an error import calc.py # Error import main.py # Error","title":"Modules"},{"location":"Coding/Python/#argparse","text":"The ArgumentParser object exposes an attribute that contains the value passed in from the command-line. This attribute takes its identifier from the dest keyword argument when invoking the add_argument() method. Python Output import argparse def get_args (): parser = argparse . ArgumentParser () parser . add_argument ( dest = 'bar' ) return parser . parse_args () def main (): args = get_args () . bar The optional value assigned to description will be displayed when running the script with the options -h or --help parser = argparse . ArgumentParser ( description = helptext ) usage: argparse_practice.py [-h] [-f bar] optional arguments: -h, --help show this help message and exit -f bar, --foo bar A help string can be provided as a keyword argument to help . parser . add_argument ( \"foo\" , help = \"bar\" ) A data type can be specified as an argument to type : parser . add_argument ( \"foo\" , type = int ) An alternative name for the dest value on the command-line (but which does not affect the identifier of the attribute on which the value is exposed) can be specified by metavar . parser . add_argument ( \"foo\" , metavar = \"bar\" ) The examples above used positional parameters (i.e. an argument). A named parameter (an option or flag, i.e. -h , --help , etc) requires - at the beginning of the string and values from the command-line to be passed after = or Space . add_argument supports the required keyword argument for named parameters. Note that use of the option on the command-line at all requires an argument to it, even if the option is not required itself. parser . add_argument ( '-r' , '--radius' , type = int , required = True , help = 'radius' ) A flag option can be created by defining an action keyword parameter. ( src ) parser . add_argument ( '-o' , '--on' , help = 'A boolean flag' , action = 'store_true' ) add_mutually_exclusive_group() can be used to add a group of mutually exclusive arguments. In this case, add_argument() is invoked on the new object returned by this method and not directly on the ArgumentParser() object. g = ArgumentParser . add_mutually_exclusive_group () g . add_argument ( \"-v\" , \"--verbose\" , action = \"store_true\" ) g . add_argument ( \"-q\" , \"--quiet\" , \"-s\" , \"--silent\" , action = \"store_true\" , help = 'quiet/silent mode' ) User input can be restricted by providing a value for choices , which will accept any iterable value including lists, ranges, and strings: parser . add_argument ( \"foo\" , choices = [ \"bar\" , \"baz\" ]) parser . add_argument ( \"foo\" , choices = range ( 1 , 10 )) parser . add_argument ( \"foo\" , choices = 'Hello, world!' ) # equivalent to ['H','e', ...] Sources Python documentation","title":"argparse"},{"location":"Coding/Python/#asyncio","text":"The asyncio module offers an implementation of coroutines which allow tasks to control context switching to implement concurrency . The await keyword is a checkpoint that indicates where it is safe for the process to go to another coroutine, allowing total control over context switching import asyncio import time counter = 0 async def func1 (): global counter while True : counter += 1 counter -= 1 await asyncio . sleep ( 0 ) async def func2 (): global counter while True : counter += 1 counter -= 1 await asyncio . sleep ( 0 ) asyncio . gather ( func1 (), func2 ()) asyncio . get_event_loop () . run_forever () async def get_users (): users = await client . do_query ( 'select * from users' ) return users async def main (): task = asyncio . create_task ( get_users ()) # ... await task asyncio . run ( main ()) Allows the joining of multiple threads. async def get_users (): users = await client . do_query ( 'select * from users' ) return users async def main (): await asyncio . gather ( get_users (), get_users (), ) asyncio . run ( main ()) async def get_users (): users = await client . do_query ( 'select * from users' ) return users asyncio . run ( get_users ()) async def main (): users = await get_users () print ( users ) asyncio . run ( main ()) Sources: Demistifying Python's Async and Await keywords","title":"asyncio"},{"location":"Coding/Python/#azurecosmos","text":"import azure.cosmos from azure.cosmos.partition_key import PartitionKey database = cosmos_client . create_database ( 'RetailDemo' ) container = database . create_container ( id = 'WebsiteData' , partition_key = PartitionKey ( path = '/CartID' )) print ( 'Container WebsiteData created' )","title":"azure.cosmos  "},{"location":"Coding/Python/#bullet","text":"bullet.Check() implements a checkbox widget: cli = bullet . Check ( prompt = \"Choose from the following items: \" , choices = [ 'pepperoni' , 'sausage' , 'green peppers' ]) bullet.Bullet() implements a radio button widget: cli = bullet . Bullet ( prompt = \"Choose from the following items: \" , choices = [ 'red' , 'white' , 'blue' ]) The resulting object then exposes a launch() method. cli . launch ()","title":"bullet"},{"location":"Coding/Python/#click","text":"Click modifies functions using decorators whch determine the command-line arguments elements that the decorated function can see. Hello World program. Click import click @click . command () def hello (): click . echo ( 'Hello World!' ) if __name__ = '__main__' : hello () Modified Hello World import click @click . command () @click . option ( '--count' , default = 1 , help = 'number of greetings' ) @click . argument ( 'name' ) def hello ( count , name ): for x in range ( count ): click . echo ( 'Hello %s !' % name ) if __name__ == '__main__' : hello () Developing the pdfcropper tool; passing --examref changes the numbers. import click @click . command () @click . option ( '--examref' , is_flag = True ) def hello ( examref ): top , right , bottom , left = 0 , 0 , 0 , 0 if examref : top , right , bottom , left = 1 , 2 , 3 , 4 click . echo ( f 'Your numbers are: top ( { top } ), right { right } , bottom { bottom } , left { left } ' ) if __name__ == '__main__' : hello () @click.group() decorators allow nested command groups to be created. There are two ways of adding commands to command groups : - Using the group as a decorator, whereby the name of the function decorated by @click.group() is then used to decorate commands: @click . group () def group1 () pass @group1 . command () def command1 (): pass - Using the add_command method @click . group () def group1 () pass @click . command () def command1 (): pass group1 . add_command ( command1 ) For example, to imitate the nested commands available in netsh : netsh interface ip @click . group () def interface (): pass @interface . command ( 'ip' ) @click . argument ( 'args' , nargs =- 1 ) # All arguments passed in as tuple \"args\" def interface_ip ( args ): pass Docstrings of groups and commands show up as progressive help messages when they are invoked from the command-line. @click . group () def cli (): pass @click . command () def initdb (): click . echo ( 'Initialized the database' ) @click . command () def dropdb (): click . echo ( 'Dropped the database' ) cli . add_command ( initdb ) cli . add_command ( dropdb ) if __name__ == '__main__' : cli () CommandCollection flattens the structure of grouped commands so that the commands in all the contained groups appear in a single tier. It also becomes the entry-point of the script. Example from GitHub : # Three command groups cli1, cli2, and cli3 declared: @click . group () def cli1 (): pass @click . group () def cli2 (): pass @click . group () def cli3 (): pass # Three commands each belonging to a separate group @cli1 . command () def server (): pass @cli2 . command () def console (): pass @cli3 . command () def routes (): pass # CommandCollection flattens the grouped commands such that all the commands are available at once: cli = click . CommandCollection ( sources = [ cli1 , cli2 , cli3 ]) if __name__ == '__main__' : cli ()","title":"click"},{"location":"Coding/Python/#collections","text":"abc provides Mapping and MutableMapping ABCs to formalize the interfaces of dict and similar types ChainMap Lookups are performed on each mapping in order Counter Holds an integer count for each key; each new key adds to the count deque : Thread-safe double-ended queue that supports most list methods namedtuple Card = namedtuple ( 'Card' ,[ 'rank' , 'suit' ]) ` City = namedtuple ( 'City' , 'Name Country Population Coordinates' . split ( ' ' )] OrderedDict: Maintains keys in insertion order UserDict: Designed to be subclassed","title":"collections"},{"location":"Coding/Python/#colorama","text":"Colorama provides a set of enums that resolve to terminal codes when concatenated with strings. colorama . Fore . GREEN colorama . Style . RESET_ALL","title":"colorama"},{"location":"Coding/Python/#csv","text":"with open ( 'file.csv' , newline = '' ): data = [ row for row in csv . reader ( f )] csv.DictReader with open ( 'greeks.csv' ) as f : reader = csv . DictReader ( f ) for row in reader : print ( row [ 'name' ], row [ 'city' ], row [ 'dob' ])","title":"csv"},{"location":"Coding/Python/#datetime","text":"datetime . date ( 2016 , 7 , 24 ) datetime . date . today () Difference between datetime objects is a timedelta Parse strings into datetime objects datetime . strptime ( datestring , formatstring ) # Various metacharacters are defined for `strptime` datetime . datetime . strptime ( '06/30/1992' , '%m/ %d /%Y' )","title":"datetime"},{"location":"Coding/Python/#discordpy","text":"pip install discord.py client = discord.Client () Client objects expose a decorator that is used for event handlers, functions named after various events: - on_ready - on_member_join - on_error - on_message @client . event async def on_ready (): print ( f ' { client . user } has connected to Discord!' ) Another decorator is exposed for in-chat commands ( commands.Bot has to be instantiated first.) @bot . command ( name = 'roll_dice' , help = 'Simulates rolling dice.' ) async def roll ( ctx , number_of_dice : int , number_of_sides : int ): dice = [ str ( random . choice ( range ( 1 , number_of_sides + 1 ))) for _ in range ( number_of_dice ) ] await ctx . send ( ', ' . join ( dice )) client . run ( token ) bot = comands . Bot ( command_prefix = '!' )","title":"discord.py"},{"location":"Coding/Python/#dotenv","text":"pip install -U python-dotenv Load a .env file placed in the current working directory. load_dotenv () value = os . getenv ( 'key' )","title":"dotenv"},{"location":"Coding/Python/#functools","text":"For higher-order functions (functions that act on or return other functions)\\ Apply function of two arguments cumulatively to the items of iterable in order to reduce it to a single value functools . reduce ( function , iterable [, initializer ]) Calculate ((((1+2) +3) +4) +5) functools . reduce ( lambda x , y : x + y , [ 1 , 2 , 3 , 4 , 5 ]) functools.reduce(lambda a,b: a*b, range(1,6)) => 120 : factorial","title":"functools"},{"location":"Coding/Python/#glob","text":"Produce a list of strings glob . glob ( '*.py' )","title":"glob"},{"location":"Coding/Python/#heapq","text":"Support heaps , data objects where each node is either greater than or equal to its parent ( max-heap ) or less than or equal to its parent ( min-heap ) Create a heap from {iterable} heapq . heapify ( iterable ) Remove and return the smallest element of {heap} heapq . heappop ( heap ) Replace the smallest element of {heap} with {element} heapq . heapreplace ( heap , element )","title":"heapq"},{"location":"Coding/Python/#http","text":"Start an HTTP server for the current directory python http.server","title":"http"},{"location":"Coding/Python/#itertools","text":"cycle() works like next() , but it restarts from the beginning of the iterable that is passed as argument after the last element has been reached. with open ( 'raven' ) as f : raven = [ l for l in f ] itertools . cycle ( raven )","title":"itertools"},{"location":"Coding/Python/#json","text":"Deserialize Serialize import json with open ( 'starships.json' ) as f : data = json . load ( f ) import json with open ( 'starships.json' , \"w\" ) as f : json . dump ( data , f )","title":" json"},{"location":"Coding/Python/#logging","text":"import logging def main (): logging . basicConfig ( filename = '/tmp/learn-logging.log' , level = logging . ERROR , format = ' %(asctime)s %(levelname)s : %(message)s ' ) logging . info ( \"Once upon a midnight dreary,\" ) logging . warning ( 'While I pondered weak and weary,' ) logging . error ( 'Over many a quaint and curious volume of forgotten lore,' ) if __name__ == '__main__' : main ()","title":"logging"},{"location":"Coding/Python/#npyscreen","text":"Widget library and application framework built on top of ncurses . Documentation ] Three main types of object compose npyscreen applications: - Application objects manage forms and other classes - Form objects form the canvas upon which widgets are arrayed - Form general-purpose - FormMutt - Widget objects are individual controls - TitleText text entry - TitleSelectOne equivalent to radio buttons - TitleDateCombo allows picking of date on a small calendar npyscreen.wrapper_basic is the main entry point import npyscreen def myFunction ( * args ): pass if __name__ == '__main__' : npyscreen . wrapper_basic ( myFunction ) print \"Blink and you missed it!\" npyscreen.Form is equivalent to the Tk() object, which is typically instantiated as win in GUI frameworks. F = npyscreen . Form ( name = 'My Test Application' ) Several important methods are key: - create() The standard constructor calls this method, which does nothing by default and is meant to be overriden in subclasses. Widgets are defined here. npyscreen.FormMutt imitates a UI layout popularized by applications like mutt , irssi , and vim , with a title bar at the top, a command line at the bottom, and a status line directly above the command line. ACTION_CONTROLLER can be defined in the FormMutt subclass as the name of a subclass of ActionControllerSimple . Commands for the application can be defined as callbacks in the create() method. self . add_action ( ident , call_back , True ) Callbacks are called with the following arguments: call_back ( command_line , control_widget_proxy , live = True ) class ActionControllerSearch ( npyscreen . ActionControllerSimple ): def create ( self ): self . add_action ( '^/.*' , self . set_search , True ) def set_search ( self , command_line , widget_proxy , live ): self . parent . value . set_filter ( command_line [ 1 :]) self . parent . wMain . values = self . parent . value . get () self . parent . wMain . display () class FmSearchActive ( npyscreen . FormMuttActiveTraditional ): ACTION_CONTROLLER = ActionControllerSearch npyscreen.NPSAppManaged is the preferred superclass to support object-oriented implementation. class MyApplication ( npyscreen . NPSAppManaged ): pass Calling run() method of application object as main entry point. run() activates the default form, which should be given an id of MAIN if __name__ == '__main__' : TestApp = MyApplication () . run () print \"All objects, baby.\" Using a try / except block to allow for well-mannered exit in case of KeyboardInterrupt (Ctrl+C) GitHub try : App () . run () except KeyboardInterrupt : sys . exit ( 0 ) There are three methods for registering a Form object with a NPSAppManaged instance; - addForm() creates a new form and returns a weakref.proxy to it - addFormClass() register a class of Form rather than an instance - registerForm() It continually displays the Form named by its NEXT_ACTIVE_FORM attribute. Use the afterEditing method to allow exiting. class myEmployeeForm ( npyscreen . Form ): def afterEditing ( self ): self . parentApp . setNextForm ( None )","title":"npyscreen"},{"location":"Coding/Python/#numpy","text":"numpy.ndarray - 2-dimensional array - items can be fetched using the syntax a[i, j] - arrays can be sliced with syntax a[m:n, k:l] - FP:35 numpy.arange(n) build a numpy.ndarray object with numbers 0 to n-1 (FP:52) numpy.loadtxt(filename) load numbers stored in a text file (FP:53)","title":"numpy"},{"location":"Coding/Python/#optparse","text":"Instantiate the parser object parser = optparse . OptionParser ( usage = __doc__ . strip ()) # add an option parser . add_option ( '--timeout' )","title":"optparse"},{"location":"Coding/Python/#os","text":"Execute shell command given by string. The value returned is actually the exit code, not the output of the command to STDOUT. os . system ( 'ls -la' ) Store output in a variable os . popen ( 'ls -la' ) . read () Navigate filesystem os . getcwd () os . chdir ( path ) Test for existence of a file os . path . isfile ( file )","title":"os"},{"location":"Coding/Python/#pandas","text":"summary: open-source Python library used for data science operation: runs over NumPy - good for storing lists-of-lists (CSV) print(df) - prints it out in an easy to read tabular format DataFrame is the main object in pandas - head() , tail() - prints out the first, last several rows (5 by default) - optional numerical argument defines number of rows - describe() - numerical analyses, including count, unique, mean, etc - sort_values('field',ascending=False)","title":"pandas"},{"location":"Coding/Python/#pathlib","text":"Create a new pathlib object; represents a file or directory pathlib . Path ( path ) Test for existence of a file pathlib . Path . is_file ( file ) Test for existence of a directory pathlib . Path . is_dir ( dir ) Find all .py files Returns a generator pathlib . Path . glob ( '*.py' ) Open a file. This makes a file object that is automatically closed, similar to open builtin: pathlib . Path . open () Display file extension pathlib . Path . suffix () Display file size pathlib . Path . stat () . st_size","title":"pathlib"},{"location":"Coding/Python/#pyinstaller","text":"Source: RealPython tutorial Installing PyInstaller , even in a virtual environment, will install the pyinstaller executable to $HOME/.local/bin. On Windows, it is installed to another directory within LOCALAPPDATA . pip install pyinstaller PyInstaller creates primarily 3 items: - .spec file, named after the CLI script - build/ folder, which can be ignored - dist/ folder, containing the final artifact at dist/cli/cli or dist/cli/cli.exe Several options are available hidden-import name onefile pyinstaller script.py --onefile On Windows, if PyInstaller is run from a virtual environment without necessary modules installed, they may not be available for compilation into the artifact. This does not appear to be an issue with Linux. This problem appears to be specific to certain modules, like emoji .","title":"pyinstaller"},{"location":"Coding/Python/#pythonnet","text":"Docs: ? ! Developers recommend Mono version 5.20.1 Issues 939 On Ubuntu, the eoan universe repository has to be added deb https://archive.ubuntu.com/ubuntu/ eoan universe deb https://archive.ubuntu.com/ubuntu/ eoan-updates universe But I can't figure out how to add the older version, because the recommended syntax produces the error \"Unable to correct problems, you have held broken packages\" sudo apt install mono-devel = 5 .18.0.240+dfsg-3 Maybe try the tarballs on Mono's website... Or maybe there's another repo I don't know about.. apt install clang libglib2.0-dev python3-dev pip install pycparser pythonnet pip install -U setuptools","title":"pythonnet"},{"location":"Coding/Python/#random","text":"Random choice with replacement random . choice ( iterable ) Shuffle elements of an iterable in-place [FP:42] random . shuffle ( iterable )","title":"random"},{"location":"Coding/Python/#scrapy","text":"Best used to obtain one \"stream\" of data at a time, without trying to obtain data from different pages scrapy runspider spider.py -o file.json Display HTML source of the scraped page print ( response . txt ) Get {URL} fetch ( 'url' ) Select a CSS selector # Returns a `SelectorList` response . css ( 'p' ) # Retrieve full HTML elements response . css ( 'p' ) . extract () Retrieve only the text within the element response . css ( 'p::text' ) . extract () response . css ( 'p::text' ) . extract_first () response . css ( 'p::text' ) . extract ()[ 0 ] Get the href attribute value for an anchor tag response . css ( 'a' ) . attrib [ 'href' ] Launch Scrapy shell and scrape $URL scrapy shell $URL Make a default spider named {quotes} that will be restricted to {domain} scrapy genspider quotes domain scrapy runspider scrapy1 . py Run a spider, saving scraped data to a JSON file scrapy runspider spider . py - o items . json Method which contains most of the logic of the spider, especially after the yield keyword. For multiple items, a structural basis for iteration must be found and for each iteration, data is yielded Extract URL from link using standard CSS selection techniques Add the domain name to a relative link response . urljoin () Recursively call the parse method again on the next page yield scrapy . Request ( url = next_page_url , callback = self . parse ) Scrape detail pages - parse_details would be a spider method sibling to the main parse method - if a detail page has more information than the main, then the yield keyword should be in parse_details yield scrapy . Request ( url = { url }, callback = self . parse_details )","title":"scrapy"},{"location":"Coding/Python/#setuptools","text":"Setuptools is for uploading to PyPi. To create self-contained executable files, use pyinstaller . PROJECT \u251c\u2500\u2500 PROJECT # Additional code files will be placed in here \u2502 \u2514\u2500\u2500 init.py \u2514\u2500\u2500 setup.py # Containing a call to `setuptools.setup()` 1 directory, 2 files setup.py from setuptools import setup setup ( name = 'funniest' , version = '0.1' , description = 'The funniest joke in the world' , url = 'http://github.com/storborg/funniest' , author = 'Flying Circus' , author_email = 'flyingcircus@example.com' , license = 'MIT' , packages = [ 'funniest' ], zip_safe = False ) If the package has dependencies, they can be added by appending a install_requires keyword argument passing an array of the module names setup ( install_requires = [ 'markdown' , ], ) Reserve the name, upload package metadata, and create the pypi.python.org webpage python setup . py register Create a source distribution, producing a tarball inside the top-level directory python setup . py sdist Upload the source distribution python setup . py sdist upload Do all the above in a single step python setup . py register sdist upload","title":"setuptools"},{"location":"Coding/Python/#socket","text":"The socket module is Python's standard interface for the transport layer. Sockets can be classified by family AF_INET Internet AF_UNIX for UNIX sockets and type : - SOCK_STREAM TCP - SOCK_DGRAM UDP These enum values are required upon initialization of a socket object: Ortega : 25 client_socket = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) Sources: Sockets tutorial TCP server TCP Client UDP server UDP client import socket with socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) as s : s . bind (( HOST , PORT )) import socket with socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) as s : s . connect (( HOST , PORT )) import socket with socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) as s : s . bind (( HOST , PORT )) import socket msg = \"Hello, world!\" with socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) as s : s . sendto ( msg . encode (), ( HOST , PORT )) Define port on which to listen for connections. serversocket . bind (( 'localhost' , 80 )) Connect to a remote socket in one direction client_socket . connect (( 'www.packtpub.com' , 80 )) Convert a domain name into IPv4 address socket . gethostbyname ( 'packtpub.com' ) # '83.166.169.231' Defaults to localhost with no arguments s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) s . bind (( socket . gethostname (), 1234 )) Get protocol name from port number socket . getservbyport ( 80 ) # 'http' Listen to a maximum of 10 connections serversocket . listen ( 10 ) Receive bytestream from server msg = s . recv ( 1024 ) print ( msg . decode ( 'utf-8' ))","title":"socket"},{"location":"Coding/Python/#sqlite3","text":"Create a Connect connection object and employee.db (binary) if it doesn't exist conn = sqlite . connect ( 'employee.db' ) Create a Connect.Cursor object c = conn . cursor () Perform SQL commands with Connect.Cursor.execute() . Create tablename with fields field of type type ( null , integer , real , text , blob ); never use Python's native string operations (f-strings, etc) to form commands, because this method is vulnerable to SQL injection. YouTube c . execute ( '''CREATE TABLE {tablename} ( {field} {type} , {field} {type} ...)) Save changes conn . commit () Close connection conn . close ()","title":"sqlite3"},{"location":"Coding/Python/#subprocess","text":"subprocess modules allows you to spawn new processes, interact with file descriptors, and obtain exit codes. The recommended approach is to use the run() function as default, which runs a CLI command with options as a list of strings and returns a CompletedProcess instance.\\ Execute shell command Unlike os.system , subprocess.run() takes a list of arguments. subprocess . run ([ 'ls' , '-l,' . '], 0) Set capture_output to True to save output, stored as property stdout of the returned object. data = subprocess . run ([ 'ls,' - l ',' . '], 0, capture_output=True) The data is stored as a bytestring , which can be decoded to a normal string. data . stdout . decode ( 'utf-8' ) This return a CompletedProcess instance with the command's output stored under the stdout property subprocess . run ([ 'ls' , '-l' , '/dev/null' ], capture_output = True ) This will raise a CalledProcessError exception because of the non-zero exit code subprocess . run ( 'exit 1' , shell = True , check = True )","title":"subprocess"},{"location":"Coding/Python/#sys","text":"Return site-specific directory where Python files are installed sys . prefix # /usr/local/ by default","title":"sys"},{"location":"Coding/Python/#tabulate","text":"","title":"tabulate"},{"location":"Coding/Python/#termcolor","text":"Print text in a color code termcolor . cprint ( text , color )","title":"termcolor"},{"location":"Coding/Python/#threading","text":"Docs counter = 0 lock = threading . RLock () def func1 (): global counter while True : with lock : counter += 1 counter -= 1 def func2 (): global counter while True : with lock : counter += 1 counter -= 1 threading . Thrad ( target = func1 ) . start () threading . Thrad ( target = func2 ) . start () counter = 0 def func1 (): global counter while True : counter += 1 counter -= 1 def func2 (): global counter while True : counter += 1 counter -= 1 threading . Thrad ( target = func1 ) . start () threading . Thrad ( target = func2 ) . start ()","title":"threading"},{"location":"Coding/Python/#typing","text":"As tuples, their attributes are immutable class Starship ( NamedTuple ): name : str registry : str crew : int","title":"typing"},{"location":"Coding/Python/#urllib","text":"Download an RFC file from rfc-editor.org Ortega rfc_raw = urllib . request . urlopen ( url ) . read () rfc = rfc_raw . decode ()","title":"urllib"},{"location":"Coding/Python/#weakref","text":"Weak references are references to objects which return exceptions when that object has been garbage collected Create a weak reference to {object} # A weak reference created using `ref` must be dereferenced r = weakref . ref ( obj ) r () . method () r . method () # will not work # A weak reference created using `proxy` does not need to be dereferenced: weakref . proxy ( obj )","title":"weakref"},{"location":"Coding/Python/#winrm","text":"Winrm allows you to connect Linux and Windows hosts over WinRM. adamtheautomator.com Begin a WinRM session. If no errors are thrown, the session has been successfully established session = winrm . Session ( ipaddress , auth = ( username , password ))","title":"winrm"},{"location":"Coding/Python/#yaml","text":"pip install pyyaml Deserialize Serialize import yaml with open ( './starships.yaml' ) as f : starships = yaml . safe_load ( f ) There is a load method but it requires specifying one of four possible values for the Loader kwarg. import yaml with open ( './starships.yaml' , 'w' ) as f : yaml . dump ( starships , f ) Resources Introduction to YAML","title":"yaml"},{"location":"Coding/Python/#xml","text":"books.xml <?xml version=\"1.0\"?> <catalog> <book id= \"bk101\" > <author> Gambardella, Matthew </author> <title> XML Developer's Guide </title> <genre> Computer </genre> <price> 44.95 </price> <publish_date> 2000-10-01 </publish_date> <description> An in-depth look at creating applications with XML. </description> </book> <book id= \"bk102\" > <author> Ralls, Kim </author> <title> Midnight Rain </title> <genre> Fantasy </genre> <price> 5.95 </price> <publish_date> 2000-12-16 </publish_date> <description> A former architect battles corporate zombies, an evil sorceress, and her own childhood to become queen of the world. </description> </book> <book id= \"bk103\" > <author> Corets, Eva </author> <title> Maeve Ascendant </title> <genre> Fantasy </genre> <price> 5.95 </price> <publish_date> 2000-11-17 </publish_date> <description> After the collapse of a nanotechnology society in England, the young survivors lay the foundation for a new society. </description> </book> <book id= \"bk104\" > <author> Corets, Eva </author> <title> Oberon's Legacy </title> <genre> Fantasy </genre> <price> 5.95 </price> <publish_date> 2001-03-10 </publish_date> <description> In post-apocalypse England, the mysterious agent known only as Oberon helps to create a new life for the inhabitants of London. Sequel to Maeve Ascendant. </description> </book> <book id= \"bk105\" > <author> Corets, Eva </author> <title> The Sundered Grail </title> <genre> Fantasy </genre> <price> 5.95 </price> <publish_date> 2001-09-10 </publish_date> <description> The two daughters of Maeve, half-sisters, battle one another for control of England. Sequel to Oberon's Legacy. </description> </book> <book id= \"bk106\" > <author> Randall, Cynthia </author> <title> Lover Birds </title> <genre> Romance </genre> <price> 4.95 </price> <publish_date> 2000-09-02 </publish_date> <description> When Carla meets Paul at an ornithology conference, tempers fly as feathers get ruffled. </description> </book> <book id= \"bk107\" > <author> Thurman, Paula </author> <title> Splish Splash </title> <genre> Romance </genre> <price> 4.95 </price> <publish_date> 2000-11-02 </publish_date> <description> A deep sea diver finds true love twenty thousand leagues beneath the sea. </description> </book> <book id= \"bk108\" > <author> Knorr, Stefan </author> <title> Creepy Crawlies </title> <genre> Horror </genre> <price> 4.95 </price> <publish_date> 2000-12-06 </publish_date> <description> An anthology of horror stories about roaches, centipedes, scorpions and other insects. </description> </book> <book id= \"bk109\" > <author> Kress, Peter </author> <title> Paradox Lost </title> <genre> Science Fiction </genre> <price> 6.95 </price> <publish_date> 2000-11-02 </publish_date> <description> After an inadvertant trip through a Heisenberg Uncertainty Device, James Salway discovers the problems of being quantum. </description> </book> <book id= \"bk110\" > <author> O'Brien, Tim </author> <title> Microsoft .NET: The Programming Bible </title> <genre> Computer </genre> <price> 36.95 </price> <publish_date> 2000-12-09 </publish_date> <description> Microsoft's .NET initiative is explored in detail in this deep programmer's reference. </description> </book> <book id= \"bk111\" > <author> O'Brien, Tim </author> <title> MSXML3: A Comprehensive Guide </title> <genre> Computer </genre> <price> 36.95 </price> <publish_date> 2000-12-01 </publish_date> <description> The Microsoft MSXML3 parser is covered in detail, with attention to XML DOM interfaces, XSLT processing, SAX and more. </description> </book> <book id= \"bk112\" > <author> Galos, Mike </author> <title> Visual Studio 7: A Comprehensive Guide </title> <genre> Computer </genre> <price> 49.95 </price> <publish_date> 2001-04-16 </publish_date> <description> Microsoft Visual Studio 7 is explored in depth, looking at how Visual Basic, Visual C++, C#, and ASP+ are integrated into a comprehensive development environment. </description> </book> </catalog> The etree submodule contains the ElementTree object which can open a string filename to deserialize XML data using parse() , which returns an ElementTree object, representing an XML document. A Python string can also be parsed with fromstring() , which actually returns an Element object. File String tree = xml . etree . ElementTree . parse ( 'books.xml' ) tree = xml . etree . ElementTree . fromstring ( books ) The getroot() method returns an Element object of the XML document's root node. root = tree . getroot () The parsed data can be displayed using the tostring() static method, providing an Element as argument. ElementTree . tostring ( root ) Children of an element can be filtered using findall() . This returns a list of Elements. books = root . findall ( 'book' ) Any Element object exposes an attrib property which returns a dictionary of attributes. [b.attrib for b in books] Attributes can be written to an Element using the set() method. root . set ( 'foo' , 'bar' ) Attributes can also be manipulated on the attrib property with normal Python dictionary operations. Setting Deleting root . attrib [ 'foo' ] = 'bar' del ( root . attrib [ 'hello' ]) Commit changes to disk. The argument can be a string representing the filename or a file object (in which case the file must be opened as a binary). Encoding can be specified (default is UTF-8 ) and a XML declaration can also be automatically generated. String File object tree . write ( 'books.xml' , encoding = 'UTF-16' , xml_declaration = True ) with open ( 'books.xml' , 'wb' ) as f : tree . write ( f ) Find elements by element name tree . findall ( 'book' )","title":"xml"},{"location":"Coding/Python/#glossary","text":"","title":"Glossary"},{"location":"Coding/Python/#method-resolution-order","text":"Method resolution order (MRO) is the order of base classes that are searched when using super() . It is accessed with __mro__ , which returns a tuple of base classes in order of precedence, ending in object which is the root class of all classes. ( src )","title":"Method resolution order"},{"location":"Coding/Python/#non-interactive-debugging","text":"Non-interactive debugging is the most basic form of debugging, dependent on print or log statements placed within the body of code.","title":"Non-interactive debugging"},{"location":"Coding/Python/#type-slot","text":"A type slot is any of a number of fields within each magic method, including __new__() , __init__() , and __prepare__() (which returns a dictionary-like object that's used as the local namespace for all code from the class body)","title":"Type slot"},{"location":"Coding/Testing/","text":"Testing There has been a push toward adoption of unit-testing over the past decades, so much so that the amount of test code can exceed production code by up to 10 times. Testing can help to forestall software entropy , the phenomenon whereby a software project becomes progressively more complex and disorganized. There are two popular coverage metrics that quantitatively measure the quality of a test suite: Test coverage : the portion of total production code lines executed by any test Branch coverage : the portion of total number of branches traversed by any test Unit testing A unit test quickly tests a small piece of code (or \"unit\") in isolation of others. There are two schools of unit testing which differ in their intepretation of how isolation should be achieved: London (also \"mockist\") approach emphasizes segregation of the system under test from its collaborators (dependencies) using [ test doubles ][test double], in particular [ mocks ][mock]. Classical (also \"Detroit\") approach emphasizes segregation of unit tests themselves from each other, allowing them to be run independently. In classical testing, there is less emphasis on using test doubles, which are used strictly for shared dependencies AAA Conventionally, tests have a three-part structure summarized in the acronym AAA (also 3A or Given-When-Then ): Arrange : SUT and dependencies are brought to a desired state Act : methods on the SUT are called and output is captured Assert : outcome is verified Several recommendations when using this framework: Every unit test should have a single Action Avoid conditional logic in unit tests The Arrange section should be largest, but if it is too large then it should be extracted into a private method or a separate factory class. Unit tests should be loosely coupled. Placing reusable [ test fixtures ][test fixture] in the test class's constructor is an anti-pattern, unless every single test method uses the fixture. Test methods should have expressive, easily understood names. \ud83d\udee0\ufe0f Tasks \ud83d\ude80 Starship C# Python public class StarshipShould { [Theory] [InlineData(\"USS Enterprise\",\"NCC-1701\",203)] [InlineData(\"USS Constitution\",\"NCC-1700\",204)] [InlineData(\"USS Voyager\",\"NCC-74656\",141)] [InlineData(\"USS Defiant\",\"NX-74205\",50)] [InlineData(\"USS Enterprise\",\"NCC-1701-D\",1000)] public void BeValid ( string name , string registry , int crew ) { var starship = new Starship { Name = name , Registry = registry , Crew = crew }; Assert . Equal ( starship . Name , name ); Assert . Equal ( starship . Registry , registry ); Assert . Equal ( starship . Crew , crew ); } } import pytest from starships import Starship , StarshipClass , Fleet @pytest . fixture def enterprise (): return Starship ( \"USS Enterprise\" , \"NCC-1701\" , StarshipClass . CONSTITUTION ) def test_lookup_by_name ( enterprise ): starfleet = Fleet () starfleet . add ( enterprise ) assert starfleet . lookup ( enterprise . name ) == enterprise \ud83d\ude80\u2714\ufe0f StarshipValidator C# public class StarshipValidatorShould { [Theory] [InlineData(\"Jean-Luc Picard\", 2305, 7, 13)] [InlineData(\"James Kirk\", 2233, 3, 22)] public void ValidateCaptainedStarships ( string n , params int [] dob ) { var mockStarship = new Mock < IStarship >(); Captain captain = new Captain ( n , new DateTime ( dob [ 0 ], dob [ 1 ], dob [ 2 ])); mockStarship . Setup ( x => x . Captain ). Returns ( captain ); StarshipValidator starshipValidator = new StarshipValidator ( mockStarship . Object ); Assert . True ( starshipValidator . IsCaptained ()); } [Theory] [InlineData(\"USS Enterprise\",\"NCC-1701\",203)] [InlineData(\"USS Constitution\",\"NCC-1700\",204)] [InlineData(\"USS Voyager\",\"NCC-74656\",141)] [InlineData(\"USS Defiant\",\"NX-74205\",50)] [InlineData(\"USS Enterprise\",\"NCC-1701-D\",1000)] public void ValidateStarshipsWithValidRegistryNumbers ( string name , string registry , int crew ) { var starship = new Starship { Name = name , Registry = registry , Crew = crew }; StarshipValidator starshipValidator = new StarshipValidator ( starship ); Assert . True ( starshipValidator . ValidateRegistry ()); } } \ud83d\ude80\ud83c\udff9 StarshipDeployment C# Test (xUnit) public class StarshipDeploymentShould { [Fact] public void ThrowOnNullValidator () { var sut = new StarshipDeployment ( null ); Assert . Throws < ArgumentNullException >( sut ); } [Theory] [InlineData(\"Betelgeuse\")] public void EvaluateStarship ( string destination ) { var mockValidator = new Mock < IStarshipValidator >(); mockValidator . Setup ( x => x . Evaluate ()). Returns ( true ); var mockStarship = new Mock < IStarship >(); var sut = new StarshipDeployment ( mockValidator . Object as IStarshipValidator ); sut . Deploy ( mockStarship . Object as Starship , destination ); mockValidator . Verify ( x => x . Evaluate ()); } } \ud83d\udc69\u200d\ud83d\ude80 Officer \ud83d\udc69\u200d\ud83d\ude80\u2714\ufe0f CaptainSelector C# public class CaptainSelectorShould { [Theory] [InlineData('B')] [InlineData('C')] [InlineData('D')] [InlineData('F')] public void OnlyAssignGoodCaptains ( char grade ) { var mockOfficer = new Mock < IOfficer >(); mockOfficer . Setup ( x => x . Grade ). Returns ( grade ); CaptainSelector captainSelector = new CaptainSelector ( mockOfficer . Object ); bool selectionResult = captainSelector . Evaluate (); Assert . False ( selectionResult ); } } \ud83d\udcd8 Glossary Mock A mock is a test double that emulates outgoing interactions, or calls the system under test makes to change the state of a dependency. Mocks include [spies][spy]. Spike A spike is an experiment without tests to ensure that an idea will work. Once the spike succeeds, the spike code is thrown away and the logic is recreated following TDD, starting with tests. Stub A stub is a test double that emulates incoming interactions, or calls the system under test makes to get data from a dependency. Fakes provide a working implementation of the dependency, however one which is unsuitable for production (e.g. in-memory databases) Dummies are passed around like real implementations but never accessed or used. These are used to satisfy the parameters of a method. Test double Test double include a variety of objects that facilitate unit testing by replacing a production object, usually a data dependency. Test doubles can be classified on what type of interaction the object emulates: Mocks emulate outgoing interactions Stubs emulate incoming interactions","title":"Testing"},{"location":"Coding/Testing/#testing","text":"There has been a push toward adoption of unit-testing over the past decades, so much so that the amount of test code can exceed production code by up to 10 times. Testing can help to forestall software entropy , the phenomenon whereby a software project becomes progressively more complex and disorganized. There are two popular coverage metrics that quantitatively measure the quality of a test suite: Test coverage : the portion of total production code lines executed by any test Branch coverage : the portion of total number of branches traversed by any test","title":"Testing"},{"location":"Coding/Testing/#unit-testing","text":"A unit test quickly tests a small piece of code (or \"unit\") in isolation of others. There are two schools of unit testing which differ in their intepretation of how isolation should be achieved: London (also \"mockist\") approach emphasizes segregation of the system under test from its collaborators (dependencies) using [ test doubles ][test double], in particular [ mocks ][mock]. Classical (also \"Detroit\") approach emphasizes segregation of unit tests themselves from each other, allowing them to be run independently. In classical testing, there is less emphasis on using test doubles, which are used strictly for shared dependencies","title":"Unit testing"},{"location":"Coding/Testing/#aaa","text":"Conventionally, tests have a three-part structure summarized in the acronym AAA (also 3A or Given-When-Then ): Arrange : SUT and dependencies are brought to a desired state Act : methods on the SUT are called and output is captured Assert : outcome is verified Several recommendations when using this framework: Every unit test should have a single Action Avoid conditional logic in unit tests The Arrange section should be largest, but if it is too large then it should be extracted into a private method or a separate factory class. Unit tests should be loosely coupled. Placing reusable [ test fixtures ][test fixture] in the test class's constructor is an anti-pattern, unless every single test method uses the fixture. Test methods should have expressive, easily understood names.","title":"AAA"},{"location":"Coding/Testing/#tasks","text":"","title":"\ud83d\udee0&#xfe0f; Tasks"},{"location":"Coding/Testing/#starship","text":"C# Python public class StarshipShould { [Theory] [InlineData(\"USS Enterprise\",\"NCC-1701\",203)] [InlineData(\"USS Constitution\",\"NCC-1700\",204)] [InlineData(\"USS Voyager\",\"NCC-74656\",141)] [InlineData(\"USS Defiant\",\"NX-74205\",50)] [InlineData(\"USS Enterprise\",\"NCC-1701-D\",1000)] public void BeValid ( string name , string registry , int crew ) { var starship = new Starship { Name = name , Registry = registry , Crew = crew }; Assert . Equal ( starship . Name , name ); Assert . Equal ( starship . Registry , registry ); Assert . Equal ( starship . Crew , crew ); } } import pytest from starships import Starship , StarshipClass , Fleet @pytest . fixture def enterprise (): return Starship ( \"USS Enterprise\" , \"NCC-1701\" , StarshipClass . CONSTITUTION ) def test_lookup_by_name ( enterprise ): starfleet = Fleet () starfleet . add ( enterprise ) assert starfleet . lookup ( enterprise . name ) == enterprise","title":"\ud83d\ude80 Starship"},{"location":"Coding/Testing/#starshipvalidator","text":"C# public class StarshipValidatorShould { [Theory] [InlineData(\"Jean-Luc Picard\", 2305, 7, 13)] [InlineData(\"James Kirk\", 2233, 3, 22)] public void ValidateCaptainedStarships ( string n , params int [] dob ) { var mockStarship = new Mock < IStarship >(); Captain captain = new Captain ( n , new DateTime ( dob [ 0 ], dob [ 1 ], dob [ 2 ])); mockStarship . Setup ( x => x . Captain ). Returns ( captain ); StarshipValidator starshipValidator = new StarshipValidator ( mockStarship . Object ); Assert . True ( starshipValidator . IsCaptained ()); } [Theory] [InlineData(\"USS Enterprise\",\"NCC-1701\",203)] [InlineData(\"USS Constitution\",\"NCC-1700\",204)] [InlineData(\"USS Voyager\",\"NCC-74656\",141)] [InlineData(\"USS Defiant\",\"NX-74205\",50)] [InlineData(\"USS Enterprise\",\"NCC-1701-D\",1000)] public void ValidateStarshipsWithValidRegistryNumbers ( string name , string registry , int crew ) { var starship = new Starship { Name = name , Registry = registry , Crew = crew }; StarshipValidator starshipValidator = new StarshipValidator ( starship ); Assert . True ( starshipValidator . ValidateRegistry ()); } }","title":"\ud83d\ude80\u2714&#xfe0f; StarshipValidator"},{"location":"Coding/Testing/#starshipdeployment","text":"C# Test (xUnit) public class StarshipDeploymentShould { [Fact] public void ThrowOnNullValidator () { var sut = new StarshipDeployment ( null ); Assert . Throws < ArgumentNullException >( sut ); } [Theory] [InlineData(\"Betelgeuse\")] public void EvaluateStarship ( string destination ) { var mockValidator = new Mock < IStarshipValidator >(); mockValidator . Setup ( x => x . Evaluate ()). Returns ( true ); var mockStarship = new Mock < IStarship >(); var sut = new StarshipDeployment ( mockValidator . Object as IStarshipValidator ); sut . Deploy ( mockStarship . Object as Starship , destination ); mockValidator . Verify ( x => x . Evaluate ()); } }","title":"\ud83d\ude80\ud83c\udff9 StarshipDeployment"},{"location":"Coding/Testing/#officer","text":"","title":"\ud83d\udc69\u200d\ud83d\ude80 Officer"},{"location":"Coding/Testing/#captainselector","text":"C# public class CaptainSelectorShould { [Theory] [InlineData('B')] [InlineData('C')] [InlineData('D')] [InlineData('F')] public void OnlyAssignGoodCaptains ( char grade ) { var mockOfficer = new Mock < IOfficer >(); mockOfficer . Setup ( x => x . Grade ). Returns ( grade ); CaptainSelector captainSelector = new CaptainSelector ( mockOfficer . Object ); bool selectionResult = captainSelector . Evaluate (); Assert . False ( selectionResult ); } }","title":"\ud83d\udc69\u200d\ud83d\ude80\u2714&#xfe0f; CaptainSelector"},{"location":"Coding/Testing/#glossary","text":"Mock A mock is a test double that emulates outgoing interactions, or calls the system under test makes to change the state of a dependency. Mocks include [spies][spy]. Spike A spike is an experiment without tests to ensure that an idea will work. Once the spike succeeds, the spike code is thrown away and the logic is recreated following TDD, starting with tests. Stub A stub is a test double that emulates incoming interactions, or calls the system under test makes to get data from a dependency. Fakes provide a working implementation of the dependency, however one which is unsuitable for production (e.g. in-memory databases) Dummies are passed around like real implementations but never accessed or used. These are used to satisfy the parameters of a method. Test double Test double include a variety of objects that facilitate unit testing by replacing a production object, usually a data dependency. Test doubles can be classified on what type of interaction the object emulates: Mocks emulate outgoing interactions Stubs emulate incoming interactions","title":"\ud83d\udcd8 Glossary"},{"location":"Coding/Windows/","text":"Windows programming \ud83d\udcd8 Glossary app model App model is a term Microsoft began using after the release of UWP in 2012 to refer to the hosting model, or the rigidly defined parameters for how an application is installed, stores state, manages versions, and integrates with the operating system and other apps, that generally define an application lifecycle. Specifically, the term was used in the context of describing the security sandbox and other restrictions of the UWP app model for Microsoft Store applications. Previous to UWP, there had been no definition of an \"app model\" per se and developers of a Win32 application were free to determine these parameters individually. This caused wide disparity in implementation among applications, resulting in registry bloat and poorly managed uninstallations. The UWP App Model was specifically introduced to answer these concerns which had plagued generations of Windows. ( src ) The failure of UWP as an app model resulted in Project Reunion , an effort to reunify the bifurcated Windows development landscape. C++/CX C++/CX is a legacy language projection that uses nonstandard C++ (see C++/WinRT ). Compile a C++ to a DLL ( src ) Invoke MSVC , creating: Library.dll Library.lib Library.exp Library.obj Command-line Library.cpp Library.h Library.def cl /w4 /ld Library.cpp Library.def #include \"Library.h\" #include <stdio.h> void Cluck () { printf ( \"C-style cluck! \\n \" ); } #pragma once void Cluck (); A definitions file is broken into sections, all of which are optional. EXPORTS Cluck Now the libary can be used in an application Command-line Application.cpp cl /W4 Application.cpp /link Library.lib #include \"Library.h\" int main () { Cluck (); } C++/WinRT C#/Win32 Resources: Windows APIs Everywhere in .NET C#/WinRT .NET Core 3 and .NET Framework (\"NETFX\") applications can continue to use the Microsoft.Windows.SDK.Contracts NuGet package. C#/WinRT is the WinRT language projection for C#, created after .NET5 removed WinRT projection support for C# out of the .NET compiler. This represents a decoupling of the Windows-specific APIs from .NET It is used to create C# runtime components hosted in non-.NET languages by first building interop assemblies from Windows Metadata files using cswinrt.exe . Resources: How to call WinRT APIs from .NET5 applications COM Component Object Model was developed in the late 1980s by Microsoft. The Component Object Model is a binary standard interface specification for objects. It originated in 1993 as a renaming of OLE (Object Linking and Embedding) 2.0, used by Microsoft Office at the time to link data between applications. COM objects support a collection of interfaces, most importantly IUnknown which all COM objects must implement. IUnknown includes QueryInteface() which returns pointers to the other interfaces also implemented by the COM object. It also includes AddRef() and Release() , which manage the object's lifetime. COM uses HRESULT s, 32-bit longs, to indicate success or failure. Bit 31 indicates success (0) or failure (1). With .NET interop , a failed HRESULT turns into a thrown COMException. Common HRESULT s include S_OK , S_FALSE , E_FAIL , and many other failure codes also beginning with E_ . COM objects are created by calling CoCreateInstance() . This function, which is stored in ole32.dll , searches the Registry for the given class ID, then loads the apropriate COM Server DLL file. This DLL file then creates a class factory which then creates the COM instance, which is passed back to the client as an interface pointer. ( src ) COM classes must be registered in the Registry, at minimum by mapping the class ID (a GUID) to a DLL, in the HKEY_CLASSES_ROOT hive. This is done using the regsvr32.exe utility. ( src ) Core application Core application refers to the lifecycle of a UWP application, through which Windows offers app-specific services relating to power management, security, etc and abstracts the app itself. It offers a level of control over graphical applications comparable to that available for apps as services. ( src ) DirectX DirectX is a gaming API that was created by Eric Engstrom (d. 2020), Alex St. John, and Craig Eisler in 1994 to support game development on Windows 95. Interop assembly Interop assemblies allow .NET applications to call native code. They can be distributed along with applications that reference them. Language projections like C#/WinRT produce interop assemblies composed in C# which can then be compiled into a projection assembly . Language projection A language projection (or simply \"projection\") is a native adapter that enables programming APIs in a way that is idiomatic to a given language. Framework C# C++ WinRT C#/WinRT C++/WinRT Win32 C#/Win32 ? Resources: C#/WinRT (MSDocs) Windows APIs Everywhere in .NET TFM Target Framework Monikers (TFM) are used in NuGet packages and project files to refer to the flavor of .NET targeted by an application. Only for .NET5 , Microsoft introduced new monikers that indicate the targeted OS after a hyphen, e.g. net5.0-windows , etc. These monikers will pull in the projection assemblies that are needed to access those APIs and replace earlier use of the Microsoft.Windows.SDK.Contracts package reference. Example TMFs include: net5.0-windows10.0.19041.0 Windows 10 version 2004 net5.0-windows10.0.18362.0 Windows 10 version 1903 net5.0-windows10.0.17763.0 Windows 10 version 1809 References: Windows APIs Everywhere in .NET UWP Universal Windows Platform (UWP) refers to both a UI framework incorporating the Fluent Design System as well as an app model . When the UWP XAML framework was released in 2012, UWP was touted as a means to develop for many different device platforms, including mobile and tablet. Until recently, the UWP XAML framework was only available for applications using the UWP app model for apps destined for the Microsoft Store. However, UWP development has floundered over the past half decade as Microsoft has been unable to produce sufficient interest in its mobile and tablet devices or the Microsoft Store. Microsoft itself has ceased development of UWP apps for Office or for Xbox, for which it has turned rather to Electron. Win32 WinMD Windows metadata files (*.winmd) are machine-readable files that define Windows Runtime APIs. All public types in a .winmd file must be WinRT types. They use the same physical file format as CLR assemblies. Resources: Windows Metadata (WinMD) files WinRT The Windows Runtime is a framework introduced with Windows 8 to provide access to system resources. WinRT is separate from, although it is used by, .NET. Under the surface, WinRT is implemented as COM components. UWP XAML controls were included in WinRT until recently when they were moved to the WinUI NuGet package. Resources: Windows APIs Everywhere in .NET WinUI Windows UI Library (WinUI) 3 is a native UI framework that represents a rebranding of the UWP UI framework , which had previously only been available for applications using the UWP app model , and a move to make it available for both app models. Still in development, it promises to deliver a unified framework and all the styles and controls previously distributed in WinUI 2, which in turn is a NuGet packaging containing the UWP XAML controls and styles. Unfortunately, because the UWP framework had been available only for the UWP app model , it did not experience wide adoption among developers who prefered the flexibility of the older Win32 \"app model\" (or rather, the lack of one).","title":"Windows programming"},{"location":"Coding/Windows/#windows-programming","text":"","title":"Windows programming"},{"location":"Coding/Windows/#glossary","text":"app model App model is a term Microsoft began using after the release of UWP in 2012 to refer to the hosting model, or the rigidly defined parameters for how an application is installed, stores state, manages versions, and integrates with the operating system and other apps, that generally define an application lifecycle. Specifically, the term was used in the context of describing the security sandbox and other restrictions of the UWP app model for Microsoft Store applications. Previous to UWP, there had been no definition of an \"app model\" per se and developers of a Win32 application were free to determine these parameters individually. This caused wide disparity in implementation among applications, resulting in registry bloat and poorly managed uninstallations. The UWP App Model was specifically introduced to answer these concerns which had plagued generations of Windows. ( src ) The failure of UWP as an app model resulted in Project Reunion , an effort to reunify the bifurcated Windows development landscape.","title":"\ud83d\udcd8 Glossary"},{"location":"Coding/Windows/#ccx","text":"C++/CX is a legacy language projection that uses nonstandard C++ (see C++/WinRT ). Compile a C++ to a DLL ( src ) Invoke MSVC , creating: Library.dll Library.lib Library.exp Library.obj Command-line Library.cpp Library.h Library.def cl /w4 /ld Library.cpp Library.def #include \"Library.h\" #include <stdio.h> void Cluck () { printf ( \"C-style cluck! \\n \" ); } #pragma once void Cluck (); A definitions file is broken into sections, all of which are optional. EXPORTS Cluck Now the libary can be used in an application Command-line Application.cpp cl /W4 Application.cpp /link Library.lib #include \"Library.h\" int main () { Cluck (); }","title":"C++/CX"},{"location":"Coding/Windows/#cwinrt","text":"","title":"C++/WinRT "},{"location":"Coding/Windows/#cwin32","text":"Resources: Windows APIs Everywhere in .NET","title":"C#/Win32 "},{"location":"Coding/Windows/#cwinrt_1","text":".NET Core 3 and .NET Framework (\"NETFX\") applications can continue to use the Microsoft.Windows.SDK.Contracts NuGet package. C#/WinRT is the WinRT language projection for C#, created after .NET5 removed WinRT projection support for C# out of the .NET compiler. This represents a decoupling of the Windows-specific APIs from .NET It is used to create C# runtime components hosted in non-.NET languages by first building interop assemblies from Windows Metadata files using cswinrt.exe . Resources: How to call WinRT APIs from .NET5 applications","title":"C#/WinRT  "},{"location":"Coding/Windows/#com","text":"Component Object Model was developed in the late 1980s by Microsoft. The Component Object Model is a binary standard interface specification for objects. It originated in 1993 as a renaming of OLE (Object Linking and Embedding) 2.0, used by Microsoft Office at the time to link data between applications. COM objects support a collection of interfaces, most importantly IUnknown which all COM objects must implement. IUnknown includes QueryInteface() which returns pointers to the other interfaces also implemented by the COM object. It also includes AddRef() and Release() , which manage the object's lifetime. COM uses HRESULT s, 32-bit longs, to indicate success or failure. Bit 31 indicates success (0) or failure (1). With .NET interop , a failed HRESULT turns into a thrown COMException. Common HRESULT s include S_OK , S_FALSE , E_FAIL , and many other failure codes also beginning with E_ . COM objects are created by calling CoCreateInstance() . This function, which is stored in ole32.dll , searches the Registry for the given class ID, then loads the apropriate COM Server DLL file. This DLL file then creates a class factory which then creates the COM instance, which is passed back to the client as an interface pointer. ( src ) COM classes must be registered in the Registry, at minimum by mapping the class ID (a GUID) to a DLL, in the HKEY_CLASSES_ROOT hive. This is done using the regsvr32.exe utility. ( src )","title":"COM"},{"location":"Coding/Windows/#core-application","text":"Core application refers to the lifecycle of a UWP application, through which Windows offers app-specific services relating to power management, security, etc and abstracts the app itself. It offers a level of control over graphical applications comparable to that available for apps as services. ( src )","title":"Core application"},{"location":"Coding/Windows/#directx","text":"DirectX is a gaming API that was created by Eric Engstrom (d. 2020), Alex St. John, and Craig Eisler in 1994 to support game development on Windows 95.","title":"DirectX"},{"location":"Coding/Windows/#interop-assembly","text":"Interop assemblies allow .NET applications to call native code. They can be distributed along with applications that reference them. Language projections like C#/WinRT produce interop assemblies composed in C# which can then be compiled into a projection assembly .","title":"Interop assembly"},{"location":"Coding/Windows/#language-projection","text":"A language projection (or simply \"projection\") is a native adapter that enables programming APIs in a way that is idiomatic to a given language. Framework C# C++ WinRT C#/WinRT C++/WinRT Win32 C#/Win32 ? Resources: C#/WinRT (MSDocs) Windows APIs Everywhere in .NET","title":"Language projection"},{"location":"Coding/Windows/#tfm","text":"Target Framework Monikers (TFM) are used in NuGet packages and project files to refer to the flavor of .NET targeted by an application. Only for .NET5 , Microsoft introduced new monikers that indicate the targeted OS after a hyphen, e.g. net5.0-windows , etc. These monikers will pull in the projection assemblies that are needed to access those APIs and replace earlier use of the Microsoft.Windows.SDK.Contracts package reference. Example TMFs include: net5.0-windows10.0.19041.0 Windows 10 version 2004 net5.0-windows10.0.18362.0 Windows 10 version 1903 net5.0-windows10.0.17763.0 Windows 10 version 1809 References: Windows APIs Everywhere in .NET","title":"TFM"},{"location":"Coding/Windows/#uwp","text":"Universal Windows Platform (UWP) refers to both a UI framework incorporating the Fluent Design System as well as an app model . When the UWP XAML framework was released in 2012, UWP was touted as a means to develop for many different device platforms, including mobile and tablet. Until recently, the UWP XAML framework was only available for applications using the UWP app model for apps destined for the Microsoft Store. However, UWP development has floundered over the past half decade as Microsoft has been unable to produce sufficient interest in its mobile and tablet devices or the Microsoft Store. Microsoft itself has ceased development of UWP apps for Office or for Xbox, for which it has turned rather to Electron.","title":"UWP"},{"location":"Coding/Windows/#win32","text":"","title":"Win32"},{"location":"Coding/Windows/#winmd","text":"Windows metadata files (*.winmd) are machine-readable files that define Windows Runtime APIs. All public types in a .winmd file must be WinRT types. They use the same physical file format as CLR assemblies. Resources: Windows Metadata (WinMD) files","title":"WinMD"},{"location":"Coding/Windows/#winrt","text":"The Windows Runtime is a framework introduced with Windows 8 to provide access to system resources. WinRT is separate from, although it is used by, .NET. Under the surface, WinRT is implemented as COM components. UWP XAML controls were included in WinRT until recently when they were moved to the WinUI NuGet package. Resources: Windows APIs Everywhere in .NET","title":"WinRT"},{"location":"Coding/Windows/#winui","text":"Windows UI Library (WinUI) 3 is a native UI framework that represents a rebranding of the UWP UI framework , which had previously only been available for applications using the UWP app model , and a move to make it available for both app models. Still in development, it promises to deliver a unified framework and all the styles and controls previously distributed in WinUI 2, which in turn is a NuGet packaging containing the UWP XAML controls and styles. Unfortunately, because the UWP framework had been available only for the UWP app model , it did not experience wide adoption among developers who prefered the flexibility of the older Win32 \"app model\" (or rather, the lack of one).","title":"WinUI"},{"location":"Coding/Wired%20Brain%20Coffee/","text":"\u2615 Wired Brain Coffee Basic layout MainPage.xaml <Window x:Class= \"WiredBrainCoffee.UWP.MainWindow\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" > <Grid> <Grid.RowDefinitions> <RowDefinition Height= \"Auto\" /> <RowDefinition/> </Grid.RowDefinitions> <Grid.ColumnDefinitions> <ColumnDefinition Width= \"350\" /> <ColumnDefinition Width= \"*\" /> </Grid.ColumnDefinitions> <Border Grid.ColumnSpan= \"2\" Background= \"#f05a28\" > <StackPanel Orientation= \"Horizontal\" HorizontalAlignment= \"Center\" > <Image Height= \"90\" Margin= \"5\" Source= \"/Images/logo.png\" /> <TextBlock Text= \"Employee Manager\" FontSize= \"40\" VerticalAlignment= \"Center\" /> </StackPanel> </Border> <!-- Sidebar --> <Grid Grid.Row= \"1\" > <Grid.RowDefinitions> <RowDefinition Height= \"Auto\" /> <RowDefinition /> </Grid.RowDefinitions> <Button Content= \"Refresh\" Margin= \"10\" /> <ListView Grid.Row= \"1\" /> </Grid> <!--MainArea--> <Grid Grid.Row= \"1\" Grid.Column= \"1\" > <Grid.RowDefinitions> <RowDefinition Height= \"Auto\" /> <RowDefinition Height= \"Auto\" /> <RowDefinition Height= \"Auto\" /> <RowDefinition Height= \"Auto\" /> <RowDefinition Height= \"*\" /> </Grid.RowDefinitions> <TextBox Header= \"Firstname\" Margin= \"10\" /> <DatePicker Grid.Row= \"1\" Header= \"Entry date\" Margin= \"10\" /> <ComboBox Grid.Row= \"2\" Header= \"Job role\" Margin= \"10\" HorizontalAlignment= \"Stretch\" /> <CheckBox Grid.Row= \"3\" Content= \"Is coffee drinker?\" Margin= \"10\" /> <Button Grid.Row= \"4\" Content= \"Save\" Margin= \"10 10 10 30\" VerticalAlignment= \"Bottom\" HorizontalAlignment= \"Left\" /> </Grid> </Grid> </Window> Custom control Controls/HeaderControl.xaml <UserControl x:Class= \"WiredBrainCoffee.UWP.Controls.HeaderControl\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP.Controls\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" d:DesignHeight= \"300\" d:DesignWidth= \"400\" > <Border Background= \"#F05A28\" > <Grid> <Grid.ColumnDefinitions> <ColumnDefinition/> <ColumnDefinition Width= \"Auto\" /> </Grid.ColumnDefinitions> <StackPanel Orientation= \"Horizontal\" HorizontalAlignment= \"Center\" > <Image Source= \"/Images/WiredBrainLogo.png\" Height= \"90\" /> <TextBlock Text= \"Wired Brain Coffee\" FontSize= \"40\" VerticalAlignment= \"Center\" /> </StackPanel> <Button HorizontalAlignment= \"Right\" Grid.Column= \"1\" Margin= \"10\" > <SymbolIcon Symbol= \"AlignRight\" /> </Button> </Grid> </Border> </UserControl> MainPage.xaml <Page x:Class= \"WiredBrainCoffee.UWP.MainPage\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" xmlns:controls= \"using:WiredBrainCoffee.UWP.Controls\" mc:Ignorable= \"d\" Background= \"{ThemeResource ApplicationPageBackgroundThemeBrush}\" > <Grid> <Grid.RowDefinitions> <RowDefinition Height= \"Auto\" /> <RowDefinition/> </Grid.RowDefinitions> <Grid.ColumnDefinitions> <ColumnDefinition Width= \"350\" /> <ColumnDefinition/> </Grid.ColumnDefinitions> <!-- Header --> <controls:HeaderControl Grid.ColumnSpan= \"2\" /> <!-- Sidebar --> <Grid Grid.Row= \"1\" > <Grid.RowDefinitions> <RowDefinition Height= \"Auto\" /> <RowDefinition/> </Grid.RowDefinitions> <StackPanel Orientation= \"Horizontal\" > <Button Margin= \"10\" > <SymbolIcon Symbol= \"AddFriend\" /> </Button> <Button Margin= \"10\" > <SymbolIcon Symbol= \"Delete\" /> </Button> </StackPanel> <ListView Grid.Row= \"1\" > <ListViewItem> Aristotle </ListViewItem> <ListViewItem> Euclid </ListViewItem> <ListViewItem> Plato </ListViewItem> <ListViewItem> Socrates </ListViewItem> </ListView> </Grid> <StackPanel Grid.Row= \"1\" Grid.Column= \"1\" > <TextBox Header= \"First name\" Margin= \"10\" /> <TextBox Header= \"Last name\" Margin= \"10\" /> <CheckBox Content= \"Drinks coffee\" Margin= \"10\" /> </StackPanel> </Grid> </Page> Sidebar Setting an x:Name attribute on an element allows it to be manipulated in C#. ( src ) MainPage.xaml.cs private void btn_MoveSideBar_Click ( object sender , RoutedEventArgs e ) { int column = Grid . GetColumn ( customerListGrid ); int newcolumn ; if ( column == 0 ) { newcolumn = 2 ; btn_MoveSideBar_Symbol . Symbol = Symbol . AlignLeft ; } else { newcolumn = 0 ; btn_MoveSideBar_Symbol . Symbol = Symbol . AlignRight ; } Grid . SetColumn ( customerListGrid , newcolumn ); } Data provider A data provider class accomodates the need for mock data while also loosely coupling the data with the source. ( src ) DataProviders/CustomerDataProvider.cs using Newtonsoft.Json ; using System ; using System.Collections.Generic ; using System.Threading.Tasks ; using System.IO ; using Windows.Storage ; using Windows.Storage.Streams ; using WiredBrainCoffee.UWP.Models ; namespace WiredBrainCoffee.UWP.DataProviders { class CustomerDataProvider { private static readonly string _customersFileName = \"customers.json\" ; private static readonly StorageFolder _localFolder = ApplicationData . Current . LocalFolder ; public async Task < IEnumerable < Customer >> LoadCustomersAsync () { var storageFile = await _localFolder . TryGetItemAsync ( _customersFileName ) as StorageFile ; List < Customer > customerList = null ; if ( storageFile == null ) { customerList = new List < Customer > { new Customer { FirstName = \"Clark\" , LastName = \"Kent\" , IsCoffeeDrinker = true }, new Customer { FirstName = \"Bruce\" , LastName = \"Wayne\" , IsCoffeeDrinker = false }, new Customer { FirstName = \"Diana\" , LastName = \"Prince\" , IsCoffeeDrinker = true } }; } else { using ( var stream = await storageFile . OpenAsync ( FileAccessMode . Read )) { using ( var dataReader = new DataReader ( stream )) { await dataReader . LoadAsync (( uint ) stream . Size ); var json = dataReader . ReadString (( uint ) stream . Size ); customerList = JsonConvert . DeserializeObject < List < Customer >>( json ); } } } return customerList ; } public async Task SaveCustomersAsync ( IEnumerable < Customer > customers ) { var storageFile = await _localFolder . CreateFileAsync ( _customersFileName , CreationCollisionOption . ReplaceExisting ); using ( var stream = await storageFile . OpenAsync ( FileAccessMode . ReadWrite )) { using ( var dataWriter = new DataWriter ( stream )) { var json = JsonConvert . SerializeObject ( customers , Formatting . Indented ); dataWriter . WriteString ( json ); await dataWriter . StoreAsync (); } } } } } Models/Customer.cs namespace WiredBrainCoffee.UWP.Models { public class Customer { public string FirstName { get ; set ; } public string LastName { get ; set ; } public bool IsCoffeeDrinker { get ; set ; } } } Event hooks are used to populate the ListView with data from the data provider. MainPage.xaml.cs public MainPage () { this . InitializeComponent (); this . Loaded += MainPage_LoadedAsync ; App . Current . Suspending += App_SuspendingAsync ; _customerDataProvider = new CustomerDataProvider (); } private async void App_SuspendingAsync ( object sender , Windows . ApplicationModel . SuspendingEventArgs e ) { var deferral = e . SuspendingOperation . GetDeferral (); await _customerDataProvider . SaveCustomersAsync ( customerListView . Items . OfType < Customer >()); deferral . Complete (); } private async void MainPage_LoadedAsync ( object sender , RoutedEventArgs e ) { customerListView . Items . Clear (); var customers = await _customerDataProvider . LoadCustomersAsync (); foreach ( var customer in customers ) { customerListView . Items . Add ( customer ); } } Data binding using events Synchronize the customer detail textboxes to the selected item in the ListView. A rough form of data binding is possible with event handling . ( src ) First implement an event handler when the ListView.SelectionChanged event is fired. MainPage.xaml <ListView Grid.Row= \"1\" x:Name= \"customerListView\" DisplayMemberPath= \"FirstName\" SelectionChanged= \"customerListView_SelectionChanged\" /> MainPage.xaml.cs private void customerListView_SelectionChanged ( object sender , SelectionChangedEventArgs e ) { var customer = customerListView . SelectedItem as Customer ; txtFirstName . Text = customer ?. FirstName ?? \"\" ; txtLastName . Text = customer ?. LastName ?? \"\" ; chkDrinksCoffee . IsChecked = customer ?. IsCoffeeDrinker ; } Implement event handlers on the controls in the main area ( TextBox.TextChanged and CheckBox.Checked and CheckBox.Unchedked events) when changes are made. MainPage.xaml <StackPanel Grid.Row= \"1\" Grid.Column= \"1\" > <TextBox x:Name= \"txtFirstName\" Header= \"First name\" Margin= \"10\" TextChanged= \"UpdateCustomer\" /> <TextBox x:Name= \"txtLastName\" Header= \"Last name\" Margin= \"10\" TextChanged= \"UpdateCustomer\" /> <CheckBox x:Name= \"chkDrinksCoffee\" Content= \"Caffeine fiend\" Margin= \"10\" Checked= \"UpdateCustomer\" Unchecked= \"UpdateCustomer\" /> </StackPanel> MainPage.xaml.cs private void UpdateCustomer ( object sender , RoutedEventArgs e ) { var customer = customerListView . SelectedItem as Customer ; if ( customer != null ) { customer . FirstName = txtFirstName . Text ; customer . LastName = txtLastName . Text ; customer . IsCoffeeDrinker = chkDrinksCoffee . IsChecked . GetValueOrDefault (); } } Update ListView ListView still won't update as a result of changes. In order to implement this, you have to raise the PropertyChanged event. We implement the INotifyPropertyChanged interface and make it the base class of Customer. Also, we implement a helper method to fire the event handler whenever a property is changed. This helper is invoked every time a property is set. The CallerMemberName attribute passes the name of the calling property as a string, and allows us to avoid placing typeof(FirstName) , etc with every invocation. ( src ) Models/Customer.cs using System.ComponentModel ; using System.Runtime.CompilerServices ; namespace WiredBrainCoffee.UWP.Models { public class Observable : INotifyPropertyChanged { public event PropertyChangedEventHandler PropertyChanged ; protected virtual void OnPropertyChanged ([ CallerMemberName ] string propertyName = null ) { PropertyChanged ?. Invoke ( this , new PropertyChangedEventArgs ( propertyName )); } } public class Customer : Observable { private string firstName ; private string lastName ; private bool isCoffeeDrinker ; public string FirstName { get => firstName ; set { firstName = value ; OnPropertyChanged (); } } public string LastName { get => lastName ; set { lastName = value ; OnPropertyChanged (); } } public bool IsCoffeeDrinker { get => isCoffeeDrinker ; set { isCoffeeDrinker = value ; OnPropertyChanged (); } } } } Add/remove customers Implement event handlers for the Add and Delete buttons. ( src ) MainPage.xaml.cs private void DeleteCustomer_Click ( object sender , RoutedEventArgs e ) { var customer = customerListView . SelectedItem ; if ( customer != null ) { customerListView . Items . Remove ( customer ); } } private void AddCustomer_Click ( object sender , RoutedEventArgs e ) { var customer = new Customer { FirstName = \"New\" }; customerListView . Items . Add ( customer ); customerListView . SelectedItem = customer ; } MainPage.xaml <Button x:Name= \"AddCustomer\" Margin= \"10\" Click= \"AddCustomer_Click\" > <SymbolIcon Symbol= \"AddFriend\" /> </Button> <Button x:Name= \"DeleteCustomer\" Margin= \"10\" Click= \"DeleteCustomer_Click\" > <SymbolIcon Symbol= \"Delete\" /> </Button> Custom control We abstract controls in the main area of the app into a new CustomerDetailControl. As before, we cut the UI elements into a new XAML file and reference the new control in MainPage. However, now, customerListView is inaccessible. The lynchpin is forming a property on customerDetailControl that is populated with the customer object by the SelectionChanged event handler MainPage.xaml.cs private void customerListView_SelectionChanged ( object sender , SelectionChangedEventArgs e ) { var customer = customerListView . SelectedItem as Customer ; customerDetailControl . Customer = customer ; } Controls/CustomerDetailControl.xaml <UserControl x:Class= \"WiredBrainCoffee.UWP.Controls.CustomerDetailControl\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP.Controls\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" d:DesignHeight= \"300\" d:DesignWidth= \"400\" > <StackPanel> <TextBox x:Name= \"txtFirstName\" Header= \"First name\" Margin= \"10\" TextChanged= \"UpdateCustomer\" /> <TextBox x:Name= \"txtLastName\" Header= \"Last name\" Margin= \"10\" TextChanged= \"UpdateCustomer\" /> <CheckBox x:Name= \"chkDrinksCoffee\" Content= \"Caffeine fiend\" Margin= \"10\" Checked= \"UpdateCustomer\" Unchecked= \"UpdateCustomer\" /> </StackPanel> </UserControl> Controls/CustomerDetailControl.xaml.cs using Windows.UI.Xaml ; using Windows.UI.Xaml.Controls ; using WiredBrainCoffee.UWP.Models ; namespace WiredBrainCoffee.UWP.Controls { public sealed partial class CustomerDetailControl : UserControl { public CustomerDetailControl () { this . InitializeComponent (); } private Customer _customer ; public Customer Customer { get { return _customer ; } set { _customer = value ; txtFirstName . Text = _customer ?. FirstName ?? \"\" ; txtLastName . Text = _customer ?. LastName ?? \"\" ; chkDrinksCoffee . IsChecked = _customer ?. IsCoffeeDrinker ; } } private void UpdateCustomer ( object sender , RoutedEventArgs e ) { if ( Customer != null ) { Customer . FirstName = txtFirstName . Text ; Customer . LastName = txtLastName . Text ; Customer . IsCoffeeDrinker = chkDrinksCoffee . IsChecked . GetValueOrDefault (); } } } } Assign mock content We combine two different namespace mappings (one for the Customer model and another for the CustomerDetailControl) to prepopulate the CustomerDetailControl with a customer defined in XAML. Because CustomerDetailControl exposes a public Customer property, this data can be assigned to the Customer property using property-element syntax. ( src ) MainPage.xaml <controls:CustomerDetailControl x:Name= \"customerDetailControl\" Grid.Row= \"1\" Grid.Column= \"1\" > <controls:CustomerDetailControl.Customer> <model:Customer FirstName= \"Clark\" LastName= \"Kent\" IsCoffeeDrinker= \"True\" /> </controls:CustomerDetailControl.Customer> </controls:CustomerDetailControl> In order to be able to assign the customer as direct content without specifying the property explicitly, the custom control class has to be decorated with the ContentProperty attribute. This is because by default any direct child is assigned to the Content property, which does not exist for this custom control. Using the ContentProperty allows us to specify a property to which to assign direct children. Controls/CustomerDetailControl.xaml.cs [ContentProperty(Name = nameof(Customer))] public sealed partial class CustomerDetailControl : UserControl { /* ... */ } MainPage.xaml <controls:CustomerDetailControl x:Name= \"customerDetailControl\" Grid.Row= \"1\" Grid.Column= \"1\" > <model:Customer FirstName= \"Clark\" LastName= \"Kent\" IsCoffeeDrinker= \"True\" /> </controls:CustomerDetailControl> XAML Type conversion Passing the customer as an attribute requires custom logic to parse the string. The target model is then decorated with the CreateFromString attribute. This is only for custom classes: primitive types and enumerations can be parsed by the XAML processor automatically. Models/CustomerConverter.cs namespace WiredBrainCoffee.UWP.Models { public static class CustomerConverter { public static Customer ParseStringAsCustomer ( string s ) { string [] values = s . Split ( ';' ); return new Customer { FirstName = values [ 0 ], LastName = values [ 1 ], IsCoffeeDrinker = bool . Parse ( values [ 2 ]) }; } } } Models/Customer.cs [CreateFromString(MethodName =\"WiredBrainCoffee.UWP.Models.CustomerConverter.ParseCustomerFromString\")] public class Customer : Observable { /* ... */ } StaticResource You can use the StaticResource Markup Extension to define the equivalent of XAML variables to store elements for attribution using attribute syntax. Every UI element has a property named Resources to which you can assign elements. Unlike the Items property of a ListView, however, this property is a Dictionary type, which means you must specify a key for these values (i.e. specify x:Key ). Because the XAML processor looks for resources as it crawls up the element tree, these resources can be organized at any level of the application, even in the App.xaml where it will become available to other files: ( src ) MainPage.xaml <Page.Resources> <model:Customer x:Key= \"Shazam\" FirstName= \"William\" LastName= \"William Batson\" IsCoffeeDrinker= \"false\" /> </Page.Resources> <!-- ... --> <controls:CustomerDetailControl Customer= \"{StaticResource Shazam}\" /> However, mocking data in XAML is an anti-pattern; Resource dictionaries are typically used for colors and predefined strings. Resource dictionaries are consolidated into their own files: ( src ) Resources/Brushes.xaml <ResourceDictionary xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" > <SolidColorBrush x:Key= \"customerListBackgroundBrush\" Color= \"#EEEEEE\" /> </ResourceDictionary> Resources/Strings.xaml <ResourceDictionary xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" > <x:String x:Key= \"applicationTitle\" > Wired Brain Coffee </x:String> </ResourceDictionary> These can then be referenced from App.xaml and are available for assignment in any appropriate attribute App.xaml <Application x:Class= \"WiredBrainCoffee.UWP.App\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP\" > <Application.Resources> <ResourceDictionary> <ResourceDictionary.MergedDictionaries> <ResourceDictionary Source= \"Resources/Brushes.xaml\" /> <ResourceDictionary Source= \"Resources/Strings.xaml\" /> </ResourceDictionary.MergedDictionaries> </ResourceDictionary> </Application.Resources> </Application> Controls/HeaderControl.xaml <UserControl x:Class= \"WiredBrainCoffee.UWP.Controls.HeaderControl\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP.Controls\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" d:DesignHeight= \"300\" d:DesignWidth= \"400\" > <Border Background= \"#F05A28\" > <Grid> <Grid.ColumnDefinitions> <ColumnDefinition/> <ColumnDefinition Width= \"Auto\" /> </Grid.ColumnDefinitions> <StackPanel Orientation= \"Horizontal\" HorizontalAlignment= \"Center\" > <Image Source= \"/Images/WiredBrainLogo.png\" Height= \"90\" /> <TextBlock Text= \"{StaticResource applicationTitle}\" FontSize= \"40\" VerticalAlignment= \"Center\" /> </StackPanel> <Button x:Name= \"ButtonMove\" HorizontalAlignment= \"Right\" Grid.Column= \"1\" Margin= \"10\" Click= \"ButtonMove_Click\" > <SymbolIcon x:Name= \"ButtonMove_Symbol\" Symbol= \"AlignRight\" /> </Button> </Grid> </Border> </UserControl> ThemeResource The ThemeResource Markup Extension makes UWP-specific theme resource dictionaries available. These same resources are available using StaticResource, but with ThemeResource they will be updated if the user changes his Windows theme from light to dark. ( src ) Resources/Brushes.xaml <ResourceDictionary xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" > <ResourceDictionary.ThemeDictionaries> <ResourceDictionary x:Key= \"Dark\" > <SolidColorBrush x:Key= \"customerListBackgroundBrush\" Color= \"#222222\" /> </ResourceDictionary> <ResourceDictionary x:Key= \"Light\" > <SolidColorBrush x:Key= \"customListBackgroundBrush\" Color= \"#EEEEEE\" /> </ResourceDictionary> </ResourceDictionary.ThemeDictionaries> </ResourceDictionary> Theme selection A specific theme can be specified at any element by specifying a RequestedTheme attribute. However, this property cannot be changed at runtime. App.xaml <Application x:Class= \"WiredBrainCoffee.UWP.App\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP\" RequestedTheme= \"Dark\" > <Application.Resources> <ResourceDictionary> <ResourceDictionary.MergedDictionaries> <ResourceDictionary Source= \"Resources/Brushes.xaml\" /> <ResourceDictionary Source= \"Resources/Strings.xaml\" /> </ResourceDictionary.MergedDictionaries> </ResourceDictionary> </Application.Resources> </Application> A button to manually change theme involves simply assigning an ElementTheme enum value to the MainPage's RequestedTheme property. However, because on startup this has an ApplicationTheme enum value that is not evailable in ElementTheme , the MainPage's constructor must be changed to set the correct theme enum. Without this change, the first click after the application's startup will not change the theme at all, only set the correct ElementTheme . ( src ) MainPage.xaml <!-- Header --> <controls:HeaderControl Grid.ColumnSpan= \"3\" /> <Button Grid.ColumnSpan= \"3\" Click= \"ChangeTheme\" Margin= \"10\" VerticalAlignment= \"Top\" HorizontalAlignment= \"Right\" > <SymbolIcon Symbol= \"Placeholder\" /> </Button> MainPage.xaml.cs public MainPage () { this . InitializeComponent (); this . Loaded += MainPage_LoadedAsync ; App . Current . Suspending += App_SuspendingAsync ; _customerDataProvider = new CustomerDataProvider (); RequestedTheme = App . Current . RequestedTheme == ApplicationTheme . Dark ? ElementTheme . Dark : ElementTheme . Light ; } private void ChangeTheme ( object sender , RoutedEventArgs e ) { this . RequestedTheme = RequestedTheme == ElementTheme . Dark ? ElementTheme . Light : ElementTheme . Dark ; } Color theme The Fluent XAML Theme Editor on the Microsoft Store can generate ThemeResource dictionaries Data binding Use the Binding markup extension to establish a binding on the CustomerDetailControl to the Customer property of customerListView Here, the Customer property is the target property , and the SelectedItem property of customerListView is the source property . So this data binding makes the information in the customerDetailControl (target) dependent on which item is selected (source). MainPage.xaml.cs <controls:CustomerDetailControl x:Name= \"customerDetailControl\" Grid.Row= \"1\" Grid.Column= \"1\" Customer= \"{Binding ElementName=customerListView,Path=SelectedItem,Mode=OneWay}\" > However, the target of a data binding needs to be a Dependency Property . The purpose of dependency properties is to provide a way to compute the value of a property based on the value of other inputs. The Visual Studio snippet for a dependency property is propdp . A dependency property includes a static readonly field of type DependencyProperty and a normal property that works as a frontend for that field by wrapping GetValue and SetValue . We implement the logic to update the controls with the selected customer as a callback function passed as the second argument of the PropertyMetadata object in the dependency property definition. This callback must be a static void function, and as such it has no access to the instantiated objects we have already named with x:Name . However, these objects are retrievable from the DependencyObject and DependencyPropertyChangedEventArgs parameters that are passed to the callback. ( src ) Controls/CustomerDetailControl.xaml.cs public Customer Customer { get { return ( Customer ) GetValue ( CustomerProperty ); } set { SetValue ( CustomerProperty , value ); } } // Using a DependencyProperty as the backing store for Customer. This enables animation, styling, binding, etc... public static readonly DependencyProperty CustomerProperty = DependencyProperty . Register ( \"Customer\" , typeof ( Customer ), typeof ( CustomerDetailControl ), new PropertyMetadata ( null , CustomerChangedCallback )); private static void CustomerChangedCallback ( DependencyObject d , DependencyPropertyChangedEventArgs e ) { if ( d is CustomerDetailControl customerDetailControl ) { var customer = e . NewValue as Customer ; customerDetailControl . txtFirstName . Text = customer ?. FirstName ?? \"\" ; customerDetailControl . txtLastName . Text = customer ?. LastName ?? \"\" ; customerDetailControl . chkDrinksCoffee . IsChecked = customer ?. IsCoffeeDrinker ; } } This bound the customerDetailControl to the item selected in customerListView. Now we implement the data bindings on each control of customerDetailControl. We give the root UserControl an x:Name so that we can refer to it in the binding markup extensions of the children as the value of ElementName . We can also remove the x:Name s of the individual controls, as well any trace of the event handlers! ( src ) Controls/CustomerDetailControl.xaml <UserControl x:Class= \"WiredBrainCoffee.UWP.Controls.CustomerDetailControl\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP.Controls\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" d:DesignHeight= \"300\" d:DesignWidth= \"400\" x:Name= \"root\" > <StackPanel> <TextBox Header= \"First name\" Margin= \"10\" Text= \"{Binding ElementName=root,Path=Customer.FirstName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" /> <TextBox Header= \"Last name\" Margin= \"10\" Text= \"{Binding ElementName=root,Path=Customer.LastName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" /> <CheckBox Content= \"Caffeine fiend\" Margin= \"10\" IsChecked= \"{Binding ElementName=root,Path=Customer.FirstName,Mode=TwoWay}\" /> </StackPanel> </UserControl> ViewModel In the work above, we used the binding markup extension to bind one element to another, using that other element as a data source <TextBlock Text= \"{Binding ElementName=root,...}\" > We can use the MVVM pattern to assign the object to be bound to customerDetailControl as a data context . Every UI element has a DataContext property that can be set, and if it is set to an object then it can be placed there as a default data source that the XAML processor will find as it walks up the element tree. ( src ) This will allow us to simplify the markup, removing the x:Name from the root and the ElementName from the data bindings of the children. First we create the ViewModel, which incorporates some of the logic from the former App_SuspendingAsync and MainPage_LoadedAsync event handler methods. The ViewModel can dispose of the references to customerDetailControl and customerListView and replace them with its own Customers property. ViewModel/MainViewModel.cs using System.Collections.ObjectModel ; using System.Threading.Tasks ; using WiredBrainCoffee.UWP.DataProviders ; using WiredBrainCoffee.UWP.Models ; namespace WiredBrainCoffee.UWP.ViewModel { public class MainViewModel { public ObservableCollection < Customer > Customers { get ; } public MainViewModel () { _customerDataProvider = new CustomerDataProvider (); Customers = new ObservableCollection < Customer >(); } private CustomerDataProvider _customerDataProvider ; public async Task LoadAsync () { Customers . Clear (); var customers = await _customerDataProvider . LoadCustomersAsync (); foreach ( var customer in customers ) { Customers . Add ( customer ); } } public async Task SaveAsync () { await _customerDataProvider . SaveCustomersAsync ( Customers ); } } } To further decouple the ViewModel from the data provider, in order to facilitate testing, we extract an interface from CustomerDataProvider. DataProviders/ICustomerDataProvider.cs using System.Collections.Generic ; using System.Threading.Tasks ; using WiredBrainCoffee.UWP.Models ; namespace WiredBrainCoffee.UWP.DataProviders { public interface ICustomerDataProvider { Task < IEnumerable < Customer >> LoadCustomersAsync (); Task SaveCustomersAsync ( IEnumerable < Customer > customers ); } } Now we implement an ICustomerDataProvider parameter to the ViewModel constructor, and remember to pass in a new data provider as an argument implementing that interface. The private field _customerDataProvider can be removed. ViewModel/MainViewModel.cs private ICustomerDataProvider _customerDataProvider ; public MainViewModel ( ICustomerDataProvider customerDataProvider ) { _customerDataProvider = customerDataProvider ; Customers = new ObservableCollection < Customer >(); } MainPage.xaml.cs this . ViewModel = new MainViewModel ( new CustomerDataProvider ()); DataContext = ViewModel ; Finally, since we have a data context on MainPage, we can set it as a source for the ListView MainPage.xaml <ListView ItemsSource= \"{Binding Customers,Mode=OneWay}\" Grid.Row= \"1\" x:Name= \"customerListView\" DisplayMemberPath= \"FirstName\" > Binding the selected customer At this moment, customerDetailControl is still tied to customerListView's SelectedItem property directly, and not through the ViewModel. To change this, we implement a SelectedCustomer property on the ViewModel that will be bound to both. We reuse the Observable base class that implement the INotifyPropertyChanged interface. This allows us to use the OnPropertyChanged() method in the setter of the new SelectedItem property. We replace the element binding of customerListView with a binding to the SelectedCustomer property in the data context. ViewModel/MainViewModel.cs public class MainViewModel : Observable { private Customer selectedCustomer ; public Customer SelectedCustomer { get { return selectedCustomer ; } set { selectedCustomer = value ; OnPropertyChanged (); } } // ... } MainPage.xaml <ListView ItemsSource= \"{Binding Customers,Mode=OneWay}\" Grid.Row= \"1\" x:Name= \"customerListView\" DisplayMemberPath= \"FirstName\" SelectedItem= \"{Binding SelectedCustomer,Mode=TwoWay}\" > </ListView> DataTemplate At this moment, customerListView is being populated by a single property of each Customer - their first name. If we want to compose more complex information, we can assign DataTemplate to the ListView's ItemTemplate property. This will create the enclosed controls for each element in the ListView. Remember to remove the DisplayMemberPath attribute! <ListView ItemsSource= \"{Binding Customers,Mode=OneWay}\" Grid.Row= \"1\" x:Name= \"customerListView\" SelectedItem= \"{Binding SelectedCustomer,Mode=TwoWay}\" > <ListView.ItemTemplate> <DataTemplate> <StackPanel Orientation= \"Horizontal\" > <TextBlock Text= \"{Binding FirstName}\" /> <TextBlock Text= \"{Binding LastName}\" Margin= \"5 0 0 0\" FontWeight= \"Bold\" /> </StackPanel> </DataTemplate> </ListView.ItemTemplate> </ListView> x:Bind There are two data binding types available in XAML ( src ) Binding markup extension resolves the binding path at runtime. x:Bind resolves the binding path at compile-time , generating C# code and offering better performance and compile-time errors. You can also step into the compiled code, providing a better debugging experience. x:Bind should generally be preferred, however it is available only in UWP. Binding markup extension can have several different data sources, depending on defined attributes. ElementName Source RelativeSource If none of these are defined, then the binding markup extension resolves to the DataContext property. x:Bind, in contrast, binds only to the parent Page or UserControl element. So any property of MainPage will be accessible, and any property of that object will also be accessible using dot notation. Most bindings are easily translated between the two types if the ViewModel has already been implemented as a property of MainPage: Binding markup extension x:Bind public MainPage () { this . InitializeComponent ; this . ViewModel = new MainViewModel (); DataContext = ViewModel ; } <ListView ItemsSource= \"{Binding Customers,Mode=OneWay}\" > <!-- ...--> </ListView> public MainPage () { this . InitializeComponent ; this . ViewModel = new MainViewModel (); // DataContext = ViewModel; } <ListView ItemsSource= \"{x:Bind ViewModel.Customers,Mode=OneWay}\" > <!-- ...--> </ListView> Notably, the default binding mode of the Binding markup extension is OneWay x:Bind is OneTime , although this can be changed by setting x:DefaultBindMode on the root element. Set explicitly Changing default binding mode <Page> <Listview ItemsSource= \"{x:Bind ViewModelCustomers,Mode=OneWay}\" /> </Page> <Page x:DefaultBindMode= \"OneWay\" > <Listview ItemsSource= \"{x:Bind ViewModelCustomers}\" /> </Page> customDetailControl, which previously used the binding markup extension but set the root element as an explicitly named source property, is notably simplified after replacing with x:Bind . We can now directly access the user control's property. Before After <UserControl x:Class= \"WiredBrainCoffee.UWP.Controls.CustomerDetailControl\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP.Controls\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" d:DesignHeight= \"300\" d:DesignWidth= \"400\" x:Name= \"root\" > <StackPanel> <TextBox Header= \"First name\" Margin= \"10\" Text= \"{Binding ElementName=root,Path=Customer.FirstName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" /> <TextBox Header= \"Last name\" Margin= \"10\" Text= \"{Binding ElementName=root,Path=Customer.LastName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" /> <CheckBox Content= \"Caffeine fiend\" Margin= \"10\" IsChecked= \"{Binding ElementName=root,Path=Customer.FirstName,Mode=TwoWay}\" /> </StackPanel> </UserControl> <UserControl x:Class= \"WiredBrainCoffee.UWP.Controls.CustomerDetailControl\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP.Controls\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" d:DesignHeight= \"300\" d:DesignWidth= \"400\" > <StackPanel> <TextBox Header= \"First name\" Margin= \"10\" Text= \"{x:Bind Customer.FirstName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" /> <TextBox Header= \"Last name\" Margin= \"10\" Text= \"{x:Bind Customer.LastName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" /> <CheckBox Content= \"Caffeine fiend\" Margin= \"10\" IsChecked= \"{x:Bind Customer.IsCoffeeDrinker,Mode=TwoWay}\" /> </StackPanel> </UserControl> x:Bind can also be implemented in the ListView's ItemTemplate, so long as the x:DataType attribute is set on DataTemplate. We must also remember to set the Mode binding property, since x:Bind's default is OneTime! ( src ) <ListView Grid.Row= \"1\" ItemsSource= \"{x:Bind ViewModel.Customers}\" SelectedItem= \"{x:Bind ViewModel.SelectedCustomer,Mode=TwoWay}\" > <ListView.ItemTemplate> <DataTemplate x:DataType= \"model:Customer\" > <StackPanel Orientation= \"Horizontal\" > <TextBlock Text= \"{x:Bind FirstName}\" /> <TextBlock Text= \"{x:Bind LastName}\" Margin= \"5 0 0 0\" FontWeight= \"Bold\" /> </StackPanel> </DataTemplate> </ListView.ItemTemplate> </ListView> x:Bind can also hide or reveal controls depending on boolean value. A new boolean property is formed on the ViewModel, and we wire the OnPropertyChanged event handler to it. We also bind this value to the Visibility attribute of customerDetailControl. This will hide the customerDetailControl on application startup before the user selects a customer. ( src ) public Customer SelectedCustomer { get { return selectedCustomer ; } set { if ( selectedCustomer != value ) { selectedCustomer = value ; OnPropertyChanged (); OnPropertyChanged ( nameof ( IsCustomerSelected )); } } } public bool IsCustomerSelected => SelectedCustomer != null ; <controls:CustomerDetailControl x:Name= \"customerDetailControl\" Grid.Row= \"1\" Grid.Column= \"1\" Customer= \"{x:Bind ViewModel.SelectedCustomer,Mode=TwoWay}\" Visibility= \"{x:Bind ViewModel.IsCustomerSelected}\" /> We can also implement a third TextBlock in the ListView's ItemTemplate to show a string depending on the value of the CheckBox. <ListView Grid.Row= \"1\" ItemsSource= \"{x:Bind ViewModel.Customers}\" SelectedItem= \"{x:Bind ViewModel.SelectedCustomer,Mode=TwoWay}\" > <ListView.ItemTemplate> <DataTemplate x:DataType= \"model:Customer\" > <StackPanel Orientation= \"Horizontal\" > <TextBlock Text= \"Dev\" Opacity= \"0.5\" Visibility= \"{x:Bind IsCoffeeDrinker}\" Margin= \"0 0 5 0\" /> <TextBlock Text= \"{x:Bind FirstName}\" /> <TextBlock Text= \"{x:Bind LastName}\" Margin= \"5 0 0 0\" FontWeight= \"Bold\" /> </StackPanel> </DataTemplate> </ListView.ItemTemplate> </ListView> Toggling visibility In order to fully implement the logic of the MVVM pattern, we should move functionality that deals with the logic of the app as a whole to the ViewModel. That would include the Add and Delete buttons. ( src ) Before After <CommandBar> <AppBarButton x:Name= \"AddCustomer\" Click= \"AddCustomer_Click\" Label= \"Add\" > <SymbolIcon Symbol= \"Add\" /> </AppBarButton> <AppBarButton x:Name= \"DeleteCustomer\" Click= \"DeleteCustomer_Click\" Label= \"Delete\" > <SymbolIcon Symbol= \"Delete\" /> </AppBarButton> <AppBarButton x:Name= \"btn_MoveSideBar\" Click= \"btn_MoveSideBar_Click\" Label= \"Move sidebar\" > <SymbolIcon x:Name= \"btn_MoveSideBar_Symbol\" Symbol= \"AlignRight\" /> </AppBarButton> </CommandBar> <CommandBar> <AppBarButton x:Name= \"AddCustomer\" Click= \"{x:Bind ViewModel.AddCustomer_Click}\" Label= \"Add\" > <SymbolIcon Symbol= \"Add\" /> </AppBarButton> <AppBarButton x:Name= \"DeleteCustomer\" Click= \"{x:Bind ViewModel.DeleteCustomer_Click}\" Label= \"Delete\" > <SymbolIcon Symbol= \"Delete\" /> </AppBarButton> <AppBarButton x:Name= \"btn_MoveSideBar\" Click= \"btn_MoveSideBar_Click\" Label= \"Move sidebar\" > <SymbolIcon x:Name= \"btn_MoveSideBar_Symbol\" Symbol= \"AlignRight\" /> </AppBarButton> </CommandBar> Styling You can define a style that has to be used more than once by declaring a Style element on a UserControl's Resources property. ( src ) <UserControl> <UserControl.Resources> <Style x:Key= \"myTextBoxStyle\" TargetType= \"TextBox\" > <Style.Setters> <Setter Property= \"Margin\" Value= \"10\" /> <Setter Property= \"CornerRadius\" Value= \"10\" /> </Style.Setters> </Style> </UserControl.Resources> </UserControl> This Style can then be used as a StaticResource, setting the value of the Style attribute.","title":"\u2615 Wired Brain Coffee"},{"location":"Coding/Wired%20Brain%20Coffee/#wired-brain-coffee","text":"","title":"\u2615 Wired Brain Coffee"},{"location":"Coding/Wired%20Brain%20Coffee/#basic-layout","text":"MainPage.xaml <Window x:Class= \"WiredBrainCoffee.UWP.MainWindow\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" > <Grid> <Grid.RowDefinitions> <RowDefinition Height= \"Auto\" /> <RowDefinition/> </Grid.RowDefinitions> <Grid.ColumnDefinitions> <ColumnDefinition Width= \"350\" /> <ColumnDefinition Width= \"*\" /> </Grid.ColumnDefinitions> <Border Grid.ColumnSpan= \"2\" Background= \"#f05a28\" > <StackPanel Orientation= \"Horizontal\" HorizontalAlignment= \"Center\" > <Image Height= \"90\" Margin= \"5\" Source= \"/Images/logo.png\" /> <TextBlock Text= \"Employee Manager\" FontSize= \"40\" VerticalAlignment= \"Center\" /> </StackPanel> </Border> <!-- Sidebar --> <Grid Grid.Row= \"1\" > <Grid.RowDefinitions> <RowDefinition Height= \"Auto\" /> <RowDefinition /> </Grid.RowDefinitions> <Button Content= \"Refresh\" Margin= \"10\" /> <ListView Grid.Row= \"1\" /> </Grid> <!--MainArea--> <Grid Grid.Row= \"1\" Grid.Column= \"1\" > <Grid.RowDefinitions> <RowDefinition Height= \"Auto\" /> <RowDefinition Height= \"Auto\" /> <RowDefinition Height= \"Auto\" /> <RowDefinition Height= \"Auto\" /> <RowDefinition Height= \"*\" /> </Grid.RowDefinitions> <TextBox Header= \"Firstname\" Margin= \"10\" /> <DatePicker Grid.Row= \"1\" Header= \"Entry date\" Margin= \"10\" /> <ComboBox Grid.Row= \"2\" Header= \"Job role\" Margin= \"10\" HorizontalAlignment= \"Stretch\" /> <CheckBox Grid.Row= \"3\" Content= \"Is coffee drinker?\" Margin= \"10\" /> <Button Grid.Row= \"4\" Content= \"Save\" Margin= \"10 10 10 30\" VerticalAlignment= \"Bottom\" HorizontalAlignment= \"Left\" /> </Grid> </Grid> </Window>","title":"Basic layout"},{"location":"Coding/Wired%20Brain%20Coffee/#custom-control","text":"Controls/HeaderControl.xaml <UserControl x:Class= \"WiredBrainCoffee.UWP.Controls.HeaderControl\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP.Controls\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" d:DesignHeight= \"300\" d:DesignWidth= \"400\" > <Border Background= \"#F05A28\" > <Grid> <Grid.ColumnDefinitions> <ColumnDefinition/> <ColumnDefinition Width= \"Auto\" /> </Grid.ColumnDefinitions> <StackPanel Orientation= \"Horizontal\" HorizontalAlignment= \"Center\" > <Image Source= \"/Images/WiredBrainLogo.png\" Height= \"90\" /> <TextBlock Text= \"Wired Brain Coffee\" FontSize= \"40\" VerticalAlignment= \"Center\" /> </StackPanel> <Button HorizontalAlignment= \"Right\" Grid.Column= \"1\" Margin= \"10\" > <SymbolIcon Symbol= \"AlignRight\" /> </Button> </Grid> </Border> </UserControl> MainPage.xaml <Page x:Class= \"WiredBrainCoffee.UWP.MainPage\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" xmlns:controls= \"using:WiredBrainCoffee.UWP.Controls\" mc:Ignorable= \"d\" Background= \"{ThemeResource ApplicationPageBackgroundThemeBrush}\" > <Grid> <Grid.RowDefinitions> <RowDefinition Height= \"Auto\" /> <RowDefinition/> </Grid.RowDefinitions> <Grid.ColumnDefinitions> <ColumnDefinition Width= \"350\" /> <ColumnDefinition/> </Grid.ColumnDefinitions> <!-- Header --> <controls:HeaderControl Grid.ColumnSpan= \"2\" /> <!-- Sidebar --> <Grid Grid.Row= \"1\" > <Grid.RowDefinitions> <RowDefinition Height= \"Auto\" /> <RowDefinition/> </Grid.RowDefinitions> <StackPanel Orientation= \"Horizontal\" > <Button Margin= \"10\" > <SymbolIcon Symbol= \"AddFriend\" /> </Button> <Button Margin= \"10\" > <SymbolIcon Symbol= \"Delete\" /> </Button> </StackPanel> <ListView Grid.Row= \"1\" > <ListViewItem> Aristotle </ListViewItem> <ListViewItem> Euclid </ListViewItem> <ListViewItem> Plato </ListViewItem> <ListViewItem> Socrates </ListViewItem> </ListView> </Grid> <StackPanel Grid.Row= \"1\" Grid.Column= \"1\" > <TextBox Header= \"First name\" Margin= \"10\" /> <TextBox Header= \"Last name\" Margin= \"10\" /> <CheckBox Content= \"Drinks coffee\" Margin= \"10\" /> </StackPanel> </Grid> </Page>","title":"Custom control"},{"location":"Coding/Wired%20Brain%20Coffee/#sidebar","text":"Setting an x:Name attribute on an element allows it to be manipulated in C#. ( src ) MainPage.xaml.cs private void btn_MoveSideBar_Click ( object sender , RoutedEventArgs e ) { int column = Grid . GetColumn ( customerListGrid ); int newcolumn ; if ( column == 0 ) { newcolumn = 2 ; btn_MoveSideBar_Symbol . Symbol = Symbol . AlignLeft ; } else { newcolumn = 0 ; btn_MoveSideBar_Symbol . Symbol = Symbol . AlignRight ; } Grid . SetColumn ( customerListGrid , newcolumn ); }","title":"Sidebar"},{"location":"Coding/Wired%20Brain%20Coffee/#data-provider","text":"A data provider class accomodates the need for mock data while also loosely coupling the data with the source. ( src ) DataProviders/CustomerDataProvider.cs using Newtonsoft.Json ; using System ; using System.Collections.Generic ; using System.Threading.Tasks ; using System.IO ; using Windows.Storage ; using Windows.Storage.Streams ; using WiredBrainCoffee.UWP.Models ; namespace WiredBrainCoffee.UWP.DataProviders { class CustomerDataProvider { private static readonly string _customersFileName = \"customers.json\" ; private static readonly StorageFolder _localFolder = ApplicationData . Current . LocalFolder ; public async Task < IEnumerable < Customer >> LoadCustomersAsync () { var storageFile = await _localFolder . TryGetItemAsync ( _customersFileName ) as StorageFile ; List < Customer > customerList = null ; if ( storageFile == null ) { customerList = new List < Customer > { new Customer { FirstName = \"Clark\" , LastName = \"Kent\" , IsCoffeeDrinker = true }, new Customer { FirstName = \"Bruce\" , LastName = \"Wayne\" , IsCoffeeDrinker = false }, new Customer { FirstName = \"Diana\" , LastName = \"Prince\" , IsCoffeeDrinker = true } }; } else { using ( var stream = await storageFile . OpenAsync ( FileAccessMode . Read )) { using ( var dataReader = new DataReader ( stream )) { await dataReader . LoadAsync (( uint ) stream . Size ); var json = dataReader . ReadString (( uint ) stream . Size ); customerList = JsonConvert . DeserializeObject < List < Customer >>( json ); } } } return customerList ; } public async Task SaveCustomersAsync ( IEnumerable < Customer > customers ) { var storageFile = await _localFolder . CreateFileAsync ( _customersFileName , CreationCollisionOption . ReplaceExisting ); using ( var stream = await storageFile . OpenAsync ( FileAccessMode . ReadWrite )) { using ( var dataWriter = new DataWriter ( stream )) { var json = JsonConvert . SerializeObject ( customers , Formatting . Indented ); dataWriter . WriteString ( json ); await dataWriter . StoreAsync (); } } } } } Models/Customer.cs namespace WiredBrainCoffee.UWP.Models { public class Customer { public string FirstName { get ; set ; } public string LastName { get ; set ; } public bool IsCoffeeDrinker { get ; set ; } } } Event hooks are used to populate the ListView with data from the data provider. MainPage.xaml.cs public MainPage () { this . InitializeComponent (); this . Loaded += MainPage_LoadedAsync ; App . Current . Suspending += App_SuspendingAsync ; _customerDataProvider = new CustomerDataProvider (); } private async void App_SuspendingAsync ( object sender , Windows . ApplicationModel . SuspendingEventArgs e ) { var deferral = e . SuspendingOperation . GetDeferral (); await _customerDataProvider . SaveCustomersAsync ( customerListView . Items . OfType < Customer >()); deferral . Complete (); } private async void MainPage_LoadedAsync ( object sender , RoutedEventArgs e ) { customerListView . Items . Clear (); var customers = await _customerDataProvider . LoadCustomersAsync (); foreach ( var customer in customers ) { customerListView . Items . Add ( customer ); } }","title":"Data provider"},{"location":"Coding/Wired%20Brain%20Coffee/#data-binding-using-events","text":"Synchronize the customer detail textboxes to the selected item in the ListView. A rough form of data binding is possible with event handling . ( src ) First implement an event handler when the ListView.SelectionChanged event is fired. MainPage.xaml <ListView Grid.Row= \"1\" x:Name= \"customerListView\" DisplayMemberPath= \"FirstName\" SelectionChanged= \"customerListView_SelectionChanged\" /> MainPage.xaml.cs private void customerListView_SelectionChanged ( object sender , SelectionChangedEventArgs e ) { var customer = customerListView . SelectedItem as Customer ; txtFirstName . Text = customer ?. FirstName ?? \"\" ; txtLastName . Text = customer ?. LastName ?? \"\" ; chkDrinksCoffee . IsChecked = customer ?. IsCoffeeDrinker ; } Implement event handlers on the controls in the main area ( TextBox.TextChanged and CheckBox.Checked and CheckBox.Unchedked events) when changes are made. MainPage.xaml <StackPanel Grid.Row= \"1\" Grid.Column= \"1\" > <TextBox x:Name= \"txtFirstName\" Header= \"First name\" Margin= \"10\" TextChanged= \"UpdateCustomer\" /> <TextBox x:Name= \"txtLastName\" Header= \"Last name\" Margin= \"10\" TextChanged= \"UpdateCustomer\" /> <CheckBox x:Name= \"chkDrinksCoffee\" Content= \"Caffeine fiend\" Margin= \"10\" Checked= \"UpdateCustomer\" Unchecked= \"UpdateCustomer\" /> </StackPanel> MainPage.xaml.cs private void UpdateCustomer ( object sender , RoutedEventArgs e ) { var customer = customerListView . SelectedItem as Customer ; if ( customer != null ) { customer . FirstName = txtFirstName . Text ; customer . LastName = txtLastName . Text ; customer . IsCoffeeDrinker = chkDrinksCoffee . IsChecked . GetValueOrDefault (); } }","title":"Data binding using events"},{"location":"Coding/Wired%20Brain%20Coffee/#update-listview","text":"ListView still won't update as a result of changes. In order to implement this, you have to raise the PropertyChanged event. We implement the INotifyPropertyChanged interface and make it the base class of Customer. Also, we implement a helper method to fire the event handler whenever a property is changed. This helper is invoked every time a property is set. The CallerMemberName attribute passes the name of the calling property as a string, and allows us to avoid placing typeof(FirstName) , etc with every invocation. ( src ) Models/Customer.cs using System.ComponentModel ; using System.Runtime.CompilerServices ; namespace WiredBrainCoffee.UWP.Models { public class Observable : INotifyPropertyChanged { public event PropertyChangedEventHandler PropertyChanged ; protected virtual void OnPropertyChanged ([ CallerMemberName ] string propertyName = null ) { PropertyChanged ?. Invoke ( this , new PropertyChangedEventArgs ( propertyName )); } } public class Customer : Observable { private string firstName ; private string lastName ; private bool isCoffeeDrinker ; public string FirstName { get => firstName ; set { firstName = value ; OnPropertyChanged (); } } public string LastName { get => lastName ; set { lastName = value ; OnPropertyChanged (); } } public bool IsCoffeeDrinker { get => isCoffeeDrinker ; set { isCoffeeDrinker = value ; OnPropertyChanged (); } } } }","title":"Update ListView"},{"location":"Coding/Wired%20Brain%20Coffee/#addremove-customers","text":"Implement event handlers for the Add and Delete buttons. ( src ) MainPage.xaml.cs private void DeleteCustomer_Click ( object sender , RoutedEventArgs e ) { var customer = customerListView . SelectedItem ; if ( customer != null ) { customerListView . Items . Remove ( customer ); } } private void AddCustomer_Click ( object sender , RoutedEventArgs e ) { var customer = new Customer { FirstName = \"New\" }; customerListView . Items . Add ( customer ); customerListView . SelectedItem = customer ; } MainPage.xaml <Button x:Name= \"AddCustomer\" Margin= \"10\" Click= \"AddCustomer_Click\" > <SymbolIcon Symbol= \"AddFriend\" /> </Button> <Button x:Name= \"DeleteCustomer\" Margin= \"10\" Click= \"DeleteCustomer_Click\" > <SymbolIcon Symbol= \"Delete\" /> </Button>","title":"Add/remove customers"},{"location":"Coding/Wired%20Brain%20Coffee/#custom-control_1","text":"We abstract controls in the main area of the app into a new CustomerDetailControl. As before, we cut the UI elements into a new XAML file and reference the new control in MainPage. However, now, customerListView is inaccessible. The lynchpin is forming a property on customerDetailControl that is populated with the customer object by the SelectionChanged event handler MainPage.xaml.cs private void customerListView_SelectionChanged ( object sender , SelectionChangedEventArgs e ) { var customer = customerListView . SelectedItem as Customer ; customerDetailControl . Customer = customer ; } Controls/CustomerDetailControl.xaml <UserControl x:Class= \"WiredBrainCoffee.UWP.Controls.CustomerDetailControl\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP.Controls\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" d:DesignHeight= \"300\" d:DesignWidth= \"400\" > <StackPanel> <TextBox x:Name= \"txtFirstName\" Header= \"First name\" Margin= \"10\" TextChanged= \"UpdateCustomer\" /> <TextBox x:Name= \"txtLastName\" Header= \"Last name\" Margin= \"10\" TextChanged= \"UpdateCustomer\" /> <CheckBox x:Name= \"chkDrinksCoffee\" Content= \"Caffeine fiend\" Margin= \"10\" Checked= \"UpdateCustomer\" Unchecked= \"UpdateCustomer\" /> </StackPanel> </UserControl> Controls/CustomerDetailControl.xaml.cs using Windows.UI.Xaml ; using Windows.UI.Xaml.Controls ; using WiredBrainCoffee.UWP.Models ; namespace WiredBrainCoffee.UWP.Controls { public sealed partial class CustomerDetailControl : UserControl { public CustomerDetailControl () { this . InitializeComponent (); } private Customer _customer ; public Customer Customer { get { return _customer ; } set { _customer = value ; txtFirstName . Text = _customer ?. FirstName ?? \"\" ; txtLastName . Text = _customer ?. LastName ?? \"\" ; chkDrinksCoffee . IsChecked = _customer ?. IsCoffeeDrinker ; } } private void UpdateCustomer ( object sender , RoutedEventArgs e ) { if ( Customer != null ) { Customer . FirstName = txtFirstName . Text ; Customer . LastName = txtLastName . Text ; Customer . IsCoffeeDrinker = chkDrinksCoffee . IsChecked . GetValueOrDefault (); } } } }","title":"Custom control"},{"location":"Coding/Wired%20Brain%20Coffee/#assign-mock-content","text":"We combine two different namespace mappings (one for the Customer model and another for the CustomerDetailControl) to prepopulate the CustomerDetailControl with a customer defined in XAML. Because CustomerDetailControl exposes a public Customer property, this data can be assigned to the Customer property using property-element syntax. ( src ) MainPage.xaml <controls:CustomerDetailControl x:Name= \"customerDetailControl\" Grid.Row= \"1\" Grid.Column= \"1\" > <controls:CustomerDetailControl.Customer> <model:Customer FirstName= \"Clark\" LastName= \"Kent\" IsCoffeeDrinker= \"True\" /> </controls:CustomerDetailControl.Customer> </controls:CustomerDetailControl> In order to be able to assign the customer as direct content without specifying the property explicitly, the custom control class has to be decorated with the ContentProperty attribute. This is because by default any direct child is assigned to the Content property, which does not exist for this custom control. Using the ContentProperty allows us to specify a property to which to assign direct children. Controls/CustomerDetailControl.xaml.cs [ContentProperty(Name = nameof(Customer))] public sealed partial class CustomerDetailControl : UserControl { /* ... */ } MainPage.xaml <controls:CustomerDetailControl x:Name= \"customerDetailControl\" Grid.Row= \"1\" Grid.Column= \"1\" > <model:Customer FirstName= \"Clark\" LastName= \"Kent\" IsCoffeeDrinker= \"True\" /> </controls:CustomerDetailControl>","title":"Assign mock content"},{"location":"Coding/Wired%20Brain%20Coffee/#xaml-type-conversion","text":"Passing the customer as an attribute requires custom logic to parse the string. The target model is then decorated with the CreateFromString attribute. This is only for custom classes: primitive types and enumerations can be parsed by the XAML processor automatically. Models/CustomerConverter.cs namespace WiredBrainCoffee.UWP.Models { public static class CustomerConverter { public static Customer ParseStringAsCustomer ( string s ) { string [] values = s . Split ( ';' ); return new Customer { FirstName = values [ 0 ], LastName = values [ 1 ], IsCoffeeDrinker = bool . Parse ( values [ 2 ]) }; } } } Models/Customer.cs [CreateFromString(MethodName =\"WiredBrainCoffee.UWP.Models.CustomerConverter.ParseCustomerFromString\")] public class Customer : Observable { /* ... */ }","title":"XAML Type conversion"},{"location":"Coding/Wired%20Brain%20Coffee/#staticresource","text":"You can use the StaticResource Markup Extension to define the equivalent of XAML variables to store elements for attribution using attribute syntax. Every UI element has a property named Resources to which you can assign elements. Unlike the Items property of a ListView, however, this property is a Dictionary type, which means you must specify a key for these values (i.e. specify x:Key ). Because the XAML processor looks for resources as it crawls up the element tree, these resources can be organized at any level of the application, even in the App.xaml where it will become available to other files: ( src ) MainPage.xaml <Page.Resources> <model:Customer x:Key= \"Shazam\" FirstName= \"William\" LastName= \"William Batson\" IsCoffeeDrinker= \"false\" /> </Page.Resources> <!-- ... --> <controls:CustomerDetailControl Customer= \"{StaticResource Shazam}\" /> However, mocking data in XAML is an anti-pattern; Resource dictionaries are typically used for colors and predefined strings. Resource dictionaries are consolidated into their own files: ( src ) Resources/Brushes.xaml <ResourceDictionary xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" > <SolidColorBrush x:Key= \"customerListBackgroundBrush\" Color= \"#EEEEEE\" /> </ResourceDictionary> Resources/Strings.xaml <ResourceDictionary xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" > <x:String x:Key= \"applicationTitle\" > Wired Brain Coffee </x:String> </ResourceDictionary> These can then be referenced from App.xaml and are available for assignment in any appropriate attribute App.xaml <Application x:Class= \"WiredBrainCoffee.UWP.App\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP\" > <Application.Resources> <ResourceDictionary> <ResourceDictionary.MergedDictionaries> <ResourceDictionary Source= \"Resources/Brushes.xaml\" /> <ResourceDictionary Source= \"Resources/Strings.xaml\" /> </ResourceDictionary.MergedDictionaries> </ResourceDictionary> </Application.Resources> </Application> Controls/HeaderControl.xaml <UserControl x:Class= \"WiredBrainCoffee.UWP.Controls.HeaderControl\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP.Controls\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" d:DesignHeight= \"300\" d:DesignWidth= \"400\" > <Border Background= \"#F05A28\" > <Grid> <Grid.ColumnDefinitions> <ColumnDefinition/> <ColumnDefinition Width= \"Auto\" /> </Grid.ColumnDefinitions> <StackPanel Orientation= \"Horizontal\" HorizontalAlignment= \"Center\" > <Image Source= \"/Images/WiredBrainLogo.png\" Height= \"90\" /> <TextBlock Text= \"{StaticResource applicationTitle}\" FontSize= \"40\" VerticalAlignment= \"Center\" /> </StackPanel> <Button x:Name= \"ButtonMove\" HorizontalAlignment= \"Right\" Grid.Column= \"1\" Margin= \"10\" Click= \"ButtonMove_Click\" > <SymbolIcon x:Name= \"ButtonMove_Symbol\" Symbol= \"AlignRight\" /> </Button> </Grid> </Border> </UserControl>","title":"StaticResource"},{"location":"Coding/Wired%20Brain%20Coffee/#themeresource","text":"The ThemeResource Markup Extension makes UWP-specific theme resource dictionaries available. These same resources are available using StaticResource, but with ThemeResource they will be updated if the user changes his Windows theme from light to dark. ( src ) Resources/Brushes.xaml <ResourceDictionary xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" > <ResourceDictionary.ThemeDictionaries> <ResourceDictionary x:Key= \"Dark\" > <SolidColorBrush x:Key= \"customerListBackgroundBrush\" Color= \"#222222\" /> </ResourceDictionary> <ResourceDictionary x:Key= \"Light\" > <SolidColorBrush x:Key= \"customListBackgroundBrush\" Color= \"#EEEEEE\" /> </ResourceDictionary> </ResourceDictionary.ThemeDictionaries> </ResourceDictionary>","title":"ThemeResource"},{"location":"Coding/Wired%20Brain%20Coffee/#theme-selection","text":"A specific theme can be specified at any element by specifying a RequestedTheme attribute. However, this property cannot be changed at runtime. App.xaml <Application x:Class= \"WiredBrainCoffee.UWP.App\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP\" RequestedTheme= \"Dark\" > <Application.Resources> <ResourceDictionary> <ResourceDictionary.MergedDictionaries> <ResourceDictionary Source= \"Resources/Brushes.xaml\" /> <ResourceDictionary Source= \"Resources/Strings.xaml\" /> </ResourceDictionary.MergedDictionaries> </ResourceDictionary> </Application.Resources> </Application> A button to manually change theme involves simply assigning an ElementTheme enum value to the MainPage's RequestedTheme property. However, because on startup this has an ApplicationTheme enum value that is not evailable in ElementTheme , the MainPage's constructor must be changed to set the correct theme enum. Without this change, the first click after the application's startup will not change the theme at all, only set the correct ElementTheme . ( src ) MainPage.xaml <!-- Header --> <controls:HeaderControl Grid.ColumnSpan= \"3\" /> <Button Grid.ColumnSpan= \"3\" Click= \"ChangeTheme\" Margin= \"10\" VerticalAlignment= \"Top\" HorizontalAlignment= \"Right\" > <SymbolIcon Symbol= \"Placeholder\" /> </Button> MainPage.xaml.cs public MainPage () { this . InitializeComponent (); this . Loaded += MainPage_LoadedAsync ; App . Current . Suspending += App_SuspendingAsync ; _customerDataProvider = new CustomerDataProvider (); RequestedTheme = App . Current . RequestedTheme == ApplicationTheme . Dark ? ElementTheme . Dark : ElementTheme . Light ; } private void ChangeTheme ( object sender , RoutedEventArgs e ) { this . RequestedTheme = RequestedTheme == ElementTheme . Dark ? ElementTheme . Light : ElementTheme . Dark ; }","title":"Theme selection"},{"location":"Coding/Wired%20Brain%20Coffee/#color-theme","text":"The Fluent XAML Theme Editor on the Microsoft Store can generate ThemeResource dictionaries","title":"Color theme"},{"location":"Coding/Wired%20Brain%20Coffee/#data-binding","text":"Use the Binding markup extension to establish a binding on the CustomerDetailControl to the Customer property of customerListView Here, the Customer property is the target property , and the SelectedItem property of customerListView is the source property . So this data binding makes the information in the customerDetailControl (target) dependent on which item is selected (source). MainPage.xaml.cs <controls:CustomerDetailControl x:Name= \"customerDetailControl\" Grid.Row= \"1\" Grid.Column= \"1\" Customer= \"{Binding ElementName=customerListView,Path=SelectedItem,Mode=OneWay}\" > However, the target of a data binding needs to be a Dependency Property . The purpose of dependency properties is to provide a way to compute the value of a property based on the value of other inputs. The Visual Studio snippet for a dependency property is propdp . A dependency property includes a static readonly field of type DependencyProperty and a normal property that works as a frontend for that field by wrapping GetValue and SetValue . We implement the logic to update the controls with the selected customer as a callback function passed as the second argument of the PropertyMetadata object in the dependency property definition. This callback must be a static void function, and as such it has no access to the instantiated objects we have already named with x:Name . However, these objects are retrievable from the DependencyObject and DependencyPropertyChangedEventArgs parameters that are passed to the callback. ( src ) Controls/CustomerDetailControl.xaml.cs public Customer Customer { get { return ( Customer ) GetValue ( CustomerProperty ); } set { SetValue ( CustomerProperty , value ); } } // Using a DependencyProperty as the backing store for Customer. This enables animation, styling, binding, etc... public static readonly DependencyProperty CustomerProperty = DependencyProperty . Register ( \"Customer\" , typeof ( Customer ), typeof ( CustomerDetailControl ), new PropertyMetadata ( null , CustomerChangedCallback )); private static void CustomerChangedCallback ( DependencyObject d , DependencyPropertyChangedEventArgs e ) { if ( d is CustomerDetailControl customerDetailControl ) { var customer = e . NewValue as Customer ; customerDetailControl . txtFirstName . Text = customer ?. FirstName ?? \"\" ; customerDetailControl . txtLastName . Text = customer ?. LastName ?? \"\" ; customerDetailControl . chkDrinksCoffee . IsChecked = customer ?. IsCoffeeDrinker ; } } This bound the customerDetailControl to the item selected in customerListView. Now we implement the data bindings on each control of customerDetailControl. We give the root UserControl an x:Name so that we can refer to it in the binding markup extensions of the children as the value of ElementName . We can also remove the x:Name s of the individual controls, as well any trace of the event handlers! ( src ) Controls/CustomerDetailControl.xaml <UserControl x:Class= \"WiredBrainCoffee.UWP.Controls.CustomerDetailControl\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP.Controls\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" d:DesignHeight= \"300\" d:DesignWidth= \"400\" x:Name= \"root\" > <StackPanel> <TextBox Header= \"First name\" Margin= \"10\" Text= \"{Binding ElementName=root,Path=Customer.FirstName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" /> <TextBox Header= \"Last name\" Margin= \"10\" Text= \"{Binding ElementName=root,Path=Customer.LastName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" /> <CheckBox Content= \"Caffeine fiend\" Margin= \"10\" IsChecked= \"{Binding ElementName=root,Path=Customer.FirstName,Mode=TwoWay}\" /> </StackPanel> </UserControl>","title":"Data binding"},{"location":"Coding/Wired%20Brain%20Coffee/#viewmodel","text":"In the work above, we used the binding markup extension to bind one element to another, using that other element as a data source <TextBlock Text= \"{Binding ElementName=root,...}\" > We can use the MVVM pattern to assign the object to be bound to customerDetailControl as a data context . Every UI element has a DataContext property that can be set, and if it is set to an object then it can be placed there as a default data source that the XAML processor will find as it walks up the element tree. ( src ) This will allow us to simplify the markup, removing the x:Name from the root and the ElementName from the data bindings of the children. First we create the ViewModel, which incorporates some of the logic from the former App_SuspendingAsync and MainPage_LoadedAsync event handler methods. The ViewModel can dispose of the references to customerDetailControl and customerListView and replace them with its own Customers property. ViewModel/MainViewModel.cs using System.Collections.ObjectModel ; using System.Threading.Tasks ; using WiredBrainCoffee.UWP.DataProviders ; using WiredBrainCoffee.UWP.Models ; namespace WiredBrainCoffee.UWP.ViewModel { public class MainViewModel { public ObservableCollection < Customer > Customers { get ; } public MainViewModel () { _customerDataProvider = new CustomerDataProvider (); Customers = new ObservableCollection < Customer >(); } private CustomerDataProvider _customerDataProvider ; public async Task LoadAsync () { Customers . Clear (); var customers = await _customerDataProvider . LoadCustomersAsync (); foreach ( var customer in customers ) { Customers . Add ( customer ); } } public async Task SaveAsync () { await _customerDataProvider . SaveCustomersAsync ( Customers ); } } } To further decouple the ViewModel from the data provider, in order to facilitate testing, we extract an interface from CustomerDataProvider. DataProviders/ICustomerDataProvider.cs using System.Collections.Generic ; using System.Threading.Tasks ; using WiredBrainCoffee.UWP.Models ; namespace WiredBrainCoffee.UWP.DataProviders { public interface ICustomerDataProvider { Task < IEnumerable < Customer >> LoadCustomersAsync (); Task SaveCustomersAsync ( IEnumerable < Customer > customers ); } } Now we implement an ICustomerDataProvider parameter to the ViewModel constructor, and remember to pass in a new data provider as an argument implementing that interface. The private field _customerDataProvider can be removed. ViewModel/MainViewModel.cs private ICustomerDataProvider _customerDataProvider ; public MainViewModel ( ICustomerDataProvider customerDataProvider ) { _customerDataProvider = customerDataProvider ; Customers = new ObservableCollection < Customer >(); } MainPage.xaml.cs this . ViewModel = new MainViewModel ( new CustomerDataProvider ()); DataContext = ViewModel ; Finally, since we have a data context on MainPage, we can set it as a source for the ListView MainPage.xaml <ListView ItemsSource= \"{Binding Customers,Mode=OneWay}\" Grid.Row= \"1\" x:Name= \"customerListView\" DisplayMemberPath= \"FirstName\" >","title":"ViewModel"},{"location":"Coding/Wired%20Brain%20Coffee/#binding-the-selected-customer","text":"At this moment, customerDetailControl is still tied to customerListView's SelectedItem property directly, and not through the ViewModel. To change this, we implement a SelectedCustomer property on the ViewModel that will be bound to both. We reuse the Observable base class that implement the INotifyPropertyChanged interface. This allows us to use the OnPropertyChanged() method in the setter of the new SelectedItem property. We replace the element binding of customerListView with a binding to the SelectedCustomer property in the data context. ViewModel/MainViewModel.cs public class MainViewModel : Observable { private Customer selectedCustomer ; public Customer SelectedCustomer { get { return selectedCustomer ; } set { selectedCustomer = value ; OnPropertyChanged (); } } // ... } MainPage.xaml <ListView ItemsSource= \"{Binding Customers,Mode=OneWay}\" Grid.Row= \"1\" x:Name= \"customerListView\" DisplayMemberPath= \"FirstName\" SelectedItem= \"{Binding SelectedCustomer,Mode=TwoWay}\" > </ListView>","title":"Binding the selected customer"},{"location":"Coding/Wired%20Brain%20Coffee/#datatemplate","text":"At this moment, customerListView is being populated by a single property of each Customer - their first name. If we want to compose more complex information, we can assign DataTemplate to the ListView's ItemTemplate property. This will create the enclosed controls for each element in the ListView. Remember to remove the DisplayMemberPath attribute! <ListView ItemsSource= \"{Binding Customers,Mode=OneWay}\" Grid.Row= \"1\" x:Name= \"customerListView\" SelectedItem= \"{Binding SelectedCustomer,Mode=TwoWay}\" > <ListView.ItemTemplate> <DataTemplate> <StackPanel Orientation= \"Horizontal\" > <TextBlock Text= \"{Binding FirstName}\" /> <TextBlock Text= \"{Binding LastName}\" Margin= \"5 0 0 0\" FontWeight= \"Bold\" /> </StackPanel> </DataTemplate> </ListView.ItemTemplate> </ListView>","title":"DataTemplate"},{"location":"Coding/Wired%20Brain%20Coffee/#xbind","text":"There are two data binding types available in XAML ( src ) Binding markup extension resolves the binding path at runtime. x:Bind resolves the binding path at compile-time , generating C# code and offering better performance and compile-time errors. You can also step into the compiled code, providing a better debugging experience. x:Bind should generally be preferred, however it is available only in UWP. Binding markup extension can have several different data sources, depending on defined attributes. ElementName Source RelativeSource If none of these are defined, then the binding markup extension resolves to the DataContext property. x:Bind, in contrast, binds only to the parent Page or UserControl element. So any property of MainPage will be accessible, and any property of that object will also be accessible using dot notation. Most bindings are easily translated between the two types if the ViewModel has already been implemented as a property of MainPage: Binding markup extension x:Bind public MainPage () { this . InitializeComponent ; this . ViewModel = new MainViewModel (); DataContext = ViewModel ; } <ListView ItemsSource= \"{Binding Customers,Mode=OneWay}\" > <!-- ...--> </ListView> public MainPage () { this . InitializeComponent ; this . ViewModel = new MainViewModel (); // DataContext = ViewModel; } <ListView ItemsSource= \"{x:Bind ViewModel.Customers,Mode=OneWay}\" > <!-- ...--> </ListView> Notably, the default binding mode of the Binding markup extension is OneWay x:Bind is OneTime , although this can be changed by setting x:DefaultBindMode on the root element. Set explicitly Changing default binding mode <Page> <Listview ItemsSource= \"{x:Bind ViewModelCustomers,Mode=OneWay}\" /> </Page> <Page x:DefaultBindMode= \"OneWay\" > <Listview ItemsSource= \"{x:Bind ViewModelCustomers}\" /> </Page> customDetailControl, which previously used the binding markup extension but set the root element as an explicitly named source property, is notably simplified after replacing with x:Bind . We can now directly access the user control's property. Before After <UserControl x:Class= \"WiredBrainCoffee.UWP.Controls.CustomerDetailControl\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP.Controls\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" d:DesignHeight= \"300\" d:DesignWidth= \"400\" x:Name= \"root\" > <StackPanel> <TextBox Header= \"First name\" Margin= \"10\" Text= \"{Binding ElementName=root,Path=Customer.FirstName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" /> <TextBox Header= \"Last name\" Margin= \"10\" Text= \"{Binding ElementName=root,Path=Customer.LastName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" /> <CheckBox Content= \"Caffeine fiend\" Margin= \"10\" IsChecked= \"{Binding ElementName=root,Path=Customer.FirstName,Mode=TwoWay}\" /> </StackPanel> </UserControl> <UserControl x:Class= \"WiredBrainCoffee.UWP.Controls.CustomerDetailControl\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP.Controls\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" d:DesignHeight= \"300\" d:DesignWidth= \"400\" > <StackPanel> <TextBox Header= \"First name\" Margin= \"10\" Text= \"{x:Bind Customer.FirstName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" /> <TextBox Header= \"Last name\" Margin= \"10\" Text= \"{x:Bind Customer.LastName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" /> <CheckBox Content= \"Caffeine fiend\" Margin= \"10\" IsChecked= \"{x:Bind Customer.IsCoffeeDrinker,Mode=TwoWay}\" /> </StackPanel> </UserControl> x:Bind can also be implemented in the ListView's ItemTemplate, so long as the x:DataType attribute is set on DataTemplate. We must also remember to set the Mode binding property, since x:Bind's default is OneTime! ( src ) <ListView Grid.Row= \"1\" ItemsSource= \"{x:Bind ViewModel.Customers}\" SelectedItem= \"{x:Bind ViewModel.SelectedCustomer,Mode=TwoWay}\" > <ListView.ItemTemplate> <DataTemplate x:DataType= \"model:Customer\" > <StackPanel Orientation= \"Horizontal\" > <TextBlock Text= \"{x:Bind FirstName}\" /> <TextBlock Text= \"{x:Bind LastName}\" Margin= \"5 0 0 0\" FontWeight= \"Bold\" /> </StackPanel> </DataTemplate> </ListView.ItemTemplate> </ListView> x:Bind can also hide or reveal controls depending on boolean value. A new boolean property is formed on the ViewModel, and we wire the OnPropertyChanged event handler to it. We also bind this value to the Visibility attribute of customerDetailControl. This will hide the customerDetailControl on application startup before the user selects a customer. ( src ) public Customer SelectedCustomer { get { return selectedCustomer ; } set { if ( selectedCustomer != value ) { selectedCustomer = value ; OnPropertyChanged (); OnPropertyChanged ( nameof ( IsCustomerSelected )); } } } public bool IsCustomerSelected => SelectedCustomer != null ; <controls:CustomerDetailControl x:Name= \"customerDetailControl\" Grid.Row= \"1\" Grid.Column= \"1\" Customer= \"{x:Bind ViewModel.SelectedCustomer,Mode=TwoWay}\" Visibility= \"{x:Bind ViewModel.IsCustomerSelected}\" /> We can also implement a third TextBlock in the ListView's ItemTemplate to show a string depending on the value of the CheckBox. <ListView Grid.Row= \"1\" ItemsSource= \"{x:Bind ViewModel.Customers}\" SelectedItem= \"{x:Bind ViewModel.SelectedCustomer,Mode=TwoWay}\" > <ListView.ItemTemplate> <DataTemplate x:DataType= \"model:Customer\" > <StackPanel Orientation= \"Horizontal\" > <TextBlock Text= \"Dev\" Opacity= \"0.5\" Visibility= \"{x:Bind IsCoffeeDrinker}\" Margin= \"0 0 5 0\" /> <TextBlock Text= \"{x:Bind FirstName}\" /> <TextBlock Text= \"{x:Bind LastName}\" Margin= \"5 0 0 0\" FontWeight= \"Bold\" /> </StackPanel> </DataTemplate> </ListView.ItemTemplate> </ListView>","title":"x:Bind"},{"location":"Coding/Wired%20Brain%20Coffee/#toggling-visibility","text":"In order to fully implement the logic of the MVVM pattern, we should move functionality that deals with the logic of the app as a whole to the ViewModel. That would include the Add and Delete buttons. ( src ) Before After <CommandBar> <AppBarButton x:Name= \"AddCustomer\" Click= \"AddCustomer_Click\" Label= \"Add\" > <SymbolIcon Symbol= \"Add\" /> </AppBarButton> <AppBarButton x:Name= \"DeleteCustomer\" Click= \"DeleteCustomer_Click\" Label= \"Delete\" > <SymbolIcon Symbol= \"Delete\" /> </AppBarButton> <AppBarButton x:Name= \"btn_MoveSideBar\" Click= \"btn_MoveSideBar_Click\" Label= \"Move sidebar\" > <SymbolIcon x:Name= \"btn_MoveSideBar_Symbol\" Symbol= \"AlignRight\" /> </AppBarButton> </CommandBar> <CommandBar> <AppBarButton x:Name= \"AddCustomer\" Click= \"{x:Bind ViewModel.AddCustomer_Click}\" Label= \"Add\" > <SymbolIcon Symbol= \"Add\" /> </AppBarButton> <AppBarButton x:Name= \"DeleteCustomer\" Click= \"{x:Bind ViewModel.DeleteCustomer_Click}\" Label= \"Delete\" > <SymbolIcon Symbol= \"Delete\" /> </AppBarButton> <AppBarButton x:Name= \"btn_MoveSideBar\" Click= \"btn_MoveSideBar_Click\" Label= \"Move sidebar\" > <SymbolIcon x:Name= \"btn_MoveSideBar_Symbol\" Symbol= \"AlignRight\" /> </AppBarButton> </CommandBar>","title":"Toggling visibility"},{"location":"Coding/Wired%20Brain%20Coffee/#styling","text":"You can define a style that has to be used more than once by declaring a Style element on a UserControl's Resources property. ( src ) <UserControl> <UserControl.Resources> <Style x:Key= \"myTextBoxStyle\" TargetType= \"TextBox\" > <Style.Setters> <Setter Property= \"Margin\" Value= \"10\" /> <Setter Property= \"CornerRadius\" Value= \"10\" /> </Style.Setters> </Style> </UserControl.Resources> </UserControl> This Style can then be used as a StaticResource, setting the value of the Style attribute.","title":"Styling"},{"location":"Coding/WinUI/","text":"WinUI Syntax Every XAML element maps to a C# class , and every XAML attribute maps to a class property or an event . There are several syntaxes available for use that correspond to various methods of declaring objects: Object-element syntax has the type's name within angle brackets, similar to HTML. When the object contains other objects, it is called a container . If the object does not contain other objects, it can be declared with a self-closing tag: Attribute syntax has the property value set by declaring an attribute. In property-element syntax , signified by a period in the element name, where the portion of the element following the dot representing the identifier of the property. Content-property syntax is similar to the property element syntax in that a property's value is set by a child. However, in this case the XAML processor interpolates the correct property element, typically Content. Some controls can accept more than one child element. In the background, this is actually using the content-property syntax to assign to the Children property. Object-element Attribute Property-element Content-property Multiple children <Canvas> <Rectangle /> </Canvas> <Rectangle Name= \"rectangle1\" Width= \"100\" Height= \"100\" Fill= \"Blue\" /> <Rectangle Name= \"rectangle1\" Width= \"100\" Height= \"100\" > <Rectangle.Fill> <SolidColorBrush Color= \"Blue\" /> </Rectangle.Fill> </Rectangle> <Button> Add customer </Button> <StackPanel> <TextBlock> Hello </TextBlock> <TextBlock> World </TextBlock> </StackPanel> Namespaces A XAML file usually declares a default XAML namespace in its root element. This default namespace defines elements that can be declared without qualifying them by a prefix (e.g. x: ). XAML namespaces apply only to the specific element on which they are declared and its children, which explains why they are typically placed on the root element. ( src ) Most XAML files have two xmlns declarations xmlns maps a default XAML namespace xmlns:x maps a separate XAML namespace for XAML-defined language elements to the x: prefix: xmlns:mc indicates and supports a markup compatibility mode for reading XAML. xmlns:mc=\"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable=\"d\" Directives x:Name uniquely identifies object elements for access to the object from code-behind x:Key sets a unique key for each resource in a ResourceDictionary x:Class specifies the CLR namespace and class name for the class that provides code-behind for a XAML page. The x:Class directive configures XAML markup compilation to join partial classes between markup and code-behind. In this example it can be seen how it refers to the MainPage class within the Project namespace. ( src ) x:Bind is a form of data-binding Language elements are commonly used features necessary for Windows Runtime apps. For example, to join any code-behind to a XAML file through a partial class, you must name that class as the x:Class attribute in the root element of the XAML file. XAML Code-behind <Page x:Class= \"Project.MainPage\" /> namespace Project { public sealed partial class MainPage : Page { public MainPage () { this . InitializeComponent (); } } } Attached property Attached properties can be modified or queried in C# code as well, as long as the XAML element has an x:Name defined, which gives that particular element an identifier. For the column property of Grid , the static methods GetColumn and SetColumn are available. Here a button press changes the column value of an enclosing Grid and toggles between two different Symbol values. ( src ) private void ButtonMove_Click ( object sender , RoutedEventArgs e ) { int column = Grid . GetColumn ( customerListGrid ); int newcolumn = column == 0 ? 2 : 0 ; Grid . SetColumn ( customerListGrid , newcolumn ); moveSymbolIcon . Symbol = newcolumn == 0 ? Symbol . Forward : Symbol . Back ; } Event handling Event hooks can be used to subscribe to events. For example, the MainPage class exposes a Loaded event that can be used to fill a prototype app with dogfood data, rather than putting hardcoding it in the XAML markup. Notably, these event handlers must return only void types. public sealed partial class MainPage : Page { public MainPage () { this . InitializeComponent (); this . Loaded += MainPage_Loaded ; } private void MainPage_Loaded ( object sender , RoutedEventArgs e ) { throw new NotImplementedException (); } } Some XAML controls have attributes that map to events: Element Property Button Click CheckBox Checked , Unchecked ListView SelectionChanged TextBox TextChanged Defining an event handler in XAML is equivalent to doing so in C#. XAML C# <Button Content= \"Add customer\" Click= \"Button_ClickHandler\" > var btn = new Button { Content = \"Add customer\" this . Click = // No clue if this is right... }; SelectionChanged TextChanged Markup Code-behind <ListView x:Name= \"customerListView\" SelectionChanged= \"CustomerListView_SelectionChanged\" /> private void CustomerListView_SelectionChanged ( object sender , SelectionChangedEventArgs e ) { var customer = customerListView . SelectedItem as Customer ; txtFirstName . Text = customer ?. FirstName ?? \"\" ; txtLastName . Text = customer ?. LastName ?? \"\" ; chkIsDeveloper . IsChecked = customer ?. IsDeveloper ; } Markup Code-behind <StackPanel> <TextBox x:Name= \"txtFirstName\" Header= \"Firstname\" TextChanged= \"TextBox_TextChanged\" Margin= \"10\" /> <TextBox x:Name= \"txtLastName\" Header= \"Lastname\" TextChanged= \"TextBox_TextChanged\" Margin= \"10\" /> <CheckBox x:Name= \"chkIsDeveloper\" Content= \"IsDeveloper\" Margin= \"10\" Checked= \"CheckBox_IsCheckedChanged\" Unchecked= \"CheckBox_IsCheckedChanged\" /> </StackPanel> private void TextBox_TextChanged ( object sender , TextChangedEventArgs e ) { UpdateCustomer (); } private void CheckBox_IsCheckedChanged ( object sender , RoutedEventArgs e ) { UpdateCustomer (); } private void UpdateCustomer () { var customer = customerListView . SelectedItem as Customer ; if ( customer != null ) { customer . FirstName = txtFirstName . Text ; customer . LastName = txtLastName . Text ; customer . IsDeveloper = chkIsDeveloper . IsChecked .{{ c4 :: GetValueOrDefault }}(); } } Handlers for Loaded and Unloaded are automatically attached to any Page that uses the NavigationHelper class. Loaded Unloaded Custom controls You can take a group of controls and create a custom control using what is called a namespace mapping , where a C# class is made available in the XAML markup. More specifically, a custom control defined in XAML along with its accompanying code-behind file must use UserControl as its base class. In this sense, a custom control is really a special case of a namespace mapping, which can also be used to populate an application with mock data during development. Markup Code-behind <UserControl x:Class= \"Project.Controls.CustomControl\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP.Controls\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" > <TextBlock Text= \"Hello, world!\" /> </UserControl> namespace Project.Controls { public sealed partial class CustomControl : UserControl { public CustomControl () { this . InitializeComponent (); } } } Mock data The most basic method of adding mock data is by hardcoding data in the XAML markup . Somewhat more sophisticated is the option of hardcoding data in the code-behind . The x:Bind directive can be used to bind an IEnumerable data source to either the Items or ItemsSource attributes. An ObservableCollection is preferred in WinUI programming because it raises events when properties are changed, but Lists and Arrays also work. If the collection is made of objects, the DisplayMemberPath allows the specification of a particular property on those objects to be displayed. Notably, classes specifically need to be used, and not structs, for the members of these collections. With namespace mapping , classes within the C# namespace can be used in XAML markup. ( src ) Most robust of all is creating a Data Provider class which will fall back to mock data when the data source is not available. This will allow any number of other data sources to be plugged in, such as databases or REST services. ( src ) Hardcoding in XAML Hardcoding in C# Namespace mapping Data provider <ListView> <ListViewItem> Aristotle </ListViewItem> <ListViewItem> Euclid </ListViewItem> <ListViewItem> Plato </ListViewItem> <ListViewItem> Socrates </ListViewItem> </ListView> Markup Code-behind <Page> <ListView ItemsSource= \"{x:Bind Starships}\" SelectedItem= \"{x:Bind Starships[0]}\" DisplayMemberPath= \"Display\" /> </Page> namespace Starships.Models { class Starship { public string Name ; public string Registry ; public int Crew ; public string Display { get { return Name + Registry ; } } } } Markup Code-behind <Page xmlns:model= \"using:Project.Models\" > <ListView DisplayMemberPath= \"Display\" > <model:Starship Name= \"USS Enterprise\" Registry= \"NCC-1701\" Crew= \"140\" /> </ListView> </Page> namespace Starships.Models { class Starship { public string Name ; public string Registry ; public int Crew ; public string Display { get { return Name + Registry ; } } } } Themes Windows 10 themes ( \"Light\" , \"Dark\" , and \"HighContrast\" can be specified as a property of the Application element: Light Dark Theme colors <Application x:Class= \"Scratchpad.App\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:Scratchpad\" RequestedTheme= \"Light\" > <Application.Resources> <ResourceDictionary> <ResourceDictionary.MergedDictionaries> <XamlControlsResources xmlns= \"using:Microsoft.UI.Xaml.Controls\" /> <!-- Other merged dictionaries here --> </ResourceDictionary.MergedDictionaries> <!-- Other app resources here --> </ResourceDictionary> </Application.Resources> </Application> <Application x:Class= \"Scratchpad.App\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:Scratchpad\" RequestedTheme= \"Dark\" > <Application.Resources> <ResourceDictionary> <ResourceDictionary.MergedDictionaries> <XamlControlsResources xmlns= \"using:Microsoft.UI.Xaml.Controls\" /> <!-- Other merged dictionaries here --> </ResourceDictionary.MergedDictionaries> <!-- Other app resources here --> </ResourceDictionary> </Application.Resources> </Application> Layout The layout panels in XAML serve a similar function to the geometry manager methods in tkinter. There are various layout panels available. Grid RelativePanel StackPanel VariableSizeWrapGrid Data binding There are two data binding types available in XAML ( src ) Binding markup extension resolves the binding path at runtime. It can accept data sources from the binding properties ElementName , Source , and RelativeSource . If none of these are defined, then the binding markup extension resolves to the DataContext property. x:Bind resolves the binding path at compile-time , generating C# code and offering better performance and compile-time errors. You can also step into the compiled code, providing a better debugging experience. x:Bind should generally be preferred, however it is available only in UWP. x:Bind , in contrast, binds only to the parent Page or UserControl element. So any property of MainPage will be accessible, and any property of that object will also be accessible using dot notation. Most bindings are easily translated between the two types: Binding markup extension x:Bind public MainPage () { this . InitializeComponent ; this . ViewModel = new MainViewModel (); DataContext = ViewModel ; } <ListView ItemsSource= \"{Binding Customers,Mode=OneWay}\" > <!-- ...--> </ListView> public MainPage () { this . InitializeComponent ; this . ViewModel = new MainViewModel (); // DataContext = ViewModel; } <ListView ItemsSource= \"{x:Bind ViewModel.Customers,Mode=OneWay}\" > <!-- ...--> </ListView> Notably, the default binding mode of the binding markup extension is OneWay **, whereas that of x:Bind is ** OneTime , although this can be changed by setting x:DefaultBindMode on the root element. Changing default bind mode Set explicitly <Page x:DefaultBindMode= \"OneWay\" > <Listview ItemsSource= \"{x:Bind ViewModelCustomers}\" /> </Page> <Page> <Listview ItemsSource= \"{x:Bind ViewModelCustomers,Mode=OneWay}\" /> </Page> Type conversion By default, XAML attribute values are strings. In order for the XAML processor to interpret them as objects, they must be converted. Type converters can convert string representations of attribute values to property elements. For example, a type converter is what is used with the XAML declaration HorizontalAlignment=\"Left\" , which is mapped to a specific enumeration within the Windows.UI.XAML namespace. ref In UWP, the XAML processor integrates the conversion logic to convert the Margin declaration to a Thickness object. But in WPF , TypeConverters are used. Markup Code-behind <Button Margin= \"10,20,10,30\" Content= \"Click me\" /> var btn = new Button { Margin = new Thickness ( 10 , 20 , 10 , 30 ); }; Markup Extensions Markup extensions are placed between curly braces { and } and create a reference to a ResourceDictionary . They can be used to unify styling across an application. The most common markup extensions are: StaticResource refers to resources defined in resource dictionaries ThemeResource for Windows native themeing colors Binding for data binding expressions, which require the bound property to be a dependency property Here, the background of the grid is bound to a color from the Windows-native theming and the TextBlock's Foreground property is bound to a color defined in a resource dictionary defined on the same page. Multiple properties can be set at the same time by setting a Style property element. <Page> <Page.Resources> <Style TargetType= \"Button\" x:Key= \"MyButtonStyle\" > <Setter Property= \"Background\" Value= \"Blue\" /> <Setter Property= \"FontFamily\" Value= \"Arial Black\" /> <Setter Property= \"FontSize\" Value= \"36\" /> </Style> </Page.Resources> <Button Content= \"Hello world\" Style= \"{StaticResource MyButtonStyle}\" /> </Page> <Page> <Page.Resources> <SolidColorBrush x:Key= \"MyBrush\" Color= \"Brown\" /> </Page.Resources> <Grid Background= \"{ThemeResource ApplicationPageBackgroundThemeBrush}\" > <TextBlock Text= \"Hello World\" Foreground= \"{StaticResource MyBrush}\" /> </Grid> </Page> src Dependency properties Only dependency properties can be targets for data binding in UWP and WPF . The propdp snippet in Visual Studio can be used to create one. The DependencyObject class, which is a base class of all UI elements in UWP and WPF, exposes the GetValue and SetValue methods, which are used to ... Multi-instance A multi-instance application is one that can run in several instances, which is necessary to allow users to open new windows. A collection of templates is available as a Visual Studio extension . These templates modify the appxmanifest file by setting the SupportsMultipleInstances attribute to true: <Package ... xmlns:desktop4= \"http://schemas.microsoft.com/appx/manifest/desktop/windows10/4\" xmlns:iot2= \"http://schemas.microsoft.com/appx/manifest/iot/windows10/2\" IgnorableNamespaces= \"uap mp desktop4 iot2\" > ... <Applications> <Application Id= \"App\" ... desktop4:SupportsMultipleInstances= \"true\" iot2:SupportsMultipleInstances= \"true\" > ... </Application> </Applications> ... </Package> Resources: Create a multi-instance UWP app Patterns Action on focus UI elements expose the GotFocus event hook for when a user clicks or tabs into a control. Example handler selecting all text in a TextBox: ( src ) Markup Code-behind <Page> <TextBox GotFocus= \"TextBox_GotFocus\" /> </Page> void TextBox_GotFocus ( object sender , RoutedEventArgs e ) { TextBox textBox = sender as TextBox ; textBox . SelectAll (); } List/Details The list/details pattern has a master pane (usually a ListView ) and a details pane for content. MVVM In the Model, View, ViewModel (MVVM) pattern, which implicitly relies on OOP principles, the Model represents the data model for the objects being manipulated, and the ViewModel is the model for the View , that is, the state of the application as represented in a class. In WinUI, the project that contains the Views (that is, the XAML files) must first add references to the projects where the Models and ViewModel are contained. These will allow the code-behind file of the MainWindow to reference those classes. The class representing the ViewModel is instantiated and assigned to an attribute. That class's methods can then be called by using the x:Bind attribute syntax on, for instance, a ListView element. Markup Code-behind <Window> <ListView ItemsSource= \"{x:Bind ViewModel.Employees, Mode=OneWay}\" SelectedItem= \"{x:Bind ViewModel.SelectedEmployee, Mode=OneWay}\" DisplayMemberPath= \"FirstName\" /> </Window> using EmployeeManager.DataAccess ; using EmployeeManager.ViewModel ; using Microsoft.UI.Xaml ; namespace EmployeeManager.WinUI { public sealed partial class MainWindow : Window { public MainWindow () { ViewModel = new MainViewModel ( new EmployeeDataProvider ()); this . InitializeComponent (); } public MainViewModel ViewModel { get ; } } } Navigation App layout typically begins with the choice of navigation model , which defines how users navigate between pages in the app. There are two common navigation models: left nav and top nav Examples List and details <Page x:Class= \"Superheroes.MainPage\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:Superheroes\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" Background= \"{ThemeResource ApplicationPageBackgroundThemeBrush}\" > <StackPanel> <CommandBar> <AppBarButton x:Name= \"AddHero\" Click= \"AddHero_Click\" Label= \"Add hero\" > <SymbolIcon Symbol= \"Add\" /> </AppBarButton> <AppBarButton x:Name= \"DelHero\" Click= \"DelHero_Click\" Label= \"Remove hero\" > <SymbolIcon Symbol= \"Delete\" /> </AppBarButton> </CommandBar> <TextBox Header= \"First name\" Text= \"{Binding ElementName=heroesListView,Path=SelectedItem.FirstName,Mode=TwoWay}\" /> <TextBox Header= \"Last name\" Text= \"{Binding ElementName=heroesListView,Path=SelectedItem.LastName,Mode=TwoWay}\" /> <TextBox Header= \"Alias\" Text= \"{Binding ElementName=heroesListView,Path=SelectedItem.Superhero,Mode=TwoWay}\" /> <ListView x:Name= \"heroesListView\" ItemsSource= \"{Binding Heroes,Mode=TwoWay}\" SelectedItem= \"{Binding SelectedHero,Mode=TwoWay}\" > <ListView.ItemTemplate> <DataTemplate> <StackPanel Orientation= \"Horizontal\" > <TextBlock Text= \"{Binding Superhero}\" FontWeight= \"Bold\" /> <TextBlock Text= \"{Binding FirstName}\" Margin= \"5 0 0 0\" /> <TextBlock Text= \"{Binding LastName}\" Margin= \"5 0 0 0\" /> </StackPanel> </DataTemplate> </ListView.ItemTemplate> </ListView> </StackPanel> </Page> In order to link two controls, some form of data binding is necessary. Here, TextBox elements are individually bound to the string properties of the selected ListView element with the Binding markup extension using the ElementName binding property. The selected item and the path to the string property are combined using dot notation and assigned to the Path binding property. Custom control The textboxes can also be consolidated into a custom control using UserControl . In this case, the custom control must expose a target property that will be bound to the ListView's SelectedItem property. Individual TextBoxes bound to ListView UserControl <TextBox Header= \"First name\" Text= \"{Binding ElementName=heroesListView,Path=SelectedItem.FirstName,Mode=TwoWay}\" /> <TextBox Header= \"Last name\" Text= \"{Binding ElementName=heroesListView,Path=SelectedItem.LastName,Mode=TwoWay}\" /> <TextBox Header= \"Alias\" Text= \"{Binding ElementName=heroesListView,Path=SelectedItem.Superhero,Mode=TwoWay}\" /> <!-- <local:MyTextBoxes Hero=\"{Binding ElementName=heroesListView,Path=SelectedItem,Mode=TwoWay}\"/> --> <!--<TextBox Header=\"First name\" Text=\"{Binding ElementName=heroesListView,Path=SelectedItem.FirstName,Mode=TwoWay}\"/> <TextBox Header=\"Last name\" Text=\"{Binding ElementName=heroesListView,Path=SelectedItem.LastName,Mode=TwoWay}\"/> <TextBox Header=\"Alias\" Text=\"{Binding ElementName=heroesListView,Path=SelectedItem.Superhero,Mode=TwoWay}\"/>--> <local:MyTextBoxes Hero= \"{Binding ElementName=heroesListView,Path=SelectedItem,Mode=TwoWay}\" /> In order to accept the data binding, the target property must be implemented as a dependency property . This dependency property works through a static callback function which sets the individual textbox values to the bound target property. Naively, the callback function can be made to change the individual TextBox elements, provided they have x:Name defined. But a better technique is using the Binding markup extension to bind the individual TextBoxes to the root node using the ElementName binding property. This requires the root node to have x:Name defined. Both MyTextBoxes's binding to heroesListView and the individual TextBox bindings to the Hero property of root need to be TwoWay in order for changes made in the TextBox to take effect. Notably, the ListView does not reflect any changes made yet. Callback Binding <TextBox x:Name= \"FirstNameTextbox\" Header= \"First name\" Margin= \"10\" /> <TextBox x:Name= \"LastNameTextbox\" Header= \"Last name\" Margin= \"10\" /> <TextBox x:Name= \"SuperheroTextbox\" Header= \"Alias\" Margin= \"10\" /> private static void HeroChangedCallback ( DependencyObject d , DependencyPropertyChangedEventArgs e ) { if ( d is MyTextBoxes myTextBoxes ) { var hero = e . NewValue as Hero ; myTextBoxes . FirstNameTextbox . Text = hero . FirstName ; myTextBoxes . LastNameTextbox . Text = hero . LastName ; myTextBoxes . SuperheroTextbox . Text = hero . Superhero ; } } <TextBox x:Name= \"FirstNameTextbox\" Header= \"First name\" Margin= \"10\" Text= \"{Binding ElementName=root,Path=Hero.FirstName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" /> <TextBox x:Name= \"LastNameTextbox\" Header= \"Last name\" Margin= \"10\" Text= \"{Binding ElementName=root,Path=Hero.LastName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" /> <TextBox x:Name= \"SuperheroTextbox\" Header= \"Alias\" Margin= \"10\" Text= \"{Binding ElementName=root,Path=Hero.Superhero,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" /> private static void HeroChangedCallback ( DependencyObject d , DependencyPropertyChangedEventArgs e ) { //if (d is MyTextBoxes myTextBoxes) //{ // var hero = e.NewValue as Hero; // myTextBoxes.FirstNameTextbox.Text = hero.FirstName; // myTextBoxes.LastNameTextbox.Text = hero.LastName; // myTextBoxes.SuperheroTextbox.Text = hero.Superhero; //} } ListView In order for the ListView to update, the underlying model must raise the PropertyChanged event. I really don't understand this very well, so here is the Observable class that implements the INotifyPropertyChanged interface. The model must inherit from this base class and the Set accessor of any property should fire OnPropertyChanged() . public class Observable : INotifyPropertyChanged { public event PropertyChangedEventHandler PropertyChanged ; protected void OnPropertyChanged ([ CallerMemberName ] string propertyName = null ) { PropertyChanged ?. Invoke ( this , new PropertyChangedEventArgs ( propertyName )); } } ViewModel x:Bind Binding markup extensions are trivially changed to compiled data bindings by replacing Binding with x:Bind . !!! Note that ListView will enter an infinite loop if the SelectedHero property does not incorporate additional logic. ```csharp public Hero SelectedHero { get { return _selectedHero; } set { if (_selectedHero != value) { _selectedHero = value; OnPropertyChanged(); OnPropertyChanged(nameof(IsHeroSelected)); } } } ``` Binding markup extension Compiled data binding <ListView x:Name= \"heroesListView\" ItemsSource= \"{Binding Heroes,Mode=OneWay}\" SelectedItem= \"{Binding SelectedHero,Mode=TwoWay}\" Margin= \"0\" > <TextBox x:Name= \"FirstNameTextbox\" Header= \"First name\" Margin= \"10\" Text= \"{Binding ElementName=root,Path=Hero.FirstName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" GotFocus= \"Textbox_GotFocus\" /> <TextBox x:Name= \"LastNameTextbox\" Header= \"Last name\" Margin= \"10\" Text= \"{Binding ElementName=root,Path=Hero.LastName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" GotFocus= \"Textbox_GotFocus\" /> <TextBox x:Name= \"SuperheroTextbox\" Header= \"Alias\" Margin= \"10\" Text= \"{Binding ElementName=root,Path=Hero.Superhero,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" GotFocus= \"Textbox_GotFocus\" /> <ListView x:Name= \"heroesListView\" ItemsSource= \"{x:Bind ViewModel.Heroes,Mode=OneWay}\" SelectedItem= \"{x:Bind ViewModel.SelectedHero,Mode=TwoWay}\" Margin= \"0\" > <TextBox x:Name= \"FirstNameTextbox\" Header= \"First name\" Margin= \"10\" Text= \"{x:Bind Hero.FirstName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" GotFocus= \"Textbox_GotFocus\" /> <TextBox x:Name= \"LastNameTextbox\" Header= \"Last name\" Margin= \"10\" Text= \"{x:Bind Hero.LastName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" GotFocus= \"Textbox_GotFocus\" /> <TextBox x:Name= \"SuperheroTextbox\" Header= \"Alias\" Margin= \"10\" Text= \"{x:Bind Hero.Superhero,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" GotFocus= \"Textbox_GotFocus\" /> x:Bind can also be implemented in the ListView's ItemTemplate, so long as the x:DataType attribute is set on DataTemplate. We must also remember to set the Mode binding property, since x:Bind's default is OneTime! Binding markup extension Compiled data binding <ListView.ItemTemplate> <DataTemplate> <StackPanel Orientation= \"Horizontal\" > <TextBlock Text= \"{Binding Superhero}\" FontWeight= \"Bold\" /> <TextBlock Text= \"{Binding FirstName}\" Margin= \"5 0 0 0\" /> <TextBlock Text= \"{Binding LastName}\" Margin= \"5 0 0 0\" /> </StackPanel> </DataTemplate> </ListView.ItemTemplate> <ListView.ItemTemplate> <DataTemplate x:DataType= \"local:Hero\" > <StackPanel Orientation= \"Horizontal\" > <TextBlock Text= \"{x:Bind Superhero}\" FontWeight= \"Bold\" /> <TextBlock Text= \"{x:Bind FirstName}\" Margin= \"5 0 0 0\" /> <TextBlock Text= \"{x:Bind LastName}\" Margin= \"5 0 0 0\" /> </StackPanel> </DataTemplate> </ListView.ItemTemplate> x:Bind can also hide or reveal controls depending on boolean value. A new boolean field is formed on the ViewModel. public class ViewModel { public bool IsHeroSelected => SelectedHero != null ; } Before After <CommandBar> <AppBarButton x:Name= \"AddHero\" Click= \"AddHero_Click\" Label= \"Add hero\" > <SymbolIcon Symbol= \"Add\" /> </AppBarButton> <AppBarButton x:Name= \"DelHero\" Click= \"DelHero_Click\" Label= \"Remove hero\" > <SymbolIcon Symbol= \"Delete\" /> </AppBarButton> </CommandBar> <CommandBar> <AppBarButton x:Name= \"AddHero\" Click= \"AddHero_Click\" Label= \"Add hero\" > <SymbolIcon Symbol= \"Add\" /> </AppBarButton> <AppBarButton x:Name= \"DelHero\" Click= \"DelHero_Click\" Label= \"Remove hero\" IsEnabled= \"{x:Bind ViewModel.IsHeroSelected,Mode=OneWay}\" > <SymbolIcon Symbol= \"Delete\" /> </AppBarButton> </CommandBar> History Windows has a long history of introducing UI frameworks to facilitate the creation of GUI applications: MFC (1992) was based on Native C++ WinForms (2002) was based on .NET Framework WPF (2006) was also based on .NET Framework UWP XAML (2012) was based on C++ and .NET WinUI 2 is a NuGet package containing controls and styles for UWP Apps, intended to decouple UWP applications from the latest version of Windows WinUI 3 (2020) is meant to provide a UX framework for both Win32 and UWP applications XAML is a declarative markup language used to create UIs for .NET Core apps. The logic of the app is separated in code-behind files that are joined to the markup through partial class definitions. In Visual Studio this is emphasized by the fact that the code-behind file is literally presented as a child node of the XAML document. The root element (of which there must be only one in order to be a valid XAML file) contains attributes that define the XAML namespaces for the program that will be parsing the XAML file, or a namescope .","title":"WinUI"},{"location":"Coding/WinUI/#winui","text":"","title":"WinUI"},{"location":"Coding/WinUI/#syntax","text":"Every XAML element maps to a C# class , and every XAML attribute maps to a class property or an event . There are several syntaxes available for use that correspond to various methods of declaring objects: Object-element syntax has the type's name within angle brackets, similar to HTML. When the object contains other objects, it is called a container . If the object does not contain other objects, it can be declared with a self-closing tag: Attribute syntax has the property value set by declaring an attribute. In property-element syntax , signified by a period in the element name, where the portion of the element following the dot representing the identifier of the property. Content-property syntax is similar to the property element syntax in that a property's value is set by a child. However, in this case the XAML processor interpolates the correct property element, typically Content. Some controls can accept more than one child element. In the background, this is actually using the content-property syntax to assign to the Children property. Object-element Attribute Property-element Content-property Multiple children <Canvas> <Rectangle /> </Canvas> <Rectangle Name= \"rectangle1\" Width= \"100\" Height= \"100\" Fill= \"Blue\" /> <Rectangle Name= \"rectangle1\" Width= \"100\" Height= \"100\" > <Rectangle.Fill> <SolidColorBrush Color= \"Blue\" /> </Rectangle.Fill> </Rectangle> <Button> Add customer </Button> <StackPanel> <TextBlock> Hello </TextBlock> <TextBlock> World </TextBlock> </StackPanel>","title":"Syntax"},{"location":"Coding/WinUI/#namespaces","text":"A XAML file usually declares a default XAML namespace in its root element. This default namespace defines elements that can be declared without qualifying them by a prefix (e.g. x: ). XAML namespaces apply only to the specific element on which they are declared and its children, which explains why they are typically placed on the root element. ( src ) Most XAML files have two xmlns declarations xmlns maps a default XAML namespace xmlns:x maps a separate XAML namespace for XAML-defined language elements to the x: prefix: xmlns:mc indicates and supports a markup compatibility mode for reading XAML. xmlns:mc=\"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable=\"d\"","title":"Namespaces"},{"location":"Coding/WinUI/#directives","text":"x:Name uniquely identifies object elements for access to the object from code-behind x:Key sets a unique key for each resource in a ResourceDictionary x:Class specifies the CLR namespace and class name for the class that provides code-behind for a XAML page. The x:Class directive configures XAML markup compilation to join partial classes between markup and code-behind. In this example it can be seen how it refers to the MainPage class within the Project namespace. ( src ) x:Bind is a form of data-binding Language elements are commonly used features necessary for Windows Runtime apps. For example, to join any code-behind to a XAML file through a partial class, you must name that class as the x:Class attribute in the root element of the XAML file. XAML Code-behind <Page x:Class= \"Project.MainPage\" /> namespace Project { public sealed partial class MainPage : Page { public MainPage () { this . InitializeComponent (); } } }","title":"Directives"},{"location":"Coding/WinUI/#attached-property","text":"Attached properties can be modified or queried in C# code as well, as long as the XAML element has an x:Name defined, which gives that particular element an identifier. For the column property of Grid , the static methods GetColumn and SetColumn are available. Here a button press changes the column value of an enclosing Grid and toggles between two different Symbol values. ( src ) private void ButtonMove_Click ( object sender , RoutedEventArgs e ) { int column = Grid . GetColumn ( customerListGrid ); int newcolumn = column == 0 ? 2 : 0 ; Grid . SetColumn ( customerListGrid , newcolumn ); moveSymbolIcon . Symbol = newcolumn == 0 ? Symbol . Forward : Symbol . Back ; }","title":"Attached property"},{"location":"Coding/WinUI/#event-handling","text":"Event hooks can be used to subscribe to events. For example, the MainPage class exposes a Loaded event that can be used to fill a prototype app with dogfood data, rather than putting hardcoding it in the XAML markup. Notably, these event handlers must return only void types. public sealed partial class MainPage : Page { public MainPage () { this . InitializeComponent (); this . Loaded += MainPage_Loaded ; } private void MainPage_Loaded ( object sender , RoutedEventArgs e ) { throw new NotImplementedException (); } } Some XAML controls have attributes that map to events: Element Property Button Click CheckBox Checked , Unchecked ListView SelectionChanged TextBox TextChanged Defining an event handler in XAML is equivalent to doing so in C#. XAML C# <Button Content= \"Add customer\" Click= \"Button_ClickHandler\" > var btn = new Button { Content = \"Add customer\" this . Click = // No clue if this is right... }; SelectionChanged TextChanged Markup Code-behind <ListView x:Name= \"customerListView\" SelectionChanged= \"CustomerListView_SelectionChanged\" /> private void CustomerListView_SelectionChanged ( object sender , SelectionChangedEventArgs e ) { var customer = customerListView . SelectedItem as Customer ; txtFirstName . Text = customer ?. FirstName ?? \"\" ; txtLastName . Text = customer ?. LastName ?? \"\" ; chkIsDeveloper . IsChecked = customer ?. IsDeveloper ; } Markup Code-behind <StackPanel> <TextBox x:Name= \"txtFirstName\" Header= \"Firstname\" TextChanged= \"TextBox_TextChanged\" Margin= \"10\" /> <TextBox x:Name= \"txtLastName\" Header= \"Lastname\" TextChanged= \"TextBox_TextChanged\" Margin= \"10\" /> <CheckBox x:Name= \"chkIsDeveloper\" Content= \"IsDeveloper\" Margin= \"10\" Checked= \"CheckBox_IsCheckedChanged\" Unchecked= \"CheckBox_IsCheckedChanged\" /> </StackPanel> private void TextBox_TextChanged ( object sender , TextChangedEventArgs e ) { UpdateCustomer (); } private void CheckBox_IsCheckedChanged ( object sender , RoutedEventArgs e ) { UpdateCustomer (); } private void UpdateCustomer () { var customer = customerListView . SelectedItem as Customer ; if ( customer != null ) { customer . FirstName = txtFirstName . Text ; customer . LastName = txtLastName . Text ; customer . IsDeveloper = chkIsDeveloper . IsChecked .{{ c4 :: GetValueOrDefault }}(); } } Handlers for Loaded and Unloaded are automatically attached to any Page that uses the NavigationHelper class. Loaded Unloaded","title":"Event handling"},{"location":"Coding/WinUI/#custom-controls","text":"You can take a group of controls and create a custom control using what is called a namespace mapping , where a C# class is made available in the XAML markup. More specifically, a custom control defined in XAML along with its accompanying code-behind file must use UserControl as its base class. In this sense, a custom control is really a special case of a namespace mapping, which can also be used to populate an application with mock data during development. Markup Code-behind <UserControl x:Class= \"Project.Controls.CustomControl\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:WiredBrainCoffee.UWP.Controls\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" > <TextBlock Text= \"Hello, world!\" /> </UserControl> namespace Project.Controls { public sealed partial class CustomControl : UserControl { public CustomControl () { this . InitializeComponent (); } } }","title":"Custom controls"},{"location":"Coding/WinUI/#mock-data","text":"The most basic method of adding mock data is by hardcoding data in the XAML markup . Somewhat more sophisticated is the option of hardcoding data in the code-behind . The x:Bind directive can be used to bind an IEnumerable data source to either the Items or ItemsSource attributes. An ObservableCollection is preferred in WinUI programming because it raises events when properties are changed, but Lists and Arrays also work. If the collection is made of objects, the DisplayMemberPath allows the specification of a particular property on those objects to be displayed. Notably, classes specifically need to be used, and not structs, for the members of these collections. With namespace mapping , classes within the C# namespace can be used in XAML markup. ( src ) Most robust of all is creating a Data Provider class which will fall back to mock data when the data source is not available. This will allow any number of other data sources to be plugged in, such as databases or REST services. ( src ) Hardcoding in XAML Hardcoding in C# Namespace mapping Data provider <ListView> <ListViewItem> Aristotle </ListViewItem> <ListViewItem> Euclid </ListViewItem> <ListViewItem> Plato </ListViewItem> <ListViewItem> Socrates </ListViewItem> </ListView> Markup Code-behind <Page> <ListView ItemsSource= \"{x:Bind Starships}\" SelectedItem= \"{x:Bind Starships[0]}\" DisplayMemberPath= \"Display\" /> </Page> namespace Starships.Models { class Starship { public string Name ; public string Registry ; public int Crew ; public string Display { get { return Name + Registry ; } } } } Markup Code-behind <Page xmlns:model= \"using:Project.Models\" > <ListView DisplayMemberPath= \"Display\" > <model:Starship Name= \"USS Enterprise\" Registry= \"NCC-1701\" Crew= \"140\" /> </ListView> </Page> namespace Starships.Models { class Starship { public string Name ; public string Registry ; public int Crew ; public string Display { get { return Name + Registry ; } } } }","title":"Mock data"},{"location":"Coding/WinUI/#themes","text":"Windows 10 themes ( \"Light\" , \"Dark\" , and \"HighContrast\" can be specified as a property of the Application element: Light Dark Theme colors <Application x:Class= \"Scratchpad.App\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:Scratchpad\" RequestedTheme= \"Light\" > <Application.Resources> <ResourceDictionary> <ResourceDictionary.MergedDictionaries> <XamlControlsResources xmlns= \"using:Microsoft.UI.Xaml.Controls\" /> <!-- Other merged dictionaries here --> </ResourceDictionary.MergedDictionaries> <!-- Other app resources here --> </ResourceDictionary> </Application.Resources> </Application> <Application x:Class= \"Scratchpad.App\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:Scratchpad\" RequestedTheme= \"Dark\" > <Application.Resources> <ResourceDictionary> <ResourceDictionary.MergedDictionaries> <XamlControlsResources xmlns= \"using:Microsoft.UI.Xaml.Controls\" /> <!-- Other merged dictionaries here --> </ResourceDictionary.MergedDictionaries> <!-- Other app resources here --> </ResourceDictionary> </Application.Resources> </Application>","title":"Themes"},{"location":"Coding/WinUI/#layout","text":"The layout panels in XAML serve a similar function to the geometry manager methods in tkinter. There are various layout panels available. Grid RelativePanel StackPanel VariableSizeWrapGrid","title":"Layout"},{"location":"Coding/WinUI/#data-binding","text":"There are two data binding types available in XAML ( src ) Binding markup extension resolves the binding path at runtime. It can accept data sources from the binding properties ElementName , Source , and RelativeSource . If none of these are defined, then the binding markup extension resolves to the DataContext property. x:Bind resolves the binding path at compile-time , generating C# code and offering better performance and compile-time errors. You can also step into the compiled code, providing a better debugging experience. x:Bind should generally be preferred, however it is available only in UWP. x:Bind , in contrast, binds only to the parent Page or UserControl element. So any property of MainPage will be accessible, and any property of that object will also be accessible using dot notation. Most bindings are easily translated between the two types: Binding markup extension x:Bind public MainPage () { this . InitializeComponent ; this . ViewModel = new MainViewModel (); DataContext = ViewModel ; } <ListView ItemsSource= \"{Binding Customers,Mode=OneWay}\" > <!-- ...--> </ListView> public MainPage () { this . InitializeComponent ; this . ViewModel = new MainViewModel (); // DataContext = ViewModel; } <ListView ItemsSource= \"{x:Bind ViewModel.Customers,Mode=OneWay}\" > <!-- ...--> </ListView> Notably, the default binding mode of the binding markup extension is OneWay **, whereas that of x:Bind is ** OneTime , although this can be changed by setting x:DefaultBindMode on the root element. Changing default bind mode Set explicitly <Page x:DefaultBindMode= \"OneWay\" > <Listview ItemsSource= \"{x:Bind ViewModelCustomers}\" /> </Page> <Page> <Listview ItemsSource= \"{x:Bind ViewModelCustomers,Mode=OneWay}\" /> </Page>","title":"Data binding"},{"location":"Coding/WinUI/#type-conversion","text":"By default, XAML attribute values are strings. In order for the XAML processor to interpret them as objects, they must be converted. Type converters can convert string representations of attribute values to property elements. For example, a type converter is what is used with the XAML declaration HorizontalAlignment=\"Left\" , which is mapped to a specific enumeration within the Windows.UI.XAML namespace. ref In UWP, the XAML processor integrates the conversion logic to convert the Margin declaration to a Thickness object. But in WPF , TypeConverters are used. Markup Code-behind <Button Margin= \"10,20,10,30\" Content= \"Click me\" /> var btn = new Button { Margin = new Thickness ( 10 , 20 , 10 , 30 ); };","title":"Type conversion"},{"location":"Coding/WinUI/#markup-extensions","text":"Markup extensions are placed between curly braces { and } and create a reference to a ResourceDictionary . They can be used to unify styling across an application. The most common markup extensions are: StaticResource refers to resources defined in resource dictionaries ThemeResource for Windows native themeing colors Binding for data binding expressions, which require the bound property to be a dependency property Here, the background of the grid is bound to a color from the Windows-native theming and the TextBlock's Foreground property is bound to a color defined in a resource dictionary defined on the same page. Multiple properties can be set at the same time by setting a Style property element. <Page> <Page.Resources> <Style TargetType= \"Button\" x:Key= \"MyButtonStyle\" > <Setter Property= \"Background\" Value= \"Blue\" /> <Setter Property= \"FontFamily\" Value= \"Arial Black\" /> <Setter Property= \"FontSize\" Value= \"36\" /> </Style> </Page.Resources> <Button Content= \"Hello world\" Style= \"{StaticResource MyButtonStyle}\" /> </Page> <Page> <Page.Resources> <SolidColorBrush x:Key= \"MyBrush\" Color= \"Brown\" /> </Page.Resources> <Grid Background= \"{ThemeResource ApplicationPageBackgroundThemeBrush}\" > <TextBlock Text= \"Hello World\" Foreground= \"{StaticResource MyBrush}\" /> </Grid> </Page> src","title":"Markup Extensions"},{"location":"Coding/WinUI/#dependency-properties","text":"Only dependency properties can be targets for data binding in UWP and WPF . The propdp snippet in Visual Studio can be used to create one. The DependencyObject class, which is a base class of all UI elements in UWP and WPF, exposes the GetValue and SetValue methods, which are used to ...","title":"Dependency properties"},{"location":"Coding/WinUI/#multi-instance","text":"A multi-instance application is one that can run in several instances, which is necessary to allow users to open new windows. A collection of templates is available as a Visual Studio extension . These templates modify the appxmanifest file by setting the SupportsMultipleInstances attribute to true: <Package ... xmlns:desktop4= \"http://schemas.microsoft.com/appx/manifest/desktop/windows10/4\" xmlns:iot2= \"http://schemas.microsoft.com/appx/manifest/iot/windows10/2\" IgnorableNamespaces= \"uap mp desktop4 iot2\" > ... <Applications> <Application Id= \"App\" ... desktop4:SupportsMultipleInstances= \"true\" iot2:SupportsMultipleInstances= \"true\" > ... </Application> </Applications> ... </Package> Resources: Create a multi-instance UWP app","title":"Multi-instance"},{"location":"Coding/WinUI/#patterns","text":"","title":"Patterns"},{"location":"Coding/WinUI/#action-on-focus","text":"UI elements expose the GotFocus event hook for when a user clicks or tabs into a control. Example handler selecting all text in a TextBox: ( src ) Markup Code-behind <Page> <TextBox GotFocus= \"TextBox_GotFocus\" /> </Page> void TextBox_GotFocus ( object sender , RoutedEventArgs e ) { TextBox textBox = sender as TextBox ; textBox . SelectAll (); }","title":"Action on focus"},{"location":"Coding/WinUI/#listdetails","text":"The list/details pattern has a master pane (usually a ListView ) and a details pane for content.","title":"List/Details"},{"location":"Coding/WinUI/#mvvm","text":"In the Model, View, ViewModel (MVVM) pattern, which implicitly relies on OOP principles, the Model represents the data model for the objects being manipulated, and the ViewModel is the model for the View , that is, the state of the application as represented in a class. In WinUI, the project that contains the Views (that is, the XAML files) must first add references to the projects where the Models and ViewModel are contained. These will allow the code-behind file of the MainWindow to reference those classes. The class representing the ViewModel is instantiated and assigned to an attribute. That class's methods can then be called by using the x:Bind attribute syntax on, for instance, a ListView element. Markup Code-behind <Window> <ListView ItemsSource= \"{x:Bind ViewModel.Employees, Mode=OneWay}\" SelectedItem= \"{x:Bind ViewModel.SelectedEmployee, Mode=OneWay}\" DisplayMemberPath= \"FirstName\" /> </Window> using EmployeeManager.DataAccess ; using EmployeeManager.ViewModel ; using Microsoft.UI.Xaml ; namespace EmployeeManager.WinUI { public sealed partial class MainWindow : Window { public MainWindow () { ViewModel = new MainViewModel ( new EmployeeDataProvider ()); this . InitializeComponent (); } public MainViewModel ViewModel { get ; } } }","title":"MVVM"},{"location":"Coding/WinUI/#navigation","text":"App layout typically begins with the choice of navigation model , which defines how users navigate between pages in the app. There are two common navigation models: left nav and top nav","title":"Navigation"},{"location":"Coding/WinUI/#examples","text":"","title":"Examples"},{"location":"Coding/WinUI/#list-and-details","text":"<Page x:Class= \"Superheroes.MainPage\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:Superheroes\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" Background= \"{ThemeResource ApplicationPageBackgroundThemeBrush}\" > <StackPanel> <CommandBar> <AppBarButton x:Name= \"AddHero\" Click= \"AddHero_Click\" Label= \"Add hero\" > <SymbolIcon Symbol= \"Add\" /> </AppBarButton> <AppBarButton x:Name= \"DelHero\" Click= \"DelHero_Click\" Label= \"Remove hero\" > <SymbolIcon Symbol= \"Delete\" /> </AppBarButton> </CommandBar> <TextBox Header= \"First name\" Text= \"{Binding ElementName=heroesListView,Path=SelectedItem.FirstName,Mode=TwoWay}\" /> <TextBox Header= \"Last name\" Text= \"{Binding ElementName=heroesListView,Path=SelectedItem.LastName,Mode=TwoWay}\" /> <TextBox Header= \"Alias\" Text= \"{Binding ElementName=heroesListView,Path=SelectedItem.Superhero,Mode=TwoWay}\" /> <ListView x:Name= \"heroesListView\" ItemsSource= \"{Binding Heroes,Mode=TwoWay}\" SelectedItem= \"{Binding SelectedHero,Mode=TwoWay}\" > <ListView.ItemTemplate> <DataTemplate> <StackPanel Orientation= \"Horizontal\" > <TextBlock Text= \"{Binding Superhero}\" FontWeight= \"Bold\" /> <TextBlock Text= \"{Binding FirstName}\" Margin= \"5 0 0 0\" /> <TextBlock Text= \"{Binding LastName}\" Margin= \"5 0 0 0\" /> </StackPanel> </DataTemplate> </ListView.ItemTemplate> </ListView> </StackPanel> </Page> In order to link two controls, some form of data binding is necessary. Here, TextBox elements are individually bound to the string properties of the selected ListView element with the Binding markup extension using the ElementName binding property. The selected item and the path to the string property are combined using dot notation and assigned to the Path binding property.","title":"List and details"},{"location":"Coding/WinUI/#custom-control","text":"The textboxes can also be consolidated into a custom control using UserControl . In this case, the custom control must expose a target property that will be bound to the ListView's SelectedItem property. Individual TextBoxes bound to ListView UserControl <TextBox Header= \"First name\" Text= \"{Binding ElementName=heroesListView,Path=SelectedItem.FirstName,Mode=TwoWay}\" /> <TextBox Header= \"Last name\" Text= \"{Binding ElementName=heroesListView,Path=SelectedItem.LastName,Mode=TwoWay}\" /> <TextBox Header= \"Alias\" Text= \"{Binding ElementName=heroesListView,Path=SelectedItem.Superhero,Mode=TwoWay}\" /> <!-- <local:MyTextBoxes Hero=\"{Binding ElementName=heroesListView,Path=SelectedItem,Mode=TwoWay}\"/> --> <!--<TextBox Header=\"First name\" Text=\"{Binding ElementName=heroesListView,Path=SelectedItem.FirstName,Mode=TwoWay}\"/> <TextBox Header=\"Last name\" Text=\"{Binding ElementName=heroesListView,Path=SelectedItem.LastName,Mode=TwoWay}\"/> <TextBox Header=\"Alias\" Text=\"{Binding ElementName=heroesListView,Path=SelectedItem.Superhero,Mode=TwoWay}\"/>--> <local:MyTextBoxes Hero= \"{Binding ElementName=heroesListView,Path=SelectedItem,Mode=TwoWay}\" /> In order to accept the data binding, the target property must be implemented as a dependency property . This dependency property works through a static callback function which sets the individual textbox values to the bound target property. Naively, the callback function can be made to change the individual TextBox elements, provided they have x:Name defined. But a better technique is using the Binding markup extension to bind the individual TextBoxes to the root node using the ElementName binding property. This requires the root node to have x:Name defined. Both MyTextBoxes's binding to heroesListView and the individual TextBox bindings to the Hero property of root need to be TwoWay in order for changes made in the TextBox to take effect. Notably, the ListView does not reflect any changes made yet. Callback Binding <TextBox x:Name= \"FirstNameTextbox\" Header= \"First name\" Margin= \"10\" /> <TextBox x:Name= \"LastNameTextbox\" Header= \"Last name\" Margin= \"10\" /> <TextBox x:Name= \"SuperheroTextbox\" Header= \"Alias\" Margin= \"10\" /> private static void HeroChangedCallback ( DependencyObject d , DependencyPropertyChangedEventArgs e ) { if ( d is MyTextBoxes myTextBoxes ) { var hero = e . NewValue as Hero ; myTextBoxes . FirstNameTextbox . Text = hero . FirstName ; myTextBoxes . LastNameTextbox . Text = hero . LastName ; myTextBoxes . SuperheroTextbox . Text = hero . Superhero ; } } <TextBox x:Name= \"FirstNameTextbox\" Header= \"First name\" Margin= \"10\" Text= \"{Binding ElementName=root,Path=Hero.FirstName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" /> <TextBox x:Name= \"LastNameTextbox\" Header= \"Last name\" Margin= \"10\" Text= \"{Binding ElementName=root,Path=Hero.LastName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" /> <TextBox x:Name= \"SuperheroTextbox\" Header= \"Alias\" Margin= \"10\" Text= \"{Binding ElementName=root,Path=Hero.Superhero,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" /> private static void HeroChangedCallback ( DependencyObject d , DependencyPropertyChangedEventArgs e ) { //if (d is MyTextBoxes myTextBoxes) //{ // var hero = e.NewValue as Hero; // myTextBoxes.FirstNameTextbox.Text = hero.FirstName; // myTextBoxes.LastNameTextbox.Text = hero.LastName; // myTextBoxes.SuperheroTextbox.Text = hero.Superhero; //} }","title":"Custom control"},{"location":"Coding/WinUI/#listview","text":"In order for the ListView to update, the underlying model must raise the PropertyChanged event. I really don't understand this very well, so here is the Observable class that implements the INotifyPropertyChanged interface. The model must inherit from this base class and the Set accessor of any property should fire OnPropertyChanged() . public class Observable : INotifyPropertyChanged { public event PropertyChangedEventHandler PropertyChanged ; protected void OnPropertyChanged ([ CallerMemberName ] string propertyName = null ) { PropertyChanged ?. Invoke ( this , new PropertyChangedEventArgs ( propertyName )); } }","title":"ListView"},{"location":"Coding/WinUI/#viewmodel","text":"","title":"ViewModel"},{"location":"Coding/WinUI/#xbind","text":"Binding markup extensions are trivially changed to compiled data bindings by replacing Binding with x:Bind . !!! Note that ListView will enter an infinite loop if the SelectedHero property does not incorporate additional logic. ```csharp public Hero SelectedHero { get { return _selectedHero; } set { if (_selectedHero != value) { _selectedHero = value; OnPropertyChanged(); OnPropertyChanged(nameof(IsHeroSelected)); } } } ``` Binding markup extension Compiled data binding <ListView x:Name= \"heroesListView\" ItemsSource= \"{Binding Heroes,Mode=OneWay}\" SelectedItem= \"{Binding SelectedHero,Mode=TwoWay}\" Margin= \"0\" > <TextBox x:Name= \"FirstNameTextbox\" Header= \"First name\" Margin= \"10\" Text= \"{Binding ElementName=root,Path=Hero.FirstName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" GotFocus= \"Textbox_GotFocus\" /> <TextBox x:Name= \"LastNameTextbox\" Header= \"Last name\" Margin= \"10\" Text= \"{Binding ElementName=root,Path=Hero.LastName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" GotFocus= \"Textbox_GotFocus\" /> <TextBox x:Name= \"SuperheroTextbox\" Header= \"Alias\" Margin= \"10\" Text= \"{Binding ElementName=root,Path=Hero.Superhero,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" GotFocus= \"Textbox_GotFocus\" /> <ListView x:Name= \"heroesListView\" ItemsSource= \"{x:Bind ViewModel.Heroes,Mode=OneWay}\" SelectedItem= \"{x:Bind ViewModel.SelectedHero,Mode=TwoWay}\" Margin= \"0\" > <TextBox x:Name= \"FirstNameTextbox\" Header= \"First name\" Margin= \"10\" Text= \"{x:Bind Hero.FirstName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" GotFocus= \"Textbox_GotFocus\" /> <TextBox x:Name= \"LastNameTextbox\" Header= \"Last name\" Margin= \"10\" Text= \"{x:Bind Hero.LastName,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" GotFocus= \"Textbox_GotFocus\" /> <TextBox x:Name= \"SuperheroTextbox\" Header= \"Alias\" Margin= \"10\" Text= \"{x:Bind Hero.Superhero,Mode=TwoWay,UpdateSourceTrigger=PropertyChanged}\" GotFocus= \"Textbox_GotFocus\" /> x:Bind can also be implemented in the ListView's ItemTemplate, so long as the x:DataType attribute is set on DataTemplate. We must also remember to set the Mode binding property, since x:Bind's default is OneTime! Binding markup extension Compiled data binding <ListView.ItemTemplate> <DataTemplate> <StackPanel Orientation= \"Horizontal\" > <TextBlock Text= \"{Binding Superhero}\" FontWeight= \"Bold\" /> <TextBlock Text= \"{Binding FirstName}\" Margin= \"5 0 0 0\" /> <TextBlock Text= \"{Binding LastName}\" Margin= \"5 0 0 0\" /> </StackPanel> </DataTemplate> </ListView.ItemTemplate> <ListView.ItemTemplate> <DataTemplate x:DataType= \"local:Hero\" > <StackPanel Orientation= \"Horizontal\" > <TextBlock Text= \"{x:Bind Superhero}\" FontWeight= \"Bold\" /> <TextBlock Text= \"{x:Bind FirstName}\" Margin= \"5 0 0 0\" /> <TextBlock Text= \"{x:Bind LastName}\" Margin= \"5 0 0 0\" /> </StackPanel> </DataTemplate> </ListView.ItemTemplate> x:Bind can also hide or reveal controls depending on boolean value. A new boolean field is formed on the ViewModel. public class ViewModel { public bool IsHeroSelected => SelectedHero != null ; } Before After <CommandBar> <AppBarButton x:Name= \"AddHero\" Click= \"AddHero_Click\" Label= \"Add hero\" > <SymbolIcon Symbol= \"Add\" /> </AppBarButton> <AppBarButton x:Name= \"DelHero\" Click= \"DelHero_Click\" Label= \"Remove hero\" > <SymbolIcon Symbol= \"Delete\" /> </AppBarButton> </CommandBar> <CommandBar> <AppBarButton x:Name= \"AddHero\" Click= \"AddHero_Click\" Label= \"Add hero\" > <SymbolIcon Symbol= \"Add\" /> </AppBarButton> <AppBarButton x:Name= \"DelHero\" Click= \"DelHero_Click\" Label= \"Remove hero\" IsEnabled= \"{x:Bind ViewModel.IsHeroSelected,Mode=OneWay}\" > <SymbolIcon Symbol= \"Delete\" /> </AppBarButton> </CommandBar>","title":"x:Bind"},{"location":"Coding/WinUI/#history","text":"Windows has a long history of introducing UI frameworks to facilitate the creation of GUI applications: MFC (1992) was based on Native C++ WinForms (2002) was based on .NET Framework WPF (2006) was also based on .NET Framework UWP XAML (2012) was based on C++ and .NET WinUI 2 is a NuGet package containing controls and styles for UWP Apps, intended to decouple UWP applications from the latest version of Windows WinUI 3 (2020) is meant to provide a UX framework for both Win32 and UWP applications XAML is a declarative markup language used to create UIs for .NET Core apps. The logic of the app is separated in code-behind files that are joined to the markup through partial class definitions. In Visual Studio this is emphasized by the fact that the code-behind file is literally presented as a child node of the XAML document. The root element (of which there must be only one in order to be a valid XAML file) contains attributes that define the XAML namespaces for the program that will be parsing the XAML file, or a namescope .","title":"History"},{"location":"Coding/WinUI/API/","text":"API ComboBox Important attributes: Items or ItemsSource specify the collection (preferably IObservableCollection to support event handling on value changes) to be used to populate the control. SelectedItem defines the element that appears selected by the control by default. If not defined, no element will be selected. DisplayMemberPath defines the name of the property to be used to display each individual choice. <ComboBox ItemsSource= \"{x:Bind Items}\" SelectedItem= \"{x:Bind Items[0]}\" DisplayMemberPath= \"Display\" /> CommandBar CommandBar is a lightweight control that can organize a bar of buttons. <CommandBar> <AppBarButton x:Name= \"AddCustomer\" Click= \"AddCustomer_Click\" Label= \"Add\" > <SymbolIcon Symbol= \"AddFriend\" /> </AppBarButton> <AppBarButton x:Name= \"DeleteCustomer\" Click= \"DeleteCustomer_Click\" Label= \"Delete\" > <SymbolIcon Symbol= \"Delete\" /> </AppBarButton> <AppBarButton x:Name= \"btn_MoveSideBar\" Click= \"btn_MoveSideBar_Click\" Label= \"Move sidebar\" > <SymbolIcon x:Name= \"btn_MoveSideBar_Symbol\" Symbol= \"AlignRight\" /> </AppBarButton> </CommandBar> DataGrid Dialog boxes In XAML, the MessageDialog object can be used to create a modal dialog box (i.e. one that does not allow interaction with the main window until the dialog box has been cleared). The MessageDialog object can take a string argument containing the text of the dialog box. It is actually displayed by calling the object's ShowAsync() method. Because this is an asynchronous call, it must be await ed, which requires the System namespace. ( src ) using Microsoft.UI.Xaml ; using Microsoft.UI.Xaml.Controls ; using Windows.UI.Popups ; using System ; namespace WiredBrainCoffee.UWP { public sealed partial class MainPage : Page { public MainPage () { this . InitializeComponent (); } private async void AddCustomer ( object sender , RoutedEventArgs e ) { var messageDialog = new MessageDialog ( \"Customer added!\" ); await messageDialog . ShowAsync (); } } } Grid The Grid layout panel is analogous to the grid geometry manager in tkinter. However, in XAML you are forced to explicitly declare RowDefinition ** and ** ColumnDefinition elements. Whereas in tkinter, the widget being placed declares its own grid position. If the grid is sparse, the empty rows and columns appear to be ignored. Grid star-sizing works similar to flex-grow and flex-shrink CSS style statements used with Flexbox. Rows Columns <Grid> <Grid.RowDefinitions> <RowDefinition/> <RowDefinition/> <RowDefinition/> <RowDefinition/> <RowDefinition/> <RowDefinition/> <RowDefinition/> <RowDefinition/> </Grid.RowDefinitions> <Rectangle Fill= \"LightGray\" /> <Rectangle Fill= \"LightSteelBlue\" Grid.Row= \"1\" /> <Rectangle Fill= \"LightBlue\" Grid.Row= \"2\" /> <Rectangle Fill= \"LightCyan\" Grid.Row= \"3\" /> <Rectangle Fill= \"LightSeaGreen\" Grid.Row= \"4\" /> <Rectangle Fill= \"LightGreen\" Grid.Row= \"5\" /> <Rectangle Fill= \"LightGoldenrodYellow\" Grid.Row= \"6\" /> <Rectangle Fill= \"LightSalmon\" Grid.Row= \"7\" /> <Rectangle Fill= \"LightCoral\" Grid.Row= \"8\" /> </Grid> <Grid> <Grid.ColumnDefinitions> <ColumnDefinition/> <ColumnDefinition/> <ColumnDefinition/> <ColumnDefinition/> <ColumnDefinition/> <ColumnDefinition/> <ColumnDefinition/> <ColumnDefinition/> </Grid.ColumnDefinitions> <Rectangle Fill= \"LightGray\" /> <Rectangle Fill= \"LightSteelBlue\" Grid.Column= \"1\" /> <Rectangle Fill= \"LightBlue\" Grid.Column= \"2\" /> <Rectangle Fill= \"LightCyan\" Grid.Column= \"3\" /> <Rectangle Fill= \"LightSeaGreen\" Grid.Column= \"4\" /> <Rectangle Fill= \"LightGreen\" Grid.Column= \"5\" /> <Rectangle Fill= \"LightGoldenrodYellow\" Grid.Column= \"6\" /> <Rectangle Fill= \"LightSalmon\" Grid.Column= \"7\" /> <Rectangle Fill= \"LightCoral\" Grid.Column= \"8\" /> </Grid> ListDetailsView ListDetailsView is a custom control available from the Windows Community Toolkit (Nuget package Microsoft.Toolkit.UWP ) that implements the Master/Details pattern . Illustration Basic structure XAML xmlns:toolkit=\"using:Microsoft.Toolkit.Uwp.UI.Controls : namespace toolkit:ListDetailsView toolkit:ListDetailsView. ItemTemplate property-element for how items are rendered in sidebar toolkit:ListDetailsView. DetailsTemplate property-element for how items are rendered in the main pane toolkit:ListDetailsView. NoSelectionContentTemplate property-element for how the main pane is rendered with no item selected toolkit:ListDetailsView. AppCommandBar <Page <!-- ... -- > xmlns:toolkit=\"using:Microsoft.Toolkit.Uwp.UI.Controls\"> <toolkit:ListDetailsView> <toolkit:ListDetailsView.ItemTemplate> </toolkit:ListDetailsView.ItemTemplate> <toolkit:ListDetailsView.DetailsTemplate> </toolkit:ListDetailsView.DetailsTemplate> <toolkit:ListDetailsView.NoSelectionContentTemplate> </toolkit:ListDetailsView.NoSelectionContentTemplate> <toolkit:ListDetailsView.AppCommandBar> </toolkit:ListDetailsView.AppCommandBar> </toolkit:ListDetailsView> </Page> ListView <ListView ItemsSource= \"{x:Bind Items}\" SelectedItem= \"{x:Bind Items[0]}\" DisplayMemberPath= \"Display\" /> Important attributes: Items or ItemsSource specify the collection (preferably IObservableCollection to support event handling on value changes) to be used to populate the control. SelectedItem defines the element that appears selected by the control by default. If not defined, no element will be selected. DisplayMemberPath defines the name of the property to be used to display each individual choice. MainWindow.xaml MainWindow.xaml.cs <Window x:Class= \"Scratchpad1.MainWindow\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:Scratchpad1\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" > <StackPanel Orientation= \"Horizontal\" HorizontalAlignment= \"Center\" VerticalAlignment= \"Center\" > <ComboBox ItemsSource= \"{x:Bind Items}\" SelectedItem= \"{x:Bind Items[0]}\" DisplayMemberPath= \"Name\" /> </StackPanel> </Window> using System.Collections.Generic ; using Microsoft.UI.Xaml ; namespace Scratchpad1 { public class Starship { public string Name { get ; set ; } public string Registry { get ; set ; } public int Crew { get ; set ; } public Starship ( string name , string registry , int crew ) { Name = name ; Registry = registry ; Crew = crew ; } } public sealed partial class MainWindow : Window { //List<string> Items = new List<string>(); Starship [] Items = new Starship [ 3 ]; public MainWindow () { this . InitializeComponent (); Items [ 0 ]= new Starship ( \"USS Enterprise\" , \"NCC-1701\" , 204 ); Items [ 1 ]= new Starship ( \"USS Constitution\" , \"NCC-1700\" , 203 ); Items [ 2 ]= new Starship ( \"USS Defiant\" , \"NCC-1764\" , 202 ); } } } NavigationView NavigationView provides top-level navigation by implementing a collapsible navigation bar (\"hamburger menu\") that is reactive to window size. It organizes two areas - Pane and Content - into two layout options that differ in the placement of the Pane. The Header content is placed above the Content in both layouts. Left Top It has various display modes that can be specified by setting PaneDisplayMode . By default, PaneDisplayMode is set to Auto , which enables adaptive (i.e. reactive) behavior, switching between Left, LeftCompact, and LeftMinimal display modes depending on window size. Left LeftCompact LeftMinimal Top The Pane is the central feature of NavigationView, and it can contain many items organized into several sections. MenuItems is the main section containing NavigationView items at the beginning of the control. FooterMenuItems is similar but they are added to the end of the control. PaneTitle can accept text, which will appear next to the menu button. PaneHeader is similar but can accept non-text content. PaneFooter can also accept any content. NavigationView items can be of various types: NavigationViewItemHeader can visually organize other navigation items NavigationViewItem exposes Content and Icon properties. NavigationViewItemSeparator AutoSuggestBox Settings button visible by default but can be hidden by setting IsSettingsVisible Left Top MenuItems <Page> <NavigationView> <NavigationView.MenuItems/> </NavigationView> </Page> Navigation using NavigationView is not automatically implemented but relies on event handling. NavigationView raises an ItemInvoked event when selected, and if the selection has changed then SelectionChanged is also raised. NavigationView also feature a Back button, which can be disabled or removed by setting IsBackButtonVisible or IsBackEnabled to false. If enabled, this button raises the BackRequested event. Typical implementations pair NavigationView with a Frame nested within ScrollViewer to support navigating to different views while supporting the back button (see below). XAML Code-behind <Page> <Grid> <NavigationView> <ScrollViewer> <Frame x:Name= \"ContentFrame\" /> </ScrollViewer> </NavigationView> </Grid> </Page> private void Navigation_ItemInvoked ( NavigationView sender , NavigationViewItemInvokedArgs args ) { ContentFrame . Navigate ( typeof ( Page1 )); } Page The Page element in UWP is equivalent to Window in WPF. Page elements can only accept a single Content sub-element, necessitating the use of a layout panel like Grid , StackPanel , etc. RelativePanel RelativePanel allows children to declare attributes (e.g. RelativePanel.RightOf ) to specify position relative to the x:Name of other children. This is useful in building responsive layouts. Supports several attached properties that allow elements to be aligned with siblings or with the panel itself. Panel alignment relations like AlignLeftWithPanel , AlignTopWithPanel , AlignRightWithPanel , AlignBottomWithPanel , align controls to the border of the RelativePanel containing them. Sibling alignment relationships like AlignLeftWith , AlignTopWith , AlignVerticalCenterWith etc. specify the name of a sibling control to provide alignment. Sibling positional relations like LeftOf , Above , RightOf , and Below also specify a sibling control. ResourceDictionary Resource dictionaries Here, Buttons will now be able to be styled using a markup extension <Button Style= \"{StaticResource SubmitButton}\" Content= \"Submit\" /> App.xaml /ResourceDictionaries/ButtonDictionary.xaml <Application> <Application.Resources> <ResourceDictionary Source= \"ResourceDictionaries/ButtonDictionary.xaml\" /> </Application.Resources> </Application> <ResourceDictionary> <Style TargetType= \"Button\" x:Key= \"SubmitButton\" > <Setter Property= \"Background\" Value= \"Green\" /> <Setter Property= \"Padding\" Value= \"5\" /> </Style> </ResourceDictionary> Managing a consistent style will typically necessitate using multiple resource dictionaries. But some elements can only contain a single ResourceDictionary element. The solution is to place a ResourceDictionary.MergedDictionaries property element within the outermost ResourceDictionary . Multiple ResourceDictionary objects can be placed as children of it. <Page> <Page.Resources> <ResourceDictionary> <ResourceDictionary.MergedDictionaries> <ResourceDictionary Source= \"Dictionary1.xaml\" /> <ResourceDictionary Source= \"Dictionary2.xaml\" /> </ResourceDictionary.MergedDictionaries> </ResourceDictionary> </Page.Resources> </Page> Sources ResourceDictionary YouTube SplitView SplitView can be used to implement hamburger-style navigation. SplitView has two attributes into which controls can be placed, Pane and Content . Pane is not displayed by default. However, by setting the SplitView instance's IsPaneOpen attribute to True it can be displayed. The DisplayMode attribute controls how the Pane interacts with Content with opened: Overlay : Pane covers up Content Inline : Pane pushes Content to the right. CompactInline : Where Pane will fit Pane elements closely, if CompactPaneLength is not specified CompactOverley : Pane's dimensions can be specified using CompactPaneLength and OpenPaneLength StackPanel The StackPanel layout panel in XAML is similar in function to the pack() geometry manager in tkinter, although its default behavior appears to horizontally center elements and stack them vertically. Notably, StackPanel does not support scroll bars. ( src ) TabView Textbox XAML tkinter <Window x:Class= \"EmployeeManager.WinUI.MainWindow\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:EmployeeManager.WinUI\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" > <TextBox Header= \"First name\" /> </Window> import tkinter as tk from tkinter.ttk import Entry from tkinter.ttk import LabelFrame win = tk . Tk () frame = LabelFrame ( win , text = \"First name\" ) frame . pack () Entry ( frame ) . pack () tk . mainloop () VariableSizedWrapGrid VariableSizedWrapGrid can be used to define a field of tiles similar to an HTML flex container ( display: flex; with flex-wrap: wrap; ). The Orientation property is similar to a flex container's flex-direction , in that the direction of alignment can be specified. <Page> <VariableSizedWrapGrid Orientation= \"Horizontal\" ItemWidth= \"100\" ItemHeight= \"100\" > <Rectangle Fill= \"LightGray\" /> <Rectangle Fill= \"LightSteelBlue\" /> <Rectangle Fill= \"LightBlue\" /> <Rectangle Fill= \"LightCyan\" /> <Rectangle Fill= \"LightSeaGreen\" /> <Rectangle Fill= \"LightGreen\" /> <Rectangle Fill= \"LightGoldenrodYellow\" /> <Rectangle Fill= \"LightSalmon\" /> <Rectangle Fill= \"LightCoral\" /> <Rectangle Fill= \"Gray\" /> <Rectangle Fill= \"SteelBlue\" /> <Rectangle Fill= \"CadetBlue\" /> <Rectangle Fill= \"Cyan\" /> <Rectangle Fill= \"SeaGreen\" /> <Rectangle Fill= \"Green\" /> <Rectangle Fill= \"Goldenrod\" /> <Rectangle Fill= \"Salmon\" /> <Rectangle Fill= \"Coral\" /> </VariableSizedWrapGrid> </Page> Notably, the horizontal or vertical alignment of XAML controls is defined on each control, whereas in HTML alignment is specified at the level of the enclosing container. XAML HTML and CSS <TextBlock Content= \"Hello, world!\" HorizontalAlignment= \"Left\" VerticalAlignment= \"Top\" /> .container p Hello, world! . container { text-align : right top ; }","title":"API"},{"location":"Coding/WinUI/API/#api","text":"","title":"API"},{"location":"Coding/WinUI/API/#combobox","text":"Important attributes: Items or ItemsSource specify the collection (preferably IObservableCollection to support event handling on value changes) to be used to populate the control. SelectedItem defines the element that appears selected by the control by default. If not defined, no element will be selected. DisplayMemberPath defines the name of the property to be used to display each individual choice. <ComboBox ItemsSource= \"{x:Bind Items}\" SelectedItem= \"{x:Bind Items[0]}\" DisplayMemberPath= \"Display\" />","title":"ComboBox"},{"location":"Coding/WinUI/API/#commandbar","text":"CommandBar is a lightweight control that can organize a bar of buttons. <CommandBar> <AppBarButton x:Name= \"AddCustomer\" Click= \"AddCustomer_Click\" Label= \"Add\" > <SymbolIcon Symbol= \"AddFriend\" /> </AppBarButton> <AppBarButton x:Name= \"DeleteCustomer\" Click= \"DeleteCustomer_Click\" Label= \"Delete\" > <SymbolIcon Symbol= \"Delete\" /> </AppBarButton> <AppBarButton x:Name= \"btn_MoveSideBar\" Click= \"btn_MoveSideBar_Click\" Label= \"Move sidebar\" > <SymbolIcon x:Name= \"btn_MoveSideBar_Symbol\" Symbol= \"AlignRight\" /> </AppBarButton> </CommandBar>","title":"CommandBar"},{"location":"Coding/WinUI/API/#datagrid","text":"","title":"DataGrid"},{"location":"Coding/WinUI/API/#dialog-boxes","text":"In XAML, the MessageDialog object can be used to create a modal dialog box (i.e. one that does not allow interaction with the main window until the dialog box has been cleared). The MessageDialog object can take a string argument containing the text of the dialog box. It is actually displayed by calling the object's ShowAsync() method. Because this is an asynchronous call, it must be await ed, which requires the System namespace. ( src ) using Microsoft.UI.Xaml ; using Microsoft.UI.Xaml.Controls ; using Windows.UI.Popups ; using System ; namespace WiredBrainCoffee.UWP { public sealed partial class MainPage : Page { public MainPage () { this . InitializeComponent (); } private async void AddCustomer ( object sender , RoutedEventArgs e ) { var messageDialog = new MessageDialog ( \"Customer added!\" ); await messageDialog . ShowAsync (); } } }","title":"Dialog boxes"},{"location":"Coding/WinUI/API/#grid","text":"The Grid layout panel is analogous to the grid geometry manager in tkinter. However, in XAML you are forced to explicitly declare RowDefinition ** and ** ColumnDefinition elements. Whereas in tkinter, the widget being placed declares its own grid position. If the grid is sparse, the empty rows and columns appear to be ignored. Grid star-sizing works similar to flex-grow and flex-shrink CSS style statements used with Flexbox. Rows Columns <Grid> <Grid.RowDefinitions> <RowDefinition/> <RowDefinition/> <RowDefinition/> <RowDefinition/> <RowDefinition/> <RowDefinition/> <RowDefinition/> <RowDefinition/> </Grid.RowDefinitions> <Rectangle Fill= \"LightGray\" /> <Rectangle Fill= \"LightSteelBlue\" Grid.Row= \"1\" /> <Rectangle Fill= \"LightBlue\" Grid.Row= \"2\" /> <Rectangle Fill= \"LightCyan\" Grid.Row= \"3\" /> <Rectangle Fill= \"LightSeaGreen\" Grid.Row= \"4\" /> <Rectangle Fill= \"LightGreen\" Grid.Row= \"5\" /> <Rectangle Fill= \"LightGoldenrodYellow\" Grid.Row= \"6\" /> <Rectangle Fill= \"LightSalmon\" Grid.Row= \"7\" /> <Rectangle Fill= \"LightCoral\" Grid.Row= \"8\" /> </Grid> <Grid> <Grid.ColumnDefinitions> <ColumnDefinition/> <ColumnDefinition/> <ColumnDefinition/> <ColumnDefinition/> <ColumnDefinition/> <ColumnDefinition/> <ColumnDefinition/> <ColumnDefinition/> </Grid.ColumnDefinitions> <Rectangle Fill= \"LightGray\" /> <Rectangle Fill= \"LightSteelBlue\" Grid.Column= \"1\" /> <Rectangle Fill= \"LightBlue\" Grid.Column= \"2\" /> <Rectangle Fill= \"LightCyan\" Grid.Column= \"3\" /> <Rectangle Fill= \"LightSeaGreen\" Grid.Column= \"4\" /> <Rectangle Fill= \"LightGreen\" Grid.Column= \"5\" /> <Rectangle Fill= \"LightGoldenrodYellow\" Grid.Column= \"6\" /> <Rectangle Fill= \"LightSalmon\" Grid.Column= \"7\" /> <Rectangle Fill= \"LightCoral\" Grid.Column= \"8\" /> </Grid>","title":"Grid"},{"location":"Coding/WinUI/API/#listdetailsview","text":"ListDetailsView is a custom control available from the Windows Community Toolkit (Nuget package Microsoft.Toolkit.UWP ) that implements the Master/Details pattern . Illustration Basic structure XAML xmlns:toolkit=\"using:Microsoft.Toolkit.Uwp.UI.Controls : namespace toolkit:ListDetailsView toolkit:ListDetailsView. ItemTemplate property-element for how items are rendered in sidebar toolkit:ListDetailsView. DetailsTemplate property-element for how items are rendered in the main pane toolkit:ListDetailsView. NoSelectionContentTemplate property-element for how the main pane is rendered with no item selected toolkit:ListDetailsView. AppCommandBar <Page <!-- ... -- > xmlns:toolkit=\"using:Microsoft.Toolkit.Uwp.UI.Controls\"> <toolkit:ListDetailsView> <toolkit:ListDetailsView.ItemTemplate> </toolkit:ListDetailsView.ItemTemplate> <toolkit:ListDetailsView.DetailsTemplate> </toolkit:ListDetailsView.DetailsTemplate> <toolkit:ListDetailsView.NoSelectionContentTemplate> </toolkit:ListDetailsView.NoSelectionContentTemplate> <toolkit:ListDetailsView.AppCommandBar> </toolkit:ListDetailsView.AppCommandBar> </toolkit:ListDetailsView> </Page>","title":"ListDetailsView "},{"location":"Coding/WinUI/API/#listview","text":"<ListView ItemsSource= \"{x:Bind Items}\" SelectedItem= \"{x:Bind Items[0]}\" DisplayMemberPath= \"Display\" /> Important attributes: Items or ItemsSource specify the collection (preferably IObservableCollection to support event handling on value changes) to be used to populate the control. SelectedItem defines the element that appears selected by the control by default. If not defined, no element will be selected. DisplayMemberPath defines the name of the property to be used to display each individual choice. MainWindow.xaml MainWindow.xaml.cs <Window x:Class= \"Scratchpad1.MainWindow\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:Scratchpad1\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" > <StackPanel Orientation= \"Horizontal\" HorizontalAlignment= \"Center\" VerticalAlignment= \"Center\" > <ComboBox ItemsSource= \"{x:Bind Items}\" SelectedItem= \"{x:Bind Items[0]}\" DisplayMemberPath= \"Name\" /> </StackPanel> </Window> using System.Collections.Generic ; using Microsoft.UI.Xaml ; namespace Scratchpad1 { public class Starship { public string Name { get ; set ; } public string Registry { get ; set ; } public int Crew { get ; set ; } public Starship ( string name , string registry , int crew ) { Name = name ; Registry = registry ; Crew = crew ; } } public sealed partial class MainWindow : Window { //List<string> Items = new List<string>(); Starship [] Items = new Starship [ 3 ]; public MainWindow () { this . InitializeComponent (); Items [ 0 ]= new Starship ( \"USS Enterprise\" , \"NCC-1701\" , 204 ); Items [ 1 ]= new Starship ( \"USS Constitution\" , \"NCC-1700\" , 203 ); Items [ 2 ]= new Starship ( \"USS Defiant\" , \"NCC-1764\" , 202 ); } } }","title":"ListView"},{"location":"Coding/WinUI/API/#navigationview","text":"NavigationView provides top-level navigation by implementing a collapsible navigation bar (\"hamburger menu\") that is reactive to window size. It organizes two areas - Pane and Content - into two layout options that differ in the placement of the Pane. The Header content is placed above the Content in both layouts. Left Top It has various display modes that can be specified by setting PaneDisplayMode . By default, PaneDisplayMode is set to Auto , which enables adaptive (i.e. reactive) behavior, switching between Left, LeftCompact, and LeftMinimal display modes depending on window size. Left LeftCompact LeftMinimal Top The Pane is the central feature of NavigationView, and it can contain many items organized into several sections. MenuItems is the main section containing NavigationView items at the beginning of the control. FooterMenuItems is similar but they are added to the end of the control. PaneTitle can accept text, which will appear next to the menu button. PaneHeader is similar but can accept non-text content. PaneFooter can also accept any content. NavigationView items can be of various types: NavigationViewItemHeader can visually organize other navigation items NavigationViewItem exposes Content and Icon properties. NavigationViewItemSeparator AutoSuggestBox Settings button visible by default but can be hidden by setting IsSettingsVisible Left Top MenuItems <Page> <NavigationView> <NavigationView.MenuItems/> </NavigationView> </Page> Navigation using NavigationView is not automatically implemented but relies on event handling. NavigationView raises an ItemInvoked event when selected, and if the selection has changed then SelectionChanged is also raised. NavigationView also feature a Back button, which can be disabled or removed by setting IsBackButtonVisible or IsBackEnabled to false. If enabled, this button raises the BackRequested event. Typical implementations pair NavigationView with a Frame nested within ScrollViewer to support navigating to different views while supporting the back button (see below). XAML Code-behind <Page> <Grid> <NavigationView> <ScrollViewer> <Frame x:Name= \"ContentFrame\" /> </ScrollViewer> </NavigationView> </Grid> </Page> private void Navigation_ItemInvoked ( NavigationView sender , NavigationViewItemInvokedArgs args ) { ContentFrame . Navigate ( typeof ( Page1 )); }","title":"NavigationView"},{"location":"Coding/WinUI/API/#page","text":"The Page element in UWP is equivalent to Window in WPF. Page elements can only accept a single Content sub-element, necessitating the use of a layout panel like Grid , StackPanel , etc.","title":"Page"},{"location":"Coding/WinUI/API/#relativepanel","text":"RelativePanel allows children to declare attributes (e.g. RelativePanel.RightOf ) to specify position relative to the x:Name of other children. This is useful in building responsive layouts. Supports several attached properties that allow elements to be aligned with siblings or with the panel itself. Panel alignment relations like AlignLeftWithPanel , AlignTopWithPanel , AlignRightWithPanel , AlignBottomWithPanel , align controls to the border of the RelativePanel containing them. Sibling alignment relationships like AlignLeftWith , AlignTopWith , AlignVerticalCenterWith etc. specify the name of a sibling control to provide alignment. Sibling positional relations like LeftOf , Above , RightOf , and Below also specify a sibling control.","title":"RelativePanel"},{"location":"Coding/WinUI/API/#resourcedictionary","text":"Resource dictionaries Here, Buttons will now be able to be styled using a markup extension <Button Style= \"{StaticResource SubmitButton}\" Content= \"Submit\" /> App.xaml /ResourceDictionaries/ButtonDictionary.xaml <Application> <Application.Resources> <ResourceDictionary Source= \"ResourceDictionaries/ButtonDictionary.xaml\" /> </Application.Resources> </Application> <ResourceDictionary> <Style TargetType= \"Button\" x:Key= \"SubmitButton\" > <Setter Property= \"Background\" Value= \"Green\" /> <Setter Property= \"Padding\" Value= \"5\" /> </Style> </ResourceDictionary> Managing a consistent style will typically necessitate using multiple resource dictionaries. But some elements can only contain a single ResourceDictionary element. The solution is to place a ResourceDictionary.MergedDictionaries property element within the outermost ResourceDictionary . Multiple ResourceDictionary objects can be placed as children of it. <Page> <Page.Resources> <ResourceDictionary> <ResourceDictionary.MergedDictionaries> <ResourceDictionary Source= \"Dictionary1.xaml\" /> <ResourceDictionary Source= \"Dictionary2.xaml\" /> </ResourceDictionary.MergedDictionaries> </ResourceDictionary> </Page.Resources> </Page> Sources ResourceDictionary YouTube","title":"ResourceDictionary"},{"location":"Coding/WinUI/API/#splitview","text":"SplitView can be used to implement hamburger-style navigation. SplitView has two attributes into which controls can be placed, Pane and Content . Pane is not displayed by default. However, by setting the SplitView instance's IsPaneOpen attribute to True it can be displayed. The DisplayMode attribute controls how the Pane interacts with Content with opened: Overlay : Pane covers up Content Inline : Pane pushes Content to the right. CompactInline : Where Pane will fit Pane elements closely, if CompactPaneLength is not specified CompactOverley : Pane's dimensions can be specified using CompactPaneLength and OpenPaneLength","title":"SplitView"},{"location":"Coding/WinUI/API/#stackpanel","text":"The StackPanel layout panel in XAML is similar in function to the pack() geometry manager in tkinter, although its default behavior appears to horizontally center elements and stack them vertically. Notably, StackPanel does not support scroll bars. ( src )","title":"StackPanel"},{"location":"Coding/WinUI/API/#tabview","text":"","title":"TabView"},{"location":"Coding/WinUI/API/#textbox","text":"XAML tkinter <Window x:Class= \"EmployeeManager.WinUI.MainWindow\" xmlns= \"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x= \"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local= \"using:EmployeeManager.WinUI\" xmlns:d= \"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc= \"http://schemas.openxmlformats.org/markup-compatibility/2006\" mc:Ignorable= \"d\" > <TextBox Header= \"First name\" /> </Window> import tkinter as tk from tkinter.ttk import Entry from tkinter.ttk import LabelFrame win = tk . Tk () frame = LabelFrame ( win , text = \"First name\" ) frame . pack () Entry ( frame ) . pack () tk . mainloop ()","title":"Textbox"},{"location":"Coding/WinUI/API/#variablesizedwrapgrid","text":"VariableSizedWrapGrid can be used to define a field of tiles similar to an HTML flex container ( display: flex; with flex-wrap: wrap; ). The Orientation property is similar to a flex container's flex-direction , in that the direction of alignment can be specified. <Page> <VariableSizedWrapGrid Orientation= \"Horizontal\" ItemWidth= \"100\" ItemHeight= \"100\" > <Rectangle Fill= \"LightGray\" /> <Rectangle Fill= \"LightSteelBlue\" /> <Rectangle Fill= \"LightBlue\" /> <Rectangle Fill= \"LightCyan\" /> <Rectangle Fill= \"LightSeaGreen\" /> <Rectangle Fill= \"LightGreen\" /> <Rectangle Fill= \"LightGoldenrodYellow\" /> <Rectangle Fill= \"LightSalmon\" /> <Rectangle Fill= \"LightCoral\" /> <Rectangle Fill= \"Gray\" /> <Rectangle Fill= \"SteelBlue\" /> <Rectangle Fill= \"CadetBlue\" /> <Rectangle Fill= \"Cyan\" /> <Rectangle Fill= \"SeaGreen\" /> <Rectangle Fill= \"Green\" /> <Rectangle Fill= \"Goldenrod\" /> <Rectangle Fill= \"Salmon\" /> <Rectangle Fill= \"Coral\" /> </VariableSizedWrapGrid> </Page> Notably, the horizontal or vertical alignment of XAML controls is defined on each control, whereas in HTML alignment is specified at the level of the enclosing container. XAML HTML and CSS <TextBlock Content= \"Hello, world!\" HorizontalAlignment= \"Left\" VerticalAlignment= \"Top\" /> .container p Hello, world! . container { text-align : right top ; }","title":"VariableSizedWrapGrid"},{"location":"GTK/","text":"Overview Resources PyGObject API reference PyGObject docs Documentation for Rust GTK bindings GTK3 GTK4 GTK is an open-source cross-platform widget toolkit developed by The GNOME Project for creating GUI applications. Major desktop environments including GNOME and Xfce are based on GTK. GTK was originally designed for use in GIMP as a replacement for the previous Motif toolkit which was unsatisfactory. Since GTK 2.8 (2005), GTK uses the Cairo library to render vector graphics. Building GTK application UIs can be done procedurally by defining UI elements in code or declaratively in XML interfaces. Interfaces XML interfaces can be defined in XML files or less commonly as string literals which are then loaded in the constructor for Gtk.Builder . Interfaces are also known as \"Glade files\" after the popular Glade UI design application. <?xml version=\"1.0\" encoding=\"UTF-8\"?> <interface> # (1) <requires lib= \"gtk+\" version= \"3.40\" /> <!-- (2) --> <object class= \"GtkApplicationWindow\" id= \"window\" > <signal name= \"destroy\" handler= \"on_main_window_destroy\" /> <!-- (3) --> <child> <!-- (4) --> <object class= \"GtkBox\" > <property name= \"orientation\" > vertical </property> <!-- (5) --> <object class= \"GtkLabel\" id= \"label\" > <property name= \"label\" > Hello, World! </property> </object> </object> </child> </object> </interface> The root node in these files is the interface element itself. The first direct child of an interface is the requires element, with a version number that specifies the required version of GTK. If this interface used Gtk.ApplicationWindow instead of Gtk.Window , this number would have to be 3.40 because that is the version this class was introduced. Signals and callbacks can be specified in the markup on the signal element, which is the direct child to the object emitting the signal and also self-closing. Callback method names are specified in the handler attribute Container widgets like Gtk.Box wrap every child in a child element. Note that the interface element does not need a but is immediate parent to the outermost container of the UI. Each property of an object is a property element with the name of the property provided in a name attribute and the value provided as the element's value. The UI is then loaded into the application using Gtk.Builder . Individual UI elements can be bound if they have an id attribute assigned. Rust Python fn main () { let app = gtk :: Application :: builder () . application_id ( \"org.example.gtk-app\" ) . build (); app . connect_activate ( build_ui ); app . run (); } fn build_ui ( app : & Application ) : { let builder = gtk :: Builder :: from_string ( include_str! ( \"window.ui\" )); let window : ApplicationWindow = builder . object ( \"window\" ) . expect ( \"Error loading ApplicationWindow!\" ); window . set_application ( Some ( app )); window . show_all (); window . present (); } class Application ( Gtk . Application ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( application_id = \"org.example.gtk-app\" ) def do_activate ( self ): builder = Gtk . Builder . new_from_file ( \"window.ui\" ) self . window = builder . get_object ( \"window\" ) self . window . show_all () self . window . present () def run ( self ): super () . run () Gtk . main () # (1) For some reason, one of the differences between a PyGTK script that specifies its UI procedurally in code4 and one that takes it from an interface file is that Gtk.main() must be explicitly called somewhere. The Application object's run() method can be overridden to call it, or it can be placed in the script's entrypoint. Builder pattern A typical ways of building UIs procedurally in gtk-rs is the Builder design pattern . This pattern supports procedural construction of the object using a chain of method calls, ending in build() . Note, this is not to be confused with the Gtk.Builder API that is actually used for declarative UI specification in interfaces . gtk-rs use gtk4 as gtk ; use gtk :: prelude :: * ; use gtk :: { Application , ApplicationWindow }; fn main () { let app = Application :: builder () . application_id ( \"org.example.HelloWorld\" ) . build (); app . connect_activate ( | app | { let window = ApplicationWindow :: builder () . application ( app ) . default_width ( 320 ) . default_height ( 200 ) . title ( \"Hello, World!\" ) . build (); window . show (); }); app . run (); } Other objects, like Box , do not expose this API. connect_activate This simple (nonfunctional) example produces an error that demands a handler be implemented for the activate signal. use gtk :: prelude :: * ; use gtk :: Application ; fn main () { let app = Application :: builder () . application_id ( \"org.gtk-rs.example\" ) . build (); app . run (); } Here the activate signal is bound to the build_ui() function. Alternatively, a closure can be used for simple windows. ApplicationWindow's present() and show() methods appear to be interchangeable. Function Closure fn main () { // ... app . connect_activate ( build_ui ); app . run (); } fn build_ui ( app : & Application ) { let window = ApplicationWindow :: builder () . application ( app ) . title ( \"My GTK App\" ) . build (); window . present (); } fn main () { // ... app . connect_activate ( | app | { let window = ApplicationWindow :: builder () . application ( app ) . title ( \"Hello, World!\" ) . build (); window . show (); }); app . run (); } Widgets are added as children of containers like ApplicationWindow or Box . This can be done using one of two ways: child() builder helper method, passing an immutable reference to the widget. set_child() method after instantiation, but this time passing a Some() result containing the immutable reference to the widget. child() set_child() let window = ApplicationWindow :: builder () . application ( app ) . title ( \"Hello, World!\" ) . child ( & button ) . build (); let window = ApplicationWindow :: builder () . application ( app ) . title ( \"Hello, World!\" ) . build (); window . set_child ( Some ( & button ));","title":"Overview"},{"location":"GTK/#overview","text":"Resources PyGObject API reference PyGObject docs Documentation for Rust GTK bindings GTK3 GTK4 GTK is an open-source cross-platform widget toolkit developed by The GNOME Project for creating GUI applications. Major desktop environments including GNOME and Xfce are based on GTK. GTK was originally designed for use in GIMP as a replacement for the previous Motif toolkit which was unsatisfactory. Since GTK 2.8 (2005), GTK uses the Cairo library to render vector graphics. Building GTK application UIs can be done procedurally by defining UI elements in code or declaratively in XML interfaces.","title":"Overview"},{"location":"GTK/#interfaces","text":"XML interfaces can be defined in XML files or less commonly as string literals which are then loaded in the constructor for Gtk.Builder . Interfaces are also known as \"Glade files\" after the popular Glade UI design application. <?xml version=\"1.0\" encoding=\"UTF-8\"?> <interface> # (1) <requires lib= \"gtk+\" version= \"3.40\" /> <!-- (2) --> <object class= \"GtkApplicationWindow\" id= \"window\" > <signal name= \"destroy\" handler= \"on_main_window_destroy\" /> <!-- (3) --> <child> <!-- (4) --> <object class= \"GtkBox\" > <property name= \"orientation\" > vertical </property> <!-- (5) --> <object class= \"GtkLabel\" id= \"label\" > <property name= \"label\" > Hello, World! </property> </object> </object> </child> </object> </interface> The root node in these files is the interface element itself. The first direct child of an interface is the requires element, with a version number that specifies the required version of GTK. If this interface used Gtk.ApplicationWindow instead of Gtk.Window , this number would have to be 3.40 because that is the version this class was introduced. Signals and callbacks can be specified in the markup on the signal element, which is the direct child to the object emitting the signal and also self-closing. Callback method names are specified in the handler attribute Container widgets like Gtk.Box wrap every child in a child element. Note that the interface element does not need a but is immediate parent to the outermost container of the UI. Each property of an object is a property element with the name of the property provided in a name attribute and the value provided as the element's value. The UI is then loaded into the application using Gtk.Builder . Individual UI elements can be bound if they have an id attribute assigned. Rust Python fn main () { let app = gtk :: Application :: builder () . application_id ( \"org.example.gtk-app\" ) . build (); app . connect_activate ( build_ui ); app . run (); } fn build_ui ( app : & Application ) : { let builder = gtk :: Builder :: from_string ( include_str! ( \"window.ui\" )); let window : ApplicationWindow = builder . object ( \"window\" ) . expect ( \"Error loading ApplicationWindow!\" ); window . set_application ( Some ( app )); window . show_all (); window . present (); } class Application ( Gtk . Application ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( application_id = \"org.example.gtk-app\" ) def do_activate ( self ): builder = Gtk . Builder . new_from_file ( \"window.ui\" ) self . window = builder . get_object ( \"window\" ) self . window . show_all () self . window . present () def run ( self ): super () . run () Gtk . main () # (1) For some reason, one of the differences between a PyGTK script that specifies its UI procedurally in code4 and one that takes it from an interface file is that Gtk.main() must be explicitly called somewhere. The Application object's run() method can be overridden to call it, or it can be placed in the script's entrypoint.","title":"Interfaces"},{"location":"GTK/#builder-pattern","text":"A typical ways of building UIs procedurally in gtk-rs is the Builder design pattern . This pattern supports procedural construction of the object using a chain of method calls, ending in build() . Note, this is not to be confused with the Gtk.Builder API that is actually used for declarative UI specification in interfaces . gtk-rs use gtk4 as gtk ; use gtk :: prelude :: * ; use gtk :: { Application , ApplicationWindow }; fn main () { let app = Application :: builder () . application_id ( \"org.example.HelloWorld\" ) . build (); app . connect_activate ( | app | { let window = ApplicationWindow :: builder () . application ( app ) . default_width ( 320 ) . default_height ( 200 ) . title ( \"Hello, World!\" ) . build (); window . show (); }); app . run (); } Other objects, like Box , do not expose this API.","title":"Builder pattern"},{"location":"GTK/#connect_activate","text":"This simple (nonfunctional) example produces an error that demands a handler be implemented for the activate signal. use gtk :: prelude :: * ; use gtk :: Application ; fn main () { let app = Application :: builder () . application_id ( \"org.gtk-rs.example\" ) . build (); app . run (); } Here the activate signal is bound to the build_ui() function. Alternatively, a closure can be used for simple windows. ApplicationWindow's present() and show() methods appear to be interchangeable. Function Closure fn main () { // ... app . connect_activate ( build_ui ); app . run (); } fn build_ui ( app : & Application ) { let window = ApplicationWindow :: builder () . application ( app ) . title ( \"My GTK App\" ) . build (); window . present (); } fn main () { // ... app . connect_activate ( | app | { let window = ApplicationWindow :: builder () . application ( app ) . title ( \"Hello, World!\" ) . build (); window . show (); }); app . run (); } Widgets are added as children of containers like ApplicationWindow or Box . This can be done using one of two ways: child() builder helper method, passing an immutable reference to the widget. set_child() method after instantiation, but this time passing a Some() result containing the immutable reference to the widget. child() set_child() let window = ApplicationWindow :: builder () . application ( app ) . title ( \"Hello, World!\" ) . child ( & button ) . build (); let window = ApplicationWindow :: builder () . application ( app ) . title ( \"Hello, World!\" ) . build (); window . set_child ( Some ( & button ));","title":"connect_activate"},{"location":"GTK/API/","text":"Gio Action Gio.Action is a way to expose any single task an application or widget does by a name. Classes like Gio.MenuItem and Gtk.ModelButton support properties to set an action name. These actions can be collected into a Gio.ActionGroup . Gio.ActionMap are interfaces implemented by Gtk.ApplicationWindow Gtk ActionGroup Adjustment Gtk.Adjustment is not a widget per se but is used in many widgets, including spin buttons, view ports, and children of Gtk.Range . page increment and page size refer to actions taken when the user presses PgUp or PgDn Gtk . Adjustment . new ( initial_value , lower_range , upper_range , step_increment , page_increment , page_size ) Alignment Gtk.Alignment controls the alignment and size of its child widget. Application Subclasses of Gtk.Application encapsulate application behavior, including application startup and CLI processing. In practice it is simply a wrapper for the ApplicationWindow class which is instantiated in the do_activate() hook. Notably, the Application subclass provides the value for the application_id kwarg passed to the Gtk.Application constructor. This value is validated, and any simple string is not silently accepted. Application must expose several important methods: do_activate() def do_activate ( self ): self . window = ApplicationWindow ( application = self , title = \"Hello, World!\" ) self . window . show_all () self . window . present () do_startup() ApplicationWindow The Gtk.ApplicationWindow class is the main visible window for the application, and the only window for \"single-instance\" applications (which is the default). The ApplicationWindow class was introduced in GTK 3.4. When an action has the prefix win. it specifies that the ApplicationWindow subclass will process the signal. Assistant Gtk.Assistant widgets are used to build wizards. Box Builder Gtk.Builder allows the use of interfaces to define widget layouts. Individual UI elements can be bound if they have an id attribute assigned. Rust Python fn main () { let app = gtk :: Application :: builder () . application_id ( \"org.example.gtk-app\" ) . build (); app . connect_activate ( build_ui ); app . run (); } fn build_ui ( app : & Application ) : { let builder = gtk :: Builder :: from_string ( include_str! ( \"window.ui\" )); let window : ApplicationWindow = builder . object ( \"window\" ) . expect ( \"Error loading ApplicationWindow!\" ); window . set_application ( Some ( app )); window . show_all (); window . present (); } class Application ( Gtk . Application ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( application_id = \"org.example.gtk-app\" ) def do_activate ( self ): builder = Gtk . Builder . new_from_file ( \"window.ui\" ) self . window = builder . get_object ( \"window\" ) self . window . show_all () self . window . present () def run ( self ): super () . run () Gtk . main () CheckButton Gtk.CheckButton s are checkboxes. clone glib::clone! is used to facilitate passing strong or weak references into closures. Strong Weak use glib ; use glib_macros :: clone ; use std :: rc :: Rc ; let v = Rc :: new ( 1 ); let closure = clone ! ( @ strong v => move | x | { println! ( \"v: {}, x: {}\" , v , x ); }); closure ( 2 ); use glib ; use glib_macros :: clone ; use std :: rc :: Rc ; let u = Rc :: new ( 2 ); let closure = clone ! ( @ weak u => move | x | { println! ( \"u: {}, x: {}\" , u , x ); }); closure ( 3 ); Container Both Gtk.ApplicationWindow and Gtk.Window classes indirectly derive from the abstract class Gtk.Container . The main purpose of a container subclass is to allow a parent widget to contain one or more child widgets, and there are two types: Dialog Gtk.Dialog provides a convenient way to prompt the user for a small amount of input. It is a widget that can be instantiated and customized in its own right as well as a parent to various subclasses. dialog = Gtk . Dialog ( title = \"Hello, World!\" , parent = parent ) Dialogs are split into two parts: Content area containing interactive widgets Action area containing buttons These areas are both combined in a vertical Box that is assigned to the vbox field. The action area is packed to the end of this vbox, so the pack_start() method is used to add widgets to the content area. Dialog boxes can be modal , meaning they prevent interaction with the main window while open, or nonmodal . Modal Nonmodal dialog = Gtk . Dialog ( title = \"Hello, World!\" , parent = parent , modal = True ) dialog = Gtk . Dialog ( title = \"Hello, World!\" , parent = parent , modal = False ) Gtk.MessageDialog is a subtype of Dialog meant to simplify the process of creating simple dialogs. Buttons are added procedurally using add_button() , passing a display string (with support for mnemonics using _ ) and a ResponseType enum (they once could be added on instantiation by passing a tuple to the buttons keyword argument). dialog . add_button ( \"_OK\" , Gtk . ResponseType . OK ) Methods: add_button() Entry Unlike other widgets, Gtk.Entry can be instantiated without using a specific constructor. entry = Gtk . Entry () Default text can be provided by passing a string to the text keyword argument or with the set_text() setter method: kwarg setter entry = Gtk . Entry ( text = \"Hello, World!\" ) entry . set_text ( \"Hello, World!\" ) A password field can be made by concealing text by passing False to visibility or with the set_visibility() setter: kwarg setter password = Gtk . Entry ( visibility = False ) password . set_visibility ( False ) get_text() retrieve contents (string) set_visibility(bool) conceal text EventBox Gtk.EventBox is a container widget that allows event handling for widgets like Gtk.Label that do not have an associated GDK window. The event box can be positioned above or below the windows of its child with set_above_child() (False by default.) An EventBox must also have a Gtk.EventMask enum set to specify the type of events the widget may receive. This enum is passed as a value to set_events() . In the following example, an event handler is connected to the EventBox to handle button_press_event . This event handler changes the text of the Label after a double-click. import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk , Gdk class AppWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_border_width ( 10 ) self . set_size_request ( 200 , 50 ) eventbox = Gtk . EventBox . new () label = Gtk . Label . new ( \"Double-Click Me!\" ) eventbox . set_above_child ( False ) eventbox . connect ( \"button_press_event\" , self . on_button_pressed , label ) eventbox . add ( label ) self . add ( eventbox ) eventbox . set_events ( Gdk . EventMask . BUTTON_PRESS_MASK ) eventbox . realize () def on_button_pressed ( self , eventbox , event , label ): if event . type == Gdk . EventType . _2BUTTON_PRESS : text = label . get_text () if text [ 0 ] == 'D' : label . set_text ( \"I Was Double-Clicked!\" ) else : label . set_text ( \"Double-Click Me Again!\" ) return False class Application ( Gtk . Application ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , application_id = \"org.example.myapp\" , ** kwargs ) self . window = None def do_activate ( self ): if not self . window : self . window = AppWindow ( application = self , title = \"Hello World!\" ) self . window . show_all () self . window . present () if __name__ == \"__main__\" : app = Application () app . run () FileChooserDialog Gtk.FileChooserDialog is one of the important subtypes of Gtk.Dialog . Like other dialogs, it is provided a title and parent window on instantiation. Additionally a FileChooserAction enum must be specified. FileChooserAction s include: Gtk.FileChooserAction.SAVE Gtk.FileChooserAction.OPEN Gtk.FileChooserAction.SELECT_FOLDER Gtk.FileChooserAction.CREATE_FOLDER dialog = Gtk . FileChooserDialog ( title = \"Save file as ...\" , parent = parent , action = FileChooserAction . SAVE ) Selected files are then retrieved using dialog . get_filenames () | Setter | Property | Description | | --------------------------------------------------------------------------------------------------------------------------------- | ----------------- | | set_current_folder | | Specify directory in filesystem where FileChooser will start | | set_current_name | | For FileChooserAction.SAVE, suggest a filename | | set_select_multiple | select_multiple | For FileChooserAction.OPEN or SELECT_FOLDER, allow multiple file or folder selections | Grid Gtk.Grid allows children to be packed in a two-dimensional grid. Grids are instantiated with new() and widgets are laid out by calling attach() (see Login for an example). attach() lay out a widget providing column and row numbers followed by column and row spans grid . attach ( label , 0 , 0 , 1 , 1 ) HeaderBar Gtk.HeaderBar allows the titlebar to be customized. Like other widgets, it can be configured on instantiation by providing values to keyword arguments or by using setters. Adding to a window HeaderBars are added with set_titlebar() . This is unlike other widgets which are assigned to an ApplicationWindow or Window using pack_start() , pack_end() , or add() , kwarg setter class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) headerbar = Gtk . HeaderBar ( title = f \"Hello, World!\" , subtitle = \"HeaderBar example\" , show_close_button = True ) self . set_titlebar ( headerbar ) class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) headerbar = Gtk . HeaderBar () headerbar . set_title ( f \"Hello, World!\" ) headerbar . set_subtitle ( \"HeaderBar example\" ) headerbar . set_show_close_button ( True ) self . set_titlebar ( headerbar ) Label Note that Gtk.Label sets its text with \"label\" and not \"text\" as you may expect from the corresponding setter. kwarg setter label = Gtk . Label ( label = \"Hello, World!\" ) label . set_text ( \"Hello, World!\" ) ListBox Gtk.ListBox is a vertical container of Gtk.ListBoxRow children used as an alternative to TreeView when the children need to be interactive, as in a list of settings. ListBox ListStore Gtk.ListStore is one of the two major classes that serves as combination schema and database backing Gtk.TreeView , the other being Gtk.TreeStore . It is instantiated with a sequence of data types, similar to a database schema. These can be standard Python types or GObjects (which are mapped to the Python types anyway): Python types GObject types import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk liststore = Gtk . ListStore (( str , int , str )) import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk , GObject liststore = Gtk . ListStore (( GObject . TYPE_STRING , GOBject ) . TYPE_INT , GObject . TYPE_STRING )) This object then exposes an append method which is used to add records: liststore . append ([ \"Socrates\" , 350 , \"Athens\" ]) The store is then associated with the treeview with set_model treeview . set_model ( liststore ) MenuBar Gtk.MenuBar is populated with Gtk.MenuItems , corresponding to the expandable menu items (i.e. \"File\", \"Edit\", and \"Help\"). Gtk.Menu is actually used for the submenu, which like MenuBar is also populared with MenuItems. A Menu is attached to the MenuItem of a MenuBar by using the set_submenu() method on the Menu object. This setter does not have a corresponding kwarg, so all menus have to be constructed procedurally. import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk class AppWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_size_request ( 250 , - 1 ) menubar = Gtk . MenuBar . new () self . add ( menubar ) file = Gtk . MenuItem . new_with_label ( \"File\" ) menubar . append ( file ) filemenu = Gtk . Menu . new () file . set_submenu ( filemenu ) new = Gtk . MenuItem . new_with_label ( \"New\" ) open = Gtk . MenuItem . new_with_label ( \"Open\" ) filemenu . append ( new ) filemenu . append ( open ) edit = Gtk . MenuItem . new_with_label ( \"Edit\" ) menubar . append ( edit ) editmenu = Gtk . Menu . new () edit . set_submenu ( editmenu ) cut = Gtk . MenuItem . new_with_label ( \"Cut\" ) copy = Gtk . MenuItem . new_with_label ( \"Copy\" ) paste = Gtk . MenuItem . new_with_label ( \"Paste\" ) editmenu . append ( cut ) editmenu . append ( copy ) editmenu . append ( paste ) help = Gtk . MenuItem . new_with_label ( \"Help\" ) menubar . append ( help ) helpmenu = Gtk . Menu . new () help . set_submenu ( helpmenu ) contents = Gtk . MenuItem . new_with_label ( \"Help\" ) about = Gtk . MenuItem . new_with_label ( \"About\" ) helpmenu . append ( contents ) helpmenu . append ( about ) class Application ( Gtk . Application ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , application_id = \"org.example.myapp\" , ** kwargs ) self . window = None def do_activate ( self ): if not self . window : self . window = AppWindow ( application = self , title = \"Menu Bars\" ) self . window . show_all () self . window . present () if __name__ == \"__main__\" : app = Application () app . run () Notebook Gtk.Notebook is a layout container that organizes content into tabbed pages. It is instantiated with the new() method and pages are appended with the append_page() method, passing content and label widgets as arguments. The tab bar can be placed using set_tab_pos() , passing a Gtk.PositionType enum Top Right Bottom Left notebook = Gtk . Notebook . new () # notebook.set_tab_pos(Gtk.PositionType.TOP) notebook = Gtk . Notebook . new () notebook . set_tab_pos ( Gtk . PositionType . RIGHT ) notebook = Gtk . Notebook . new () notebook . set_tab_pos ( Gtk . PositionType . BOTTOM ) notebook = Gtk . Notebook . new () notebook . set_tab_pos ( Gtk . PositionType . LEFT ) notebook = Gtk . Notebook . new () label = Gtk . Label . new ( \"Tab title\" ) child = Gtk . Label . new ( \"Tab content\" ) notebook . append_page ( child , label ) The label widget is commonly Gtk.Label but can also be a Gtk.Box . The tab bar can be made scrollable using set_scrollable() , passing a bool. Scale Gtk.Scale widgets are sliders, and they can be instantiated in one of two ways: new() passing an Adjustment object new_with_range(min, max, step) passing values for minimum, maximum, and step Scale values are stored as doubles, so integers have to be simulated by reducing the number of digits to 0 using set_digits() . By default, the number of digits is set to that of the step value. ScrolledWindow Gtk.ScrolledWindow is a decorator container that accepts a single child widget. Widgets that implement the Gtk.Scrollable interface have native scrolling suppport, like Gtk.TreeView , Gtk.TextView, and Gtk.Layout. Other widgets have to use Gtk.Viewport as an adaptor, and must be added to a Viewport which is then added to the ScrolledWindow. It is instantiated with the new() method, optionally passing two Adjustment objects that affect horizontal and vertical scrolling behavior when stepping or paging. scrolled_win = Gtk . ScrolledWindow . new ( None , None ) Statusbar Gtk.Statusbar (note the lowercase b ) stores a stack of messages, the topmost of which is displayed. Before adding messages, a context identifier , a unique unsigned integer associated with a context description string, must be retrieved from the newly created Statusbar by passing a string value to get_context_id() . This allows messages to be categorized and pushed to separate stacks. statusbar . push ( context_id , message ) Switch Gtk.Switch allows a user to toggle a boolean value. Switch exposes getters and setters for both state (which is represented by the trough color) and active (switch position) properties. State is the backend to activ, and they are kept in sync. import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_border_width ( 10 ) box_outer = Gtk . Box ( orientation = Gtk . Orientation . VERTICAL , spacing = 6 ) listbox = Gtk . ListBox ( selection_mode = Gtk . SelectionMode . NONE ) row = Gtk . ListBoxRow () hbox = Gtk . Box ( orientation = Gtk . Orientation . HORIZONTAL , spacing = 50 ) label1 = Gtk . Label ( label = \"Automatic Date & Time\" , xalign = 0 ) hbox . add ( label1 ) self . switch = Gtk . Switch ( valign = Gtk . Align . CENTER , state = False ) hbox . add ( self . switch ) row . add ( hbox ) listbox . add ( row ) box_outer . add ( listbox ) self . add ( box_outer ) button = Gtk . Button ( label = \"Click\" ) button . connect ( \"clicked\" , self . on_button_clicked ) box_outer . add ( button ) def on_button_clicked ( self , button ): print ( f \"Value of get_active(): { self . switch . get_active () } \" ) print ( f \"Value of get_state(): { self . switch . get_state () } \" ) class Application ( Gtk . Application ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . window = None def do_activate ( self ): if not self . window : self . window = ApplicationWindow ( application = self ) self . window . show_all () self . window . present () if __name__ == \"__main__\" : app = Application () app . run () TreeView In order to create a tree or list in GTK, the Gtk.TreeView widget is paired with a Gtk.TreeModel interface, the most typical implementation of which is Gtk.ListStore or Gtk.TreeStore . TreeView is a complicated widget that must be constructed procedurally: Gtk.TreeView is instantiated. A ListStore is specified as data model and passed in as the value of the model kwarg. The ListStore specifies the schema of the data as a collection of types. treeview = Gtk . TreeView ( model = Gtk . ListStore . new (( str ))) Alternatively, the ListStore can be specified after instantiation. treeview = Gtk . TreeView . new () treeview . set_model ( Gtk . ListStore . new ([ str ])) A Gtk.TreeViewColumn is created for every column in the model. These require a Gtk.CellRenderer to be defined. The TreeViewColumn is added to the treeview by calling the append_column() method on the treeview. The text kwarg appears to refer to the column of the data store to use for the column's values. treeview . append_column ( Gtk . TreeViewColumn ( \"Greeks\" , Gtk . CellRendererText . new (), text = 0 )) Items are added to the ListStore procedurally using the append() method. Note that the method takes only a single argument, so collections like lists or tuples must be used. liststore . append (( \"Socrates\" ,)) liststore . append (( \"Plato\" ,)) liststore . append (( \"Aristotle\" ,)) Changing the number of columns affects the types used to define the ListStore, the appended records, as well as the number of columns added to the TreeView itself. 1 column 2 columns import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . gen_treeview () scrolled_win = Gtk . ScrolledWindow . new ( None , None ) scrolled_win . set_policy ( Gtk . PolicyType . AUTOMATIC , Gtk . PolicyType . AUTOMATIC ) scrolled_win . add ( self . treeview ) self . add ( scrolled_win ) self . set_size_request ( 200 , 200 ) def get_liststore ( self ): store = Gtk . ListStore . new (( str ,)) store . append (( \"Socrates\" ,)) store . append (( \"Plato\" ,)) store . append (( \"Aristotle\" ,)) return store def gen_treeview ( self ): self . treeview = Gtk . TreeView . new () self . treeview . set_model ( self . get_liststore ()) self . treeview . append_column ( Gtk . TreeViewColumn ( \"Greeks\" , Gtk . CellRendererText . new (), text = 0 )) class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.myapp' ) def do_activate ( self ): self . window = ApplicationWindow ( application = self , title = \"Greeks\" ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = Application () app . run () import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . gen_treeview () scrolled_win = Gtk . ScrolledWindow . new ( None , None ) scrolled_win . set_policy ( Gtk . PolicyType . AUTOMATIC , Gtk . PolicyType . AUTOMATIC ) scrolled_win . add ( self . treeview ) self . add ( scrolled_win ) self . set_size_request ( 200 , 200 ) def get_liststore ( self ): store = Gtk . ListStore . new (( str , str )) store . append ([ \"Socrates\" , \"Athens\" ]) store . append ([ \"Plato\" , \"Athens\" ]) store . append ([ \"Aristotle\" , \"Athens\" ]) return store def gen_treeview ( self ): self . treeview = Gtk . TreeView . new () self . treeview . set_model ( self . get_liststore ()) self . treeview . append_column ( Gtk . TreeViewColumn ( \"Greeks\" , Gtk . CellRendererText . new (), text = 0 )) self . treeview . append_column ( Gtk . TreeViewColumn ( \"Place of birth\" , Gtk . CellRendererText . new (), text = 1 )) class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.myapp' ) def do_activate ( self ): self . window = ApplicationWindow ( application = self , title = \"Greeks\" ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = Application () app . run () The model backing a TreeView (usually a ListStore ), can be retrieved with the get_model() method. treeview . get_model () . append (( 'foo' , 'bar' )) TreeView emits several signals: row_activated when a row is double-clicked, with the following implicit argument widget refering to the emitting TreeView widget itself path is a TreePath . column is of type TreeViewcolumn treeview . connect ( \"row_activated\" , self . on_row_activated , widget , path , column ) def on_row_activated ( self , widget , path , column ): row = path . get_indices ()[ 0 ] print ( f \"row= { path . get_indices ()[ 0 ] } ,col= { column . props . title } \" ) print ( widget . get_model ()[ row ][:]) TreePath Gtk.TreePath is a type used to implement the rows of a TreeView . Although it prints to an integer with the print statement, it cannot be treated as one. A path object can be passed as the index to a TreeModel like ListStore , as can an integer. The row number of a TreePath from a normal list-style TreeView can be retrieved with the get_indices() method. row = path . get_indices ()[ 0 ] # Using TreePath object as index to model model [ path ][:] # Using row integer as index to model model [ row ][:] Another method on TreePath, get_depth() always returns 1 for list-style TreeViews, but may be more useful for tree-style TreeViews. TreeSelection Gtk.TreeSelection objects represent selection information for each tree view. TreeViewColumn Gtk.TreeViewColumn represents a visible column in a Treeview . Its props property exposes many associated values, including title. print ( column . props . title ) A column is made sortable by calling set_sort_column_id() , passing the column of the model to sort by. column . set_sort_column_id ( 0 ) Window Gtk.Window","title":"API"},{"location":"GTK/API/#gio","text":"","title":"Gio"},{"location":"GTK/API/#action","text":"Gio.Action is a way to expose any single task an application or widget does by a name. Classes like Gio.MenuItem and Gtk.ModelButton support properties to set an action name. These actions can be collected into a Gio.ActionGroup . Gio.ActionMap are interfaces implemented by Gtk.ApplicationWindow","title":"Action"},{"location":"GTK/API/#gtk","text":"","title":"Gtk"},{"location":"GTK/API/#actiongroup_1","text":"","title":"ActionGroup"},{"location":"GTK/API/#adjustment","text":"Gtk.Adjustment is not a widget per se but is used in many widgets, including spin buttons, view ports, and children of Gtk.Range . page increment and page size refer to actions taken when the user presses PgUp or PgDn Gtk . Adjustment . new ( initial_value , lower_range , upper_range , step_increment , page_increment , page_size )","title":"Adjustment"},{"location":"GTK/API/#alignment","text":"Gtk.Alignment controls the alignment and size of its child widget.","title":"Alignment"},{"location":"GTK/API/#application","text":"Subclasses of Gtk.Application encapsulate application behavior, including application startup and CLI processing. In practice it is simply a wrapper for the ApplicationWindow class which is instantiated in the do_activate() hook. Notably, the Application subclass provides the value for the application_id kwarg passed to the Gtk.Application constructor. This value is validated, and any simple string is not silently accepted. Application must expose several important methods: do_activate() def do_activate ( self ): self . window = ApplicationWindow ( application = self , title = \"Hello, World!\" ) self . window . show_all () self . window . present () do_startup()","title":"Application"},{"location":"GTK/API/#applicationwindow","text":"The Gtk.ApplicationWindow class is the main visible window for the application, and the only window for \"single-instance\" applications (which is the default). The ApplicationWindow class was introduced in GTK 3.4. When an action has the prefix win. it specifies that the ApplicationWindow subclass will process the signal.","title":"ApplicationWindow"},{"location":"GTK/API/#assistant","text":"Gtk.Assistant widgets are used to build wizards.","title":"Assistant"},{"location":"GTK/API/#box","text":"","title":"Box"},{"location":"GTK/API/#builder","text":"Gtk.Builder allows the use of interfaces to define widget layouts. Individual UI elements can be bound if they have an id attribute assigned. Rust Python fn main () { let app = gtk :: Application :: builder () . application_id ( \"org.example.gtk-app\" ) . build (); app . connect_activate ( build_ui ); app . run (); } fn build_ui ( app : & Application ) : { let builder = gtk :: Builder :: from_string ( include_str! ( \"window.ui\" )); let window : ApplicationWindow = builder . object ( \"window\" ) . expect ( \"Error loading ApplicationWindow!\" ); window . set_application ( Some ( app )); window . show_all (); window . present (); } class Application ( Gtk . Application ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( application_id = \"org.example.gtk-app\" ) def do_activate ( self ): builder = Gtk . Builder . new_from_file ( \"window.ui\" ) self . window = builder . get_object ( \"window\" ) self . window . show_all () self . window . present () def run ( self ): super () . run () Gtk . main ()","title":"Builder"},{"location":"GTK/API/#checkbutton","text":"Gtk.CheckButton s are checkboxes.","title":"CheckButton"},{"location":"GTK/API/#clone","text":"glib::clone! is used to facilitate passing strong or weak references into closures. Strong Weak use glib ; use glib_macros :: clone ; use std :: rc :: Rc ; let v = Rc :: new ( 1 ); let closure = clone ! ( @ strong v => move | x | { println! ( \"v: {}, x: {}\" , v , x ); }); closure ( 2 ); use glib ; use glib_macros :: clone ; use std :: rc :: Rc ; let u = Rc :: new ( 2 ); let closure = clone ! ( @ weak u => move | x | { println! ( \"u: {}, x: {}\" , u , x ); }); closure ( 3 );","title":"clone"},{"location":"GTK/API/#container","text":"Both Gtk.ApplicationWindow and Gtk.Window classes indirectly derive from the abstract class Gtk.Container . The main purpose of a container subclass is to allow a parent widget to contain one or more child widgets, and there are two types:","title":"Container"},{"location":"GTK/API/#dialog","text":"Gtk.Dialog provides a convenient way to prompt the user for a small amount of input. It is a widget that can be instantiated and customized in its own right as well as a parent to various subclasses. dialog = Gtk . Dialog ( title = \"Hello, World!\" , parent = parent ) Dialogs are split into two parts: Content area containing interactive widgets Action area containing buttons These areas are both combined in a vertical Box that is assigned to the vbox field. The action area is packed to the end of this vbox, so the pack_start() method is used to add widgets to the content area. Dialog boxes can be modal , meaning they prevent interaction with the main window while open, or nonmodal . Modal Nonmodal dialog = Gtk . Dialog ( title = \"Hello, World!\" , parent = parent , modal = True ) dialog = Gtk . Dialog ( title = \"Hello, World!\" , parent = parent , modal = False ) Gtk.MessageDialog is a subtype of Dialog meant to simplify the process of creating simple dialogs. Buttons are added procedurally using add_button() , passing a display string (with support for mnemonics using _ ) and a ResponseType enum (they once could be added on instantiation by passing a tuple to the buttons keyword argument). dialog . add_button ( \"_OK\" , Gtk . ResponseType . OK ) Methods: add_button()","title":"Dialog"},{"location":"GTK/API/#entry","text":"Unlike other widgets, Gtk.Entry can be instantiated without using a specific constructor. entry = Gtk . Entry () Default text can be provided by passing a string to the text keyword argument or with the set_text() setter method: kwarg setter entry = Gtk . Entry ( text = \"Hello, World!\" ) entry . set_text ( \"Hello, World!\" ) A password field can be made by concealing text by passing False to visibility or with the set_visibility() setter: kwarg setter password = Gtk . Entry ( visibility = False ) password . set_visibility ( False ) get_text() retrieve contents (string) set_visibility(bool) conceal text","title":"Entry"},{"location":"GTK/API/#eventbox","text":"Gtk.EventBox is a container widget that allows event handling for widgets like Gtk.Label that do not have an associated GDK window. The event box can be positioned above or below the windows of its child with set_above_child() (False by default.) An EventBox must also have a Gtk.EventMask enum set to specify the type of events the widget may receive. This enum is passed as a value to set_events() . In the following example, an event handler is connected to the EventBox to handle button_press_event . This event handler changes the text of the Label after a double-click. import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk , Gdk class AppWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_border_width ( 10 ) self . set_size_request ( 200 , 50 ) eventbox = Gtk . EventBox . new () label = Gtk . Label . new ( \"Double-Click Me!\" ) eventbox . set_above_child ( False ) eventbox . connect ( \"button_press_event\" , self . on_button_pressed , label ) eventbox . add ( label ) self . add ( eventbox ) eventbox . set_events ( Gdk . EventMask . BUTTON_PRESS_MASK ) eventbox . realize () def on_button_pressed ( self , eventbox , event , label ): if event . type == Gdk . EventType . _2BUTTON_PRESS : text = label . get_text () if text [ 0 ] == 'D' : label . set_text ( \"I Was Double-Clicked!\" ) else : label . set_text ( \"Double-Click Me Again!\" ) return False class Application ( Gtk . Application ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , application_id = \"org.example.myapp\" , ** kwargs ) self . window = None def do_activate ( self ): if not self . window : self . window = AppWindow ( application = self , title = \"Hello World!\" ) self . window . show_all () self . window . present () if __name__ == \"__main__\" : app = Application () app . run ()","title":"EventBox"},{"location":"GTK/API/#filechooserdialog","text":"Gtk.FileChooserDialog is one of the important subtypes of Gtk.Dialog . Like other dialogs, it is provided a title and parent window on instantiation. Additionally a FileChooserAction enum must be specified. FileChooserAction s include: Gtk.FileChooserAction.SAVE Gtk.FileChooserAction.OPEN Gtk.FileChooserAction.SELECT_FOLDER Gtk.FileChooserAction.CREATE_FOLDER dialog = Gtk . FileChooserDialog ( title = \"Save file as ...\" , parent = parent , action = FileChooserAction . SAVE ) Selected files are then retrieved using dialog . get_filenames () | Setter | Property | Description | | --------------------------------------------------------------------------------------------------------------------------------- | ----------------- | | set_current_folder | | Specify directory in filesystem where FileChooser will start | | set_current_name | | For FileChooserAction.SAVE, suggest a filename | | set_select_multiple | select_multiple | For FileChooserAction.OPEN or SELECT_FOLDER, allow multiple file or folder selections |","title":"FileChooserDialog"},{"location":"GTK/API/#grid","text":"Gtk.Grid allows children to be packed in a two-dimensional grid. Grids are instantiated with new() and widgets are laid out by calling attach() (see Login for an example). attach() lay out a widget providing column and row numbers followed by column and row spans grid . attach ( label , 0 , 0 , 1 , 1 )","title":"Grid"},{"location":"GTK/API/#headerbar","text":"Gtk.HeaderBar allows the titlebar to be customized. Like other widgets, it can be configured on instantiation by providing values to keyword arguments or by using setters. Adding to a window HeaderBars are added with set_titlebar() . This is unlike other widgets which are assigned to an ApplicationWindow or Window using pack_start() , pack_end() , or add() , kwarg setter class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) headerbar = Gtk . HeaderBar ( title = f \"Hello, World!\" , subtitle = \"HeaderBar example\" , show_close_button = True ) self . set_titlebar ( headerbar ) class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) headerbar = Gtk . HeaderBar () headerbar . set_title ( f \"Hello, World!\" ) headerbar . set_subtitle ( \"HeaderBar example\" ) headerbar . set_show_close_button ( True ) self . set_titlebar ( headerbar )","title":"HeaderBar"},{"location":"GTK/API/#label","text":"Note that Gtk.Label sets its text with \"label\" and not \"text\" as you may expect from the corresponding setter. kwarg setter label = Gtk . Label ( label = \"Hello, World!\" ) label . set_text ( \"Hello, World!\" )","title":"Label"},{"location":"GTK/API/#listbox","text":"Gtk.ListBox is a vertical container of Gtk.ListBoxRow children used as an alternative to TreeView when the children need to be interactive, as in a list of settings. ListBox","title":"ListBox"},{"location":"GTK/API/#liststore","text":"Gtk.ListStore is one of the two major classes that serves as combination schema and database backing Gtk.TreeView , the other being Gtk.TreeStore . It is instantiated with a sequence of data types, similar to a database schema. These can be standard Python types or GObjects (which are mapped to the Python types anyway): Python types GObject types import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk liststore = Gtk . ListStore (( str , int , str )) import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk , GObject liststore = Gtk . ListStore (( GObject . TYPE_STRING , GOBject ) . TYPE_INT , GObject . TYPE_STRING )) This object then exposes an append method which is used to add records: liststore . append ([ \"Socrates\" , 350 , \"Athens\" ]) The store is then associated with the treeview with set_model treeview . set_model ( liststore )","title":"ListStore"},{"location":"GTK/API/#menubar","text":"Gtk.MenuBar is populated with Gtk.MenuItems , corresponding to the expandable menu items (i.e. \"File\", \"Edit\", and \"Help\"). Gtk.Menu is actually used for the submenu, which like MenuBar is also populared with MenuItems. A Menu is attached to the MenuItem of a MenuBar by using the set_submenu() method on the Menu object. This setter does not have a corresponding kwarg, so all menus have to be constructed procedurally. import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk class AppWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_size_request ( 250 , - 1 ) menubar = Gtk . MenuBar . new () self . add ( menubar ) file = Gtk . MenuItem . new_with_label ( \"File\" ) menubar . append ( file ) filemenu = Gtk . Menu . new () file . set_submenu ( filemenu ) new = Gtk . MenuItem . new_with_label ( \"New\" ) open = Gtk . MenuItem . new_with_label ( \"Open\" ) filemenu . append ( new ) filemenu . append ( open ) edit = Gtk . MenuItem . new_with_label ( \"Edit\" ) menubar . append ( edit ) editmenu = Gtk . Menu . new () edit . set_submenu ( editmenu ) cut = Gtk . MenuItem . new_with_label ( \"Cut\" ) copy = Gtk . MenuItem . new_with_label ( \"Copy\" ) paste = Gtk . MenuItem . new_with_label ( \"Paste\" ) editmenu . append ( cut ) editmenu . append ( copy ) editmenu . append ( paste ) help = Gtk . MenuItem . new_with_label ( \"Help\" ) menubar . append ( help ) helpmenu = Gtk . Menu . new () help . set_submenu ( helpmenu ) contents = Gtk . MenuItem . new_with_label ( \"Help\" ) about = Gtk . MenuItem . new_with_label ( \"About\" ) helpmenu . append ( contents ) helpmenu . append ( about ) class Application ( Gtk . Application ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , application_id = \"org.example.myapp\" , ** kwargs ) self . window = None def do_activate ( self ): if not self . window : self . window = AppWindow ( application = self , title = \"Menu Bars\" ) self . window . show_all () self . window . present () if __name__ == \"__main__\" : app = Application () app . run ()","title":"MenuBar"},{"location":"GTK/API/#notebook","text":"Gtk.Notebook is a layout container that organizes content into tabbed pages. It is instantiated with the new() method and pages are appended with the append_page() method, passing content and label widgets as arguments. The tab bar can be placed using set_tab_pos() , passing a Gtk.PositionType enum Top Right Bottom Left notebook = Gtk . Notebook . new () # notebook.set_tab_pos(Gtk.PositionType.TOP) notebook = Gtk . Notebook . new () notebook . set_tab_pos ( Gtk . PositionType . RIGHT ) notebook = Gtk . Notebook . new () notebook . set_tab_pos ( Gtk . PositionType . BOTTOM ) notebook = Gtk . Notebook . new () notebook . set_tab_pos ( Gtk . PositionType . LEFT ) notebook = Gtk . Notebook . new () label = Gtk . Label . new ( \"Tab title\" ) child = Gtk . Label . new ( \"Tab content\" ) notebook . append_page ( child , label ) The label widget is commonly Gtk.Label but can also be a Gtk.Box . The tab bar can be made scrollable using set_scrollable() , passing a bool.","title":"Notebook"},{"location":"GTK/API/#scale","text":"Gtk.Scale widgets are sliders, and they can be instantiated in one of two ways: new() passing an Adjustment object new_with_range(min, max, step) passing values for minimum, maximum, and step Scale values are stored as doubles, so integers have to be simulated by reducing the number of digits to 0 using set_digits() . By default, the number of digits is set to that of the step value.","title":"Scale"},{"location":"GTK/API/#scrolledwindow","text":"Gtk.ScrolledWindow is a decorator container that accepts a single child widget. Widgets that implement the Gtk.Scrollable interface have native scrolling suppport, like Gtk.TreeView , Gtk.TextView, and Gtk.Layout. Other widgets have to use Gtk.Viewport as an adaptor, and must be added to a Viewport which is then added to the ScrolledWindow. It is instantiated with the new() method, optionally passing two Adjustment objects that affect horizontal and vertical scrolling behavior when stepping or paging. scrolled_win = Gtk . ScrolledWindow . new ( None , None )","title":"ScrolledWindow"},{"location":"GTK/API/#statusbar","text":"Gtk.Statusbar (note the lowercase b ) stores a stack of messages, the topmost of which is displayed. Before adding messages, a context identifier , a unique unsigned integer associated with a context description string, must be retrieved from the newly created Statusbar by passing a string value to get_context_id() . This allows messages to be categorized and pushed to separate stacks. statusbar . push ( context_id , message )","title":"Statusbar"},{"location":"GTK/API/#switch","text":"Gtk.Switch allows a user to toggle a boolean value. Switch exposes getters and setters for both state (which is represented by the trough color) and active (switch position) properties. State is the backend to activ, and they are kept in sync. import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_border_width ( 10 ) box_outer = Gtk . Box ( orientation = Gtk . Orientation . VERTICAL , spacing = 6 ) listbox = Gtk . ListBox ( selection_mode = Gtk . SelectionMode . NONE ) row = Gtk . ListBoxRow () hbox = Gtk . Box ( orientation = Gtk . Orientation . HORIZONTAL , spacing = 50 ) label1 = Gtk . Label ( label = \"Automatic Date & Time\" , xalign = 0 ) hbox . add ( label1 ) self . switch = Gtk . Switch ( valign = Gtk . Align . CENTER , state = False ) hbox . add ( self . switch ) row . add ( hbox ) listbox . add ( row ) box_outer . add ( listbox ) self . add ( box_outer ) button = Gtk . Button ( label = \"Click\" ) button . connect ( \"clicked\" , self . on_button_clicked ) box_outer . add ( button ) def on_button_clicked ( self , button ): print ( f \"Value of get_active(): { self . switch . get_active () } \" ) print ( f \"Value of get_state(): { self . switch . get_state () } \" ) class Application ( Gtk . Application ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . window = None def do_activate ( self ): if not self . window : self . window = ApplicationWindow ( application = self ) self . window . show_all () self . window . present () if __name__ == \"__main__\" : app = Application () app . run ()","title":"Switch"},{"location":"GTK/API/#treeview","text":"In order to create a tree or list in GTK, the Gtk.TreeView widget is paired with a Gtk.TreeModel interface, the most typical implementation of which is Gtk.ListStore or Gtk.TreeStore . TreeView is a complicated widget that must be constructed procedurally: Gtk.TreeView is instantiated. A ListStore is specified as data model and passed in as the value of the model kwarg. The ListStore specifies the schema of the data as a collection of types. treeview = Gtk . TreeView ( model = Gtk . ListStore . new (( str ))) Alternatively, the ListStore can be specified after instantiation. treeview = Gtk . TreeView . new () treeview . set_model ( Gtk . ListStore . new ([ str ])) A Gtk.TreeViewColumn is created for every column in the model. These require a Gtk.CellRenderer to be defined. The TreeViewColumn is added to the treeview by calling the append_column() method on the treeview. The text kwarg appears to refer to the column of the data store to use for the column's values. treeview . append_column ( Gtk . TreeViewColumn ( \"Greeks\" , Gtk . CellRendererText . new (), text = 0 )) Items are added to the ListStore procedurally using the append() method. Note that the method takes only a single argument, so collections like lists or tuples must be used. liststore . append (( \"Socrates\" ,)) liststore . append (( \"Plato\" ,)) liststore . append (( \"Aristotle\" ,)) Changing the number of columns affects the types used to define the ListStore, the appended records, as well as the number of columns added to the TreeView itself. 1 column 2 columns import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . gen_treeview () scrolled_win = Gtk . ScrolledWindow . new ( None , None ) scrolled_win . set_policy ( Gtk . PolicyType . AUTOMATIC , Gtk . PolicyType . AUTOMATIC ) scrolled_win . add ( self . treeview ) self . add ( scrolled_win ) self . set_size_request ( 200 , 200 ) def get_liststore ( self ): store = Gtk . ListStore . new (( str ,)) store . append (( \"Socrates\" ,)) store . append (( \"Plato\" ,)) store . append (( \"Aristotle\" ,)) return store def gen_treeview ( self ): self . treeview = Gtk . TreeView . new () self . treeview . set_model ( self . get_liststore ()) self . treeview . append_column ( Gtk . TreeViewColumn ( \"Greeks\" , Gtk . CellRendererText . new (), text = 0 )) class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.myapp' ) def do_activate ( self ): self . window = ApplicationWindow ( application = self , title = \"Greeks\" ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = Application () app . run () import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . gen_treeview () scrolled_win = Gtk . ScrolledWindow . new ( None , None ) scrolled_win . set_policy ( Gtk . PolicyType . AUTOMATIC , Gtk . PolicyType . AUTOMATIC ) scrolled_win . add ( self . treeview ) self . add ( scrolled_win ) self . set_size_request ( 200 , 200 ) def get_liststore ( self ): store = Gtk . ListStore . new (( str , str )) store . append ([ \"Socrates\" , \"Athens\" ]) store . append ([ \"Plato\" , \"Athens\" ]) store . append ([ \"Aristotle\" , \"Athens\" ]) return store def gen_treeview ( self ): self . treeview = Gtk . TreeView . new () self . treeview . set_model ( self . get_liststore ()) self . treeview . append_column ( Gtk . TreeViewColumn ( \"Greeks\" , Gtk . CellRendererText . new (), text = 0 )) self . treeview . append_column ( Gtk . TreeViewColumn ( \"Place of birth\" , Gtk . CellRendererText . new (), text = 1 )) class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.myapp' ) def do_activate ( self ): self . window = ApplicationWindow ( application = self , title = \"Greeks\" ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = Application () app . run () The model backing a TreeView (usually a ListStore ), can be retrieved with the get_model() method. treeview . get_model () . append (( 'foo' , 'bar' )) TreeView emits several signals: row_activated when a row is double-clicked, with the following implicit argument widget refering to the emitting TreeView widget itself path is a TreePath . column is of type TreeViewcolumn treeview . connect ( \"row_activated\" , self . on_row_activated , widget , path , column ) def on_row_activated ( self , widget , path , column ): row = path . get_indices ()[ 0 ] print ( f \"row= { path . get_indices ()[ 0 ] } ,col= { column . props . title } \" ) print ( widget . get_model ()[ row ][:])","title":"TreeView"},{"location":"GTK/API/#treepath","text":"Gtk.TreePath is a type used to implement the rows of a TreeView . Although it prints to an integer with the print statement, it cannot be treated as one. A path object can be passed as the index to a TreeModel like ListStore , as can an integer. The row number of a TreePath from a normal list-style TreeView can be retrieved with the get_indices() method. row = path . get_indices ()[ 0 ] # Using TreePath object as index to model model [ path ][:] # Using row integer as index to model model [ row ][:] Another method on TreePath, get_depth() always returns 1 for list-style TreeViews, but may be more useful for tree-style TreeViews.","title":"TreePath"},{"location":"GTK/API/#treeselection","text":"Gtk.TreeSelection objects represent selection information for each tree view.","title":"TreeSelection"},{"location":"GTK/API/#treeviewcolumn_1","text":"Gtk.TreeViewColumn represents a visible column in a Treeview . Its props property exposes many associated values, including title. print ( column . props . title ) A column is made sortable by calling set_sort_column_id() , passing the column of the model to sort by. column . set_sort_column_id ( 0 )","title":"TreeViewColumn"},{"location":"GTK/API/#window","text":"Gtk.Window","title":"Window"},{"location":"GTK/Glade/","text":"Glade The application will not close correctly without explicitly binding the destroy signal. This is because signal handlers specified in the markup must be mapped to actual methods in the code. This is done with the connect_signals() method, which can be used in two different ways depending on the object passed: Class that implements the named methods exactly Dictionary that maps handler values from markup to function names Handler object Dictionary class Handlers (): def on_window_destroy ( self ): Gtk . main_quit () builder . connect_signals ( Handlers ()) handlers = { \"on_window_destroy\" : Gtk . main_quit } builder . connect_signals ( handlers ) import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk import sys class Application ( Gtk . Application ): def __init__ ( self , * args , name = \"World\" , ** kwargs ): self . name = name super () . __init__ ( * args , application_id = \"org.example.myapp\" , ** kwargs ) def do_activate ( self ): builder = Gtk . Builder . new_from_file ( 'hwp.glade' ) self . window = builder . get_object ( 'window' ) self . window . connect ( \"destroy\" , Gtk . main_quit ) self . window . set_title ( f 'Hello, { self . name } !' ) label = builder . get_object ( 'label' ) label . set_text ( f 'Hello, { self . name } !' ) self . window . show_all () self . window . present () def run ( self ): super () . run () Gtk . main () if __name__ == '__main__' : app = Application ( name = sys . argv [ - 1 ]) app . run () Hello, World! (interactive) All of the examples in this task extend the following application markup and differ exclusively in the implementation of the on_button_clicked() signal handler. <?xml version=\"1.0\" encoding=\"UTF-8\"?> <!-- Generated with glade 3.38.2 --> <interface> <requires lib= \"gtk+\" version= \"3.4\" /> <object class= \"GtkApplicationWindow\" id= \"window\" > <property name= \"width-request\" > 200 </property> <property name= \"height-request\" > 200 </property> <property name= \"can-focus\" > False </property> <signal name= \"destroy\" handler= \"on_window_destroy\" swapped= \"no\" /> <child> <object class= \"GtkBox\" > <property name= \"can-focus\" > False </property> <property name= \"orientation\" > vertical </property> <child> <object class= \"GtkEntry\" id= \"entry\" > <property name= \"can-focus\" > True </property> <property name= \"show-emoji-icon\" > True </property> </object> <packing> <property name= \"expand\" > False </property> <property name= \"fill\" > True </property> <property name= \"position\" > 0 </property> </packing> </child> <child> <object class= \"GtkButton\" id= \"button\" > <property name= \"label\" > Greet </property> <property name= \"can-focus\" > True </property> <property name= \"receives-default\" > False </property> <signal name= \"clicked\" handler= \"on_button_clicked\" swapped= \"no\" /> </object> <packing> <property name= \"expand\" > False </property> <property name= \"fill\" > True </property> <property name= \"position\" > 1 </property> </packing> </child> <child> <object class= \"GtkLabel\" id= \"label\" > <property name= \"can-focus\" > False </property> </object> <packing> <property name= \"expand\" > True </property> <property name= \"fill\" > True </property> <property name= \"position\" > 2 </property> </packing> </child> </object> </child> </object> </interface> Label MessageDialog import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk import sys class Application ( Gtk . Application ): def __init__ ( self , name = \"World\" , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . name = name def do_activate ( self ): self . builder = Gtk . Builder . new_from_file ( 'hwi.glade' ) self . window = self . builder . get_object ( 'window' ) self . window . set_title ( f \"Hello, { self . name } !\" ) self . builder . connect_signals ( self ) self . entry = self . builder . get_object ( 'entry' ) self . entry . set_text ( self . name ) self . label = self . builder . get_object ( 'label' ) self . window . show_all () self . window . present () def on_button_clicked ( self , button ): self . label . set_text ( f 'Hello, { self . entry . get_text () } !' ) self . window . set_title ( f 'Hello, { self . entry . get_text () } !' ) def on_window_destroy ( self , arg ): Gtk . main_quit () def run ( self ): super () . run () Gtk . main () if __name__ == '__main__' : try : app = Application ( sys . argv [ 1 ]) except IndexError : app = Application () app . run () import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk import sys class Application ( Gtk . Application ): def __init__ ( self , name = \"World\" , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . name = name def do_activate ( self ): self . builder = Gtk . Builder . new_from_file ( 'hwi.glade' ) self . window = self . builder . get_object ( 'window' ) self . window . set_title ( f \"Hello, { self . name } !\" ) self . builder . connect_signals ( self ) self . entry = self . builder . get_object ( 'entry' ) self . entry . set_text ( self . name ) self . label = self . builder . get_object ( 'label' ) self . window . show_all () self . window . present () def on_button_clicked ( self , button ): dialog = Gtk . MessageDialog ( message_type = Gtk . MessageType . INFO , text = f \"Hello, { self . entry . get_text () } \" , parent = self . window , ) dialog . add_button ( \"O_K\" , Gtk . ResponseType . OK ) dialog . run () dialog . destroy () def on_window_destroy ( self , arg ): Gtk . main_quit () def run ( self ): super () . run () Gtk . main () if __name__ == '__main__' : try : app = Application ( sys . argv [ 1 ]) except IndexError : app = Application () app . run () Menu GTK objects are declared using either special predefined elements (e.g. menu ) or an object element with the class name itself specified in the class attribute. These syntaxes do not appear to be entirely interchangeable. Predefined element object element <menu id= \"app-menu\" > <object class= \"GtkMenu\" id= \"app-menu\" > Here, each menu item is described by one of three attribute, identifiable in the attribute XML tags: action which names an action and class to handle the signal target specifies the string that displays in the menu item label whether or not the target attribute string should be translated <?xml version=\"1.0\" encoding=\"UTF-8\"?> <interface> <menu id= \"app-menu\" > <section> <attribute name= \"label\" translatable= \"yes\" > Change label </attribute> <item> <attribute name= \"action\" > win.change_label </attribute> <attribute name= \"target\" > String 1 </attribute> <attribute name= \"label\" translatable= \"yes\" > String 1 </attribute> </item> <item> <attribute name= \"action\" > win.change_label </attribute> <attribute name= \"target\" > String 2 </attribute> <attribute name= \"label\" translatable= \"yes\" > String 2 </attribute> </item> </section> </menu> </interface> Examples Hello, World! Basic example <?xml version=\"1.0\" encoding=\"UTF-8\"?> <!-- Generated with glade 3.38.2 --> <interface> <requires lib= \"gtk+\" version= \"3.24\" /> <object class= \"GtkApplicationWindow\" > <property name= \"can-focus\" > False </property> <child> <object class= \"GtkLabel\" > <property name= \"visible\" > True </property> <property name= \"can-focus\" > False </property> <property name= \"label\" translatable= \"yes\" > Hello, World! </property> </object> </child> </object> </interface> <?xml version=\"1.0\" encoding=\"UTF-8\"?> <interface> <menu id= \"app-menu\" > <section> <attribute name= \"label\" translatable= \"yes\" > Change label </attribute> <item> <attribute name= \"action\" > win.change_label </attribute> <attribute name= \"target\" > String 1 </attribute> <attribute name= \"label\" translatable= \"yes\" > String 1 </attribute> </item> <item> <attribute name= \"action\" > win.change_label </attribute> <attribute name= \"target\" > String 2 </attribute> <attribute name= \"label\" translatable= \"yes\" > String 2 </attribute> </item> <item> <attribute name= \"action\" > win.change_label </attribute> <attribute name= \"target\" > String 3 </attribute> <attribute name= \"label\" translatable= \"yes\" > String 3 </attribute> </item> </section> <section> <item> <attribute name= \"action\" > win.maximize </attribute> <attribute name= \"label\" translatable= \"yes\" > Maximize </attribute> </item> </section> <section> <item> <attribute name= \"action\" > app.about </attribute> <attribute name= \"label\" translatable= \"yes\" > _About </attribute> </item> <item> <attribute name= \"action\" > app.quit </attribute> <attribute name= \"label\" translatable= \"yes\" > _Quit </attribute> <attribute name= \"accel\" > &lt; Primary &gt; q </attribute> </item> </section> </menu> </interface>","title":"Glade"},{"location":"GTK/Glade/#glade","text":"The application will not close correctly without explicitly binding the destroy signal. This is because signal handlers specified in the markup must be mapped to actual methods in the code. This is done with the connect_signals() method, which can be used in two different ways depending on the object passed: Class that implements the named methods exactly Dictionary that maps handler values from markup to function names Handler object Dictionary class Handlers (): def on_window_destroy ( self ): Gtk . main_quit () builder . connect_signals ( Handlers ()) handlers = { \"on_window_destroy\" : Gtk . main_quit } builder . connect_signals ( handlers ) import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk import sys class Application ( Gtk . Application ): def __init__ ( self , * args , name = \"World\" , ** kwargs ): self . name = name super () . __init__ ( * args , application_id = \"org.example.myapp\" , ** kwargs ) def do_activate ( self ): builder = Gtk . Builder . new_from_file ( 'hwp.glade' ) self . window = builder . get_object ( 'window' ) self . window . connect ( \"destroy\" , Gtk . main_quit ) self . window . set_title ( f 'Hello, { self . name } !' ) label = builder . get_object ( 'label' ) label . set_text ( f 'Hello, { self . name } !' ) self . window . show_all () self . window . present () def run ( self ): super () . run () Gtk . main () if __name__ == '__main__' : app = Application ( name = sys . argv [ - 1 ]) app . run ()","title":"Glade"},{"location":"GTK/Glade/#hello-world-interactive","text":"All of the examples in this task extend the following application markup and differ exclusively in the implementation of the on_button_clicked() signal handler. <?xml version=\"1.0\" encoding=\"UTF-8\"?> <!-- Generated with glade 3.38.2 --> <interface> <requires lib= \"gtk+\" version= \"3.4\" /> <object class= \"GtkApplicationWindow\" id= \"window\" > <property name= \"width-request\" > 200 </property> <property name= \"height-request\" > 200 </property> <property name= \"can-focus\" > False </property> <signal name= \"destroy\" handler= \"on_window_destroy\" swapped= \"no\" /> <child> <object class= \"GtkBox\" > <property name= \"can-focus\" > False </property> <property name= \"orientation\" > vertical </property> <child> <object class= \"GtkEntry\" id= \"entry\" > <property name= \"can-focus\" > True </property> <property name= \"show-emoji-icon\" > True </property> </object> <packing> <property name= \"expand\" > False </property> <property name= \"fill\" > True </property> <property name= \"position\" > 0 </property> </packing> </child> <child> <object class= \"GtkButton\" id= \"button\" > <property name= \"label\" > Greet </property> <property name= \"can-focus\" > True </property> <property name= \"receives-default\" > False </property> <signal name= \"clicked\" handler= \"on_button_clicked\" swapped= \"no\" /> </object> <packing> <property name= \"expand\" > False </property> <property name= \"fill\" > True </property> <property name= \"position\" > 1 </property> </packing> </child> <child> <object class= \"GtkLabel\" id= \"label\" > <property name= \"can-focus\" > False </property> </object> <packing> <property name= \"expand\" > True </property> <property name= \"fill\" > True </property> <property name= \"position\" > 2 </property> </packing> </child> </object> </child> </object> </interface> Label MessageDialog import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk import sys class Application ( Gtk . Application ): def __init__ ( self , name = \"World\" , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . name = name def do_activate ( self ): self . builder = Gtk . Builder . new_from_file ( 'hwi.glade' ) self . window = self . builder . get_object ( 'window' ) self . window . set_title ( f \"Hello, { self . name } !\" ) self . builder . connect_signals ( self ) self . entry = self . builder . get_object ( 'entry' ) self . entry . set_text ( self . name ) self . label = self . builder . get_object ( 'label' ) self . window . show_all () self . window . present () def on_button_clicked ( self , button ): self . label . set_text ( f 'Hello, { self . entry . get_text () } !' ) self . window . set_title ( f 'Hello, { self . entry . get_text () } !' ) def on_window_destroy ( self , arg ): Gtk . main_quit () def run ( self ): super () . run () Gtk . main () if __name__ == '__main__' : try : app = Application ( sys . argv [ 1 ]) except IndexError : app = Application () app . run () import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk import sys class Application ( Gtk . Application ): def __init__ ( self , name = \"World\" , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . name = name def do_activate ( self ): self . builder = Gtk . Builder . new_from_file ( 'hwi.glade' ) self . window = self . builder . get_object ( 'window' ) self . window . set_title ( f \"Hello, { self . name } !\" ) self . builder . connect_signals ( self ) self . entry = self . builder . get_object ( 'entry' ) self . entry . set_text ( self . name ) self . label = self . builder . get_object ( 'label' ) self . window . show_all () self . window . present () def on_button_clicked ( self , button ): dialog = Gtk . MessageDialog ( message_type = Gtk . MessageType . INFO , text = f \"Hello, { self . entry . get_text () } \" , parent = self . window , ) dialog . add_button ( \"O_K\" , Gtk . ResponseType . OK ) dialog . run () dialog . destroy () def on_window_destroy ( self , arg ): Gtk . main_quit () def run ( self ): super () . run () Gtk . main () if __name__ == '__main__' : try : app = Application ( sys . argv [ 1 ]) except IndexError : app = Application () app . run ()","title":"Hello, World! (interactive)"},{"location":"GTK/Glade/#menu","text":"GTK objects are declared using either special predefined elements (e.g. menu ) or an object element with the class name itself specified in the class attribute. These syntaxes do not appear to be entirely interchangeable. Predefined element object element <menu id= \"app-menu\" > <object class= \"GtkMenu\" id= \"app-menu\" > Here, each menu item is described by one of three attribute, identifiable in the attribute XML tags: action which names an action and class to handle the signal target specifies the string that displays in the menu item label whether or not the target attribute string should be translated <?xml version=\"1.0\" encoding=\"UTF-8\"?> <interface> <menu id= \"app-menu\" > <section> <attribute name= \"label\" translatable= \"yes\" > Change label </attribute> <item> <attribute name= \"action\" > win.change_label </attribute> <attribute name= \"target\" > String 1 </attribute> <attribute name= \"label\" translatable= \"yes\" > String 1 </attribute> </item> <item> <attribute name= \"action\" > win.change_label </attribute> <attribute name= \"target\" > String 2 </attribute> <attribute name= \"label\" translatable= \"yes\" > String 2 </attribute> </item> </section> </menu> </interface>","title":"Menu"},{"location":"GTK/Glade/#examples","text":"Hello, World! Basic example <?xml version=\"1.0\" encoding=\"UTF-8\"?> <!-- Generated with glade 3.38.2 --> <interface> <requires lib= \"gtk+\" version= \"3.24\" /> <object class= \"GtkApplicationWindow\" > <property name= \"can-focus\" > False </property> <child> <object class= \"GtkLabel\" > <property name= \"visible\" > True </property> <property name= \"can-focus\" > False </property> <property name= \"label\" translatable= \"yes\" > Hello, World! </property> </object> </child> </object> </interface> <?xml version=\"1.0\" encoding=\"UTF-8\"?> <interface> <menu id= \"app-menu\" > <section> <attribute name= \"label\" translatable= \"yes\" > Change label </attribute> <item> <attribute name= \"action\" > win.change_label </attribute> <attribute name= \"target\" > String 1 </attribute> <attribute name= \"label\" translatable= \"yes\" > String 1 </attribute> </item> <item> <attribute name= \"action\" > win.change_label </attribute> <attribute name= \"target\" > String 2 </attribute> <attribute name= \"label\" translatable= \"yes\" > String 2 </attribute> </item> <item> <attribute name= \"action\" > win.change_label </attribute> <attribute name= \"target\" > String 3 </attribute> <attribute name= \"label\" translatable= \"yes\" > String 3 </attribute> </item> </section> <section> <item> <attribute name= \"action\" > win.maximize </attribute> <attribute name= \"label\" translatable= \"yes\" > Maximize </attribute> </item> </section> <section> <item> <attribute name= \"action\" > app.about </attribute> <attribute name= \"label\" translatable= \"yes\" > _About </attribute> </item> <item> <attribute name= \"action\" > app.quit </attribute> <attribute name= \"label\" translatable= \"yes\" > _Quit </attribute> <attribute name= \"accel\" > &lt; Primary &gt; q </attribute> </item> </section> </menu> </interface>","title":"Examples"},{"location":"GTK/Glossary/","text":"\ud83d\udcd8 Glossary Action Actions have prefixes that determine where they are sent for processing: Window-specific actions are specified with win. (i.e. win.change_label ), which specifies that ApplicationWindow processes the signal. Application-wide actions are prefixed with app. Create new actions with Gio.SimpleAction import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gio # --snip-- quit_action = Gio . SimpleAction . new ( \"quit\" , None ) quit_action . connect ( \"activate\" , self . on_quit ) activate open Callback Event handler for GTK signals Container Decorator containers derive from Gtk.Bin and can hold only a single child, like ApplicationWindow , and are so called because they add functionality to the child widget. Layout containers derive directly from Gtk.Container and are used to arrange multiple child widgets. GLib GLib is a bundle of three low-level system libraries written in C: GLib GObject GIO It originated in the GTK+ project but was abstracted away before the release of GTK+ version 2. GObject GObject ( GLib Object System) is a library written in C that provides an object-oriented API. Prior to being abstracted into its own library, the object system formed part of the GTK+ codebase. Pop-up menu Context menu Signal A signal is a notification to the application that the user has performed an action. A signal must be connected to a callback method so that when the signal is emitted the method is executed. It is possible to connect signals at any point in applications, but it is considered good form to initialize callbacks before calling gtk_main() or present() . Signals in GTK are similar to events in other GUI frameworks, although the term \" events \" is also used in GTK to refer to special signals emitted by the X Windows System. GTK event handlers are methods that begin with do_ , i.e. do_startup . handle-local-options","title":"\ud83d\udcd8 Glossary"},{"location":"GTK/Glossary/#glossary","text":"Action Actions have prefixes that determine where they are sent for processing: Window-specific actions are specified with win. (i.e. win.change_label ), which specifies that ApplicationWindow processes the signal. Application-wide actions are prefixed with app. Create new actions with Gio.SimpleAction import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gio # --snip-- quit_action = Gio . SimpleAction . new ( \"quit\" , None ) quit_action . connect ( \"activate\" , self . on_quit ) activate open Callback Event handler for GTK signals Container Decorator containers derive from Gtk.Bin and can hold only a single child, like ApplicationWindow , and are so called because they add functionality to the child widget. Layout containers derive directly from Gtk.Container and are used to arrange multiple child widgets.","title":"\ud83d\udcd8 Glossary"},{"location":"GTK/Glossary/#glib","text":"GLib is a bundle of three low-level system libraries written in C: GLib GObject GIO It originated in the GTK+ project but was abstracted away before the release of GTK+ version 2.","title":"GLib"},{"location":"GTK/Glossary/#gobject","text":"GObject ( GLib Object System) is a library written in C that provides an object-oriented API. Prior to being abstracted into its own library, the object system formed part of the GTK+ codebase. Pop-up menu Context menu Signal A signal is a notification to the application that the user has performed an action. A signal must be connected to a callback method so that when the signal is emitted the method is executed. It is possible to connect signals at any point in applications, but it is considered good form to initialize callbacks before calling gtk_main() or present() . Signals in GTK are similar to events in other GUI frameworks, although the term \" events \" is also used in GTK to refer to special signals emitted by the X Windows System. GTK event handlers are methods that begin with do_ , i.e. do_startup . handle-local-options","title":"GObject"},{"location":"GTK/Examples/GNOME-Tweaks/","text":"GNOME Tweaks GnomeTweaks is the application class and is defined in gtweak/app.py . Window is the window class, and its constructor takes the application and liststore subclass instances as arguments. class GnomeTweaks ( Gtk . Application ): def do_activate ( self ): if not self . win : model = TweakModel () self . win = Window ( self , model ) self . win . show_all () self . win . present () TweakModel inherits from Gtk.ListStore and stores tweak groups which correspond to the pages of settings in the app. These tweak groups are defined as classes in Python modules placed in gtweak/tweaks , each of them subclassing parent classes like GSettingsSwitchTweak and GetterSetterSwitchTweak, which are defined in gtweak/widgets.py . Each of the tweak group modules defines a top-level list named TWEAK_GROUPS with only a single element - an instance of ListBoxTweakGroup , which is also defined in widgets.py. ListBoxTweakGroup, in turn, is a subclass of Gtk.ListBox and TweakGroup (tweakmodel.py) by multiple inheritance. Some TWEAK_GROUP s like that of the Desktop group are only populated conditionally. Desktop General from gtweak.widgets import ListBoxTweakGroup , GSettingsSwitchTweak , Title dicons = GSettingsSwitchTweak ( _ ( \"Show Icons\" ), \"org.gnome.desktop.background\" , \"show-desktop-icons\" ) home = GSettingsSwitchTweak ( _ ( \"Home\" ), \"org.gnome.nautilus.desktop\" , \"home-icon-visible\" , depends_on = dicons , schema_filename = \"org.gnome.nautilus.gschema.xml\" ) TWEAK_GROUPS = [] if home . loaded : TWEAK_GROUPS . append ( ListBoxTweakGroup ( _ ( \"Desktop\" ), Title ( _ ( \"Icons on Desktop\" ), \"\" , uid = \"title-theme\" , top = True ), dicons , home , GSettingsSwitchTweak ( _ ( \"Network Servers\" ), \"org.gnome.nautilus.desktop\" , \"network-icon-visible\" , depends_on = dicons , schema_filename = \"org.gnome.nautilus.gschema.xml\" ), GSettingsSwitchTweak ( _ ( \"Trash\" ), \"org.gnome.nautilus.desktop\" , \"trash-icon-visible\" , depends_on = dicons , schema_filename = \"org.gnome.nautilus.gschema.xml\" ), GSettingsSwitchTweak ( _ ( \"Mounted Volumes\" ), \"org.gnome.nautilus.desktop\" , \"volumes-visible\" , depends_on = dicons , schema_filename = \"org.gnome.nautilus.gschema.xml\" ), )) TWEAK_GROUPS = [ ListBoxTweakGroup ( _ ( \"General\" ), GSettingsSwitchTweak ( _ ( \"Animations\" ), \"org.gnome.desktop.interface\" , \"enable-animations\" ), IgnoreLidSwitchTweak (), # Don't show this setting in the Ubuntu session since this setting is in gnome-control-center there GSettingsSwitchTweak ( _ ( \"Over-Amplification\" ), \"org.gnome.desktop.sound\" , \"allow-volume-above-100-percent\" , desc = _ ( \"Allows raising the volume above 100%. This can result in a loss of audio quality; it is better to increase application volume settings, if possible.\" ), loaded = _shell_not_ubuntu ), ), ] ListBoxTweakGroup is instantiated with a series of arguments, starting with its name and followed by individual tweaks or settings. These tweaks are also defined as classes in gtweak/widgets.py , although some like IgnoreLidSwitchTweak are defined inline with the TWEAK_GROUP lists where they are instantiated. I'm still not sure what the _() means, but it is apparently some function call. All tweaks share some features: Their names reflect the control used in their interface (\"Switch\", \"SpinButton\", \"ComboEnum\", etc) in the form \" GSettings...Tweak They subclass Gtk.Box , _GSettingsTweak , and _DependableMixin by multiple inheritance They instantiate a horizontally oriented Gtk.Box with a curious syntax. Gtk . Box . __init__ ( self , orientation = Gtk . Orientation . HORIZONTAL ) They instantiate _GSettingsTweak with a similar syntax _GSettingsTweak . __init__ ( self , name , schema_name , key_name , ** options ) The rest of the widget is defined procedurally and added to self in the constructor as normal By far the most common tweak is GSettingsSwitchTweakValue . GSettingsSwitchTweak GSettingsSwitchTweakValue GSettingsFontButtonTweak GSettingsRangeTweak GSettingsSpinButtonTweak GSettingsComboEnumTweak GSettingsComboTweak class GSettingsSwitchTweak ( Gtk . Box , _GSettingsTweak , _DependableMixin ): def __init__ ( self , name , schema_name , key_name , ** options ): Gtk . Box . __init__ ( self , orientation = Gtk . Orientation . HORIZONTAL ) _GSettingsTweak . __init__ ( self , name , schema_name , key_name , ** options ) w = Gtk . Switch () self . settings . bind ( key_name , w , \"active\" , Gio . SettingsBindFlags . DEFAULT ) self . add_dependency_on_tweak ( options . get ( \"depends_on\" ), options . get ( \"depends_how\" ) ) vbox1 = Gtk . Box ( orientation = Gtk . Orientation . VERTICAL ) vbox1 . props . spacing = UI_BOX_SPACING lbl = Gtk . Label ( label = name ) lbl . props . ellipsize = Pango . EllipsizeMode . END lbl . props . xalign = 0.0 vbox1 . pack_start ( lbl , True , True , 0 ) if options . get ( \"desc\" ): description = options . get ( \"desc\" ) lbl_desc = Gtk . Label () lbl_desc . props . xalign = 0.0 lbl_desc . set_line_wrap ( True ) lbl_desc . get_style_context () . add_class ( \"dim-label\" ) lbl_desc . set_markup ( \"<span size='small'>\" + GLib . markup_escape_text ( description ) + \"</span>\" ) vbox1 . pack_start ( lbl_desc , True , True , 0 ) vbox2 = Gtk . Box ( orientation = Gtk . Orientation . VERTICAL ) vbox2_upper = Gtk . Box () vbox2_lower = Gtk . Box () vbox2 . pack_start ( vbox2_upper , True , True , 0 ) vbox2 . pack_start ( w , False , False , 0 ) vbox2 . pack_start ( vbox2_lower , True , True , 0 ) self . pack_start ( vbox1 , True , True , 0 ) self . pack_start ( vbox2 , False , False , 0 ) self . widget_for_size_group = None class GSettingsSwitchTweakValue ( Gtk . Box , _GSettingsTweak ): def __init__ ( self , name , schema_name , key_name , ** options ): Gtk . Box . __init__ ( self , orientation = Gtk . Orientation . HORIZONTAL ) _GSettingsTweak . __init__ ( self , name , schema_name , key_name , ** options ) sw = Gtk . Switch () sw . set_active ( self . get_active ()) sw . connect ( \"notify::active\" , self . _on_toggled ) vbox1 = Gtk . Box ( orientation = Gtk . Orientation . VERTICAL ) vbox1 . props . spacing = UI_BOX_SPACING lbl = Gtk . Label ( label = name ) lbl . props . ellipsize = Pango . EllipsizeMode . END lbl . props . xalign = 0.0 vbox1 . pack_start ( lbl , True , True , 0 ) if options . get ( \"desc\" ): description = options . get ( \"desc\" ) lbl_desc = Gtk . Label () lbl_desc . props . xalign = 0.0 lbl_desc . set_line_wrap ( True ) lbl_desc . get_style_context () . add_class ( \"dim-label\" ) lbl_desc . set_markup ( \"<span size='small'>\" + GLib . markup_escape_text ( description ) + \"</span>\" ) vbox1 . pack_start ( lbl_desc , True , True , 0 ) vbox2 = Gtk . Box ( orientation = Gtk . Orientation . VERTICAL ) vbox2_upper = Gtk . Box () vbox2_lower = Gtk . Box () vbox2 . pack_start ( vbox2_upper , True , True , 0 ) vbox2 . pack_start ( sw , False , False , 0 ) vbox2 . pack_start ( vbox2_lower , True , True , 0 ) self . pack_start ( vbox1 , True , True , 0 ) self . pack_start ( vbox2 , False , False , 0 ) self . widget_for_size_group = None def _on_toggled ( self , sw , pspec ): self . set_active ( sw . get_active ()) def set_active ( self , v ): raise NotImplementedError () def get_active ( self ): raise NotImplementedError () class GSettingsFontButtonTweak ( Gtk . Box , _GSettingsTweak , _DependableMixin ): def __init__ ( self , name , schema_name , key_name , ** options ): Gtk . Box . __init__ ( self , orientation = Gtk . Orientation . HORIZONTAL ) _GSettingsTweak . __init__ ( self , name , schema_name , key_name , ** options ) w = Gtk . FontButton () w . set_use_font ( True ) self . settings . bind ( key_name , w , \"font-name\" , Gio . SettingsBindFlags . DEFAULT ) build_label_beside_widget ( name , w , hbox = self ) self . widget_for_size_group = w class GSettingsRangeTweak ( Gtk . Box , _GSettingsTweak , _DependableMixin ): def __init__ ( self , name , schema_name , key_name , ** options ): Gtk . Box . __init__ ( self , orientation = Gtk . Orientation . HORIZONTAL ) _GSettingsTweak . __init__ ( self , name , schema_name , key_name , ** options ) # returned variant is range:(min, max) _min , _max = self . settings . get_range ( key_name )[ 1 ] w = Gtk . HScale . new_with_range ( _min , _max , options . get ( 'adjustment_step' , 1 )) self . settings . bind ( key_name , w . get_adjustment (), \"value\" , Gio . SettingsBindFlags . DEFAULT ) build_label_beside_widget ( self . name , w , hbox = self ) self . widget_for_size_group = w class GSettingsSpinButtonTweak ( Gtk . Box , _GSettingsTweak , _DependableMixin ): def __init__ ( self , name , schema_name , key_name , ** options ): Gtk . Box . __init__ ( self , orientation = Gtk . Orientation . HORIZONTAL ) _GSettingsTweak . __init__ ( self , name , schema_name , key_name , ** options ) # returned variant is range:(min, max) _min , _max = self . settings . get_range ( key_name )[ 1 ] adjustment = Gtk . Adjustment ( value = 0 , lower = _min , upper = _max , step_increment = options . get ( 'adjustment_step' , 1 )) w = Gtk . SpinButton () w . set_adjustment ( adjustment ) w . set_digits ( options . get ( 'digits' , 0 )) self . settings . bind ( key_name , adjustment , \"value\" , Gio . SettingsBindFlags . DEFAULT ) build_label_beside_widget ( name , w , hbox = self ) self . widget_for_size_group = w self . add_dependency_on_tweak ( options . get ( \"depends_on\" ), options . get ( \"depends_how\" ) ) class GSettingsComboEnumTweak ( Gtk . Box , _GSettingsTweak , _DependableMixin ): def __init__ ( self , name , schema_name , key_name , ** options ): Gtk . Box . __init__ ( self , orientation = Gtk . Orientation . HORIZONTAL ) _GSettingsTweak . __init__ ( self , name , schema_name , key_name , ** options ) _type , values = self . settings . get_range ( key_name ) value = self . settings . get_string ( key_name ) self . settings . connect ( 'changed::' + self . key_name , self . _on_setting_changed ) w = build_combo_box_text ( value , * [( v , v . replace ( \"-\" , \" \" ) . title ()) for v in values ]) w . connect ( 'changed' , self . _on_combo_changed ) self . combo = w build_label_beside_widget ( name , w , hbox = self ) self . widget_for_size_group = w def _values_are_different ( self ): # to stop bouncing back and forth between changed signals. I suspect there must be a nicer # Gio.settings_bind way to fix this return self . settings . get_string ( self . key_name ) != \\ self . combo . get_model () . get_value ( self . combo . get_active_iter (), 0 ) def _on_setting_changed ( self , setting , key ): assert key == self . key_name val = self . settings . get_string ( key ) model = self . combo . get_model () for row in model : if val == row [ 0 ]: self . combo . set_active_iter ( row . iter ) break def _on_combo_changed ( self , combo ): val = self . combo . get_model () . get_value ( self . combo . get_active_iter (), 0 ) if self . _values_are_different (): self . settings . set_string ( self . key_name , val ) class GSettingsComboTweak ( Gtk . Box , _GSettingsTweak , _DependableMixin ): def __init__ ( self , name , schema_name , key_name , key_options , ** options ): Gtk . Box . __init__ ( self , orientation = Gtk . Orientation . HORIZONTAL ) _GSettingsTweak . __init__ ( self , name , schema_name , key_name , ** options ) # check key_options is iterable # and if supplied, check it is a list of 2-tuples assert len ( key_options ) >= 0 if len ( key_options ): assert len ( key_options [ 0 ]) == 2 self . _key_options = key_options self . combo = build_combo_box_text ( self . settings . get_string ( self . key_name ), * key_options ) self . combo . connect ( 'changed' , self . _on_combo_changed ) self . settings . connect ( 'changed::' + self . key_name , self . _on_setting_changed ) build_label_beside_widget ( name , self . combo , hbox = self ) self . widget_for_size_group = self . combo def _on_setting_changed ( self , setting , key ): assert key == self . key_name val = self . settings . get_string ( key ) model = self . combo . get_model () for row in model : if val == row [ 0 ]: self . combo . set_active_iter ( row . iter ) return self . combo . set_active ( - 1 ) def _on_combo_changed ( self , combo ): _iter = combo . get_active_iter () if _iter : value = combo . get_model () . get_value ( _iter , 0 ) self . settings . set_string ( self . key_name , value ) @property def extra_info ( self ): if self . _extra_info is None : self . _extra_info = self . settings . schema_get_summary ( self . key_name ) self . _extra_info += \" \" + \" \" . join ( op [ 0 ] for op in self . _key_options ) return self . _extra_info IgnoreLidSwitchTweak is unusual as a tweak that inherits from GetterSetterSwitchTweak, which inherits in turn from the Tweak base class that is actually in gtweak/tweakmodel.py IgnoreLidSwitchTweak GetterSetterSwitchTweak Tweak class IgnoreLidSwitchTweak ( GetterSetterSwitchTweak ): def __init__ ( self , ** options ): self . _inhibitor_name = \"gnome-tweak-tool-lid-inhibitor\" self . _inhibitor_path = \" %s / %s \" % ( gtweak . LIBEXEC_DIR , self . _inhibitor_name ) self . _dfile = AutostartFile ( None , autostart_desktop_filename = \"ignore-lid-switch-tweak.desktop\" , exec_cmd = self . _inhibitor_path ) GetterSetterSwitchTweak . __init__ ( self , _ ( \"Suspend when laptop lid is closed\" ), ** options ) def get_active ( self ): return not self . _sync_inhibitor () def set_active ( self , v ): self . _dfile . update_start_at_login ( not v ) self . _sync_inhibitor () def _sync_inhibitor ( self ): if ( self . _dfile . is_start_at_login_enabled ()): GLib . spawn_command_line_async ( self . _inhibitor_path ) return True else : bus = Gio . bus_get_sync ( Gio . BusType . SESSION , None ) bus . call ( 'org.gnome.tweak-tool.lid-inhibitor' , '/org/gnome/tweak_tool/lid_inhibitor' , 'org.gtk.Actions' , 'Activate' , GLib . Variant ( '(sava {sv} )' , ( 'quit' , [], {})), None , 0 , - 1 , None ) return False class GetterSetterSwitchTweak ( Gtk . Box , Tweak ): def __init__ ( self , name , ** options ): Gtk . Box . __init__ ( self , orientation = Gtk . Orientation . HORIZONTAL ) Tweak . __init__ ( self , name , options . get ( \"description\" , \"\" ), ** options ) sw = Gtk . Switch () sw . set_active ( self . get_active ()) sw . connect ( \"notify::active\" , self . _on_toggled ) build_label_beside_widget ( name , sw , hbox = self ) def _on_toggled ( self , sw , pspec ): self . set_active ( sw . get_active ()) def get_active ( self ): raise NotImplementedError () def set_active ( self , v ): raise NotImplementedError () class Tweak ( object ): main_window = None widget_for_size_group = None extra_info = \"\" def __init__ ( self , name , description , ** options ): self . name = name or \"\" self . description = description or \"\" self . uid = options . get ( \"uid\" , self . __class__ . __name__ ) self . group_name = options . get ( \"group_name\" , _ ( \"Miscellaneous\" )) self . loaded = options . get ( \"loaded\" , True ) self . widget_sort_hint = None self . _search_cache = None def search_matches ( self , txt ): if self . _search_cache is None : self . _search_cache = string_for_search ( self . name ) + \" \" + \\ string_for_search ( self . description ) try : self . _search_cache += \" \" + string_for_search ( self . extra_info ) except : LOG . warning ( \"Error adding search info\" , exc_info = True ) return txt in self . _search_cache def notify_logout ( self ): self . _logoutnotification = LogoutNotification () def notify_information ( self , summary , desc = \"\" ): self . _notification = Notification ( summary , desc )","title":"GNOME Tweaks"},{"location":"GTK/Examples/GNOME-Tweaks/#gnome-tweaks","text":"GnomeTweaks is the application class and is defined in gtweak/app.py . Window is the window class, and its constructor takes the application and liststore subclass instances as arguments. class GnomeTweaks ( Gtk . Application ): def do_activate ( self ): if not self . win : model = TweakModel () self . win = Window ( self , model ) self . win . show_all () self . win . present () TweakModel inherits from Gtk.ListStore and stores tweak groups which correspond to the pages of settings in the app. These tweak groups are defined as classes in Python modules placed in gtweak/tweaks , each of them subclassing parent classes like GSettingsSwitchTweak and GetterSetterSwitchTweak, which are defined in gtweak/widgets.py . Each of the tweak group modules defines a top-level list named TWEAK_GROUPS with only a single element - an instance of ListBoxTweakGroup , which is also defined in widgets.py. ListBoxTweakGroup, in turn, is a subclass of Gtk.ListBox and TweakGroup (tweakmodel.py) by multiple inheritance. Some TWEAK_GROUP s like that of the Desktop group are only populated conditionally. Desktop General from gtweak.widgets import ListBoxTweakGroup , GSettingsSwitchTweak , Title dicons = GSettingsSwitchTweak ( _ ( \"Show Icons\" ), \"org.gnome.desktop.background\" , \"show-desktop-icons\" ) home = GSettingsSwitchTweak ( _ ( \"Home\" ), \"org.gnome.nautilus.desktop\" , \"home-icon-visible\" , depends_on = dicons , schema_filename = \"org.gnome.nautilus.gschema.xml\" ) TWEAK_GROUPS = [] if home . loaded : TWEAK_GROUPS . append ( ListBoxTweakGroup ( _ ( \"Desktop\" ), Title ( _ ( \"Icons on Desktop\" ), \"\" , uid = \"title-theme\" , top = True ), dicons , home , GSettingsSwitchTweak ( _ ( \"Network Servers\" ), \"org.gnome.nautilus.desktop\" , \"network-icon-visible\" , depends_on = dicons , schema_filename = \"org.gnome.nautilus.gschema.xml\" ), GSettingsSwitchTweak ( _ ( \"Trash\" ), \"org.gnome.nautilus.desktop\" , \"trash-icon-visible\" , depends_on = dicons , schema_filename = \"org.gnome.nautilus.gschema.xml\" ), GSettingsSwitchTweak ( _ ( \"Mounted Volumes\" ), \"org.gnome.nautilus.desktop\" , \"volumes-visible\" , depends_on = dicons , schema_filename = \"org.gnome.nautilus.gschema.xml\" ), )) TWEAK_GROUPS = [ ListBoxTweakGroup ( _ ( \"General\" ), GSettingsSwitchTweak ( _ ( \"Animations\" ), \"org.gnome.desktop.interface\" , \"enable-animations\" ), IgnoreLidSwitchTweak (), # Don't show this setting in the Ubuntu session since this setting is in gnome-control-center there GSettingsSwitchTweak ( _ ( \"Over-Amplification\" ), \"org.gnome.desktop.sound\" , \"allow-volume-above-100-percent\" , desc = _ ( \"Allows raising the volume above 100%. This can result in a loss of audio quality; it is better to increase application volume settings, if possible.\" ), loaded = _shell_not_ubuntu ), ), ] ListBoxTweakGroup is instantiated with a series of arguments, starting with its name and followed by individual tweaks or settings. These tweaks are also defined as classes in gtweak/widgets.py , although some like IgnoreLidSwitchTweak are defined inline with the TWEAK_GROUP lists where they are instantiated. I'm still not sure what the _() means, but it is apparently some function call. All tweaks share some features: Their names reflect the control used in their interface (\"Switch\", \"SpinButton\", \"ComboEnum\", etc) in the form \" GSettings...Tweak They subclass Gtk.Box , _GSettingsTweak , and _DependableMixin by multiple inheritance They instantiate a horizontally oriented Gtk.Box with a curious syntax. Gtk . Box . __init__ ( self , orientation = Gtk . Orientation . HORIZONTAL ) They instantiate _GSettingsTweak with a similar syntax _GSettingsTweak . __init__ ( self , name , schema_name , key_name , ** options ) The rest of the widget is defined procedurally and added to self in the constructor as normal By far the most common tweak is GSettingsSwitchTweakValue . GSettingsSwitchTweak GSettingsSwitchTweakValue GSettingsFontButtonTweak GSettingsRangeTweak GSettingsSpinButtonTweak GSettingsComboEnumTweak GSettingsComboTweak class GSettingsSwitchTweak ( Gtk . Box , _GSettingsTweak , _DependableMixin ): def __init__ ( self , name , schema_name , key_name , ** options ): Gtk . Box . __init__ ( self , orientation = Gtk . Orientation . HORIZONTAL ) _GSettingsTweak . __init__ ( self , name , schema_name , key_name , ** options ) w = Gtk . Switch () self . settings . bind ( key_name , w , \"active\" , Gio . SettingsBindFlags . DEFAULT ) self . add_dependency_on_tweak ( options . get ( \"depends_on\" ), options . get ( \"depends_how\" ) ) vbox1 = Gtk . Box ( orientation = Gtk . Orientation . VERTICAL ) vbox1 . props . spacing = UI_BOX_SPACING lbl = Gtk . Label ( label = name ) lbl . props . ellipsize = Pango . EllipsizeMode . END lbl . props . xalign = 0.0 vbox1 . pack_start ( lbl , True , True , 0 ) if options . get ( \"desc\" ): description = options . get ( \"desc\" ) lbl_desc = Gtk . Label () lbl_desc . props . xalign = 0.0 lbl_desc . set_line_wrap ( True ) lbl_desc . get_style_context () . add_class ( \"dim-label\" ) lbl_desc . set_markup ( \"<span size='small'>\" + GLib . markup_escape_text ( description ) + \"</span>\" ) vbox1 . pack_start ( lbl_desc , True , True , 0 ) vbox2 = Gtk . Box ( orientation = Gtk . Orientation . VERTICAL ) vbox2_upper = Gtk . Box () vbox2_lower = Gtk . Box () vbox2 . pack_start ( vbox2_upper , True , True , 0 ) vbox2 . pack_start ( w , False , False , 0 ) vbox2 . pack_start ( vbox2_lower , True , True , 0 ) self . pack_start ( vbox1 , True , True , 0 ) self . pack_start ( vbox2 , False , False , 0 ) self . widget_for_size_group = None class GSettingsSwitchTweakValue ( Gtk . Box , _GSettingsTweak ): def __init__ ( self , name , schema_name , key_name , ** options ): Gtk . Box . __init__ ( self , orientation = Gtk . Orientation . HORIZONTAL ) _GSettingsTweak . __init__ ( self , name , schema_name , key_name , ** options ) sw = Gtk . Switch () sw . set_active ( self . get_active ()) sw . connect ( \"notify::active\" , self . _on_toggled ) vbox1 = Gtk . Box ( orientation = Gtk . Orientation . VERTICAL ) vbox1 . props . spacing = UI_BOX_SPACING lbl = Gtk . Label ( label = name ) lbl . props . ellipsize = Pango . EllipsizeMode . END lbl . props . xalign = 0.0 vbox1 . pack_start ( lbl , True , True , 0 ) if options . get ( \"desc\" ): description = options . get ( \"desc\" ) lbl_desc = Gtk . Label () lbl_desc . props . xalign = 0.0 lbl_desc . set_line_wrap ( True ) lbl_desc . get_style_context () . add_class ( \"dim-label\" ) lbl_desc . set_markup ( \"<span size='small'>\" + GLib . markup_escape_text ( description ) + \"</span>\" ) vbox1 . pack_start ( lbl_desc , True , True , 0 ) vbox2 = Gtk . Box ( orientation = Gtk . Orientation . VERTICAL ) vbox2_upper = Gtk . Box () vbox2_lower = Gtk . Box () vbox2 . pack_start ( vbox2_upper , True , True , 0 ) vbox2 . pack_start ( sw , False , False , 0 ) vbox2 . pack_start ( vbox2_lower , True , True , 0 ) self . pack_start ( vbox1 , True , True , 0 ) self . pack_start ( vbox2 , False , False , 0 ) self . widget_for_size_group = None def _on_toggled ( self , sw , pspec ): self . set_active ( sw . get_active ()) def set_active ( self , v ): raise NotImplementedError () def get_active ( self ): raise NotImplementedError () class GSettingsFontButtonTweak ( Gtk . Box , _GSettingsTweak , _DependableMixin ): def __init__ ( self , name , schema_name , key_name , ** options ): Gtk . Box . __init__ ( self , orientation = Gtk . Orientation . HORIZONTAL ) _GSettingsTweak . __init__ ( self , name , schema_name , key_name , ** options ) w = Gtk . FontButton () w . set_use_font ( True ) self . settings . bind ( key_name , w , \"font-name\" , Gio . SettingsBindFlags . DEFAULT ) build_label_beside_widget ( name , w , hbox = self ) self . widget_for_size_group = w class GSettingsRangeTweak ( Gtk . Box , _GSettingsTweak , _DependableMixin ): def __init__ ( self , name , schema_name , key_name , ** options ): Gtk . Box . __init__ ( self , orientation = Gtk . Orientation . HORIZONTAL ) _GSettingsTweak . __init__ ( self , name , schema_name , key_name , ** options ) # returned variant is range:(min, max) _min , _max = self . settings . get_range ( key_name )[ 1 ] w = Gtk . HScale . new_with_range ( _min , _max , options . get ( 'adjustment_step' , 1 )) self . settings . bind ( key_name , w . get_adjustment (), \"value\" , Gio . SettingsBindFlags . DEFAULT ) build_label_beside_widget ( self . name , w , hbox = self ) self . widget_for_size_group = w class GSettingsSpinButtonTweak ( Gtk . Box , _GSettingsTweak , _DependableMixin ): def __init__ ( self , name , schema_name , key_name , ** options ): Gtk . Box . __init__ ( self , orientation = Gtk . Orientation . HORIZONTAL ) _GSettingsTweak . __init__ ( self , name , schema_name , key_name , ** options ) # returned variant is range:(min, max) _min , _max = self . settings . get_range ( key_name )[ 1 ] adjustment = Gtk . Adjustment ( value = 0 , lower = _min , upper = _max , step_increment = options . get ( 'adjustment_step' , 1 )) w = Gtk . SpinButton () w . set_adjustment ( adjustment ) w . set_digits ( options . get ( 'digits' , 0 )) self . settings . bind ( key_name , adjustment , \"value\" , Gio . SettingsBindFlags . DEFAULT ) build_label_beside_widget ( name , w , hbox = self ) self . widget_for_size_group = w self . add_dependency_on_tweak ( options . get ( \"depends_on\" ), options . get ( \"depends_how\" ) ) class GSettingsComboEnumTweak ( Gtk . Box , _GSettingsTweak , _DependableMixin ): def __init__ ( self , name , schema_name , key_name , ** options ): Gtk . Box . __init__ ( self , orientation = Gtk . Orientation . HORIZONTAL ) _GSettingsTweak . __init__ ( self , name , schema_name , key_name , ** options ) _type , values = self . settings . get_range ( key_name ) value = self . settings . get_string ( key_name ) self . settings . connect ( 'changed::' + self . key_name , self . _on_setting_changed ) w = build_combo_box_text ( value , * [( v , v . replace ( \"-\" , \" \" ) . title ()) for v in values ]) w . connect ( 'changed' , self . _on_combo_changed ) self . combo = w build_label_beside_widget ( name , w , hbox = self ) self . widget_for_size_group = w def _values_are_different ( self ): # to stop bouncing back and forth between changed signals. I suspect there must be a nicer # Gio.settings_bind way to fix this return self . settings . get_string ( self . key_name ) != \\ self . combo . get_model () . get_value ( self . combo . get_active_iter (), 0 ) def _on_setting_changed ( self , setting , key ): assert key == self . key_name val = self . settings . get_string ( key ) model = self . combo . get_model () for row in model : if val == row [ 0 ]: self . combo . set_active_iter ( row . iter ) break def _on_combo_changed ( self , combo ): val = self . combo . get_model () . get_value ( self . combo . get_active_iter (), 0 ) if self . _values_are_different (): self . settings . set_string ( self . key_name , val ) class GSettingsComboTweak ( Gtk . Box , _GSettingsTweak , _DependableMixin ): def __init__ ( self , name , schema_name , key_name , key_options , ** options ): Gtk . Box . __init__ ( self , orientation = Gtk . Orientation . HORIZONTAL ) _GSettingsTweak . __init__ ( self , name , schema_name , key_name , ** options ) # check key_options is iterable # and if supplied, check it is a list of 2-tuples assert len ( key_options ) >= 0 if len ( key_options ): assert len ( key_options [ 0 ]) == 2 self . _key_options = key_options self . combo = build_combo_box_text ( self . settings . get_string ( self . key_name ), * key_options ) self . combo . connect ( 'changed' , self . _on_combo_changed ) self . settings . connect ( 'changed::' + self . key_name , self . _on_setting_changed ) build_label_beside_widget ( name , self . combo , hbox = self ) self . widget_for_size_group = self . combo def _on_setting_changed ( self , setting , key ): assert key == self . key_name val = self . settings . get_string ( key ) model = self . combo . get_model () for row in model : if val == row [ 0 ]: self . combo . set_active_iter ( row . iter ) return self . combo . set_active ( - 1 ) def _on_combo_changed ( self , combo ): _iter = combo . get_active_iter () if _iter : value = combo . get_model () . get_value ( _iter , 0 ) self . settings . set_string ( self . key_name , value ) @property def extra_info ( self ): if self . _extra_info is None : self . _extra_info = self . settings . schema_get_summary ( self . key_name ) self . _extra_info += \" \" + \" \" . join ( op [ 0 ] for op in self . _key_options ) return self . _extra_info IgnoreLidSwitchTweak is unusual as a tweak that inherits from GetterSetterSwitchTweak, which inherits in turn from the Tweak base class that is actually in gtweak/tweakmodel.py IgnoreLidSwitchTweak GetterSetterSwitchTweak Tweak class IgnoreLidSwitchTweak ( GetterSetterSwitchTweak ): def __init__ ( self , ** options ): self . _inhibitor_name = \"gnome-tweak-tool-lid-inhibitor\" self . _inhibitor_path = \" %s / %s \" % ( gtweak . LIBEXEC_DIR , self . _inhibitor_name ) self . _dfile = AutostartFile ( None , autostart_desktop_filename = \"ignore-lid-switch-tweak.desktop\" , exec_cmd = self . _inhibitor_path ) GetterSetterSwitchTweak . __init__ ( self , _ ( \"Suspend when laptop lid is closed\" ), ** options ) def get_active ( self ): return not self . _sync_inhibitor () def set_active ( self , v ): self . _dfile . update_start_at_login ( not v ) self . _sync_inhibitor () def _sync_inhibitor ( self ): if ( self . _dfile . is_start_at_login_enabled ()): GLib . spawn_command_line_async ( self . _inhibitor_path ) return True else : bus = Gio . bus_get_sync ( Gio . BusType . SESSION , None ) bus . call ( 'org.gnome.tweak-tool.lid-inhibitor' , '/org/gnome/tweak_tool/lid_inhibitor' , 'org.gtk.Actions' , 'Activate' , GLib . Variant ( '(sava {sv} )' , ( 'quit' , [], {})), None , 0 , - 1 , None ) return False class GetterSetterSwitchTweak ( Gtk . Box , Tweak ): def __init__ ( self , name , ** options ): Gtk . Box . __init__ ( self , orientation = Gtk . Orientation . HORIZONTAL ) Tweak . __init__ ( self , name , options . get ( \"description\" , \"\" ), ** options ) sw = Gtk . Switch () sw . set_active ( self . get_active ()) sw . connect ( \"notify::active\" , self . _on_toggled ) build_label_beside_widget ( name , sw , hbox = self ) def _on_toggled ( self , sw , pspec ): self . set_active ( sw . get_active ()) def get_active ( self ): raise NotImplementedError () def set_active ( self , v ): raise NotImplementedError () class Tweak ( object ): main_window = None widget_for_size_group = None extra_info = \"\" def __init__ ( self , name , description , ** options ): self . name = name or \"\" self . description = description or \"\" self . uid = options . get ( \"uid\" , self . __class__ . __name__ ) self . group_name = options . get ( \"group_name\" , _ ( \"Miscellaneous\" )) self . loaded = options . get ( \"loaded\" , True ) self . widget_sort_hint = None self . _search_cache = None def search_matches ( self , txt ): if self . _search_cache is None : self . _search_cache = string_for_search ( self . name ) + \" \" + \\ string_for_search ( self . description ) try : self . _search_cache += \" \" + string_for_search ( self . extra_info ) except : LOG . warning ( \"Error adding search info\" , exc_info = True ) return txt in self . _search_cache def notify_logout ( self ): self . _logoutnotification = LogoutNotification () def notify_information ( self , summary , desc = \"\" ): self . _notification = Notification ( summary , desc )","title":"GNOME Tweaks"},{"location":"GTK/Tasks/Boilerplate/","text":"Boilerplate Interface <?xml version=\"1.0\" encoding=\"UTF-8\"?> <interface> <requires lib= \"gtk+\" version= \"3.40\" /> <object class= \"GtkApplicationWindow\" id= \"window\" > <property name= \"title\" > My GTK App </property> <property name= \"default-width\" > 300 </property> <property name= \"default-height\" > 300 </property> </object> </interface> Rust Python gtk-rs use gtk4 :: prelude :: * ; use gtk4 :: { Application , ApplicationWindow }; fn main () { let app = Application :: builder () . application_id ( \"com.example.learning-gtk\" ) . build (); app . connect_activate ( build_ui ); app . run (); } fn build_ui ( app : & Application ) { let window = ApplicationWindow :: builder () . application ( app ) . default_width ( 300 ) . default_height ( 300 ) . title ( \"My GTK App\" ) . build (); window . present (); } PyGTK import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk # (1) class ApplicationWindow ( Gtk . ApplicationWindow ): # (2) def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) # (3) self . set_size_request ( 300 , 300 ) self . set_title ( \"My GTK App\" ) self . show_all () self . present () class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.learning-gtk' ) # (4) def do_activate ( self ): self . window = ApplicationWindow ( application = self ) if __name__ == '__main__' : app = Application () # (5) app . run () # (6) Note that the gi module's require_version() function must be called before importing Gtk. The recommended way of using the PyGTK API is to subclass and modify the Application and ApplicationWindow classes. These were introduced in GTK+ versions 3.0 and 3.4 respectively and are meant to be used as base classes. PyGTK also offers an alternative Gtk.Window class, which like ApplicationWindow is a subclass of Gtk.Container, and which still appears in many tutorials. The ApplicationWindow subclass calls the superclass's constructor. The UI is composed by adding widgets to this subclass by calling self.add() . Typically a single Box container is added to the top-level container and controls are added to that container. The Application subclass also calls its superclass's constructor and exposes a do_activate() method that instantiates the ApplicationWindow subclass and assigns that object to self.window before calling self.window.present() . The Application subclass essentially acts as a wrapper around ApplicationWindow. At the script's entrypoint, the Application subclass itself is instantiated and its run method is called. In online tutorials that use Window , typically the Application wrapper class does not appear. The Gtk.main() method must be called somewhere in the script in order for the UI to appear.","title":"Boilerplate"},{"location":"GTK/Tasks/Boilerplate/#boilerplate","text":"Interface <?xml version=\"1.0\" encoding=\"UTF-8\"?> <interface> <requires lib= \"gtk+\" version= \"3.40\" /> <object class= \"GtkApplicationWindow\" id= \"window\" > <property name= \"title\" > My GTK App </property> <property name= \"default-width\" > 300 </property> <property name= \"default-height\" > 300 </property> </object> </interface> Rust Python gtk-rs use gtk4 :: prelude :: * ; use gtk4 :: { Application , ApplicationWindow }; fn main () { let app = Application :: builder () . application_id ( \"com.example.learning-gtk\" ) . build (); app . connect_activate ( build_ui ); app . run (); } fn build_ui ( app : & Application ) { let window = ApplicationWindow :: builder () . application ( app ) . default_width ( 300 ) . default_height ( 300 ) . title ( \"My GTK App\" ) . build (); window . present (); } PyGTK import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk # (1) class ApplicationWindow ( Gtk . ApplicationWindow ): # (2) def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) # (3) self . set_size_request ( 300 , 300 ) self . set_title ( \"My GTK App\" ) self . show_all () self . present () class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.learning-gtk' ) # (4) def do_activate ( self ): self . window = ApplicationWindow ( application = self ) if __name__ == '__main__' : app = Application () # (5) app . run () # (6) Note that the gi module's require_version() function must be called before importing Gtk. The recommended way of using the PyGTK API is to subclass and modify the Application and ApplicationWindow classes. These were introduced in GTK+ versions 3.0 and 3.4 respectively and are meant to be used as base classes. PyGTK also offers an alternative Gtk.Window class, which like ApplicationWindow is a subclass of Gtk.Container, and which still appears in many tutorials. The ApplicationWindow subclass calls the superclass's constructor. The UI is composed by adding widgets to this subclass by calling self.add() . Typically a single Box container is added to the top-level container and controls are added to that container. The Application subclass also calls its superclass's constructor and exposes a do_activate() method that instantiates the ApplicationWindow subclass and assigns that object to self.window before calling self.window.present() . The Application subclass essentially acts as a wrapper around ApplicationWindow. At the script's entrypoint, the Application subclass itself is instantiated and its run method is called. In online tutorials that use Window , typically the Application wrapper class does not appear. The Gtk.main() method must be called somewhere in the script in order for the UI to appear.","title":"Boilerplate"},{"location":"GTK/Tasks/Environment/","text":"Development environment Rust Python Red Hat dnf install gtk4-devel gcc Ubuntu apt install libgtk-4-dev build-essential Red Hat dnf install python3-venv python3-wheel dnf install gcc zlib-devel bzip2 bzip2-devel readline-devel sqlite sqlite-devel openssl-devel tk-devel git python3-cairo-devel cairo-gobject-devel gobject-introspection-devel pip install pygobject Ubuntu apt install python3-gi python3-gi-cairo gir1.2-gtk-3.0 libgirepository1.0-dev pip install pygobject","title":"Development environment"},{"location":"GTK/Tasks/Environment/#development-environment","text":"Rust Python Red Hat dnf install gtk4-devel gcc Ubuntu apt install libgtk-4-dev build-essential Red Hat dnf install python3-venv python3-wheel dnf install gcc zlib-devel bzip2 bzip2-devel readline-devel sqlite sqlite-devel openssl-devel tk-devel git python3-cairo-devel cairo-gobject-devel gobject-introspection-devel pip install pygobject Ubuntu apt install python3-gi python3-gi-cairo gir1.2-gtk-3.0 libgirepository1.0-dev pip install pygobject","title":"Development environment"},{"location":"GTK/Tasks/Hello-World/","text":"Hello, World! Window frame Rust Python TODO At the moment, this example is broken because I don't know how to pass the string into the Application struct for string interpolation. use gtk4 :: prelude :: * ; use gtk4 :: { Application , ApplicationWindow }; fn main () { let name = String :: new (); if let Some ( s ) = 42 std :: env :: args (). nth ( 1 ) { name = s ; } else { name = String :: from ( \"World\" ); }; let app = Application :: builder () . application_id ( \"com.example.learning-gtk\" ) . build (); app . connect_activate ( build_ui ); println! ( \"{}\" , app . name ); app . run (); } fn build_ui ( app : & Application ) { let window = ApplicationWindow :: builder () . application ( app ) . title ( \"Hello, World!\" ) . default_height ( 300 ) . default_width ( 300 ) . build (); window . present (); } import sys import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , name , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_default_size ( 300 , 300 ) self . set_title ( f \"Hello, { name } !\" ) self . show_all () class Application ( Gtk . Application ): def __init__ ( self , name ): super () . __init__ ( application_id = \"com.example.learning-gtk\" ) self . name = name def do_activate ( self ): self . window = ApplicationWindow ( application = self , name = self . name ) if __name__ == '__main__' : if len ( sys . argv ) > 1 : app = Application ( sys . argv [ - 1 ]) else : app = Application ( \"World\" ) app . run () Label Rust Python import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk import sys class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , name , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_size_request ( 200 , 200 ) self . add ( Gtk . Label ( label = f \"Hello, { name } !\" )) class Application ( Gtk . Application ): def __init__ ( self , name = \"World\" , * args , ** kwargs ): self . name = name super () . __init__ ( * args , application_id = \"org.example.myapp\" , ** kwargs ) def do_activate ( self ): self . window = ApplicationWindow ( application = self , name , title = f \"Hello, { self . name } !\" ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = Application () app . run () Button reveal Interface <?xml version=\"1.0\" encoding=\"UTF-8\"?> <interface> <requires lib= \"gtk+\" version= \"3.40\" > <object class= \"GtkApplicationWindow\" id= \"window\" > <property name= \"title\" > My GTK App </property> <property name= \"default-width\" > 300 </property> <property name= \"default-height\" > 300 </property> <child> <object class= \"GtkButton\" id= \"button\" > <property name= \"label\" > Press me! </property> <property name= \"margin-top\" > 12 </property> <property name= \"margin-bottom\" > 12 </property> <property name= \"margin-start\" > 12 </property> <property name= \"margin-end\" > 12 </property> </object> </child> </object> </interface> Rust Python use gtk :: prelude :: * ; use gtk :: { Application , ApplicationWindow , Button }; fn main () { let app = Application :: builder () . application_id ( \"org.gtk-rs.example\" ) . build (); app . connect_activate ( build_ui ); app . run (); } fn build_ui ( app : & Application ) { let builder = gtk :: Builder :: from_string ( include_str! ( \"window.ui\" )); let window : ApplicationWindow = builder . object ( \"window\" ) . expect ( \"Could not get object `window` from builder.\" ); let button : Button = builder . object ( \"button\" ) . expect ( \"Could not get object `button` from builder.\" ); window . set_application ( Some ( app )); button . connect_clicked ( move | button | { // (1) button . set_label ( \"Hello World!\" ); }); window . set_child ( Some ( & button )); window . show_all (); window . present (); } This move keyword appears to be unnecessary. import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.myapp' ) def do_activate ( self ): builder = Gtk . Builder . new_from_file ( 'hw-button.ui' ) self . window = builder . get_object ( 'window' ) self . button = builder . get_object ( 'button' ) self . button . connect ( 'clicked' , self . on_button_clicked ) self . window . connect ( 'destroy' , Gtk . main_quit ) self . window . show_all () self . window . present () def on_button_clicked ( self , button ): self . button . set_label ( 'Hello, World!' ) def run ( self ): super () . run () Gtk . main () if __name__ == '__main__' : app = Application () app . run () Interactive Rust Python import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) box = Gtk . Box ( orientation = Gtk . Orientation . VERTICAL , spacing = 10 ) self . add ( box ) question = Gtk . Label . new ( \"What is your name?\" ) box . add ( question ) self . entry = Gtk . Entry ( text = \"World\" ) box . add ( self . entry ) button = Gtk . Button . new_with_mnemonic ( \"Greet\" ) button . connect ( \"clicked\" , self . on_button_clicked , self ) box . add ( button ) def on_button_clicked ( self , button , parent ): dialog = Gtk . MessageDialog ( message_type = Gtk . MessageType . INFO , text = f \"Hello, { parent . entry . get_text () } \" , parent = parent , ) dialog . add_button ( \"OK\" , Gtk . ResponseType . OK ) dialog . run () dialog . destroy () class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.myapp' ) def do_activate ( self ): self . window = ApplicationWindow ( application = self ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = Application () app . run () HeaderBar Rust Python import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk import sys class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_default_size ( - 1 , - 1 ) # headerbar = Gtk.HeaderBar(title=f\"Hello, {name}!\", subtitle=\"HeaderBar example\", show_close_button=True) headerbar = Gtk . HeaderBar () headerbar . set_title ( f \"Hello, World!\" ) headerbar . set_subtitle ( \"HeaderBar example\" ) headerbar . set_show_close_button ( True ) self . set_titlebar ( headerbar ) button = Gtk . Button ( label = \"Greet\" ) button . connect ( \"clicked\" , self . on_button_clicked , self ) headerbar . add ( button ) self . entry = Gtk . Entry ( text = \"World\" , name = \"entry\" ) headerbar . add ( self . entry ) def on_button_clicked ( self , button , parent ): dialog = Gtk . MessageDialog ( message_type = Gtk . MessageType . INFO , text = f \"Hello, { parent . entry . get_text () } !\" , parent = parent , ) dialog . add_button ( \"O_K\" , Gtk . ResponseType . OK ) dialog . run () dialog . destroy () class HeaderBar ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = \"org.example.headerbar\" ) def do_activate ( self ): self . window = ApplicationWindow ( application = self ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = HeaderBar () app . run ()","title":"Hello, World!"},{"location":"GTK/Tasks/Hello-World/#hello-world","text":"","title":"Hello, World!"},{"location":"GTK/Tasks/Hello-World/#window-frame","text":"Rust Python TODO At the moment, this example is broken because I don't know how to pass the string into the Application struct for string interpolation. use gtk4 :: prelude :: * ; use gtk4 :: { Application , ApplicationWindow }; fn main () { let name = String :: new (); if let Some ( s ) = 42 std :: env :: args (). nth ( 1 ) { name = s ; } else { name = String :: from ( \"World\" ); }; let app = Application :: builder () . application_id ( \"com.example.learning-gtk\" ) . build (); app . connect_activate ( build_ui ); println! ( \"{}\" , app . name ); app . run (); } fn build_ui ( app : & Application ) { let window = ApplicationWindow :: builder () . application ( app ) . title ( \"Hello, World!\" ) . default_height ( 300 ) . default_width ( 300 ) . build (); window . present (); } import sys import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , name , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_default_size ( 300 , 300 ) self . set_title ( f \"Hello, { name } !\" ) self . show_all () class Application ( Gtk . Application ): def __init__ ( self , name ): super () . __init__ ( application_id = \"com.example.learning-gtk\" ) self . name = name def do_activate ( self ): self . window = ApplicationWindow ( application = self , name = self . name ) if __name__ == '__main__' : if len ( sys . argv ) > 1 : app = Application ( sys . argv [ - 1 ]) else : app = Application ( \"World\" ) app . run ()","title":"Window frame"},{"location":"GTK/Tasks/Hello-World/#label","text":"Rust Python import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk import sys class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , name , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_size_request ( 200 , 200 ) self . add ( Gtk . Label ( label = f \"Hello, { name } !\" )) class Application ( Gtk . Application ): def __init__ ( self , name = \"World\" , * args , ** kwargs ): self . name = name super () . __init__ ( * args , application_id = \"org.example.myapp\" , ** kwargs ) def do_activate ( self ): self . window = ApplicationWindow ( application = self , name , title = f \"Hello, { self . name } !\" ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = Application () app . run ()","title":"Label"},{"location":"GTK/Tasks/Hello-World/#button-reveal","text":"Interface <?xml version=\"1.0\" encoding=\"UTF-8\"?> <interface> <requires lib= \"gtk+\" version= \"3.40\" > <object class= \"GtkApplicationWindow\" id= \"window\" > <property name= \"title\" > My GTK App </property> <property name= \"default-width\" > 300 </property> <property name= \"default-height\" > 300 </property> <child> <object class= \"GtkButton\" id= \"button\" > <property name= \"label\" > Press me! </property> <property name= \"margin-top\" > 12 </property> <property name= \"margin-bottom\" > 12 </property> <property name= \"margin-start\" > 12 </property> <property name= \"margin-end\" > 12 </property> </object> </child> </object> </interface> Rust Python use gtk :: prelude :: * ; use gtk :: { Application , ApplicationWindow , Button }; fn main () { let app = Application :: builder () . application_id ( \"org.gtk-rs.example\" ) . build (); app . connect_activate ( build_ui ); app . run (); } fn build_ui ( app : & Application ) { let builder = gtk :: Builder :: from_string ( include_str! ( \"window.ui\" )); let window : ApplicationWindow = builder . object ( \"window\" ) . expect ( \"Could not get object `window` from builder.\" ); let button : Button = builder . object ( \"button\" ) . expect ( \"Could not get object `button` from builder.\" ); window . set_application ( Some ( app )); button . connect_clicked ( move | button | { // (1) button . set_label ( \"Hello World!\" ); }); window . set_child ( Some ( & button )); window . show_all (); window . present (); } This move keyword appears to be unnecessary. import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.myapp' ) def do_activate ( self ): builder = Gtk . Builder . new_from_file ( 'hw-button.ui' ) self . window = builder . get_object ( 'window' ) self . button = builder . get_object ( 'button' ) self . button . connect ( 'clicked' , self . on_button_clicked ) self . window . connect ( 'destroy' , Gtk . main_quit ) self . window . show_all () self . window . present () def on_button_clicked ( self , button ): self . button . set_label ( 'Hello, World!' ) def run ( self ): super () . run () Gtk . main () if __name__ == '__main__' : app = Application () app . run ()","title":"Button reveal"},{"location":"GTK/Tasks/Hello-World/#interactive","text":"Rust Python import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) box = Gtk . Box ( orientation = Gtk . Orientation . VERTICAL , spacing = 10 ) self . add ( box ) question = Gtk . Label . new ( \"What is your name?\" ) box . add ( question ) self . entry = Gtk . Entry ( text = \"World\" ) box . add ( self . entry ) button = Gtk . Button . new_with_mnemonic ( \"Greet\" ) button . connect ( \"clicked\" , self . on_button_clicked , self ) box . add ( button ) def on_button_clicked ( self , button , parent ): dialog = Gtk . MessageDialog ( message_type = Gtk . MessageType . INFO , text = f \"Hello, { parent . entry . get_text () } \" , parent = parent , ) dialog . add_button ( \"OK\" , Gtk . ResponseType . OK ) dialog . run () dialog . destroy () class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.myapp' ) def do_activate ( self ): self . window = ApplicationWindow ( application = self ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = Application () app . run ()","title":"Interactive"},{"location":"GTK/Tasks/Hello-World/#headerbar","text":"Rust Python import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk import sys class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_default_size ( - 1 , - 1 ) # headerbar = Gtk.HeaderBar(title=f\"Hello, {name}!\", subtitle=\"HeaderBar example\", show_close_button=True) headerbar = Gtk . HeaderBar () headerbar . set_title ( f \"Hello, World!\" ) headerbar . set_subtitle ( \"HeaderBar example\" ) headerbar . set_show_close_button ( True ) self . set_titlebar ( headerbar ) button = Gtk . Button ( label = \"Greet\" ) button . connect ( \"clicked\" , self . on_button_clicked , self ) headerbar . add ( button ) self . entry = Gtk . Entry ( text = \"World\" , name = \"entry\" ) headerbar . add ( self . entry ) def on_button_clicked ( self , button , parent ): dialog = Gtk . MessageDialog ( message_type = Gtk . MessageType . INFO , text = f \"Hello, { parent . entry . get_text () } !\" , parent = parent , ) dialog . add_button ( \"O_K\" , Gtk . ResponseType . OK ) dialog . run () dialog . destroy () class HeaderBar ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = \"org.example.headerbar\" ) def do_activate ( self ): self . window = ApplicationWindow ( application = self ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = HeaderBar () app . run ()","title":"HeaderBar"},{"location":"GTK/Tasks/Image-Viewer/","text":"Image Viewer use gtk4 :: prelude :: * ; use gtk4 :: { Application , ApplicationWindow , Box , Picture }; fn main () { let app = Application :: builder () . application_id ( \"org.gtk-rs.example\" ) . build (); app . connect_activate ( build_ui ); app . run (); } fn build_ui ( app : & Application ) { let img = Picture :: for_filename ( \"C: \\\\ Users \\\\ 4472936 \\\\ Rust \\\\ gtk \\\\ pic.png\" ); let container = Box :: new ( gtk :: Orientation :: Vertical , 12 ); container . append ( & img ); let window = ApplicationWindow :: builder () . application ( app ) . title ( \"My GTK App\" ) . child ( & container ) . build (); window . present (); }","title":"Image Viewer"},{"location":"GTK/Tasks/Image-Viewer/#image-viewer","text":"use gtk4 :: prelude :: * ; use gtk4 :: { Application , ApplicationWindow , Box , Picture }; fn main () { let app = Application :: builder () . application_id ( \"org.gtk-rs.example\" ) . build (); app . connect_activate ( build_ui ); app . run (); } fn build_ui ( app : & Application ) { let img = Picture :: for_filename ( \"C: \\\\ Users \\\\ 4472936 \\\\ Rust \\\\ gtk \\\\ pic.png\" ); let container = Box :: new ( gtk :: Orientation :: Vertical , 12 ); container . append ( & img ); let window = ApplicationWindow :: builder () . application ( app ) . title ( \"My GTK App\" ) . child ( & container ) . build (); window . present (); }","title":"Image Viewer"},{"location":"GTK/Tasks/Starships/","text":"Starships This task implements the list/details pattern . Hardcoded data Rust Python import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) ships = [ 'USS Enterprise' , 'USS Defiant' , 'USS Voyager' ] self . treeview = Gtk . TreeView ( model = Gtk . ListStore . new (( str ,))) column = Gtk . TreeViewColumn ( \"Ship\" , Gtk . CellRendererText (), text = 0 ) self . treeview . append_column ( column ) for s in ships : self . treeview . get_model () . append (( s ,)) self . add ( self . treeview ) self . set_size_request ( - 1 , - 1 ) class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.myapp' ) def do_activate ( self ): self . window = ApplicationWindow ( application = self ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = Application () app . run () CSV import Rust Python import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk import csv class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) with open ( '/home/jasper/dogfood/csv/starships.csv' , mode = 'r' ) as f : reader = csv . reader ( f ) self . headers = [ h . title () for h in next ( reader )] self . data = [ r for r in reader ] self . treeview = Gtk . TreeView ( model = Gtk . ListStore . new (( str , str , str , str ))) for h in self . headers : column = Gtk . TreeViewColumn ( h , Gtk . CellRendererText (), text = self . headers . index ( h )) self . treeview . append_column ( column ) for r in self . data : self . treeview . get_model () . append ( r ) self . treeview . connect ( 'row-activated' , self . on_row_activated ) self . add ( self . treeview ) self . set_size_request ( - 1 , - 1 ) class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.myapp' ) def do_activate ( self ): self . window = ApplicationWindow ( application = self ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = Application () app . run () Sortable columns Rust Python import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk import csv class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) with open ( '/home/jasper/dogfood/csv/starships.csv' , mode = 'r' ) as f : reader = csv . reader ( f ) self . headers = [ h . title () for h in next ( reader )] self . data = [ r for r in reader ] self . treeview = Gtk . TreeView ( model = Gtk . ListStore . new (( str , str , str , str ))) for h in self . headers : column = Gtk . TreeViewColumn ( h , Gtk . CellRendererText (), text = self . headers . index ( h )) column . set_sort_column_id ( self . headers . index ( h )) self . treeview . append_column ( column ) for r in self . data : self . treeview . get_model () . append ( r ) self . treeview . connect ( 'row-activated' , self . on_row_activated ) self . add ( self . treeview ) self . set_size_request ( - 1 , - 1 ) class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.myapp' ) def do_activate ( self ): self . window = ApplicationWindow ( application = self ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = Application () app . run () Event handler Rust Python import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk import csv class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) with open ( '/home/jasper/dogfood/csv/starships.csv' , mode = 'r' ) as f : reader = csv . reader ( f ) self . headers = [ h . title () for h in next ( reader )] self . data = [ r for r in reader ] self . treeview = Gtk . TreeView ( model = Gtk . ListStore . new (( str , str , str , str ))) for h in self . headers : column = Gtk . TreeViewColumn ( h , Gtk . CellRendererText (), text = self . headers . index ( h )) column . set_sort_column_id ( self . headers . index ( h )) self . treeview . append_column ( column ) for r in self . data : self . treeview . get_model () . append ( r ) self . treeview . connect ( 'row-activated' , self . on_row_activated ) self . add ( self . treeview ) self . set_size_request ( - 1 , - 1 ) def on_row_activated ( self , treeview , path , col ): model = treeview . get_model () print ( f 'Using path object as index to model: { model [ path ][:] } ' ) class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.myapp' ) def do_activate ( self ): self . window = ApplicationWindow ( application = self ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = Application () app . run () MessageDialog Rust Python import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk import csv class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) with open ( '/home/jasper/dogfood/csv/starships.csv' , mode = 'r' ) as f : reader = csv . reader ( f ) self . headers = [ h . title () for h in next ( reader )] self . data = [ r for r in reader ] self . treeview = Gtk . TreeView ( model = Gtk . ListStore . new (( str , str , str , str ))) for h in self . headers : column = Gtk . TreeViewColumn ( h , Gtk . CellRendererText (), text = self . headers . index ( h )) column . set_sort_column_id ( self . headers . index ( h )) self . treeview . append_column ( column ) for r in self . data : self . treeview . get_model () . append ( r ) self . treeview . connect ( 'row-activated' , self . on_row_activated ) self . add ( self . treeview ) self . set_size_request ( - 1 , - 1 ) def on_row_activated ( self , treeview , path , col ): model = treeview . get_model () dialog = Gtk . MessageDialog ( message_type = Gtk . MessageType . INFO , text = model [ path ][:], parent = self ) dialog . add_button ( \"OK\" , Gtk . ResponseType . OK ) dialog . run () dialog . destroy () class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.myapp' ) def do_activate ( self ): self . window = ApplicationWindow ( application = self ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = Application () app . run () Dice roller Rust Python import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk import random from math import floor class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) scale_adj = Gtk . Adjustment . new ( 1 , 0 , 6 , 1 , 2 , 0 ) self . scale = Gtk . Scale . new ( Gtk . Orientation . HORIZONTAL , scale_adj ) self . scale . set_digits ( 0 ) button = Gtk . Button . new_with_label ( \"Throw\" ) button . connect ( \"clicked\" , self . on_button_clicked ) self . label = Gtk . Label . new () box = Gtk . Box . new ( Gtk . Orientation . VERTICAL , 5 ) box . pack_start ( self . scale , False , True , 0 ) box . pack_start ( button , False , True , 0 ) box . pack_start ( self . label , False , True , 0 ) self . add ( box ) self . set_size_request ( 200 , 200 ) def on_button_clicked ( self , button ): dice = floor ( self . scale . get_value ()) results = [ random . randrange ( 6 ) for i in range ( dice )] self . label . set_text ( str ( results )) class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.myapp' ) def do_activate ( self ): self . window = ApplicationWindow ( application = self ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = Application () app . run () MenuBar Rust Python import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk class AppWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_size_request ( 250 , - 1 ) menubar = Gtk . MenuBar . new () self . add ( menubar ) file = Gtk . MenuItem . new_with_label ( \"File\" ) menubar . append ( file ) filemenu = Gtk . Menu . new () file . set_submenu ( filemenu ) new = Gtk . MenuItem . new_with_label ( \"New\" ) open = Gtk . MenuItem . new_with_label ( \"Open\" ) filemenu . append ( new ) filemenu . append ( open ) edit = Gtk . MenuItem . new_with_label ( \"Edit\" ) menubar . append ( edit ) editmenu = Gtk . Menu . new () edit . set_submenu ( editmenu ) cut = Gtk . MenuItem . new_with_label ( \"Cut\" ) copy = Gtk . MenuItem . new_with_label ( \"Copy\" ) paste = Gtk . MenuItem . new_with_label ( \"Paste\" ) editmenu . append ( cut ) editmenu . append ( copy ) editmenu . append ( paste ) help = Gtk . MenuItem . new_with_label ( \"Help\" ) menubar . append ( help ) helpmenu = Gtk . Menu . new () help . set_submenu ( helpmenu ) contents = Gtk . MenuItem . new_with_label ( \"Help\" ) about = Gtk . MenuItem . new_with_label ( \"About\" ) helpmenu . append ( contents ) helpmenu . append ( about ) class Application ( Gtk . Application ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , application_id = \"org.example.myapp\" , ** kwargs ) self . window = None def do_activate ( self ): if not self . window : self . window = AppWindow ( application = self , title = \"Menu Bars\" ) self . window . show_all () self . window . present () if __name__ == \"__main__\" : app = Application () app . run ()","title":"Starships"},{"location":"GTK/Tasks/Starships/#starships","text":"This task implements the list/details pattern .","title":"Starships"},{"location":"GTK/Tasks/Starships/#hardcoded-data","text":"Rust Python import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) ships = [ 'USS Enterprise' , 'USS Defiant' , 'USS Voyager' ] self . treeview = Gtk . TreeView ( model = Gtk . ListStore . new (( str ,))) column = Gtk . TreeViewColumn ( \"Ship\" , Gtk . CellRendererText (), text = 0 ) self . treeview . append_column ( column ) for s in ships : self . treeview . get_model () . append (( s ,)) self . add ( self . treeview ) self . set_size_request ( - 1 , - 1 ) class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.myapp' ) def do_activate ( self ): self . window = ApplicationWindow ( application = self ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = Application () app . run ()","title":"Hardcoded data"},{"location":"GTK/Tasks/Starships/#csv-import","text":"Rust Python import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk import csv class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) with open ( '/home/jasper/dogfood/csv/starships.csv' , mode = 'r' ) as f : reader = csv . reader ( f ) self . headers = [ h . title () for h in next ( reader )] self . data = [ r for r in reader ] self . treeview = Gtk . TreeView ( model = Gtk . ListStore . new (( str , str , str , str ))) for h in self . headers : column = Gtk . TreeViewColumn ( h , Gtk . CellRendererText (), text = self . headers . index ( h )) self . treeview . append_column ( column ) for r in self . data : self . treeview . get_model () . append ( r ) self . treeview . connect ( 'row-activated' , self . on_row_activated ) self . add ( self . treeview ) self . set_size_request ( - 1 , - 1 ) class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.myapp' ) def do_activate ( self ): self . window = ApplicationWindow ( application = self ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = Application () app . run ()","title":"CSV import"},{"location":"GTK/Tasks/Starships/#sortable-columns","text":"Rust Python import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk import csv class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) with open ( '/home/jasper/dogfood/csv/starships.csv' , mode = 'r' ) as f : reader = csv . reader ( f ) self . headers = [ h . title () for h in next ( reader )] self . data = [ r for r in reader ] self . treeview = Gtk . TreeView ( model = Gtk . ListStore . new (( str , str , str , str ))) for h in self . headers : column = Gtk . TreeViewColumn ( h , Gtk . CellRendererText (), text = self . headers . index ( h )) column . set_sort_column_id ( self . headers . index ( h )) self . treeview . append_column ( column ) for r in self . data : self . treeview . get_model () . append ( r ) self . treeview . connect ( 'row-activated' , self . on_row_activated ) self . add ( self . treeview ) self . set_size_request ( - 1 , - 1 ) class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.myapp' ) def do_activate ( self ): self . window = ApplicationWindow ( application = self ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = Application () app . run ()","title":"Sortable columns"},{"location":"GTK/Tasks/Starships/#event-handler","text":"Rust Python import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk import csv class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) with open ( '/home/jasper/dogfood/csv/starships.csv' , mode = 'r' ) as f : reader = csv . reader ( f ) self . headers = [ h . title () for h in next ( reader )] self . data = [ r for r in reader ] self . treeview = Gtk . TreeView ( model = Gtk . ListStore . new (( str , str , str , str ))) for h in self . headers : column = Gtk . TreeViewColumn ( h , Gtk . CellRendererText (), text = self . headers . index ( h )) column . set_sort_column_id ( self . headers . index ( h )) self . treeview . append_column ( column ) for r in self . data : self . treeview . get_model () . append ( r ) self . treeview . connect ( 'row-activated' , self . on_row_activated ) self . add ( self . treeview ) self . set_size_request ( - 1 , - 1 ) def on_row_activated ( self , treeview , path , col ): model = treeview . get_model () print ( f 'Using path object as index to model: { model [ path ][:] } ' ) class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.myapp' ) def do_activate ( self ): self . window = ApplicationWindow ( application = self ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = Application () app . run ()","title":"Event handler"},{"location":"GTK/Tasks/Starships/#messagedialog","text":"Rust Python import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk import csv class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) with open ( '/home/jasper/dogfood/csv/starships.csv' , mode = 'r' ) as f : reader = csv . reader ( f ) self . headers = [ h . title () for h in next ( reader )] self . data = [ r for r in reader ] self . treeview = Gtk . TreeView ( model = Gtk . ListStore . new (( str , str , str , str ))) for h in self . headers : column = Gtk . TreeViewColumn ( h , Gtk . CellRendererText (), text = self . headers . index ( h )) column . set_sort_column_id ( self . headers . index ( h )) self . treeview . append_column ( column ) for r in self . data : self . treeview . get_model () . append ( r ) self . treeview . connect ( 'row-activated' , self . on_row_activated ) self . add ( self . treeview ) self . set_size_request ( - 1 , - 1 ) def on_row_activated ( self , treeview , path , col ): model = treeview . get_model () dialog = Gtk . MessageDialog ( message_type = Gtk . MessageType . INFO , text = model [ path ][:], parent = self ) dialog . add_button ( \"OK\" , Gtk . ResponseType . OK ) dialog . run () dialog . destroy () class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.myapp' ) def do_activate ( self ): self . window = ApplicationWindow ( application = self ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = Application () app . run ()","title":"MessageDialog"},{"location":"GTK/Tasks/Starships/#dice-roller","text":"Rust Python import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk import random from math import floor class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) scale_adj = Gtk . Adjustment . new ( 1 , 0 , 6 , 1 , 2 , 0 ) self . scale = Gtk . Scale . new ( Gtk . Orientation . HORIZONTAL , scale_adj ) self . scale . set_digits ( 0 ) button = Gtk . Button . new_with_label ( \"Throw\" ) button . connect ( \"clicked\" , self . on_button_clicked ) self . label = Gtk . Label . new () box = Gtk . Box . new ( Gtk . Orientation . VERTICAL , 5 ) box . pack_start ( self . scale , False , True , 0 ) box . pack_start ( button , False , True , 0 ) box . pack_start ( self . label , False , True , 0 ) self . add ( box ) self . set_size_request ( 200 , 200 ) def on_button_clicked ( self , button ): dice = floor ( self . scale . get_value ()) results = [ random . randrange ( 6 ) for i in range ( dice )] self . label . set_text ( str ( results )) class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.myapp' ) def do_activate ( self ): self . window = ApplicationWindow ( application = self ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = Application () app . run ()","title":"Dice roller"},{"location":"GTK/Tasks/Starships/#menubar","text":"Rust Python import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk class AppWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_size_request ( 250 , - 1 ) menubar = Gtk . MenuBar . new () self . add ( menubar ) file = Gtk . MenuItem . new_with_label ( \"File\" ) menubar . append ( file ) filemenu = Gtk . Menu . new () file . set_submenu ( filemenu ) new = Gtk . MenuItem . new_with_label ( \"New\" ) open = Gtk . MenuItem . new_with_label ( \"Open\" ) filemenu . append ( new ) filemenu . append ( open ) edit = Gtk . MenuItem . new_with_label ( \"Edit\" ) menubar . append ( edit ) editmenu = Gtk . Menu . new () edit . set_submenu ( editmenu ) cut = Gtk . MenuItem . new_with_label ( \"Cut\" ) copy = Gtk . MenuItem . new_with_label ( \"Copy\" ) paste = Gtk . MenuItem . new_with_label ( \"Paste\" ) editmenu . append ( cut ) editmenu . append ( copy ) editmenu . append ( paste ) help = Gtk . MenuItem . new_with_label ( \"Help\" ) menubar . append ( help ) helpmenu = Gtk . Menu . new () help . set_submenu ( helpmenu ) contents = Gtk . MenuItem . new_with_label ( \"Help\" ) about = Gtk . MenuItem . new_with_label ( \"About\" ) helpmenu . append ( contents ) helpmenu . append ( about ) class Application ( Gtk . Application ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , application_id = \"org.example.myapp\" , ** kwargs ) self . window = None def do_activate ( self ): if not self . window : self . window = AppWindow ( application = self , title = \"Menu Bars\" ) self . window . show_all () self . window . present () if __name__ == \"__main__\" : app = Application () app . run ()","title":"MenuBar"},{"location":"GTK/Tasks/Tasks/","text":"Tasks","title":"Tasks"},{"location":"GTK/Tasks/Tasks/#tasks","text":"","title":"Tasks"},{"location":"Health/","text":"\ud83d\udc8a Health Substance Effects Lowest cost Ashwagandha < 0.01 Alpha GPC 0.50 Bacopa Monnieri 0.10 Curcumin 0.13 D-aspartic acid 0.15 GABA Kanna 0.40 L-Carnitine 0.08 L-Tyrosine 0.07 Lion's Mane 0.17 Quercetin Resveratrol 0.12 Rhodiola 0.05 Rosemary 0.09 Taurine Ubiquinol 0.11 \u2714\ufe0f\ufe0f TODO Develop a dosage of adaptogens to use when sleep is disturbed. Incorporate notes and research from Reddit \ud83d\udcb8 Sale Product Discount \ud83e\udd57 Dietary deficiencies Substance Dosage (mg) Cheapest cost per dose Calcium 1,000 0.02 Magnesium 200 0.02 Potassium 2,000 0.01 \ud83e\udde0 Nootropics Supplement Dosage (g) Cost ($) Cost (sale) Alpha GPC 0.6 0.71 0.50 Bacopa 0.4 0.19 0.10 DMAE 0.8 0.23 0.23 Huperzine A 0.2 0.33 0.20 L-Theanine 0.2 0.30 0.22 L-Tyrosine 1.0 0.16 0.07 Lion's Mane 1.0 0.36 0.17 Kanna 0.5 0.80 0.40 Total 3.23 2.04 Gorilla Mind Smooth 2.93 Alpha GPC Bacopa monnieri Curcumin DMAE Huperzine A L-Theanine L-Tyrosine Lions Mane Rhodiola Rosemary Saffron Vitamin B12 \ud83d\udc74 Antioxidants In recent years, the mitochondrial free radical theory has emerged to explain the aging process. In this theory, oxidative stress from free radicals, which many studies suggest cause a variety of health issues, cause mitochondrial dynsfunction and eventually aging. Supplementation with antioxidants has been demonstrated to reduce these free radicals and improve health markers. Oxidative stress is a medical concept defined by the state of excess reactive oxygen species (ROS) and reactive nitrogen species (RNS) . These are endogenously produced but harmful to mitochondria because they react with lipids, proteins, and nucleic acids, progressively degrading cellular functions. or a reduction in antioxidants which detoxify them, resulting in generalized cellular damage. Oxidative stress figures prominently in the free radical theory of aging and has been linked to neurodegenerative diseases like Alzheimer's... Long-term effects on DNA from oxidative stress are similar to those caused by radiation exposure. Reactive oxygen species (ROS) include free radicals and peroxides that damage all components of the cell and can disrupt normal cellular signalling. These are produced by disturbances in the normal reduction-oxidation (redox) state of cells Alpha lipoic acid CoQ10 Curcumin Resveratrol Vitamin C Vitamin D3 Vitamin E \ud83d\udc68\u200d\ud83e\uddb2 Hair loss prevention Dutasteride Finasteride Minoxidil \ud83d\ude34 Sleep deprivation In a double-blind placebo-controlled trial, 60 mg/kg bw of caffeine was enough to overcome the effects of 24 hours of sleep deprivation in a battery of anaerobic performance tests. ( 2018 ) Subjects given 5g of creatine four times a day for seven days experienced less of a decline in performance after 24 hours of sleep deprivation in a variety of cognitive performance measures. ( 2006 ) After six weeks of administration with l-theanine (400 mg/day), boys diagnosed with ADHD experienced significantly improved sleep. ( 2011 ) Supplementation with magnesium (100 mg/day) was enough to eliminate the difference between the sleep-deprived arm and the control arm in subjects who were sleep-deprived for a month. ( 1998 ) \ud83d\udc89 Bloodwork Recommended assay Hormone LC-MS-MS Total testosterone Equilibrium dialysis Free testosterone LC-MS-MS , estradiol sensitive Estradiol HPLC DHT Equilibrium dialysis Free DHT For measuring total testosterone, LC-MS-MS is superior to ECLIA . For measuring free testosterone, equilibrium dialysis or equilibrium filtration is more accurate than EIA For measuring estradiol, LC-MS-MS is more sensitive than ECLIA For measuring DHT , high-pressure LC-MS-MS Glossary \ud83d\udcd5 Acetylcholine Acetylcholine (ACh) is an important neurotransmitter. It is broken down by the cholinergic enzyme Acetylcholinesterase (AChE). Alpha-GPC L-alpha-glycerolphosphorylcholine (Alpha GPC) is the highest quality and most bioavailable form of choline . Unlike Alpha GPC, other forms of choline were found not to be able to cross the blood-brain barrier , preventing enhancement of cognitive performance. It is not yet officially defined as one of the B vitamins, even though it is associated with them. An injectable prescription-only form of alpha-GPC exists called Delecit that is not available in the US. Product $ mg $/600 mg Swanson Alpha-GPC 300 mg x 60 capsules (Swanson sale) $14.99 18,000 0.50 Swanson Alpha-GPC 300 mg x 60 capsules (Swanson) $19.99 18,000 0.67 NOW Supplements Alpha GPC 300mg x 60 capsules $21.44 18,000 0.71 Bulk Supplements Alpha-GPC 129.96 500,000 0.15 Bulk Supplements Alpha-GPC 84.96 250,000 0.20 Bulk Supplements Alpha-GPC 64.96 100,000 0.39 Alpha lipoic acid Alpha-lipoic acid (ALA or LA, also thioctic acid ) is a powerful antioxidant that can replenish both vitamins C and E. Vitamin E requires vitamin C or coenzyme Q to regenerate after being used up in the free radical reaction . A 2011 study found that obese subjects given doses of 1,000-1,800 mg for up to 20 weeks experienced a loss of around 3% of body weight. A 2015 literature review found that ALA is only marginally effective in combating obesity in conjunction with good diet and exercise. ALA exists in two enantiomeric forms: R and S. A 2002 study found that R-ALA has greater biopotency in several metabolic pathways. Product $ mg $/300 mg BulkSupplements R-Alpha lipoic acid 100 g 111.96 100,000 0.33 BulkSupplements R-Alpha lipoic acid 500 g 481.96 500,000 0.29 Swanson R-Fraction ALA 300 mg x 30 capsules (Swanson sale) (sale) 20.49 12.29 9,000 0.68 0.41 Swanson R-Fraction ALA 300 mg x 30 capsules (Amazon) 20.98 9,000 0.70 Doctor's Best R-Lipoic Acid 100 mg x 60 capsules 15.70 6,000 0.79 Nutricost ALA 300 mg x 240 capsules 19.89 72,000 0.08 Anxiolytic Apoptosis Programmed or controlled cell death; cf. necrosis Ashwagandha Withania somnifera or Ashwagandha is a traditional Ayurvedic herb used as a memory aid. It has been shown to increase serum testosterone in infertile men. ( 2009 ) In a double-blind, placebo-controlled crossover study , healthy men administered Ashwagandha (500 mg/day) experienced improved cognitive performance outcomes. ( 2014 ) A study that compared the difference in effect of 250 and 600 mg/day found that the higher dosage was associated with significantly greater stress and anxiety reduction and increased sleep quality. ( 2019 ) Product $ g $/g Bulk Supplements Ashwagandha 1 kg 34.96 1,000.0 0.04 Swanson Ashwagandha Extract 450 mg x 60 capsules 3.21 4.59 27.0 0.12 0.17 Swanson Ultimate Ashwagandha 250mg x 60 capsules 3.99 7.99 15.0 0.26 0.53 NOW Supplements Ashwagandha 450 mg x 180 capsules 16.46 28.99 81.0 0.20 0.35 NOW supplements Ashwagandha 450 mg x 90 capsules 17.90 40.5 0.44 Nutricost Ashwagandha Extract KSM-66 600 mg x 180 capsules 40.95 108 0.38 Astragalus Product $ g $/g Swanson Full Spectrum Astragalus 470 mg x 100 capsules 2.59 3.99 47.0 0.03 0.04 Gorilla Mind Astragalus extract 750mg x 90 capsules 20.00 67.5 0.14 Swanson Astragalus 20.39 120.0 0.17 Ayurveda A traditional system of herbal medicine originating in India (ref Bacopa ) Bacopa Bacopa monnieri is an Ayurvedic herbal medicine. Studies have substantiated its use in enhancing memory by reducing inflammation in the brain. The main nootropic constituents of Bacopa are believed to be dammarane types of triterpenoid saponins known as bacosides . Most clinical studies focus on memory, to the omission of other facets of cognition like fluid intelligence, typically lasting 12 weeks. The long-term effect of Bacopa is unknown, but animal models suggest protection from age-related neurodegeneration. ( 2013 :material-file-pdf:) There are several putative mechanisms of action: Anti-oxidant/neuroprotection Acetylcholinesterase inhbition Choline acetyltransferase activation Beta-amyloid reduction Increased cerebral blood flow Monoamine potentiation and modulation A mix of Bacopa monnieri , Lycopene , Astaxanthin , and vitamin B12 ) administered orally once a day to subjects aged 60 years old or more resulted in improved cognitive performance after 8 weeks. ( 2020 ) A neuropharmacological review found that the maximum efficacious dosage was 200 mg/kg. ( 2013 ) Product $ M (g) $/g Bulk Supplements Bacopa 1 kg 212.96 1,000.0 0.21 Swanson Bacopa Monnieri 250 mg x 90 capsules sale 5.57 9.29 22.5 0.23 0.41 Bulk Supplements Bacopa 250 g 73.96 250.0 0.29 NOW Supplements Bacoba Extract 450 mg x 90 capsules 18.78 40.5 0.46 Berberine Berberine complexed with hydroxypropyl-\u03b2-cyclodextrin was recommended by MPMD to deal with carb-dense meals. This is apparently a reference to the use of berberine as a GDA , a term that has recently gained currency in the fitness industry as a way to improve how glucose is processed in the body, or \"nutrient partitioning\". The science on this topic appears to be inconclusive at best . Bioperine Product $ mg $/10 mg Swanson Bioperine 10 mg x 60 capsules (35% off sale) 2.59 600 0.04 Swanson Bioperine 10 mg x 60 capsules 3.99 600 0.07 Swanson Bioperine 10 mg x 60 capsules 9.86 600 0.16 Calcium Your dietary intake of calcium is probably 300 mg with the largest provider being 4 eggs providing 100 mg (target is 1,000 mg) Most food sources of calcium are dairy. A cup of edamame may contain about 100 mg An ounce of almonds may contain 367 mg Product $ g $/1,000 mg BulkSupplements Calcium citrate powder (1 kg) 17.96 1,000.0 0.02 Swanson Calcium Citrate & Vitamin D 315 mg x 250 tablets 6.59 10.99 78.8 0.08 0.14 Swanson Liquid Calcium & Magnesium 300 mg x 100 softgels 3.89 6.49 30.0 0.13 0.21 Swanson Calcium Citrate Plus Magnesium 150 mg x 150 capsules 3.81 4.49 22.5 0.17 0.20 Amazon Elements Calcium plus Vitamin D 500 mg x 65 tablets 5.99 10.99 32.5 0.19 0.34 Blue Diamond Almonds (BOGO sale) 9.00 2.5 3.52 Swanson Calcium Carbonate, Aspartate & Citrate 500 mg x 100 tablets 2.74 50.0 0.05 Choline Choline is a precursor to the neurotransmitter acetylcholine . It is endogenously produced and found in food. Many supplements contain choline because it is cheaply produced, but studies indicate it may not be able to cross the blood-brain barrier. Another product of choline is betaine . For individuals who have genetic polymorphisms like MTHFR and are thus far less able to produce B12 and folate, the pathway producing betaine compensates for this shortcoming, which results in deprivation of acetylcholine for cognition. Cholinergic relating to or denoting nerve cells in which acetylcholine acts as a neurotransmitter Crossover trial A trial where subjects are randomly assigned to study arms where each arm consists of two consecutive bouts of treatment separated by a washout period. A concern with crossover trials is that there is a chance that subsequent rounds of treatment may be influenced by earlier ones. Citrullus lanatus A late 2020 study found that mice given Cucumis melo and watermelon ( Citrullus lanatus ) seed extract experienced enhancement of memory and cognition. Citrus Bergamot Improves lipid profile . CoQ10 Ubiquinone, also called Coenzyme Q 10 (CoQ10) , is a coenzyme family ubiquitous to animals and bacteria. Ubiquinone is the most commonly found form in humans, an endogeneously produced lipid-soluble antioxidant that metabolizes into ubiquinol . It is one of the most popular supplements and renowned for being a powerful antioxidant. Doses are usually 100-200 mg/day, although there is no established upper limit for tolerance. A meta-analysis of 13 randomized controlled trials found that CoQ10 supplementation increased superoxide dismutase (SOD) and catalase (CAT) and decreased malondialdehyce (MDA) and diene. ( 2019 ) A meta-analysis of 19 randomized controlled trials found that CoQ10 supplementation significantly increased total antioxidant capacity (TAC), SOD, CAT, and glutathione peroxidase (GPx), as well as significantly decreased MDA. ( 2020 ) A 2021 study found that horses who were fed CoQ10 experienced persistent improvement in semen quality. Patients receiving from 300-1200 mg/day exhibited no difference in incidence of drug-related toxicities between placebo and treatment arms. ( 2002 ) Creatine monohydrate A 2018 review of six studies found that creatine can improve short-term memory and intelligence/reasoning in healthy individuals. Product $ g $/5g BulkSupplements Creatine Monohydrate powder (1 kg) 19.96 1,000 0.10 Optimum Nutrition Creatine powder 4.41 lbs 42.49 2,000 0.11 Optimum Nutrition Creatine powder 1.32 lbs 15.74 600 0.13 Optimum Nutrition Creatine powder 10.5 oz 10.53 300 0.18 Swanson Creatine 1g x 180 capsules 2-pack 17.49 360 0.25 Optimum Nutrition Creatine capsules 150 x 2.5 g servings 26.13 375 0.35 Optimum Nutrition Creatine powder 2.64 lbs 35.94 1,200 0.15 Cucumis melo Muskmelon is a species of melon that includes honeydew and cantaloupe. A late 2020 study found that mice given Cucumis melo and Citrullus lanatus seed extract experienced enhancement of memory and cognition. Curcumin Curcumin ( diferuloylmethane ) is the principal biologically active polyphenolic constituent of turmeric ( Curcuma longa ). It is a powerful antioxidant binding to COX-2 and 5-LOX and has been found to increased brain-derived neurotrophic factor (BDNF). Curcumin supplementation increases serum brain-derived neurotrophic fator (BDNF) ( 2020 ). A literature review found that ( 2020 :material-file-pdf:): Curcumin can act as a neuroprotectant antioxidant by binding to COX-2 and 5-LOX. Curcumin inhibits the activation of microglial cells and protects dopaminergic neurons against microglia-mediated neurotoxicity. Curcumin inhibits the caspase-3 apoptosis mediator, suggesting that it is an anti-apoptotic agent that would be useful in treating neurodegenerative diseases. A meta-analysis found that curcumin can improve therapy for people with symptoms of depression and anxiety ( 2019 ). Curcumin has very poor bioavailability without co-administration of piperine. ( 1998 ) A systematic review and meta-analysis found that curcumin administration was associated with a significant reduction only in systeolic blood pressure, but not in diastolic, in studies with supplementation exceeding 12 weeks. ( 2019 ) Product $ g $/g BulkSupplements Curcumin 1 kg 249.96 1,000.0 0.25 Swanson Curcumin Complex 350 mg x 120 capsules (sale) 14.24 18.99 42.0 0.34 0.45 Amazon Elements turmeric Complex 316 mg x 65 capsules 14.99 20.5 0.73 D-Aspartic acid D-Aspartic acid or D-Aspartate is an amino acid found in various tissues, including in the axon terminals and synaptic vesicles of neuronal tissue and endocrine glands. It is known to induce testosterone synthesis in the testis. A literature review concluded that exogenous D-Asp increases testosterone in animals. However, human studies which demonstrated conflicting results were noted for their short-term generation, small sample size, and other problems. ( 2017 :material-file-pdf:) Most studies used dosages of 2,600-3,000 mg/day ( Healthline ) Purported benefits of DAA are probably bunk . Product $ g $/g Nutricost D-Aspartic Acid 750 mg x 180 capsules 14.99 135 0.11 Dihydrotestosterone ( DHT ) produced after testosterone is aromatized by 5-alpha-reductase. DMAE Dimethylaminoethanol (marketed as DMAE but appearing more commonly as deanol in scientific literature) is not a precursor to acetylcholine as is commonly asserted. However it is hypothesized that it may increase acetylcholine levels by inhibiting choline metabolism. In a double-blind, placebo-controlled trial, children given 40 mg of Ritalin or 500 mg of deanol experienced similar improvements in psychometric tests . Deanol appeared to be more effective for those with learning disabilities. Product $ mg $/750 mg BulkSupplements DMAE-bitartrate powder (500 g) 20.96 500,000 0.03 NOW Supplements DMAE 250 mg x 100 capsules 7.51 25,000 0.225 Dutasteride Enantiomer molecules that are mirror images of one another Endothelium cells that line the interior surface of blood vessels and lymphatic vessels epithelium one of the four basic types of animal tissue ergogenic intended to enhance physical performance, stamina, or recovery excipient an inactive substance that serves as the vehicle or medium for a drug or other active substance. Fenugreek Several studies suggest that male subjects who took 500 mg fenugreek daily experienced increased testosterone and improved mood, energy, and libido. Product $ g $/g Swanson Fenugreek seed 610 mg x 90 capsules 2.09 3.49 54.9 0.04 0.06 Swanson Fenugreek extract 500 mg x 90 capsules 6.95 11.59 45.0 0.15 0.25 Swanson Fenugree extract featuring Testofen 18.39 36.0 0.51 Finasteride Propecia is the commercial brand name for finasteride. Fish oil Fish oil is a prominent source of the Omega-3 oils EPA and DHA . Product $ g (\u03c9-3) $/g Swanson Super EPA & DHA BOGO 20.39 92.8 0.21 Follicle-Stimulating Hormone (FSH) free radical reaction GABA Gamma aminobutyric acid (GABA) is considered an inhibitory neurotransmitter because it blocks certain brain signals and decreases activity in the nervous system. GABAergic receptors are targeted by clinically important drugs that treat anxiety, epilepsy, insomnia, and other pathophysiological disorders. ( 2015 ) Glutathione Glutathione is involved in many body processes and is a common therapy for patients with various ailments, including cancer, HIV, etc. It is endogenously produced in the liver and can also be found in various foods. Gonadotropin-Releasing Hormone (GnRH) hepatic Related to the liver hirsutism a condition causing male-pattern hair growth in women. Huperzine A Huperzine A is a lycopodium alkaloid derived from Huperzia serrata (toothed clubmoss or firmoss), which has been used therapeutically for neurological disorders. It suppresses the acetylcholinesterase enzyme which breaks down acetylcholine Huperzine A promoted neurogenesis in the hippocampus of mice in vitro and in vivo ( 2013 ) Huperzine A directly increased neurotrophic activity of rat astrocytes in vitro ( 2005 ) Product $ mcg $/200 mcg BulkSupplements Huperzine A 10 g 29.96 100,000 0.06 BulkSupplements Huperzine A 100 g 325.96 1,000,000 0.07 Swanson Huperzine A 200 mcg x 30 capsules (Swanson) 9.89 5.93 6,000 0.33 0.20 Hypothalamic-pituitary-gonadal (HPG) axis An endocrine control mechanism involved in the regulation of testosterone in males and estrogen in females. In this concept, the hypothalamus produces GnRH which binds to secretory cells of the anterior pituitary. Binding of GnRH stimulates these cells to produce LH and FSH, which then produce different effects in the sexes: production of estrogen and inhibin in the ovaries of the female and testosterone and sperm in the testes of the male. International Unit (IU) A unit of measurement for the amount of a substance that varies based on the substance being measured. Isomer Each of several compounds with the same formula but a different arrangement of atoms and different properties Kanna Kanna (Sceletium tortuosum) increases seratonin in the brain and improves mood. Product $ mg $/200 mg Swanson Sceletium Tortuosum 50 mg x 60 capsules (Swanson BOGO sale) 11.99 6,000 0.40 L-Arginine L-Arginine is an amino acid widely used in pre-workout supplements because it enhances blood flow. It works by providing nitrogren to the nitric oxide synthase (NOS) enzyme to produce nitric oxide (NO), being metabolized to L-citrulline in the process. L-citrulline, in turn, can be reconverted to L-arginine in the kidneys. Both L-citrulline and L-arginine regulate nitrogen and ammonia in the blood. Bioavailability of l-arginine can vary up to 70%, but excessive dosages can cause gastrointestinal issues. L-citrulline, in contrast, has a bioavailability of practically 100%, which is why it is favored now. L-Arginine may help reduce blood pressure by producing nitric oxide, which relaxes blood vessels. Product $ g $/g Swanson L-Arginine 850mg x 90 caps BOGO 6.59 153 0.04 Nature's Bounty L-Arginine 1000mg x 50 tablets 8.80 50 0.18 Bulk Supplements L-Arginine 1 kg 34.96 1,000 0.04 L-Carnitine Several forms of L-carnitine are available: L-carnitine L-tartrate (LCLT) is a salt of L-carnitine that increases androgen receptor uptake. Resistance-trained men given LCLT experienced an increase in AR content ( 2006 ) It is absorbed faster than other L-carnitine esters ( 2005 ) Acetyl L-carnitine (ALCAR) is an ester of L-carnitine that can pass through the blood-brain barrier, and its acetyl group ensures that it is active in the central nervous system. It is a \"substrate\" for acetylcholine . L-carnitine fumarate is marketed as a weight loss booster under the name Carnishield Product $ mg $/4000 mg Swanson L-Carnitine 500 mg x 100 tabs 7.48 50,000 0.60 Swanson Acetyl L-carnitine 500 mg x 240 caps (Swanson sale) 16.24 120,000 0.54 Swanson Acetyl L-carnitine 500 mg x 100 caps (Swanson sale) 7.69 50,000 0.62 Jarrow LCLT 500 mg x 100 capsules 19.39 50,000 1.55 Swanson L-Carnitine Fumarate 450 mg x 60 caps 10.99 27,000 1.63 NOW Foods Acetyl-L-Carnitine 500 mg x 50 caps 10.38 25,000 1.66 Oral supplementation of carnitine is problematic because of the production of TMAO which has been associated with cardiovascular disease. Some studies indicate that dietary allicin from garlic can reduce the production of TMAO . ( 2015 ) Product $ mg $/24 mg Swanson Allicin 12 mg x 100 tabs 8.60 1,200 0.17 Injectable L-carnitine is a suitable lipolytic , especially when used in conjunction with methionine, inositol, and choline (MIC) and B vitamins. L-Leucine Leucine is a dietary amino acid marketed as directly stimulating muscle growth. Untrained peri- and postmenopausal women performing resistance training experienced no ergogenic effects supplementing with leucine. ( 2020 ) Young adult men experienced no ergogenic effects supplementing with leucine. ( 2019 ) Elderly men experienced no ergogenic effects from ong-term leucine supplementation (7.5g/d) ( 2009 :material-file-pdf:) L-Theanine An amino acid found in green tea, alone it can increase the feeling of well-being , but in combination with caffeine generally increases cognitive performance. Product $ mg $/200 mg BulkSupplements L-Theanine powder (100 g) 17.96 100,000 0.04 Amazon Elements L-Theanine 200 mg x 60 capsules (sale) 10.22 12,000 0.17 Amazon Elements L-Theanine 200 mg x 60 capsules 13.49 12,000 0.22 Swanson Suntheanine L-Theanine 100 mg x 60 capsules (Swanson sale) 6.74 6,000 0.22 Swanson Suntheanine L-Theanine 100 mg x 60 capsules (Swanson) 8.99 6,000 0.30 L-Tyrosine L-Tyrosine is the most bioavailable form of Tyrosine, especially in comparison with N-acetyl-L-Tyrosine, and a precursor to the neurotransmitter dopamine , thyroxine, adrenaline and noradrenaline, and the catecholamines. It is converted by the enzyme tyrosine hydroxylase. L-Tyrosine is commonly believed to reduce and relieve stress and enhance cognitive performance. Tyrosine abolished fear expression, apparently because of its effect on the catecholaminergic system. ( 2019 ) A literature review of 15 studies found that tyrosine acutely counteracts decrements in working memory and information processing caused by cognitive load or bad weather. ( 2015 ) Product g $ $/g Swanson L-Tyrosine 500 mg x 100 capsules 50.0 3.59 5.99 0.07 0.12 Jarrow L-Tyrosine 500 mg x 100 capsules 50.0 5.38 0.10 NOW Supplements L-Tyrosine 750 mg x 90 capsules 67.5 10.73 0.16 BulkSupplements L-Tyrosine 1 kg 1,000.0 28.96 0.29 Lecithin Lecithin is a generic term designating any yellow-brownish fatty substances occurring in animl and plant tissues that are amphiphilic. Soy lecithin contains various compounds that reduce stress, including phosphatidylserine . Lecithin is widely assumed to be a provider of choline , although as such it is almost certainly inferior to Alpha GPC . Lions Mane A 2021 study found that compounds extracted from Hericium erinaceus demonstrated neurotrophic effects in isolation. In particular isohericerinol A stimulated neurogenesis by increasing Nerve Growth Factor (NGF) production. Product $ g $/g BulkSupplements Lion's Mane 500 g 35.96 500 0.07 Swanson Lion's Mane 500 mg x 60 capsules (Swanson) 10.49 30 0.35 Lipid profile Ratio between HDL (good cholesterol) and LDL (bad cholesterol) Lipolysis The breakdown of fats and other lipids by hydrolysis to release fatty acids. The process of mobilizing stored energy during fasting or exercise. The metabolic pathway through which lipid triglycerides are hydrolyzed into a glycerol and three fatty acids. Lutein Lutein is a carotenoid closely related to beta-carotene, which gives carrots and pumpkins their orange color. Luteinizing Hormone (LH) n/a Maca Maca root ( Lepidium meyenii ) is a Peruvian vegetable with a long history in South American cuisine, and as an herb it is traditionally used to enhance fertility and sex drive. Some studies suggest an ergogenic benefit to endurance activities. Maca, in particular black maca, improved learning and memory in rodents with memory impairment. A review found that it can improve sperm volume, and motility and the volume of semen in infertile and healthy men. Another review found that maca can help alleviate hot flashes and anxiety in menopausal women. ( src ) Dosage varies 1.5-5.0 g/day Product $ m $/500 mg BulkSupplements Maca 1 kg 29.96 1,000 0.02 Swanson Maca 500 mg x 100 capsules 2.50 50 0.03 Magnesium Recommended daily intake of magnesium is 420 mg, and you are short by approximately 150 mg. Product $ mg $/200 mg BulkSupplements Magnesium Citrate powder (250 g) 11.96 250,000 0.010 Swanson Triple Magnesium Complex 400 mg x 300 caps (Amazon) 9.99 120,000 0.017 Swanson Triple Magnesium Complex 400 mg x 300 caps 11.99 120,000 0.020 Melatonin Melatonin may reduce blood pressure . Mini Mental State Examination ( MMSE ) Scoring 20 or above corresponds to normal cognitive functions Minoxidil Minoxidil is a potassium channel opener originally designed to lower blood pressure. Rogaine is one of many brand names offering 5% topical minoxidil. Modafinil Modafinil is a heavily abused sleep suppressant typically prescribed to narcolepsy patients but also said to be used by fighter pilots. Myopathy Any disease that affects muscle tissue Necrosis Uncontrolled cell death as a result of shock; cf. apoptosis Nerve Growth Factor (NGF) Neurodegenerative disease Diseases that result in progressive degeneration or death of neuronal cells Niacin Vitamin B3 or niacin comes in two forms: nicotinic acid and niacinamide (also called nicotinamide). Niacin is considered an antilipemic agent because can help regulate cholesterol by lowering LDL and raising HDL. In this context, daily dosages may range from as little as 250 mg up to a maximum of 6,000 mg. It has also been shown to increase NAD+ levels in people with systemic NAD+ deficiency. It is associated with side effects in some people, including hives and skin rashes. Nicotinamide adenine dinucleotide (NAD+) NAD+ is involved in many biological processes, and its level falls as a person ages . Niacin and nicotinamide riboside have both been shown to increase the serum concentrations of NAD+ in humans. Nicotinamide riboside Nicotinamide riboside, also called niagen, is a B3 vitamin and a highly bioavailable precursor to NAD+ . Nicotinamide riboside supplementation raises NAD+ levels, which also helps reduce systolic blood pressure . Phenol An aromatic organic compound with the molecular formula C 6 H 5 OH. ( wiki ) Phosphatidylcholine Phosphatidylcholine is a phospholipid that incorporates choline as a headgroup. Humans with cognitive disorders who were fed two forms of phosphatidylcholine (~100 mg) after breakfast experienced an improvement in their [ MMSE ][ MMSE ] scores so significant that their new scores rose above the diagnostic threshold for cognitive disorder. ( 2011 ) Administration of phosphatidylcholine to mice with impaired memory due to inbreeding raised their brain acetylcholine concentration to the level of unimpaired mice and resolved their poor memory function. However, phosphatidylcholine treatment did not affect memory or acetylcholine concentrations in normal mice. ( 1995 ) During a randomized, placebo-controlled clinical trial, prenatal supplementation of phosphatidylcholine resulted in improvement of cognitive performance of fetus (P50 inhibition). Delay in development as measured by this test is associated with schizophrenia and developmental problems. ( 2013 ) This finding confirmed earlier research on the long-term benefit of prenatal choline supplementation in animals. Phosphatidylserine Phosphatidylserine (PtdSer or PS) is a phospholipid that supports a gamut of cognitive functions. It is found in the inner leaflet of neural cell membranes where it regulates the release of neurotransmitters acetylcholine, dopamine, and noradrenaline. Two sources of phosphatidylserine were available: bovine-cortex (BC-PtdSer) which has fallen out of favor due to the risk of transferring infections from prion-infected brains, and soy (S-PtdSer). In a randomized, double-blind trial, children diagnosed with ADHD but never having received drug treatment related to it experienced significantly improved ADHD symptoms and short-term memory after 2 months of phosphatidylserine supplementation (200mg/day). Exogenous dosages of 300-800 mg/d are absorbed efficiently, cross the blood-brain barrier, and reverses biochemical deterioration in nerve cells as a result of aging. ( 2015 ) Product $ mg $/100 mg Swanson Phosphatidylserine 100 mg x 90 softgels (sale) 20.89 13.57 9,000 0.23 0.15 Pleiotropic Having more than one effect, especially having multiple phenotypic expressions Polyphenol A large family of naturally occurring organic compounds composed of multiple phenol units. There may be an association with polyphenol intake and reduced blood pressure . Supplements with polyphenolic compounds include resveratrol , curcumin , quercetin , and beetroot. Potassium Recommended adequate intake for potassium was set by the Food and Nutrition Board of the Institute of Medicine at 4700 mg/day . Your daily dietary intake is only 1600 mg/day. A banana may have 450 mg 100g of broccoli may have 250 mg A cup of edamame (155 g) might have >600 mg 100g of apricot (~15 pieces) provide almost 1,162 mg (or ~80 mg a piece) A systematic review and meta-analysis found that intervening to a point where potassium was 30 mmol/day produced the greatest decrease in systolic and diastolic blood pressure. ( 2020 ) Both potassium bicarbonate and potassium citrate are alkaline sources of elemental potassium, found in dietary sources. Product $ mg $/2,000 mg BulkSupplements Potassium Citrate powder (1 kg) 18.96 1,000,000 0.04 Pygeum Product $ g $/500mg BulkSupplements pygeum 1 kg 36.96 1,000 0.02 Swanson Pygeum 125 mg x 100 capsules (sale) 5.79 2.90 12.5 0.23 0.12 Quercetin Quercetin is a polyphenolic antioxidant ubiquitous in plant food sources. A systematic review and meta-analysis found that taking at least 500 mg/day resulted in significant reduction of blood pressure. ( Serban et al. 2016 ) Product $ g $/g Nutricost Quercetin 440 mg x 240 capsules 31.95 105.6 0.30 Swanson Quercetin 475 mg x 60 capsules (25% off sale) 9.74 28.5 0.34 Jarrow Quercetin 500 mg x 100 capsules 18.99 50.0 0.38 Swanson Quercetin 800 mg x 30 capsules (BOGO) 19.38 48.0 0.40 Swanson Quercetin 475 mg x 60 capsules 12.99 28.5 0.45 Jarrow Quercetin 500 mg x 200 capsules 66.95 100.0 0.67 Resveratrol Resveratrol is a polyphenol and estrogenic antioxidant found in grape skin, peanuts, and other foods. Red wine contains less than 13 mg of resveratrol per liter. ( src ) It is confirmed as a powerful antioxidant and may potentially enhance cognitive performance. A systematic review found that supplementation with resveratrol does not have an anti-obesity effect . In contrast, an earlier systematic review cited in this one did find that resveratrol supplementation had a positive effect on weight loss. A comprehensive review found that even moderate daily supplementation (0.5-1 g) is enough to inhibit estrogen metabolism and increase SHBG. ( 2020 ) Resveratrol has poor bio-availability and a short half-life (1.5 hours). ( 2019 :material-file-pdf:). Resveratrol upregulates SHBG expression in the liver. ( 2017 ) A literature review found that resveratrol intervention lowered total cholesterol, systolic blood pressure, and fasting glucose with more significant reductions when doses were higher than 300mg/day. ( 2016 ) A double-blind, randomized, placebo-controlled crossover study found that administration of resveratrol (150 mg/day) caused no changes in metabolic risk markers in overweight and obese subjects after 4 weeks with a 4-week washout period. ( 2015 ) Rats force-fed resveratrol at 20mg/kg daily experienced increased serum concentration of gonadotrophins and testosterone by stimulating the HPG axis . Product $ mg $/100 mg BulkSupplements 100g 85.73 100,000 0.09 BulkSupplements 500g 323.48 500,000 0.06 Swanson Resveratrol 500 mg x 30 capsules (BOGO) 32.99 30,000 0.11 Swanson Resveratrol 250 mg x 30 capsules (Swanson BOGO sale) 17.99 15,000 0.12 Swanson Resveratrol 100 mg x 30 capsules (Swanson BOGO sale) 8.99 6,000 0.15 Rhodiola Rhodiola rosea is an adaptogenic herb used in traditional medicine in Europe and Asia. A 2018 meta study :material-file-pdf: examined 36 animal studies and concluded that rhodiola can improve learning and memory function, despite an earlier 2012 review that found that methodological problems with earlier experiments brought these findings into question. A clinical trial in 2000 found that young physicians experienced improvement in cognitive performance on a battery of cognitive tests called the Fatigue Test. The study concluded that rhodiola could reduce general fatigue under stressful conditions. A similar 2003 study found that young military cadets were able to resist fatigue better with rhodiola. Product $ mg $/400 mg Swanson Rhodiola 400 mg x 100 capsules (Swanson sale) 4.79 40,000 0.05 Swanson Rhodiola 400 mg x 100 capsules (Swanson) 7.99 40,000 0.08 Swanson Rhodiola 400 mg x 100 capsules (Amazon) 10.89 40,000 0.11 Now Foods Rhodiola 500 mg x 60 capsules 18.25 30,000 0.24 Ritalin Ritalin is the brand name for the stimulant methylphenidate. Rosemary Mark Moss with Northumbria University has published several studies on the cognitive benefits of various herbs, including peppermint, chamomile, rosemary, and lavender. The studies on rosemary in particular were motivated by traditional associations of rosemary with memory. Aromatherapy with rosemary oil resulted in improvement in cognitive performance ( 2017 ). Drinking rosemary water produced a small benefit to memory ( 2018 ) Cognitive benefit of rosemary aromatherapy was associated with concentration of 1,8-cineole. ( 2012 ) An interesting study contrasted the impact of aromatherapies of lavender and rosemary oils. Rosemary enhanced performance for overall quality of memory, but also impaired speed of memory compared to control. Lavender actually impaired performance of working memory. Rosemary boosted alertness in comparison to both control and lavender. Both lavender and rosemary enhanced contentment. ( 2003 ) Product $ mg $/500 mg Swanson Rosemary Extract 500 mg x 60 capules (Swanson sale) 5.13 30,000 0.09 Swanson Rosemary Extract 500 mg x 60 capules (Swanson) 7.19 30,000 0.12 Swanson Rosemary Extract 500 mg x 60 capules (Amazon) 9.99 30,000 0.17 Saffron ( Crocus sativus L. ) A 2020 review found that there was some evidence that associated saffron with improvement in cognitive performance, especially in subjects with neurodegenerative disease. Sarcopenia Age-related loss of muscle mass and function Saw palmetto Saw palmetto ( Serenoa repens ) is a dwarf palm tree native to southeast North America that has long been used as a medicinal herb by Native Americans. Studies suggest that it blocks the conversion of testosterone to [ DHT ][ DHT ], and may be effective as a treatment for androgenic alopecia. ( src ) Dosage may be 160-320 mg Product $ g $/500mg BulkSupplements Saw Palmetto 1 kg 43.96 1,000 0.02 Swanson Saw Palmetto 540 mg x 250 capsules sale 15.99 5.69 135 0.06 0.02 Swanson Saw Palmetto 540 mg x 100 capsules sale 4.59 2.75 54 0.04 0.03 Squamous Referring to the shape of cells that are wide and flat, such as those found in the lining of the mouth, esophagus, and blood vessels (cf. cuboidal and columnar) Statin A family of drugs used for treating hyperlipidaemia with a recognized capacity to prevent cardiovascular disease event Taurine Dosages of taurine in studies are commonly 500 - 2,000 mg /day, however higher doses appear to be well-tolerated. Theobromine Theobromine is a methylxanthine and stimulant found in the cacao plant and chocolate products: it is what gives dark chocolate its bitterness. Theobromine is believed to promote wakefulness and alertness and enhance cognition, among other benefits . Product $ g $/g BulkSupplements Theobromine 46.96 250 0.19 BulkSupplements Theobromine 20.96 100 0.21 Thyroxine Thyroxine, called T4 because it contains 4 iodine atoms, is the major thyroid hormone secreted by the thryoid gland. It is converted to triiodothyronine (T3) by the removal of an iodine atom mainly in the liver but also in the brain. Thyroxine production is regulated by TSH . Tryptophan Tryptophan is a precursor of the neurotransmitters serotonin and melatonin. Ubiquinol Ubiquinol (QH) is the fully reduced form of ubiquinone . Intense exercise depletes CoQ10, but supplementation with ubiquinol prevents this deprivation. Ubuiquinol is more bioavailable than CoQ10 by an order of magnitude. Like vitamin E , ubiquinol is a lipid-soluble antioxidant, which gives it the special task of protecting sensitive cell membranes. Ubiquinol is depleted before vitamin E because it reacts with radicals first, and it can also replenish depleted vitamins E and C. ( 2013 ) Healthy and well-trained fireman given 200mg/day ubiquinol for two weeks experienced increases in the biomarkers of bone formation and energy mobilization, suggesting ergogenic effects. ( 2020 ) CoQ10 was positively associated with antioxidant capacity, muscle mass, muscle strength, and muscle endurance in patients with osteoarthritis. ( 2020 ) Subjects with mild cognitive impairment who received 200 mg/day ubiquinol for a year experienced improved cerebral vasoreactivity compared to baseline and placebo, but no significant neurological improvement. ( 2021 ) In a double-blind, placebo-controlled study, ubiquinol supplementation increased physical perforrmance. ( 2013 :material-file-pdf:) Ubiquinol supplementation to mice reduced a variety of markers associated with fatigue and increased muscle and liver glycogen content, which provide energy during exercise. ( 2019 ) A systematic review and meta-analysis found that long-term supplementation with CoQ10 (most studies were 2-4 months while one was 13 months) at dosages varying from 34-225 mg/day can significantly reduce blood pressue. ( 2007 ) Product $ mg $/100 mg Jarrow QH-absorb (Publix sale) 6.71 6,000 0.11 NOW Supplements Ubiquinol 100 mg x 120 softgels 38.69 12,000 0.32 Swanson Ubiquinol 100 mg x 120 softgels (2-pack) 79.99 24,000 0.33 Swanson Ubiquinol 100 mg x 120 softgels 42.99 12,000 0.36 Vascular resistance The resistance that must be overcome to push blood through the circulatory system Vasoreactivity A >= 30% decrease in pulmonary vascular resistance (PVR) with vasodilator compared to baseline VAT Visceral adipose tissue (VAT) is fat surrounding the intra-abdominal organs which has been associated with various medical pathologies; alongside subcutaneous adipose tissue (SAT) , one of the two types of body fat tissue Visceral obesity : Abnormally high deposition of visceral adipose tissue (VAT) Vitamin B12 Vitamin B12 (cobalamin, methylcobalamin) is a cofactor and precursor of neurotransmitters acetylcholine, dopamine, GABA, norepinephrine, and serotonin. Even mild B12 deficiency was associated with cognitive decline over 8 years. The BLAtwelve Study tested the effects of Bacopa monnieri , Lycopene , Astaxanthin , and Vitamin B12, finding that a mix of the four compounds administered orally once a day to subjects aged 60 years old or more experienced improved cognitive performance after 8 weeks. Product $ mg $/mg Amazon Elements B12 5000 mcg x 65 lozenges 12.99 325 0.04 Swanson Ultra Vitamin B12 Triple Action 1000 mcg x 90 tabs 6.59 10.99 90 0.07 0.12 Vitamin C Vitamin C ( \"ascorbic acid\") is a water-soluble antioxidant that can interact directly with free radicals. Its efficacy as an antioxidant has been closely associated with that of Vitamin E , which it may regenerate by reducing the Vitamin E radical. A 2018 study suggested that Vitamin C could inhibit visceral adipocyte hypertrophy A long-term study published in 2008 found that there was no significant effect of vitamin C on cardiovascular health. The body tightly controls serum concentration of vitamin C. Dosages of 200-300 mg/day result in concentrations of 70 micromol/L, whereas dosages of 1.25 g/day produce concentrations of only 135 micromol/L. Product $ M $/1,000 mg Swanson Vitamin C 1,000 mg x 250 capsules 8.04 11.49 250 0.032 0.045 Swanson Vitamin C 500 mg x 400 capsules 7.69 10.99 200 0.039 0.055 Amazon Elements Vitamin C 1000 mg x 300 tablets 17.99 300 0.06 Vitamin D3 Vitamin D3 (cholecalciferol) can regulate lipid peroxidation as a form of neuroprotection by inducing the synthesis of parvalbumin , a protein that binds to Ca 2+ ( 2020 ). Claims that Vitamin D3 can boost testosterone are unfounded ( 2017 , 2019 ) Product $ IU $/5,000 IU Swanson D3 5000 IU x 250 softgels (Swanson sale) 8.24 1,250,000 IU 0.03 Swanson D3 5000 IU x 250 softgels (Swanson) 10.99 1,250,000 IU 0.04 Amazon Elements D3 5000 IU x 180 softgels 9.34 900,000 IU 0.05 Vitamin E Vitamin E (alpha-tocopherol) is a lipid soluble antioxidant that interferes with the propagation of lipid radicals. Its efficacy as an antioxidant has been closely associated with that of Vitamin C . Zinc Men who received 30 mg of zinc a day showed increased levels of free testosterone . The tolerable upper intake level for adult men is 40 mg/day . Product $ M $/30 mg Swanson Zinc Gluconate 30 mg a 250 tabs 3.14 4.49 7.5 0.01 0.02","title":"\ud83d\udc8a Health"},{"location":"Health/#health","text":"Substance Effects Lowest cost Ashwagandha < 0.01 Alpha GPC 0.50 Bacopa Monnieri 0.10 Curcumin 0.13 D-aspartic acid 0.15 GABA Kanna 0.40 L-Carnitine 0.08 L-Tyrosine 0.07 Lion's Mane 0.17 Quercetin Resveratrol 0.12 Rhodiola 0.05 Rosemary 0.09 Taurine Ubiquinol 0.11","title":"\ud83d\udc8a Health"},{"location":"Health/#todo","text":"Develop a dosage of adaptogens to use when sleep is disturbed. Incorporate notes and research from Reddit","title":"\u2714\ufe0f&#xfe0f; TODO"},{"location":"Health/#sale","text":"Product Discount","title":"\ud83d\udcb8 Sale"},{"location":"Health/#dietary-deficiencies","text":"Substance Dosage (mg) Cheapest cost per dose Calcium 1,000 0.02 Magnesium 200 0.02 Potassium 2,000 0.01","title":"\ud83e\udd57 Dietary deficiencies"},{"location":"Health/#nootropics","text":"Supplement Dosage (g) Cost ($) Cost (sale) Alpha GPC 0.6 0.71 0.50 Bacopa 0.4 0.19 0.10 DMAE 0.8 0.23 0.23 Huperzine A 0.2 0.33 0.20 L-Theanine 0.2 0.30 0.22 L-Tyrosine 1.0 0.16 0.07 Lion's Mane 1.0 0.36 0.17 Kanna 0.5 0.80 0.40 Total 3.23 2.04 Gorilla Mind Smooth 2.93 Alpha GPC Bacopa monnieri Curcumin DMAE Huperzine A L-Theanine L-Tyrosine Lions Mane Rhodiola Rosemary Saffron Vitamin B12","title":"\ud83e\udde0 Nootropics"},{"location":"Health/#antioxidants","text":"In recent years, the mitochondrial free radical theory has emerged to explain the aging process. In this theory, oxidative stress from free radicals, which many studies suggest cause a variety of health issues, cause mitochondrial dynsfunction and eventually aging. Supplementation with antioxidants has been demonstrated to reduce these free radicals and improve health markers. Oxidative stress is a medical concept defined by the state of excess reactive oxygen species (ROS) and reactive nitrogen species (RNS) . These are endogenously produced but harmful to mitochondria because they react with lipids, proteins, and nucleic acids, progressively degrading cellular functions. or a reduction in antioxidants which detoxify them, resulting in generalized cellular damage. Oxidative stress figures prominently in the free radical theory of aging and has been linked to neurodegenerative diseases like Alzheimer's... Long-term effects on DNA from oxidative stress are similar to those caused by radiation exposure. Reactive oxygen species (ROS) include free radicals and peroxides that damage all components of the cell and can disrupt normal cellular signalling. These are produced by disturbances in the normal reduction-oxidation (redox) state of cells Alpha lipoic acid CoQ10 Curcumin Resveratrol Vitamin C Vitamin D3 Vitamin E","title":"\ud83d\udc74 Antioxidants"},{"location":"Health/#hair-loss-prevention","text":"Dutasteride Finasteride Minoxidil","title":"\ud83d\udc68\u200d\ud83e\uddb2 Hair loss prevention"},{"location":"Health/#sleep-deprivation","text":"In a double-blind placebo-controlled trial, 60 mg/kg bw of caffeine was enough to overcome the effects of 24 hours of sleep deprivation in a battery of anaerobic performance tests. ( 2018 ) Subjects given 5g of creatine four times a day for seven days experienced less of a decline in performance after 24 hours of sleep deprivation in a variety of cognitive performance measures. ( 2006 ) After six weeks of administration with l-theanine (400 mg/day), boys diagnosed with ADHD experienced significantly improved sleep. ( 2011 ) Supplementation with magnesium (100 mg/day) was enough to eliminate the difference between the sleep-deprived arm and the control arm in subjects who were sleep-deprived for a month. ( 1998 )","title":"\ud83d\ude34 Sleep deprivation"},{"location":"Health/#bloodwork","text":"Recommended assay Hormone LC-MS-MS Total testosterone Equilibrium dialysis Free testosterone LC-MS-MS , estradiol sensitive Estradiol HPLC DHT Equilibrium dialysis Free DHT For measuring total testosterone, LC-MS-MS is superior to ECLIA . For measuring free testosterone, equilibrium dialysis or equilibrium filtration is more accurate than EIA For measuring estradiol, LC-MS-MS is more sensitive than ECLIA For measuring DHT , high-pressure LC-MS-MS","title":"\ud83d\udc89 Bloodwork"},{"location":"Health/#glossary","text":"","title":"Glossary \ud83d\udcd5"},{"location":"Health/#acetylcholine","text":"Acetylcholine (ACh) is an important neurotransmitter. It is broken down by the cholinergic enzyme Acetylcholinesterase (AChE).","title":"Acetylcholine"},{"location":"Health/#alpha-gpc","text":"L-alpha-glycerolphosphorylcholine (Alpha GPC) is the highest quality and most bioavailable form of choline . Unlike Alpha GPC, other forms of choline were found not to be able to cross the blood-brain barrier , preventing enhancement of cognitive performance. It is not yet officially defined as one of the B vitamins, even though it is associated with them. An injectable prescription-only form of alpha-GPC exists called Delecit that is not available in the US. Product $ mg $/600 mg Swanson Alpha-GPC 300 mg x 60 capsules (Swanson sale) $14.99 18,000 0.50 Swanson Alpha-GPC 300 mg x 60 capsules (Swanson) $19.99 18,000 0.67 NOW Supplements Alpha GPC 300mg x 60 capsules $21.44 18,000 0.71 Bulk Supplements Alpha-GPC 129.96 500,000 0.15 Bulk Supplements Alpha-GPC 84.96 250,000 0.20 Bulk Supplements Alpha-GPC 64.96 100,000 0.39","title":"Alpha-GPC"},{"location":"Health/#alpha-lipoic-acid","text":"Alpha-lipoic acid (ALA or LA, also thioctic acid ) is a powerful antioxidant that can replenish both vitamins C and E. Vitamin E requires vitamin C or coenzyme Q to regenerate after being used up in the free radical reaction . A 2011 study found that obese subjects given doses of 1,000-1,800 mg for up to 20 weeks experienced a loss of around 3% of body weight. A 2015 literature review found that ALA is only marginally effective in combating obesity in conjunction with good diet and exercise. ALA exists in two enantiomeric forms: R and S. A 2002 study found that R-ALA has greater biopotency in several metabolic pathways. Product $ mg $/300 mg BulkSupplements R-Alpha lipoic acid 100 g 111.96 100,000 0.33 BulkSupplements R-Alpha lipoic acid 500 g 481.96 500,000 0.29 Swanson R-Fraction ALA 300 mg x 30 capsules (Swanson sale) (sale) 20.49 12.29 9,000 0.68 0.41 Swanson R-Fraction ALA 300 mg x 30 capsules (Amazon) 20.98 9,000 0.70 Doctor's Best R-Lipoic Acid 100 mg x 60 capsules 15.70 6,000 0.79 Nutricost ALA 300 mg x 240 capsules 19.89 72,000 0.08 Anxiolytic Apoptosis Programmed or controlled cell death; cf. necrosis","title":"Alpha lipoic acid"},{"location":"Health/#ashwagandha","text":"Withania somnifera or Ashwagandha is a traditional Ayurvedic herb used as a memory aid. It has been shown to increase serum testosterone in infertile men. ( 2009 ) In a double-blind, placebo-controlled crossover study , healthy men administered Ashwagandha (500 mg/day) experienced improved cognitive performance outcomes. ( 2014 ) A study that compared the difference in effect of 250 and 600 mg/day found that the higher dosage was associated with significantly greater stress and anxiety reduction and increased sleep quality. ( 2019 ) Product $ g $/g Bulk Supplements Ashwagandha 1 kg 34.96 1,000.0 0.04 Swanson Ashwagandha Extract 450 mg x 60 capsules 3.21 4.59 27.0 0.12 0.17 Swanson Ultimate Ashwagandha 250mg x 60 capsules 3.99 7.99 15.0 0.26 0.53 NOW Supplements Ashwagandha 450 mg x 180 capsules 16.46 28.99 81.0 0.20 0.35 NOW supplements Ashwagandha 450 mg x 90 capsules 17.90 40.5 0.44 Nutricost Ashwagandha Extract KSM-66 600 mg x 180 capsules 40.95 108 0.38","title":"Ashwagandha"},{"location":"Health/#astragalus","text":"Product $ g $/g Swanson Full Spectrum Astragalus 470 mg x 100 capsules 2.59 3.99 47.0 0.03 0.04 Gorilla Mind Astragalus extract 750mg x 90 capsules 20.00 67.5 0.14 Swanson Astragalus 20.39 120.0 0.17 Ayurveda A traditional system of herbal medicine originating in India (ref Bacopa )","title":"Astragalus"},{"location":"Health/#bacopa","text":"Bacopa monnieri is an Ayurvedic herbal medicine. Studies have substantiated its use in enhancing memory by reducing inflammation in the brain. The main nootropic constituents of Bacopa are believed to be dammarane types of triterpenoid saponins known as bacosides . Most clinical studies focus on memory, to the omission of other facets of cognition like fluid intelligence, typically lasting 12 weeks. The long-term effect of Bacopa is unknown, but animal models suggest protection from age-related neurodegeneration. ( 2013 :material-file-pdf:) There are several putative mechanisms of action: Anti-oxidant/neuroprotection Acetylcholinesterase inhbition Choline acetyltransferase activation Beta-amyloid reduction Increased cerebral blood flow Monoamine potentiation and modulation A mix of Bacopa monnieri , Lycopene , Astaxanthin , and vitamin B12 ) administered orally once a day to subjects aged 60 years old or more resulted in improved cognitive performance after 8 weeks. ( 2020 ) A neuropharmacological review found that the maximum efficacious dosage was 200 mg/kg. ( 2013 ) Product $ M (g) $/g Bulk Supplements Bacopa 1 kg 212.96 1,000.0 0.21 Swanson Bacopa Monnieri 250 mg x 90 capsules sale 5.57 9.29 22.5 0.23 0.41 Bulk Supplements Bacopa 250 g 73.96 250.0 0.29 NOW Supplements Bacoba Extract 450 mg x 90 capsules 18.78 40.5 0.46","title":"Bacopa"},{"location":"Health/#berberine","text":"Berberine complexed with hydroxypropyl-\u03b2-cyclodextrin was recommended by MPMD to deal with carb-dense meals. This is apparently a reference to the use of berberine as a GDA , a term that has recently gained currency in the fitness industry as a way to improve how glucose is processed in the body, or \"nutrient partitioning\". The science on this topic appears to be inconclusive at best .","title":"Berberine"},{"location":"Health/#bioperine","text":"Product $ mg $/10 mg Swanson Bioperine 10 mg x 60 capsules (35% off sale) 2.59 600 0.04 Swanson Bioperine 10 mg x 60 capsules 3.99 600 0.07 Swanson Bioperine 10 mg x 60 capsules 9.86 600 0.16","title":"Bioperine"},{"location":"Health/#calcium","text":"Your dietary intake of calcium is probably 300 mg with the largest provider being 4 eggs providing 100 mg (target is 1,000 mg) Most food sources of calcium are dairy. A cup of edamame may contain about 100 mg An ounce of almonds may contain 367 mg Product $ g $/1,000 mg BulkSupplements Calcium citrate powder (1 kg) 17.96 1,000.0 0.02 Swanson Calcium Citrate & Vitamin D 315 mg x 250 tablets 6.59 10.99 78.8 0.08 0.14 Swanson Liquid Calcium & Magnesium 300 mg x 100 softgels 3.89 6.49 30.0 0.13 0.21 Swanson Calcium Citrate Plus Magnesium 150 mg x 150 capsules 3.81 4.49 22.5 0.17 0.20 Amazon Elements Calcium plus Vitamin D 500 mg x 65 tablets 5.99 10.99 32.5 0.19 0.34 Blue Diamond Almonds (BOGO sale) 9.00 2.5 3.52 Swanson Calcium Carbonate, Aspartate & Citrate 500 mg x 100 tablets 2.74 50.0 0.05","title":"Calcium"},{"location":"Health/#choline","text":"Choline is a precursor to the neurotransmitter acetylcholine . It is endogenously produced and found in food. Many supplements contain choline because it is cheaply produced, but studies indicate it may not be able to cross the blood-brain barrier. Another product of choline is betaine . For individuals who have genetic polymorphisms like MTHFR and are thus far less able to produce B12 and folate, the pathway producing betaine compensates for this shortcoming, which results in deprivation of acetylcholine for cognition. Cholinergic relating to or denoting nerve cells in which acetylcholine acts as a neurotransmitter Crossover trial A trial where subjects are randomly assigned to study arms where each arm consists of two consecutive bouts of treatment separated by a washout period. A concern with crossover trials is that there is a chance that subsequent rounds of treatment may be influenced by earlier ones.","title":"Choline"},{"location":"Health/#citrullus-lanatus","text":"A late 2020 study found that mice given Cucumis melo and watermelon ( Citrullus lanatus ) seed extract experienced enhancement of memory and cognition.","title":"Citrullus lanatus"},{"location":"Health/#citrus-bergamot","text":"Improves lipid profile . CoQ10 Ubiquinone, also called Coenzyme Q 10 (CoQ10) , is a coenzyme family ubiquitous to animals and bacteria. Ubiquinone is the most commonly found form in humans, an endogeneously produced lipid-soluble antioxidant that metabolizes into ubiquinol . It is one of the most popular supplements and renowned for being a powerful antioxidant. Doses are usually 100-200 mg/day, although there is no established upper limit for tolerance. A meta-analysis of 13 randomized controlled trials found that CoQ10 supplementation increased superoxide dismutase (SOD) and catalase (CAT) and decreased malondialdehyce (MDA) and diene. ( 2019 ) A meta-analysis of 19 randomized controlled trials found that CoQ10 supplementation significantly increased total antioxidant capacity (TAC), SOD, CAT, and glutathione peroxidase (GPx), as well as significantly decreased MDA. ( 2020 ) A 2021 study found that horses who were fed CoQ10 experienced persistent improvement in semen quality. Patients receiving from 300-1200 mg/day exhibited no difference in incidence of drug-related toxicities between placebo and treatment arms. ( 2002 ) Creatine monohydrate A 2018 review of six studies found that creatine can improve short-term memory and intelligence/reasoning in healthy individuals. Product $ g $/5g BulkSupplements Creatine Monohydrate powder (1 kg) 19.96 1,000 0.10 Optimum Nutrition Creatine powder 4.41 lbs 42.49 2,000 0.11 Optimum Nutrition Creatine powder 1.32 lbs 15.74 600 0.13 Optimum Nutrition Creatine powder 10.5 oz 10.53 300 0.18 Swanson Creatine 1g x 180 capsules 2-pack 17.49 360 0.25 Optimum Nutrition Creatine capsules 150 x 2.5 g servings 26.13 375 0.35 Optimum Nutrition Creatine powder 2.64 lbs 35.94 1,200 0.15","title":"Citrus Bergamot"},{"location":"Health/#cucumis-melo","text":"Muskmelon is a species of melon that includes honeydew and cantaloupe. A late 2020 study found that mice given Cucumis melo and Citrullus lanatus seed extract experienced enhancement of memory and cognition.","title":"Cucumis melo"},{"location":"Health/#curcumin","text":"Curcumin ( diferuloylmethane ) is the principal biologically active polyphenolic constituent of turmeric ( Curcuma longa ). It is a powerful antioxidant binding to COX-2 and 5-LOX and has been found to increased brain-derived neurotrophic factor (BDNF). Curcumin supplementation increases serum brain-derived neurotrophic fator (BDNF) ( 2020 ). A literature review found that ( 2020 :material-file-pdf:): Curcumin can act as a neuroprotectant antioxidant by binding to COX-2 and 5-LOX. Curcumin inhibits the activation of microglial cells and protects dopaminergic neurons against microglia-mediated neurotoxicity. Curcumin inhibits the caspase-3 apoptosis mediator, suggesting that it is an anti-apoptotic agent that would be useful in treating neurodegenerative diseases. A meta-analysis found that curcumin can improve therapy for people with symptoms of depression and anxiety ( 2019 ). Curcumin has very poor bioavailability without co-administration of piperine. ( 1998 ) A systematic review and meta-analysis found that curcumin administration was associated with a significant reduction only in systeolic blood pressure, but not in diastolic, in studies with supplementation exceeding 12 weeks. ( 2019 ) Product $ g $/g BulkSupplements Curcumin 1 kg 249.96 1,000.0 0.25 Swanson Curcumin Complex 350 mg x 120 capsules (sale) 14.24 18.99 42.0 0.34 0.45 Amazon Elements turmeric Complex 316 mg x 65 capsules 14.99 20.5 0.73 D-Aspartic acid D-Aspartic acid or D-Aspartate is an amino acid found in various tissues, including in the axon terminals and synaptic vesicles of neuronal tissue and endocrine glands. It is known to induce testosterone synthesis in the testis. A literature review concluded that exogenous D-Asp increases testosterone in animals. However, human studies which demonstrated conflicting results were noted for their short-term generation, small sample size, and other problems. ( 2017 :material-file-pdf:) Most studies used dosages of 2,600-3,000 mg/day ( Healthline ) Purported benefits of DAA are probably bunk . Product $ g $/g Nutricost D-Aspartic Acid 750 mg x 180 capsules 14.99 135 0.11 Dihydrotestosterone ( DHT ) produced after testosterone is aromatized by 5-alpha-reductase.","title":"Curcumin"},{"location":"Health/#dmae","text":"Dimethylaminoethanol (marketed as DMAE but appearing more commonly as deanol in scientific literature) is not a precursor to acetylcholine as is commonly asserted. However it is hypothesized that it may increase acetylcholine levels by inhibiting choline metabolism. In a double-blind, placebo-controlled trial, children given 40 mg of Ritalin or 500 mg of deanol experienced similar improvements in psychometric tests . Deanol appeared to be more effective for those with learning disabilities. Product $ mg $/750 mg BulkSupplements DMAE-bitartrate powder (500 g) 20.96 500,000 0.03 NOW Supplements DMAE 250 mg x 100 capsules 7.51 25,000 0.225","title":"DMAE"},{"location":"Health/#dutasteride","text":"Enantiomer molecules that are mirror images of one another Endothelium cells that line the interior surface of blood vessels and lymphatic vessels epithelium one of the four basic types of animal tissue ergogenic intended to enhance physical performance, stamina, or recovery excipient an inactive substance that serves as the vehicle or medium for a drug or other active substance.","title":"Dutasteride"},{"location":"Health/#fenugreek","text":"Several studies suggest that male subjects who took 500 mg fenugreek daily experienced increased testosterone and improved mood, energy, and libido. Product $ g $/g Swanson Fenugreek seed 610 mg x 90 capsules 2.09 3.49 54.9 0.04 0.06 Swanson Fenugreek extract 500 mg x 90 capsules 6.95 11.59 45.0 0.15 0.25 Swanson Fenugree extract featuring Testofen 18.39 36.0 0.51","title":"Fenugreek"},{"location":"Health/#finasteride","text":"Propecia is the commercial brand name for finasteride.","title":"Finasteride"},{"location":"Health/#fish-oil","text":"Fish oil is a prominent source of the Omega-3 oils EPA and DHA . Product $ g (\u03c9-3) $/g Swanson Super EPA & DHA BOGO 20.39 92.8 0.21 Follicle-Stimulating Hormone (FSH) free radical reaction","title":"Fish oil"},{"location":"Health/#gaba","text":"Gamma aminobutyric acid (GABA) is considered an inhibitory neurotransmitter because it blocks certain brain signals and decreases activity in the nervous system. GABAergic receptors are targeted by clinically important drugs that treat anxiety, epilepsy, insomnia, and other pathophysiological disorders. ( 2015 ) Glutathione Glutathione is involved in many body processes and is a common therapy for patients with various ailments, including cancer, HIV, etc. It is endogenously produced in the liver and can also be found in various foods. Gonadotropin-Releasing Hormone (GnRH) hepatic Related to the liver hirsutism a condition causing male-pattern hair growth in women.","title":"GABA"},{"location":"Health/#huperzine-a","text":"Huperzine A is a lycopodium alkaloid derived from Huperzia serrata (toothed clubmoss or firmoss), which has been used therapeutically for neurological disorders. It suppresses the acetylcholinesterase enzyme which breaks down acetylcholine Huperzine A promoted neurogenesis in the hippocampus of mice in vitro and in vivo ( 2013 ) Huperzine A directly increased neurotrophic activity of rat astrocytes in vitro ( 2005 ) Product $ mcg $/200 mcg BulkSupplements Huperzine A 10 g 29.96 100,000 0.06 BulkSupplements Huperzine A 100 g 325.96 1,000,000 0.07 Swanson Huperzine A 200 mcg x 30 capsules (Swanson) 9.89 5.93 6,000 0.33 0.20 Hypothalamic-pituitary-gonadal (HPG) axis An endocrine control mechanism involved in the regulation of testosterone in males and estrogen in females. In this concept, the hypothalamus produces GnRH which binds to secretory cells of the anterior pituitary. Binding of GnRH stimulates these cells to produce LH and FSH, which then produce different effects in the sexes: production of estrogen and inhibin in the ovaries of the female and testosterone and sperm in the testes of the male. International Unit (IU) A unit of measurement for the amount of a substance that varies based on the substance being measured. Isomer Each of several compounds with the same formula but a different arrangement of atoms and different properties","title":"Huperzine A"},{"location":"Health/#kanna","text":"Kanna (Sceletium tortuosum) increases seratonin in the brain and improves mood. Product $ mg $/200 mg Swanson Sceletium Tortuosum 50 mg x 60 capsules (Swanson BOGO sale) 11.99 6,000 0.40","title":"Kanna"},{"location":"Health/#l-arginine","text":"L-Arginine is an amino acid widely used in pre-workout supplements because it enhances blood flow. It works by providing nitrogren to the nitric oxide synthase (NOS) enzyme to produce nitric oxide (NO), being metabolized to L-citrulline in the process. L-citrulline, in turn, can be reconverted to L-arginine in the kidneys. Both L-citrulline and L-arginine regulate nitrogen and ammonia in the blood. Bioavailability of l-arginine can vary up to 70%, but excessive dosages can cause gastrointestinal issues. L-citrulline, in contrast, has a bioavailability of practically 100%, which is why it is favored now. L-Arginine may help reduce blood pressure by producing nitric oxide, which relaxes blood vessels. Product $ g $/g Swanson L-Arginine 850mg x 90 caps BOGO 6.59 153 0.04 Nature's Bounty L-Arginine 1000mg x 50 tablets 8.80 50 0.18 Bulk Supplements L-Arginine 1 kg 34.96 1,000 0.04","title":"L-Arginine"},{"location":"Health/#l-carnitine","text":"Several forms of L-carnitine are available: L-carnitine L-tartrate (LCLT) is a salt of L-carnitine that increases androgen receptor uptake. Resistance-trained men given LCLT experienced an increase in AR content ( 2006 ) It is absorbed faster than other L-carnitine esters ( 2005 ) Acetyl L-carnitine (ALCAR) is an ester of L-carnitine that can pass through the blood-brain barrier, and its acetyl group ensures that it is active in the central nervous system. It is a \"substrate\" for acetylcholine . L-carnitine fumarate is marketed as a weight loss booster under the name Carnishield Product $ mg $/4000 mg Swanson L-Carnitine 500 mg x 100 tabs 7.48 50,000 0.60 Swanson Acetyl L-carnitine 500 mg x 240 caps (Swanson sale) 16.24 120,000 0.54 Swanson Acetyl L-carnitine 500 mg x 100 caps (Swanson sale) 7.69 50,000 0.62 Jarrow LCLT 500 mg x 100 capsules 19.39 50,000 1.55 Swanson L-Carnitine Fumarate 450 mg x 60 caps 10.99 27,000 1.63 NOW Foods Acetyl-L-Carnitine 500 mg x 50 caps 10.38 25,000 1.66 Oral supplementation of carnitine is problematic because of the production of TMAO which has been associated with cardiovascular disease. Some studies indicate that dietary allicin from garlic can reduce the production of TMAO . ( 2015 ) Product $ mg $/24 mg Swanson Allicin 12 mg x 100 tabs 8.60 1,200 0.17 Injectable L-carnitine is a suitable lipolytic , especially when used in conjunction with methionine, inositol, and choline (MIC) and B vitamins.","title":"L-Carnitine"},{"location":"Health/#l-leucine","text":"Leucine is a dietary amino acid marketed as directly stimulating muscle growth. Untrained peri- and postmenopausal women performing resistance training experienced no ergogenic effects supplementing with leucine. ( 2020 ) Young adult men experienced no ergogenic effects supplementing with leucine. ( 2019 ) Elderly men experienced no ergogenic effects from ong-term leucine supplementation (7.5g/d) ( 2009 :material-file-pdf:)","title":"L-Leucine"},{"location":"Health/#l-theanine","text":"An amino acid found in green tea, alone it can increase the feeling of well-being , but in combination with caffeine generally increases cognitive performance. Product $ mg $/200 mg BulkSupplements L-Theanine powder (100 g) 17.96 100,000 0.04 Amazon Elements L-Theanine 200 mg x 60 capsules (sale) 10.22 12,000 0.17 Amazon Elements L-Theanine 200 mg x 60 capsules 13.49 12,000 0.22 Swanson Suntheanine L-Theanine 100 mg x 60 capsules (Swanson sale) 6.74 6,000 0.22 Swanson Suntheanine L-Theanine 100 mg x 60 capsules (Swanson) 8.99 6,000 0.30","title":"L-Theanine"},{"location":"Health/#l-tyrosine","text":"L-Tyrosine is the most bioavailable form of Tyrosine, especially in comparison with N-acetyl-L-Tyrosine, and a precursor to the neurotransmitter dopamine , thyroxine, adrenaline and noradrenaline, and the catecholamines. It is converted by the enzyme tyrosine hydroxylase. L-Tyrosine is commonly believed to reduce and relieve stress and enhance cognitive performance. Tyrosine abolished fear expression, apparently because of its effect on the catecholaminergic system. ( 2019 ) A literature review of 15 studies found that tyrosine acutely counteracts decrements in working memory and information processing caused by cognitive load or bad weather. ( 2015 ) Product g $ $/g Swanson L-Tyrosine 500 mg x 100 capsules 50.0 3.59 5.99 0.07 0.12 Jarrow L-Tyrosine 500 mg x 100 capsules 50.0 5.38 0.10 NOW Supplements L-Tyrosine 750 mg x 90 capsules 67.5 10.73 0.16 BulkSupplements L-Tyrosine 1 kg 1,000.0 28.96 0.29","title":"L-Tyrosine"},{"location":"Health/#lecithin","text":"Lecithin is a generic term designating any yellow-brownish fatty substances occurring in animl and plant tissues that are amphiphilic. Soy lecithin contains various compounds that reduce stress, including phosphatidylserine . Lecithin is widely assumed to be a provider of choline , although as such it is almost certainly inferior to Alpha GPC .","title":"Lecithin"},{"location":"Health/#lions-mane","text":"A 2021 study found that compounds extracted from Hericium erinaceus demonstrated neurotrophic effects in isolation. In particular isohericerinol A stimulated neurogenesis by increasing Nerve Growth Factor (NGF) production. Product $ g $/g BulkSupplements Lion's Mane 500 g 35.96 500 0.07 Swanson Lion's Mane 500 mg x 60 capsules (Swanson) 10.49 30 0.35 Lipid profile Ratio between HDL (good cholesterol) and LDL (bad cholesterol) Lipolysis The breakdown of fats and other lipids by hydrolysis to release fatty acids. The process of mobilizing stored energy during fasting or exercise. The metabolic pathway through which lipid triglycerides are hydrolyzed into a glycerol and three fatty acids. Lutein Lutein is a carotenoid closely related to beta-carotene, which gives carrots and pumpkins their orange color. Luteinizing Hormone (LH) n/a","title":"Lions Mane"},{"location":"Health/#maca","text":"Maca root ( Lepidium meyenii ) is a Peruvian vegetable with a long history in South American cuisine, and as an herb it is traditionally used to enhance fertility and sex drive. Some studies suggest an ergogenic benefit to endurance activities. Maca, in particular black maca, improved learning and memory in rodents with memory impairment. A review found that it can improve sperm volume, and motility and the volume of semen in infertile and healthy men. Another review found that maca can help alleviate hot flashes and anxiety in menopausal women. ( src ) Dosage varies 1.5-5.0 g/day Product $ m $/500 mg BulkSupplements Maca 1 kg 29.96 1,000 0.02 Swanson Maca 500 mg x 100 capsules 2.50 50 0.03","title":"Maca"},{"location":"Health/#magnesium","text":"Recommended daily intake of magnesium is 420 mg, and you are short by approximately 150 mg. Product $ mg $/200 mg BulkSupplements Magnesium Citrate powder (250 g) 11.96 250,000 0.010 Swanson Triple Magnesium Complex 400 mg x 300 caps (Amazon) 9.99 120,000 0.017 Swanson Triple Magnesium Complex 400 mg x 300 caps 11.99 120,000 0.020","title":"Magnesium"},{"location":"Health/#melatonin","text":"Melatonin may reduce blood pressure . Mini Mental State Examination ( MMSE ) Scoring 20 or above corresponds to normal cognitive functions","title":"Melatonin"},{"location":"Health/#minoxidil","text":"Minoxidil is a potassium channel opener originally designed to lower blood pressure. Rogaine is one of many brand names offering 5% topical minoxidil.","title":"Minoxidil"},{"location":"Health/#modafinil","text":"Modafinil is a heavily abused sleep suppressant typically prescribed to narcolepsy patients but also said to be used by fighter pilots. Myopathy Any disease that affects muscle tissue Necrosis Uncontrolled cell death as a result of shock; cf. apoptosis Nerve Growth Factor (NGF) Neurodegenerative disease Diseases that result in progressive degeneration or death of neuronal cells","title":"Modafinil"},{"location":"Health/#niacin","text":"Vitamin B3 or niacin comes in two forms: nicotinic acid and niacinamide (also called nicotinamide). Niacin is considered an antilipemic agent because can help regulate cholesterol by lowering LDL and raising HDL. In this context, daily dosages may range from as little as 250 mg up to a maximum of 6,000 mg. It has also been shown to increase NAD+ levels in people with systemic NAD+ deficiency. It is associated with side effects in some people, including hives and skin rashes. Nicotinamide adenine dinucleotide (NAD+) NAD+ is involved in many biological processes, and its level falls as a person ages . Niacin and nicotinamide riboside have both been shown to increase the serum concentrations of NAD+ in humans.","title":"Niacin"},{"location":"Health/#nicotinamide-riboside","text":"Nicotinamide riboside, also called niagen, is a B3 vitamin and a highly bioavailable precursor to NAD+ . Nicotinamide riboside supplementation raises NAD+ levels, which also helps reduce systolic blood pressure . Phenol An aromatic organic compound with the molecular formula C 6 H 5 OH. ( wiki )","title":"Nicotinamide riboside"},{"location":"Health/#phosphatidylcholine","text":"Phosphatidylcholine is a phospholipid that incorporates choline as a headgroup. Humans with cognitive disorders who were fed two forms of phosphatidylcholine (~100 mg) after breakfast experienced an improvement in their [ MMSE ][ MMSE ] scores so significant that their new scores rose above the diagnostic threshold for cognitive disorder. ( 2011 ) Administration of phosphatidylcholine to mice with impaired memory due to inbreeding raised their brain acetylcholine concentration to the level of unimpaired mice and resolved their poor memory function. However, phosphatidylcholine treatment did not affect memory or acetylcholine concentrations in normal mice. ( 1995 ) During a randomized, placebo-controlled clinical trial, prenatal supplementation of phosphatidylcholine resulted in improvement of cognitive performance of fetus (P50 inhibition). Delay in development as measured by this test is associated with schizophrenia and developmental problems. ( 2013 ) This finding confirmed earlier research on the long-term benefit of prenatal choline supplementation in animals.","title":"Phosphatidylcholine"},{"location":"Health/#phosphatidylserine","text":"Phosphatidylserine (PtdSer or PS) is a phospholipid that supports a gamut of cognitive functions. It is found in the inner leaflet of neural cell membranes where it regulates the release of neurotransmitters acetylcholine, dopamine, and noradrenaline. Two sources of phosphatidylserine were available: bovine-cortex (BC-PtdSer) which has fallen out of favor due to the risk of transferring infections from prion-infected brains, and soy (S-PtdSer). In a randomized, double-blind trial, children diagnosed with ADHD but never having received drug treatment related to it experienced significantly improved ADHD symptoms and short-term memory after 2 months of phosphatidylserine supplementation (200mg/day). Exogenous dosages of 300-800 mg/d are absorbed efficiently, cross the blood-brain barrier, and reverses biochemical deterioration in nerve cells as a result of aging. ( 2015 ) Product $ mg $/100 mg Swanson Phosphatidylserine 100 mg x 90 softgels (sale) 20.89 13.57 9,000 0.23 0.15 Pleiotropic Having more than one effect, especially having multiple phenotypic expressions Polyphenol A large family of naturally occurring organic compounds composed of multiple phenol units. There may be an association with polyphenol intake and reduced blood pressure . Supplements with polyphenolic compounds include resveratrol , curcumin , quercetin , and beetroot.","title":"Phosphatidylserine"},{"location":"Health/#potassium","text":"Recommended adequate intake for potassium was set by the Food and Nutrition Board of the Institute of Medicine at 4700 mg/day . Your daily dietary intake is only 1600 mg/day. A banana may have 450 mg 100g of broccoli may have 250 mg A cup of edamame (155 g) might have >600 mg 100g of apricot (~15 pieces) provide almost 1,162 mg (or ~80 mg a piece) A systematic review and meta-analysis found that intervening to a point where potassium was 30 mmol/day produced the greatest decrease in systolic and diastolic blood pressure. ( 2020 ) Both potassium bicarbonate and potassium citrate are alkaline sources of elemental potassium, found in dietary sources. Product $ mg $/2,000 mg BulkSupplements Potassium Citrate powder (1 kg) 18.96 1,000,000 0.04","title":"Potassium"},{"location":"Health/#pygeum","text":"Product $ g $/500mg BulkSupplements pygeum 1 kg 36.96 1,000 0.02 Swanson Pygeum 125 mg x 100 capsules (sale) 5.79 2.90 12.5 0.23 0.12","title":"Pygeum"},{"location":"Health/#quercetin","text":"Quercetin is a polyphenolic antioxidant ubiquitous in plant food sources. A systematic review and meta-analysis found that taking at least 500 mg/day resulted in significant reduction of blood pressure. ( Serban et al. 2016 ) Product $ g $/g Nutricost Quercetin 440 mg x 240 capsules 31.95 105.6 0.30 Swanson Quercetin 475 mg x 60 capsules (25% off sale) 9.74 28.5 0.34 Jarrow Quercetin 500 mg x 100 capsules 18.99 50.0 0.38 Swanson Quercetin 800 mg x 30 capsules (BOGO) 19.38 48.0 0.40 Swanson Quercetin 475 mg x 60 capsules 12.99 28.5 0.45 Jarrow Quercetin 500 mg x 200 capsules 66.95 100.0 0.67","title":"Quercetin"},{"location":"Health/#resveratrol","text":"Resveratrol is a polyphenol and estrogenic antioxidant found in grape skin, peanuts, and other foods. Red wine contains less than 13 mg of resveratrol per liter. ( src ) It is confirmed as a powerful antioxidant and may potentially enhance cognitive performance. A systematic review found that supplementation with resveratrol does not have an anti-obesity effect . In contrast, an earlier systematic review cited in this one did find that resveratrol supplementation had a positive effect on weight loss. A comprehensive review found that even moderate daily supplementation (0.5-1 g) is enough to inhibit estrogen metabolism and increase SHBG. ( 2020 ) Resveratrol has poor bio-availability and a short half-life (1.5 hours). ( 2019 :material-file-pdf:). Resveratrol upregulates SHBG expression in the liver. ( 2017 ) A literature review found that resveratrol intervention lowered total cholesterol, systolic blood pressure, and fasting glucose with more significant reductions when doses were higher than 300mg/day. ( 2016 ) A double-blind, randomized, placebo-controlled crossover study found that administration of resveratrol (150 mg/day) caused no changes in metabolic risk markers in overweight and obese subjects after 4 weeks with a 4-week washout period. ( 2015 ) Rats force-fed resveratrol at 20mg/kg daily experienced increased serum concentration of gonadotrophins and testosterone by stimulating the HPG axis . Product $ mg $/100 mg BulkSupplements 100g 85.73 100,000 0.09 BulkSupplements 500g 323.48 500,000 0.06 Swanson Resveratrol 500 mg x 30 capsules (BOGO) 32.99 30,000 0.11 Swanson Resveratrol 250 mg x 30 capsules (Swanson BOGO sale) 17.99 15,000 0.12 Swanson Resveratrol 100 mg x 30 capsules (Swanson BOGO sale) 8.99 6,000 0.15","title":"Resveratrol"},{"location":"Health/#rhodiola","text":"Rhodiola rosea is an adaptogenic herb used in traditional medicine in Europe and Asia. A 2018 meta study :material-file-pdf: examined 36 animal studies and concluded that rhodiola can improve learning and memory function, despite an earlier 2012 review that found that methodological problems with earlier experiments brought these findings into question. A clinical trial in 2000 found that young physicians experienced improvement in cognitive performance on a battery of cognitive tests called the Fatigue Test. The study concluded that rhodiola could reduce general fatigue under stressful conditions. A similar 2003 study found that young military cadets were able to resist fatigue better with rhodiola. Product $ mg $/400 mg Swanson Rhodiola 400 mg x 100 capsules (Swanson sale) 4.79 40,000 0.05 Swanson Rhodiola 400 mg x 100 capsules (Swanson) 7.99 40,000 0.08 Swanson Rhodiola 400 mg x 100 capsules (Amazon) 10.89 40,000 0.11 Now Foods Rhodiola 500 mg x 60 capsules 18.25 30,000 0.24 Ritalin Ritalin is the brand name for the stimulant methylphenidate.","title":"Rhodiola"},{"location":"Health/#rosemary","text":"Mark Moss with Northumbria University has published several studies on the cognitive benefits of various herbs, including peppermint, chamomile, rosemary, and lavender. The studies on rosemary in particular were motivated by traditional associations of rosemary with memory. Aromatherapy with rosemary oil resulted in improvement in cognitive performance ( 2017 ). Drinking rosemary water produced a small benefit to memory ( 2018 ) Cognitive benefit of rosemary aromatherapy was associated with concentration of 1,8-cineole. ( 2012 ) An interesting study contrasted the impact of aromatherapies of lavender and rosemary oils. Rosemary enhanced performance for overall quality of memory, but also impaired speed of memory compared to control. Lavender actually impaired performance of working memory. Rosemary boosted alertness in comparison to both control and lavender. Both lavender and rosemary enhanced contentment. ( 2003 ) Product $ mg $/500 mg Swanson Rosemary Extract 500 mg x 60 capules (Swanson sale) 5.13 30,000 0.09 Swanson Rosemary Extract 500 mg x 60 capules (Swanson) 7.19 30,000 0.12 Swanson Rosemary Extract 500 mg x 60 capules (Amazon) 9.99 30,000 0.17 Saffron ( Crocus sativus L. ) A 2020 review found that there was some evidence that associated saffron with improvement in cognitive performance, especially in subjects with neurodegenerative disease. Sarcopenia Age-related loss of muscle mass and function","title":"Rosemary"},{"location":"Health/#saw-palmetto","text":"Saw palmetto ( Serenoa repens ) is a dwarf palm tree native to southeast North America that has long been used as a medicinal herb by Native Americans. Studies suggest that it blocks the conversion of testosterone to [ DHT ][ DHT ], and may be effective as a treatment for androgenic alopecia. ( src ) Dosage may be 160-320 mg Product $ g $/500mg BulkSupplements Saw Palmetto 1 kg 43.96 1,000 0.02 Swanson Saw Palmetto 540 mg x 250 capsules sale 15.99 5.69 135 0.06 0.02 Swanson Saw Palmetto 540 mg x 100 capsules sale 4.59 2.75 54 0.04 0.03 Squamous Referring to the shape of cells that are wide and flat, such as those found in the lining of the mouth, esophagus, and blood vessels (cf. cuboidal and columnar) Statin A family of drugs used for treating hyperlipidaemia with a recognized capacity to prevent cardiovascular disease event","title":"Saw palmetto"},{"location":"Health/#taurine","text":"Dosages of taurine in studies are commonly 500 - 2,000 mg /day, however higher doses appear to be well-tolerated.","title":"Taurine"},{"location":"Health/#theobromine","text":"Theobromine is a methylxanthine and stimulant found in the cacao plant and chocolate products: it is what gives dark chocolate its bitterness. Theobromine is believed to promote wakefulness and alertness and enhance cognition, among other benefits . Product $ g $/g BulkSupplements Theobromine 46.96 250 0.19 BulkSupplements Theobromine 20.96 100 0.21 Thyroxine Thyroxine, called T4 because it contains 4 iodine atoms, is the major thyroid hormone secreted by the thryoid gland. It is converted to triiodothyronine (T3) by the removal of an iodine atom mainly in the liver but also in the brain. Thyroxine production is regulated by TSH . Tryptophan Tryptophan is a precursor of the neurotransmitters serotonin and melatonin.","title":"Theobromine"},{"location":"Health/#ubiquinol","text":"Ubiquinol (QH) is the fully reduced form of ubiquinone . Intense exercise depletes CoQ10, but supplementation with ubiquinol prevents this deprivation. Ubuiquinol is more bioavailable than CoQ10 by an order of magnitude. Like vitamin E , ubiquinol is a lipid-soluble antioxidant, which gives it the special task of protecting sensitive cell membranes. Ubiquinol is depleted before vitamin E because it reacts with radicals first, and it can also replenish depleted vitamins E and C. ( 2013 ) Healthy and well-trained fireman given 200mg/day ubiquinol for two weeks experienced increases in the biomarkers of bone formation and energy mobilization, suggesting ergogenic effects. ( 2020 ) CoQ10 was positively associated with antioxidant capacity, muscle mass, muscle strength, and muscle endurance in patients with osteoarthritis. ( 2020 ) Subjects with mild cognitive impairment who received 200 mg/day ubiquinol for a year experienced improved cerebral vasoreactivity compared to baseline and placebo, but no significant neurological improvement. ( 2021 ) In a double-blind, placebo-controlled study, ubiquinol supplementation increased physical perforrmance. ( 2013 :material-file-pdf:) Ubiquinol supplementation to mice reduced a variety of markers associated with fatigue and increased muscle and liver glycogen content, which provide energy during exercise. ( 2019 ) A systematic review and meta-analysis found that long-term supplementation with CoQ10 (most studies were 2-4 months while one was 13 months) at dosages varying from 34-225 mg/day can significantly reduce blood pressue. ( 2007 ) Product $ mg $/100 mg Jarrow QH-absorb (Publix sale) 6.71 6,000 0.11 NOW Supplements Ubiquinol 100 mg x 120 softgels 38.69 12,000 0.32 Swanson Ubiquinol 100 mg x 120 softgels (2-pack) 79.99 24,000 0.33 Swanson Ubiquinol 100 mg x 120 softgels 42.99 12,000 0.36 Vascular resistance The resistance that must be overcome to push blood through the circulatory system Vasoreactivity A >= 30% decrease in pulmonary vascular resistance (PVR) with vasodilator compared to baseline VAT Visceral adipose tissue (VAT) is fat surrounding the intra-abdominal organs which has been associated with various medical pathologies; alongside subcutaneous adipose tissue (SAT) , one of the two types of body fat tissue Visceral obesity : Abnormally high deposition of visceral adipose tissue (VAT)","title":"Ubiquinol"},{"location":"Health/#vitamin-b12","text":"Vitamin B12 (cobalamin, methylcobalamin) is a cofactor and precursor of neurotransmitters acetylcholine, dopamine, GABA, norepinephrine, and serotonin. Even mild B12 deficiency was associated with cognitive decline over 8 years. The BLAtwelve Study tested the effects of Bacopa monnieri , Lycopene , Astaxanthin , and Vitamin B12, finding that a mix of the four compounds administered orally once a day to subjects aged 60 years old or more experienced improved cognitive performance after 8 weeks. Product $ mg $/mg Amazon Elements B12 5000 mcg x 65 lozenges 12.99 325 0.04 Swanson Ultra Vitamin B12 Triple Action 1000 mcg x 90 tabs 6.59 10.99 90 0.07 0.12","title":"Vitamin B12"},{"location":"Health/#vitamin-c","text":"Vitamin C ( \"ascorbic acid\") is a water-soluble antioxidant that can interact directly with free radicals. Its efficacy as an antioxidant has been closely associated with that of Vitamin E , which it may regenerate by reducing the Vitamin E radical. A 2018 study suggested that Vitamin C could inhibit visceral adipocyte hypertrophy A long-term study published in 2008 found that there was no significant effect of vitamin C on cardiovascular health. The body tightly controls serum concentration of vitamin C. Dosages of 200-300 mg/day result in concentrations of 70 micromol/L, whereas dosages of 1.25 g/day produce concentrations of only 135 micromol/L. Product $ M $/1,000 mg Swanson Vitamin C 1,000 mg x 250 capsules 8.04 11.49 250 0.032 0.045 Swanson Vitamin C 500 mg x 400 capsules 7.69 10.99 200 0.039 0.055 Amazon Elements Vitamin C 1000 mg x 300 tablets 17.99 300 0.06","title":"Vitamin C"},{"location":"Health/#vitamin-d3","text":"Vitamin D3 (cholecalciferol) can regulate lipid peroxidation as a form of neuroprotection by inducing the synthesis of parvalbumin , a protein that binds to Ca 2+ ( 2020 ). Claims that Vitamin D3 can boost testosterone are unfounded ( 2017 , 2019 ) Product $ IU $/5,000 IU Swanson D3 5000 IU x 250 softgels (Swanson sale) 8.24 1,250,000 IU 0.03 Swanson D3 5000 IU x 250 softgels (Swanson) 10.99 1,250,000 IU 0.04 Amazon Elements D3 5000 IU x 180 softgels 9.34 900,000 IU 0.05","title":"Vitamin D3"},{"location":"Health/#vitamin-e","text":"Vitamin E (alpha-tocopherol) is a lipid soluble antioxidant that interferes with the propagation of lipid radicals. Its efficacy as an antioxidant has been closely associated with that of Vitamin C .","title":"Vitamin E"},{"location":"Health/#zinc","text":"Men who received 30 mg of zinc a day showed increased levels of free testosterone . The tolerable upper intake level for adult men is 40 mg/day . Product $ M $/30 mg Swanson Zinc Gluconate 30 mg a 250 tabs 3.14 4.49 7.5 0.01 0.02","title":"Zinc"},{"location":"Infrastructure/ADO/","text":"Azure DevOps Azure CLI There is an Azure DevOps extension for the Azure CLI. # Installation and configuring defaults az extension add --name azure-devops az devops configure --defaults organization = $ORG az devops configure --defaults project = $PROJECT # Display users az devops user list --organization $ORG # Display a single user az devops user show --organization $ORG --user $USER # The extension must be updated separately from the core Azure CLI az upgrade az extension update --name azure-devops Pipelines Pipelines are used to automate the building of source code, including executing associated tasks like unit tests and packaging. A pipeline is started by a trigger and made up of one or more stages , each of which can have one or more jobs . Every execution of a pipeline, or a run , produces an artifact . The simplest possible pipeline is a YAML file with a single line defining a trigger. Creating a new repo with a single YAML file with the following content will allow a Pipeline to then be created, but not run. trigger : none # (1) A push trigger specifies which branches cause a continuous integration build to run. trigger: none disables CI triggers. Pipelines has some functionality with the Azure DevOps extension to Azure CLI . az pipelines runs list --query '[*].result' # (1) az pipelines run --id 1 --parameters \"name=Dgiapusccu pool=Work\" # (2) az pipelines delete --id 1 Like other JSON output from the Azure CLI, JMESPATH queries can be passed to the --query option to filter results. The --parameters options appears to be relatively new and buggy. The help indicates that multiple space-delimited key-value pairs should be able to be passed, however this appears not to be the case. Agent An agent represents compute infrastructure with installed agent software. Agents can be Microsoft-hosted (i.e. Azure VMs created specifically for the job and discarded after use) or self-hosted. Agents are organized into pools ; an agent can only belong to a single pool. The agent software package provides several shell scripts that provide various ways of running the agent. run.sh allows manual, interactive execution of the agent software svc.sh allows management of the agent software as a SystemD service. The service itself is named according to the pattern vsts.agent.[ORGANIZATION].[AGENTPOOL].[AGENTNAME].service . config.sh must be run to configure the agent after installation by providing the server URL and PAT token. Self-hosted agent setup Because the agent software itself is based on .NET Core 3.1, some operating systems such as Ubuntu 22.04 are not compatible. Some like CentOS 9 have an unsupported version of OpenSSL installed, which results in the configuration script producing a libssl error. CentOS 9 provides OpenSSL 1.1.1k libraries in a separate package: dnf install compat-openssl11 Also note it appears that the git utility needs to be installed, at least on Red Hat derivatives like CentOS, although it doesn't appear to be explicitly installed by the installdependencies.sh script. This may be because git is assumed to exist on Ubuntu. Tasks This pipeline will place a short message in the user directory of the service account running the self-hosted agent. Hello, World! trigger : none jobs : - job : HelloWorld pool : Hyper-V displayName : Hello, World! timeoutInMinutes : 0 steps : - checkout : self - bash : | echo \"Hello, World!\" > ~/hello This example can be developed further to provide the ability to choose between agent pools. Note the mustache syntax used for interpolating this parameter value into the value of jobs.job.pool . trigger : none parameters : - name : pool displayName : Agent pool default : Home values : - Home - Work jobs : - job : HelloWorld pool : ${{ parameters.pool }} displayName : Hello, World! steps : - checkout : self - bash : | echo \"Hello, World!\" > ~/hello A textbox can be created by defining a parameter of type: string . trigger : none parameters : - name : pool displayName : Agent pool default : Home values : - Home - Work - name : name displayName : Name to be greeted type : string default : World jobs : - job : HelloWorld pool : ${{ parameters.pool }} displayName : Hello, World! steps : - checkout : self - bash : | echo \"Hello, ${{ parameters.name }}!\" > ~/hello Wiki ADO offers the opportunity to create wikis for repos (\"codewiki\") or projects (\"projectwiki\"). These can also be done through the CLI . ADO wikis az devops wiki list","title":"Azure DevOps"},{"location":"Infrastructure/ADO/#azure-devops","text":"","title":"Azure DevOps"},{"location":"Infrastructure/ADO/#azure-cli","text":"There is an Azure DevOps extension for the Azure CLI. # Installation and configuring defaults az extension add --name azure-devops az devops configure --defaults organization = $ORG az devops configure --defaults project = $PROJECT # Display users az devops user list --organization $ORG # Display a single user az devops user show --organization $ORG --user $USER # The extension must be updated separately from the core Azure CLI az upgrade az extension update --name azure-devops","title":"Azure CLI"},{"location":"Infrastructure/ADO/#pipelines","text":"Pipelines are used to automate the building of source code, including executing associated tasks like unit tests and packaging. A pipeline is started by a trigger and made up of one or more stages , each of which can have one or more jobs . Every execution of a pipeline, or a run , produces an artifact . The simplest possible pipeline is a YAML file with a single line defining a trigger. Creating a new repo with a single YAML file with the following content will allow a Pipeline to then be created, but not run. trigger : none # (1) A push trigger specifies which branches cause a continuous integration build to run. trigger: none disables CI triggers. Pipelines has some functionality with the Azure DevOps extension to Azure CLI . az pipelines runs list --query '[*].result' # (1) az pipelines run --id 1 --parameters \"name=Dgiapusccu pool=Work\" # (2) az pipelines delete --id 1 Like other JSON output from the Azure CLI, JMESPATH queries can be passed to the --query option to filter results. The --parameters options appears to be relatively new and buggy. The help indicates that multiple space-delimited key-value pairs should be able to be passed, however this appears not to be the case.","title":"Pipelines"},{"location":"Infrastructure/ADO/#agent","text":"An agent represents compute infrastructure with installed agent software. Agents can be Microsoft-hosted (i.e. Azure VMs created specifically for the job and discarded after use) or self-hosted. Agents are organized into pools ; an agent can only belong to a single pool. The agent software package provides several shell scripts that provide various ways of running the agent. run.sh allows manual, interactive execution of the agent software svc.sh allows management of the agent software as a SystemD service. The service itself is named according to the pattern vsts.agent.[ORGANIZATION].[AGENTPOOL].[AGENTNAME].service . config.sh must be run to configure the agent after installation by providing the server URL and PAT token. Self-hosted agent setup Because the agent software itself is based on .NET Core 3.1, some operating systems such as Ubuntu 22.04 are not compatible. Some like CentOS 9 have an unsupported version of OpenSSL installed, which results in the configuration script producing a libssl error. CentOS 9 provides OpenSSL 1.1.1k libraries in a separate package: dnf install compat-openssl11 Also note it appears that the git utility needs to be installed, at least on Red Hat derivatives like CentOS, although it doesn't appear to be explicitly installed by the installdependencies.sh script. This may be because git is assumed to exist on Ubuntu.","title":"Agent"},{"location":"Infrastructure/ADO/#tasks","text":"This pipeline will place a short message in the user directory of the service account running the self-hosted agent. Hello, World! trigger : none jobs : - job : HelloWorld pool : Hyper-V displayName : Hello, World! timeoutInMinutes : 0 steps : - checkout : self - bash : | echo \"Hello, World!\" > ~/hello This example can be developed further to provide the ability to choose between agent pools. Note the mustache syntax used for interpolating this parameter value into the value of jobs.job.pool . trigger : none parameters : - name : pool displayName : Agent pool default : Home values : - Home - Work jobs : - job : HelloWorld pool : ${{ parameters.pool }} displayName : Hello, World! steps : - checkout : self - bash : | echo \"Hello, World!\" > ~/hello A textbox can be created by defining a parameter of type: string . trigger : none parameters : - name : pool displayName : Agent pool default : Home values : - Home - Work - name : name displayName : Name to be greeted type : string default : World jobs : - job : HelloWorld pool : ${{ parameters.pool }} displayName : Hello, World! steps : - checkout : self - bash : | echo \"Hello, ${{ parameters.name }}!\" > ~/hello","title":"Tasks"},{"location":"Infrastructure/ADO/#wiki","text":"ADO offers the opportunity to create wikis for repos (\"codewiki\") or projects (\"projectwiki\"). These can also be done through the CLI . ADO wikis az devops wiki list","title":"Wiki"},{"location":"Infrastructure/Ansible/","text":"Ansible Ansible is an automation tool used for configuration management using human-readable YAML templates. Ansible is distinguished for being agentless , meaning no special software is required on the nodes it manages. Ansible host management relies on an .ini-format inventory file containing a list of IP addresses or hostnames organized in groups. Ansible roles group content in a way that allows it to be shared. Roles also have a highly standardized directory structure. Folders named handlers, tasks, etc. contain those items. A skeleton directory can be created with ansible-galaxy . ansible-galaxy init $ROLENAME Roles can then be called in a playbook: Default values Overridden values - name : Play to create shared folder hosts : ftpservers roles : - vsftpd_server - name : Play to create shared folder hosts : ftpservers roles : - role : vsftpd_server vars : ftp_config_src : vsftpd_special.conf.j2 Projects for learning There are [several areas][https://opensource.com/article/19/8/ops-tasks-ansible] where Ansible can be used in personal projects for learning purposes. Use the users module to manage users, assign groups, and define custom aliases in the profile property. Put a time limit on the availability of the sudo command Use Ansible Tower to produce a GUI interface to restart certain services. Use Ansible Tower to look for files larger than a particular size in a directory. Debug a system performance problem. Variable substitution is done by specifying the name of the placeholder variable and its value under vars as a sibling to tasks and handlers --- - hosts : webservers become : yes vars : package_name : apache2 tasks : - name : this installs a package apt : \"name={{ package_name }} update_cache=yes state=latest\" notify : enable apache handlers : - name : enable apache service : \"name={{ package_name }} enabled=yes state=started\" Conditional logic is [implemented][https://www.linuxjournal.com/content/ansible-part-iii-playbooks] with each task by defining a condition as the value for a property when : --- - hosts : webservers become : yes tasks : - name : install apache this way apt : name=apache2 update_cache=yes state=latest notify : start apache2 when : ansible_os_family == \"Debian\" - name : install apache that way yum : name=httpd state=latest notify : start httpd when : ansible_os_family == \"RedHat\" handlers : - name : start apache2 service : name=apache2 enabled=yes state=started - name : start httpd service : name=httpd enabled=yes state=started Ansible configuration can be done through many files. ansible.cfg defines how privileges are escalated, and they are commonly placed in project directories. /etc/ansible/hosts defines the clients which are to be controlled by the server Tasks Hello, World! --- - name : Hello, World! playbook hosts : all tasks : - name : Hello, World! play command : echo Hello, World! ... # (1) Ad-hoc ansible all -m command -a \"echo Hello, World!\" Configuration ansible.cfg [privilege_escalation] become = yes become_method = sudo become_user = root become_ask_pass = no remote_user = ansible [defaults] interpreter_python = auto_silent deprecation_warnings = False Display non-default settings ansible-config dump --only-changed Files Create file Delete file --- - name : Create file hosts : all tasks : - name : Create file copy : dest : /etc/motd content : \"Hello, World!\\n\" # (1) ... Alma Linux 9 requires an additional package to be installed to handle SELinux contexts: # This package is incorrectly identified as \"libselinux-python\" in the Ansible error displayed on the controller host dnf install python3-libselinux --- - name : Delete file hosts : all tasks : - name : Delete file file : path : /etc/motd state : absent ... Commands All CLI tools use the same set of modules , which expose the same API. ansible Used to run ad-hoc commands from the command-line. Display all available hosts ansible localhost --list-hosts Specify a module in an ad-hoc. ansible all -m shell -a env ansible all -a env # (1) The command module is default and does not have to be made explicit ansible-config ansible-doc # List currently installed modules ansible-doc -l # Get module-specific information ansible-doc $MODULE # Get example code ansible-doc -s $MODULE ansible-galaxy # Log in ansible-galaxy login # Search for roles ansible-galaxy search $ROLE # Install a role made publicly available by a user sudo ansible-galaxy install $USER . $ROLE # Initiate the skeleton structure of a role ansible-galaxy init $ROLENAME # Upload a role ansible-galaxy import $USERNAME $REPONAME ansible-galaxy import --no-wait $USERNAME $REPONAME # send job to background ansible-playbook Verify YAML syntax ansible-playbook --syntax-check $FILE Modules archive Compress files - name : Compress directory /path/to/foo/ into /path/to/foo.tgz archive : path : /path/to/foo dest : /path/to/foo.tgz - name : Create a bz2 archive of multiple files, rooted at /path archive : path : - /path/to/foo - /path/wong/foo dest : /path/file.tar.bz2 format : bz2 cli_config Set hostname for a switch and exit with a commit message - name : commit with comment cli_config : config : set system host-name foo commit_comment : this is a test Back up a config to a different destination file - name : configurable backup path cli_config : config : \"{{ lookup('template', 'basic/config.j2') }}\" backup : yes backup_options : filename : backup.cfg dir_path : /home/user Platform-agnostic way of pushing text-based configurations to network devices over the network_cli_connection plugin. command Safest module to execute a command and arguments on client machine, requires Python. When Ansible execute commands using the Command module, they are not processed through the user's shell (meaning environment variables like $HOME and output redirection are not available). Takes command name followed by a list of space-delimited arguments. ansible all -i inventory -m command -a 'cat /etc/motd' -u ansible - name : return motd to registered var command : cat /etc/motd register : mymotd - name : Change the working directory to somedir/ and run the command as db_owner if /path/to/database does not exist. command : /usr/bin/make_database.sh db_user db_name become : yes become_user : db_owner args : chdir : somedir/ creates : /path/to/database copy Copy files from the server to nodes (cf. file ). Useful when updating configuration files. Place a text message into a file through an ad-hoc command ansible all -i inventory -m copy -a 'content=\"Managed by Ansible\\n\" dest=/etc/motd' -b -u ansible Copy updated.conf from the control node to each client. ansible $CLIENT -b -m copy -a \"src=./updated.conf dest=/etc/ntp.conf owner=root group=root mode=0644 backup=yes\" - name : Copy a new \"ntp.conf file into place, backing up the original if it differs from the copied version copy : src : /mine/ntp.conf dest : /etc/ntp.conf owner : root group : root mode : '0644' backup : yes - name : Copy file with owner and permission, using symbolic representation copy : src : /srv/myfiles/foo.conf dest : /etc/foo.conf owner : foo group : foo mode : u=rw,g=r,o=r debug - name : Display all variables/facts known for a host debug : var : hostvars[inventory_hostname] verbosity : 4 Display content of copy module only when verbosity of 2 is specified (i.e. ansible-playbook demo.yaml -vv ) opensource.com - name : Write some content in a file /tmp/foo.txt copy : dest : /tmp/foo.txt content : | Good Morning! Awesome sunshine today. register : display_file_content - name : Debug display_file_content debug : var : display_file_content verbosity : 2 file Used for doing file manipulation on the remote system itself. The state directive can take one of several values, and indicates to Ansible what should actually be done to the target file: absent delete file or directory recursively directory create directory hard create hardlink link create symlink touch create empty file Change a file's attributes - name : Change file ownership, group and permissions file : path : /etc/foo.conf owner : foo group : foo mode : '0644' Create a symlink ansible $CLIENT -b -m file -a \"src=/etc/ntp.conf dest=/home/user/ntp.conf owner=user group=user state=link Create a folder using an ad hoc command ansible $CLIENT -b -m file -a \"path=/etc/newfolder state=directory mode=0755\" Create a folder using a playbook - name : Create a directory if it does not exist file : path : /etc/foo state : directory mode : '0755' git Manage git checkouts of repos Create git archive from repo - git : repo : https://github.com/ansible/ansible-examples.git dest : /src/ansible-examples archive : /tmp/ansible-examples.zip - git : repo : https://github.com/ansible/ansible-examples.git dest : /src/ansible-examples separate_git_dir : /src/ansible-examples.git template Works similar to mail merge in a word processor. Ansible uses the Jinja2 templating language, which has a syntax similar to Ansible variable substitution. This example creates a HTML document on each client that is customized using Ansible variables. --- - hosts : webservers become : yes tasks : - name : install apache2 apt : name=apache2 state=latest update_cache=yes when : ansible_os_family == \"Debian\" - name : install httpd yum : name=httpd state=latest when : ansible_os_family == \"RedHat\" - name : start apache2 service : name=apache2 state=started enable=yes when : ansible_os_family == \"Debian\" - name : start httpd service : name=httpd state=started enable=yes when : ansible_os_family == \"RedHat - name : install index template : src : index.html.j2 dest : /var/www/html/index.html Jinja template file < html > < h1 > This computer is running {{ ansible_os_family }}, and its hostname is: </ h1 > < h3 > {{ ansible_hostname }} </ h3 > {# this is a comment, which won't be copied to the index.html file #} </ html > Glossary Ad Hoc Type of command run in realtime by an administrator working at the terminal Ansible Galaxy Online portal where a gallery of roles made by the Ansible community can be found Ansible Tower Web-based RESTful API endpoint that provides the officially supported GUI frontend to Ansible configuration management. A commercial product that is available in two versions: Standard : $13,000/yr Premium : $17,500/yr Ansible Vault Place to keep encrypted passwords AWX Open-source project upon which Ansible Tower was built Fact System property gathered by Ansible when it executes a playbook on a node Handler Handlers are tasks that are executed when notified by a task. They are only run once, and only if the notifying task has made a change to the system. Here, Enable Apache will be called if Install Apache makes a change. If apache2 is already installed, the handler is not called. --- - hosts : webservers become : yes tasks : - name : Install Apache apt : name=apache2 update_cache=yes state=latest notify : enable apache handlers : - name : Enable Apache service : name=apache2 enabled=yes state=started Inventory Text file containing a list of servers or nodes that you are managing and configuring Module Modules are standalone scripts that enable a particular task across many OSes, services, applications, etc. Predefined modules are available in the module library , and new ones can be defined via Python or JSON. Play Script or instruction that defines the task to be carried out in a server Playbook Ansible can be used in one of two ways: Running ad hoc commands, executed in realtime by an administrator working at the terminal using the ansible command Running playbooks , YAML documents that represent a sequence of scripted actions which apply changes uniformly over a set of hosts, using the ansible-playbook command. A playbook is a YAML document that represents a sequence of scripted actions called tasks which apply changes uniformly over a set of hosts. Any ad hoc command can be rewritten as a playbook, but some modules can only be used effectively as playbooks. Playbooks can be written in two styles: Using \" k=v \" syntax, where keys and values are defined inline Using proper YAML syntax k=v YAML tasks : - name : Install htop dnf : name=htop state=latest tasks : - name : Install htop dnf : name : htop state : latest Role Organize components of playbooks, allowing them to be reused Task A single scripted action in a playbook, equivalent to an ad hoc command Vault Feature of Ansible that allows you to keep sensitive data such as passwords or keys protected at rest, rather than as plaintext in playbooks or roles Two types of vaulted content: Vaulted files , where the full file, which can contain Ansible variables or other content, is encrypted Single encrypted variables , where only specific variables within a normal \"variable file\" are encrypted. Tasks Setup An Ansible service is account is created on each node and given the ability to sudo any command without a password. useradd ansible passwd ansible /etc/sudoers.d/ansible ansible ALL =( ALL ) NOPASSWD: ALL su - ansible ssh-keygen httpd --- - name : Install httpd hosts : all tasks : - name : Install ansible.builtin.dnf : name : httpd state : latest - name : Enable service ansible.builtin.systemd : name : httpd enabled : true state : started - name : Open firewall ansible.posix.firewalld : immediate : true permanent : true service : http state : enabled ...","title":"Ansible"},{"location":"Infrastructure/Ansible/#ansible","text":"Ansible is an automation tool used for configuration management using human-readable YAML templates. Ansible is distinguished for being agentless , meaning no special software is required on the nodes it manages. Ansible host management relies on an .ini-format inventory file containing a list of IP addresses or hostnames organized in groups. Ansible roles group content in a way that allows it to be shared. Roles also have a highly standardized directory structure. Folders named handlers, tasks, etc. contain those items. A skeleton directory can be created with ansible-galaxy . ansible-galaxy init $ROLENAME Roles can then be called in a playbook: Default values Overridden values - name : Play to create shared folder hosts : ftpservers roles : - vsftpd_server - name : Play to create shared folder hosts : ftpservers roles : - role : vsftpd_server vars : ftp_config_src : vsftpd_special.conf.j2 Projects for learning There are [several areas][https://opensource.com/article/19/8/ops-tasks-ansible] where Ansible can be used in personal projects for learning purposes. Use the users module to manage users, assign groups, and define custom aliases in the profile property. Put a time limit on the availability of the sudo command Use Ansible Tower to produce a GUI interface to restart certain services. Use Ansible Tower to look for files larger than a particular size in a directory. Debug a system performance problem. Variable substitution is done by specifying the name of the placeholder variable and its value under vars as a sibling to tasks and handlers --- - hosts : webservers become : yes vars : package_name : apache2 tasks : - name : this installs a package apt : \"name={{ package_name }} update_cache=yes state=latest\" notify : enable apache handlers : - name : enable apache service : \"name={{ package_name }} enabled=yes state=started\" Conditional logic is [implemented][https://www.linuxjournal.com/content/ansible-part-iii-playbooks] with each task by defining a condition as the value for a property when : --- - hosts : webservers become : yes tasks : - name : install apache this way apt : name=apache2 update_cache=yes state=latest notify : start apache2 when : ansible_os_family == \"Debian\" - name : install apache that way yum : name=httpd state=latest notify : start httpd when : ansible_os_family == \"RedHat\" handlers : - name : start apache2 service : name=apache2 enabled=yes state=started - name : start httpd service : name=httpd enabled=yes state=started Ansible configuration can be done through many files. ansible.cfg defines how privileges are escalated, and they are commonly placed in project directories. /etc/ansible/hosts defines the clients which are to be controlled by the server","title":"Ansible"},{"location":"Infrastructure/Ansible/#tasks","text":"Hello, World! --- - name : Hello, World! playbook hosts : all tasks : - name : Hello, World! play command : echo Hello, World! ... # (1) Ad-hoc ansible all -m command -a \"echo Hello, World!\" Configuration ansible.cfg [privilege_escalation] become = yes become_method = sudo become_user = root become_ask_pass = no remote_user = ansible [defaults] interpreter_python = auto_silent deprecation_warnings = False Display non-default settings ansible-config dump --only-changed Files Create file Delete file --- - name : Create file hosts : all tasks : - name : Create file copy : dest : /etc/motd content : \"Hello, World!\\n\" # (1) ... Alma Linux 9 requires an additional package to be installed to handle SELinux contexts: # This package is incorrectly identified as \"libselinux-python\" in the Ansible error displayed on the controller host dnf install python3-libselinux --- - name : Delete file hosts : all tasks : - name : Delete file file : path : /etc/motd state : absent ...","title":"Tasks"},{"location":"Infrastructure/Ansible/#commands","text":"All CLI tools use the same set of modules , which expose the same API.","title":"Commands"},{"location":"Infrastructure/Ansible/#ansible_1","text":"Used to run ad-hoc commands from the command-line. Display all available hosts ansible localhost --list-hosts Specify a module in an ad-hoc. ansible all -m shell -a env ansible all -a env # (1) The command module is default and does not have to be made explicit","title":"ansible"},{"location":"Infrastructure/Ansible/#ansible-config","text":"","title":"ansible-config"},{"location":"Infrastructure/Ansible/#ansible-doc","text":"# List currently installed modules ansible-doc -l # Get module-specific information ansible-doc $MODULE # Get example code ansible-doc -s $MODULE","title":"ansible-doc"},{"location":"Infrastructure/Ansible/#ansible-galaxy","text":"# Log in ansible-galaxy login # Search for roles ansible-galaxy search $ROLE # Install a role made publicly available by a user sudo ansible-galaxy install $USER . $ROLE # Initiate the skeleton structure of a role ansible-galaxy init $ROLENAME # Upload a role ansible-galaxy import $USERNAME $REPONAME ansible-galaxy import --no-wait $USERNAME $REPONAME # send job to background","title":"ansible-galaxy"},{"location":"Infrastructure/Ansible/#ansible-playbook","text":"Verify YAML syntax ansible-playbook --syntax-check $FILE","title":"ansible-playbook"},{"location":"Infrastructure/Ansible/#modules","text":"","title":"Modules"},{"location":"Infrastructure/Ansible/#archive","text":"Compress files - name : Compress directory /path/to/foo/ into /path/to/foo.tgz archive : path : /path/to/foo dest : /path/to/foo.tgz - name : Create a bz2 archive of multiple files, rooted at /path archive : path : - /path/to/foo - /path/wong/foo dest : /path/file.tar.bz2 format : bz2","title":"archive"},{"location":"Infrastructure/Ansible/#cli_config","text":"Set hostname for a switch and exit with a commit message - name : commit with comment cli_config : config : set system host-name foo commit_comment : this is a test Back up a config to a different destination file - name : configurable backup path cli_config : config : \"{{ lookup('template', 'basic/config.j2') }}\" backup : yes backup_options : filename : backup.cfg dir_path : /home/user Platform-agnostic way of pushing text-based configurations to network devices over the network_cli_connection plugin.","title":"cli_config"},{"location":"Infrastructure/Ansible/#command","text":"Safest module to execute a command and arguments on client machine, requires Python. When Ansible execute commands using the Command module, they are not processed through the user's shell (meaning environment variables like $HOME and output redirection are not available). Takes command name followed by a list of space-delimited arguments. ansible all -i inventory -m command -a 'cat /etc/motd' -u ansible - name : return motd to registered var command : cat /etc/motd register : mymotd - name : Change the working directory to somedir/ and run the command as db_owner if /path/to/database does not exist. command : /usr/bin/make_database.sh db_user db_name become : yes become_user : db_owner args : chdir : somedir/ creates : /path/to/database","title":"command"},{"location":"Infrastructure/Ansible/#copy","text":"Copy files from the server to nodes (cf. file ). Useful when updating configuration files. Place a text message into a file through an ad-hoc command ansible all -i inventory -m copy -a 'content=\"Managed by Ansible\\n\" dest=/etc/motd' -b -u ansible Copy updated.conf from the control node to each client. ansible $CLIENT -b -m copy -a \"src=./updated.conf dest=/etc/ntp.conf owner=root group=root mode=0644 backup=yes\" - name : Copy a new \"ntp.conf file into place, backing up the original if it differs from the copied version copy : src : /mine/ntp.conf dest : /etc/ntp.conf owner : root group : root mode : '0644' backup : yes - name : Copy file with owner and permission, using symbolic representation copy : src : /srv/myfiles/foo.conf dest : /etc/foo.conf owner : foo group : foo mode : u=rw,g=r,o=r","title":"copy"},{"location":"Infrastructure/Ansible/#debug","text":"- name : Display all variables/facts known for a host debug : var : hostvars[inventory_hostname] verbosity : 4 Display content of copy module only when verbosity of 2 is specified (i.e. ansible-playbook demo.yaml -vv ) opensource.com - name : Write some content in a file /tmp/foo.txt copy : dest : /tmp/foo.txt content : | Good Morning! Awesome sunshine today. register : display_file_content - name : Debug display_file_content debug : var : display_file_content verbosity : 2","title":"debug"},{"location":"Infrastructure/Ansible/#file","text":"Used for doing file manipulation on the remote system itself. The state directive can take one of several values, and indicates to Ansible what should actually be done to the target file: absent delete file or directory recursively directory create directory hard create hardlink link create symlink touch create empty file Change a file's attributes - name : Change file ownership, group and permissions file : path : /etc/foo.conf owner : foo group : foo mode : '0644' Create a symlink ansible $CLIENT -b -m file -a \"src=/etc/ntp.conf dest=/home/user/ntp.conf owner=user group=user state=link Create a folder using an ad hoc command ansible $CLIENT -b -m file -a \"path=/etc/newfolder state=directory mode=0755\" Create a folder using a playbook - name : Create a directory if it does not exist file : path : /etc/foo state : directory mode : '0755'","title":"file"},{"location":"Infrastructure/Ansible/#git","text":"Manage git checkouts of repos Create git archive from repo - git : repo : https://github.com/ansible/ansible-examples.git dest : /src/ansible-examples archive : /tmp/ansible-examples.zip - git : repo : https://github.com/ansible/ansible-examples.git dest : /src/ansible-examples separate_git_dir : /src/ansible-examples.git","title":"git"},{"location":"Infrastructure/Ansible/#template","text":"Works similar to mail merge in a word processor. Ansible uses the Jinja2 templating language, which has a syntax similar to Ansible variable substitution. This example creates a HTML document on each client that is customized using Ansible variables. --- - hosts : webservers become : yes tasks : - name : install apache2 apt : name=apache2 state=latest update_cache=yes when : ansible_os_family == \"Debian\" - name : install httpd yum : name=httpd state=latest when : ansible_os_family == \"RedHat\" - name : start apache2 service : name=apache2 state=started enable=yes when : ansible_os_family == \"Debian\" - name : start httpd service : name=httpd state=started enable=yes when : ansible_os_family == \"RedHat - name : install index template : src : index.html.j2 dest : /var/www/html/index.html Jinja template file < html > < h1 > This computer is running {{ ansible_os_family }}, and its hostname is: </ h1 > < h3 > {{ ansible_hostname }} </ h3 > {# this is a comment, which won't be copied to the index.html file #} </ html >","title":"template"},{"location":"Infrastructure/Ansible/#glossary","text":"Ad Hoc Type of command run in realtime by an administrator working at the terminal Ansible Galaxy Online portal where a gallery of roles made by the Ansible community can be found Ansible Tower Web-based RESTful API endpoint that provides the officially supported GUI frontend to Ansible configuration management. A commercial product that is available in two versions: Standard : $13,000/yr Premium : $17,500/yr Ansible Vault Place to keep encrypted passwords AWX Open-source project upon which Ansible Tower was built Fact System property gathered by Ansible when it executes a playbook on a node","title":"Glossary"},{"location":"Infrastructure/Ansible/#handler","text":"Handlers are tasks that are executed when notified by a task. They are only run once, and only if the notifying task has made a change to the system. Here, Enable Apache will be called if Install Apache makes a change. If apache2 is already installed, the handler is not called. --- - hosts : webservers become : yes tasks : - name : Install Apache apt : name=apache2 update_cache=yes state=latest notify : enable apache handlers : - name : Enable Apache service : name=apache2 enabled=yes state=started Inventory Text file containing a list of servers or nodes that you are managing and configuring","title":"Handler"},{"location":"Infrastructure/Ansible/#module","text":"Modules are standalone scripts that enable a particular task across many OSes, services, applications, etc. Predefined modules are available in the module library , and new ones can be defined via Python or JSON. Play Script or instruction that defines the task to be carried out in a server","title":"Module"},{"location":"Infrastructure/Ansible/#playbook","text":"Ansible can be used in one of two ways: Running ad hoc commands, executed in realtime by an administrator working at the terminal using the ansible command Running playbooks , YAML documents that represent a sequence of scripted actions which apply changes uniformly over a set of hosts, using the ansible-playbook command. A playbook is a YAML document that represents a sequence of scripted actions called tasks which apply changes uniformly over a set of hosts. Any ad hoc command can be rewritten as a playbook, but some modules can only be used effectively as playbooks. Playbooks can be written in two styles: Using \" k=v \" syntax, where keys and values are defined inline Using proper YAML syntax k=v YAML tasks : - name : Install htop dnf : name=htop state=latest tasks : - name : Install htop dnf : name : htop state : latest Role Organize components of playbooks, allowing them to be reused Task A single scripted action in a playbook, equivalent to an ad hoc command Vault Feature of Ansible that allows you to keep sensitive data such as passwords or keys protected at rest, rather than as plaintext in playbooks or roles Two types of vaulted content: Vaulted files , where the full file, which can contain Ansible variables or other content, is encrypted Single encrypted variables , where only specific variables within a normal \"variable file\" are encrypted.","title":"Playbook"},{"location":"Infrastructure/Ansible/#tasks_1","text":"","title":"Tasks"},{"location":"Infrastructure/Ansible/#setup","text":"An Ansible service is account is created on each node and given the ability to sudo any command without a password. useradd ansible passwd ansible /etc/sudoers.d/ansible ansible ALL =( ALL ) NOPASSWD: ALL su - ansible ssh-keygen","title":"Setup"},{"location":"Infrastructure/Ansible/#httpd","text":"--- - name : Install httpd hosts : all tasks : - name : Install ansible.builtin.dnf : name : httpd state : latest - name : Enable service ansible.builtin.systemd : name : httpd enabled : true state : started - name : Open firewall ansible.posix.firewalld : immediate : true permanent : true service : http state : enabled ...","title":"httpd"},{"location":"Infrastructure/Certifications/","text":"\ud83e\udd47 Certifications Microsoft Programming Exam Name Links 70-483 \u271d Programming in C# :material-file-pdf: :material-amazon: 70-484 \u271d Essentials of developing Windows Store apps in C# :material-amazon: 70-485 \u271d Advanced Windows Store app development using C# :material-amazon: Azure Administrator Associate Exams: AZ-104 Azure AI Fundamentals Exams: AI-900 Azure AI Engineer Exams: AI-100 \u271d AI-102 Azure Database Administrator Associate Exams: DP-300 Azure Data Fundamentals Exams: DP-900 Azure Data Engineer Associate Exams: DP-200 DP-201 Azure Data Scientist Associate Exams: DP-100 Azure Developer Associate Exams: MS-600 Azure Fundamentals Exams: AZ-900 Azure Security Engineer Associate Exams: AZ-500 Azure Solutions Architect Expert Exams: AZ-303 AZ-304 Azure Stack Hub Operator Associate Exams: AZ-600 Data Analyst Associate Exams: DA-100 DevOps Engineer Expert Exams: AZ-400 Identity and Access Administrator Associate Exams: SC-300 Information Protection Administrator Associate Exams: SC-400 Microsoft 365 Developer Associate Exams: MS-600 Microsoft 365 Enterprise Administrator Expert Exams: MS-100 MS-101 Microsoft 365 Fundamentals Exams: MS-900 Microsoft 365 Messaging Administrator Associate Exams: MS-203 Microsoft 365 Modern Desktop Administrator Exams: MD-100 MD-100 Microsoft 365 Security Administrator Associate Exams: MS-500 Microsoft 365 Teams Administrator Associate Exams: MS-700 Power Platform Fundamentals Exams: PL-900 Power Platform App Maker Associate Exams: PL-100 Power Platform Developer Associate Exams: PL-400 Power Platform Functional Consultant Associate Exams: PL-200 Power Platform Solution Architect Expert Exams: PL-600 Security, Compliance, and Identity Fundamentals Exams: SC-900 Security Operations Analyst Associate Exams: SC-200","title":"\ud83e\udd47 Certifications"},{"location":"Infrastructure/Certifications/#certifications","text":"","title":"\ud83e\udd47 Certifications"},{"location":"Infrastructure/Certifications/#microsoft","text":"","title":"Microsoft"},{"location":"Infrastructure/Certifications/#programming","text":"Exam Name Links 70-483 \u271d Programming in C# :material-file-pdf: :material-amazon: 70-484 \u271d Essentials of developing Windows Store apps in C# :material-amazon: 70-485 \u271d Advanced Windows Store app development using C# :material-amazon:","title":"Programming"},{"location":"Infrastructure/Certifications/#azure-administrator-associate","text":"Exams: AZ-104","title":"Azure Administrator Associate"},{"location":"Infrastructure/Certifications/#azure-ai-fundamentals","text":"Exams: AI-900","title":"Azure AI Fundamentals"},{"location":"Infrastructure/Certifications/#azure-ai-engineer","text":"Exams: AI-100 \u271d AI-102","title":"Azure AI Engineer"},{"location":"Infrastructure/Certifications/#azure-database-administrator-associate","text":"Exams: DP-300","title":"Azure Database Administrator Associate"},{"location":"Infrastructure/Certifications/#azure-data-fundamentals","text":"Exams: DP-900","title":"Azure Data Fundamentals"},{"location":"Infrastructure/Certifications/#azure-data-engineer-associate","text":"Exams: DP-200 DP-201","title":"Azure Data Engineer Associate"},{"location":"Infrastructure/Certifications/#azure-data-scientist-associate","text":"Exams: DP-100","title":"Azure Data Scientist Associate"},{"location":"Infrastructure/Certifications/#azure-developer-associate","text":"Exams: MS-600","title":"Azure Developer Associate"},{"location":"Infrastructure/Certifications/#azure-fundamentals","text":"Exams: AZ-900","title":"Azure Fundamentals"},{"location":"Infrastructure/Certifications/#azure-security-engineer-associate","text":"Exams: AZ-500","title":"Azure Security Engineer Associate"},{"location":"Infrastructure/Certifications/#azure-solutions-architect-expert","text":"Exams: AZ-303 AZ-304","title":"Azure Solutions Architect Expert"},{"location":"Infrastructure/Certifications/#azure-stack-hub-operator-associate","text":"Exams: AZ-600","title":"Azure Stack Hub Operator Associate"},{"location":"Infrastructure/Certifications/#data-analyst-associate","text":"Exams: DA-100","title":"Data Analyst Associate"},{"location":"Infrastructure/Certifications/#devops-engineer-expert","text":"Exams: AZ-400","title":"DevOps Engineer Expert"},{"location":"Infrastructure/Certifications/#identity-and-access-administrator-associate","text":"Exams: SC-300","title":"Identity and Access Administrator Associate"},{"location":"Infrastructure/Certifications/#information-protection-administrator-associate","text":"Exams: SC-400","title":"Information Protection Administrator Associate"},{"location":"Infrastructure/Certifications/#microsoft-365-developer-associate","text":"Exams: MS-600","title":"Microsoft 365 Developer Associate"},{"location":"Infrastructure/Certifications/#microsoft-365-enterprise-administrator-expert","text":"Exams: MS-100 MS-101","title":"Microsoft 365 Enterprise Administrator Expert"},{"location":"Infrastructure/Certifications/#microsoft-365-fundamentals","text":"Exams: MS-900","title":"Microsoft 365 Fundamentals"},{"location":"Infrastructure/Certifications/#microsoft-365-messaging-administrator-associate","text":"Exams: MS-203","title":"Microsoft 365 Messaging Administrator Associate"},{"location":"Infrastructure/Certifications/#microsoft-365-modern-desktop-administrator","text":"Exams: MD-100 MD-100","title":"Microsoft 365 Modern Desktop Administrator"},{"location":"Infrastructure/Certifications/#microsoft-365-security-administrator-associate","text":"Exams: MS-500","title":"Microsoft 365 Security Administrator Associate"},{"location":"Infrastructure/Certifications/#microsoft-365-teams-administrator-associate","text":"Exams: MS-700","title":"Microsoft 365 Teams Administrator Associate"},{"location":"Infrastructure/Certifications/#power-platform-fundamentals","text":"Exams: PL-900","title":"Power Platform Fundamentals"},{"location":"Infrastructure/Certifications/#power-platform-app-maker-associate","text":"Exams: PL-100","title":"Power Platform App Maker Associate"},{"location":"Infrastructure/Certifications/#power-platform-developer-associate","text":"Exams: PL-400","title":"Power Platform Developer Associate"},{"location":"Infrastructure/Certifications/#power-platform-functional-consultant-associate","text":"Exams: PL-200","title":"Power Platform Functional Consultant Associate"},{"location":"Infrastructure/Certifications/#power-platform-solution-architect-expert","text":"Exams: PL-600","title":"Power Platform Solution Architect Expert"},{"location":"Infrastructure/Certifications/#security-compliance-and-identity-fundamentals","text":"Exams: SC-900","title":"Security, Compliance, and Identity Fundamentals"},{"location":"Infrastructure/Certifications/#security-operations-analyst-associate","text":"Exams: SC-200","title":"Security Operations Analyst Associate"},{"location":"Infrastructure/Cloud/","text":"\u2601\ufe0f Cloud Compute Containers :fontawesome-solid-save: Storage Network Development Big Data IaaS Azure VMs EC2 Compute Engine PaaS App Service Elastic Beanstalk App Engine Serverless Functions Lambda Cloud Functions Cloud Run Individual containers ACI ECS Kubernetes AKS EKS GKE Container registry Artifact Registry Archive Glacier Backups Recovery Services Vault Backup Physical media Data Box Import/Export Service Snowball Transfer Appliance Private networks VNets VPC VPC Security rules Network Security Group (NSG) Security Group Firewall Rules DNS Azure DNS Route 53 Cloud DNS NoSQL Cosmos DB DynamoDB DocumentDB Firestore Spanner CI/CD Azure Devops CodeBuild CodeCommit CodeDeploy CodePipeline Cloud Build Messaging SNS Pub/Sub Computer Vision Computer Vision Rekognition Cloud Vision Big Data Data Lake Store Redshift Athena BigQuery BigTable Dataprep Streaming data Event Hubs Service Bus Stream Analytics Kinesis Athena DataFlow Batch processing HDInsight Batch EMR Batch DataFlow Dataproc Links TODO: Cloud Storage CloudWatch Azure Functions Glacier Google Cloud Storage (GCS) gcloud gsutil Simple Notification Service \ud83d\udee0\ufe0f Administration \ud83d\udcb0 Cost management Azure quotas apply to subscriptions and are implemented with tags . Resource quotas trigger alarms when resource creation and consumption hit a threshold. These are not to be confused with resource limits which can stop resources from being created, whereas quotas can not. Spending quotas trigger alarms when spending has reached a threshold. Azure budgets can be viewed and administered in the Cost Management + Billing blade. Users must have at least the Reader role at the subscription scope to view, and Contributor to create and manage, budgets. Resource locks are used to apply restrictions across all users and roles and can be applied at subscription, resource group, or resource scopes. CanNotDelete ReadOnly effectively restricts all authorized users to the permissions granted by the Reader role Storage account keys of a locked storage account cannot be listed because the list keys operation is handled through a POST request Visual Studio Server Explorer will not be able to display files for a locked App Service resource, because that interaction requires write access VMs in a locked resource group will not be able to be started or restarted, because those operations require a POST request All child resources of the scope at which a lock is applied inherit the lock. A CanNotDelete lock applied to a DNS A record would also prevent the deletion of the DNS zone that the record resides in, as well as the resource group the zone resides in. Of the builtin roles , only two have access to the Microsoft.Authorization/* or Microsoft.Authorization/locks/* actions required to create or delete locks: Owner User Access Administrator Resource locks apply to the management plane of Azure, specifically operations sent to https://management.azure.com Managed applications create two resource groups to implement locks: One resource group to contain an overview of the service, which isn't locked Another resource group containing the infrastructure for the service, which is locked Sources: Move resources to a new resource group or subscription Some services have limitations or requirements when moving resources between groups ( src ) Source and destination subscriptions must be within the same [AAD][Azure AD] tenant Destination subscription must be registered for the resource provider of the resource being moved Account moving the resources must have at least the following permissions: Microsoft.Resources/subscriptions/resourceGroups/moveResources/action Microsoft.Resources/subscriptions/resourceGroups/write IAM All cloud providers offer IAM systems that are used to control access to resources, all of which establish a similar taxonomy of concepts. The type of user that is granted access to resources is referred to variously as a member or a security principal . Bundles of specific permissions that can be assigned to users are called roles . All cloud providers offer the ability to define custom roles and come with many ready-to-use role definitions: predefined roles or built-in roles . Roles form the basis of RBAC , which is the recommended model used by all cloud providers. Roles are associated to users by policies and role assignments . gcloud gcloud projects get-iam-policy $project The Cloud providers also still support legacy IAM systems which are deprecated. Classic administrator roles included Account Administrator, Service Administrator and Co-Administrator Primitive roles included Owner, Editor, and Viewer can still be applied to most GCP resources. Infrastructure All cloud providers divide their global services into a hierarchy of geographically defined regions , each of which is in turn divided into availability zones (what AWS calls its Global Infrastructure ). Azure datacenters contain multiple availability zones, and every Azure region has at least three availability zones. Azure services are also divided into geographies , generally coterminous with countries. Azure geographies are further divided into regional pairs . Each regional pair receives rolling updates one member at a time. Most services are regionally based, meaning the underlying hardware of that service's instance will exist in only a single Region. Some regions, like AWS GovCloud , have restricted access. Some AWS resources, however, are technically running on hardware that exists in a single Region, but presented as global. GCP regions Resources Services available on Free Tier \ud83d\udc41\ufe0f Monitoring Azure Monitor Network Watcher CloudWatch Stackdriver Trace Resources Cloud providers exhibit some variety in how resources can be organized. All cloud providers support key-value tags , many of which can be applied to the same resource. Any Azure resource can only exist in a single resource group , which can contain resources from any region or subscription. However, resource groups may not contain other resource groups. GCP projects are equivalent to Azure resource groups, in that they are containers for and direct parents to resources. However, projects can be placed within folders , which do support nested hierarchies. AWS does not have an equivalent method of organizing resources. Azure subscriptions can be organized into Management Groups , and they can be nested in a hierarchy of management groups up to a maximum depth of six levels. In AWS the Organizational Unit (OU) , which can organize user accounts (subscriptions) and the resources they contain in a nested hierarchy, appears to be equivalent. A pattern common to Azure is that of a service being implemented in two resource types, one of which determines important configuration settings shared by all instances of the service which are contained within it. This is the case for storage accounts , App Service , Azure Data Explorer clusters, etc. Description Tenant Organization Organization Corresponds to a company or organization Management group Organizational Unit Logical container for user accounts and the resources created by that user Subscription Member account ? Credential associated with an individual Folder Organize resources and their parents in a nested hierarchy Resource group Project Logical container that is the direct parent to any resource, tied to a Region Tag Tag Label Key-value pairs that are used to organize resources The resource hierarchy organizes GCP resources in 3 levels below Domain Domain Organization corresponds to a company or organization. A single cloud identity is associated with a single organization and can have super admins Billing Account tracks charges and billing account admins can set budgets. Payments Profile is a Google-level resource that is used to pay for all Google services. \ud83d\udee0\ufe0f Support AWS offers various support plan tiers that provide 24/7 email, chat, and phone access to AWS cloud support engineers. Basic Support Plan Developer Support Plan (greater of $29 or 3% of monthly account usage) Business Support Plan Enterprise Support Plan (>$15,000/mo.) offers a Technical Account Manager (TAM) , a dedicated guide and advocate AWS documentation is available in several places: AWS documentation AWS Knowledge Center is a sprawling FAQ AWS security resources AWS forums Professional Services team makes white papers and webinars publicly available Tags Azure tags: Tag names have a limit of 512 characters (128 characters for storage accounts) Tag values have a limit of 256 characters. Resources and resource groups are limited to 15 tags. VMs cannot exceed 2048 characters for all tag names and values combined. Infrastructure as Code All cloud providers support ways of provisioning resources declaratively. Azure [ARM][ARM] templates are JSON, but [Bicep][Bicep] is a domain-specific language and command-line utility that can be used to generate templates from simpler, YAML-like syntax. \ud83d\udda5\ufe0f Compute IaaS All cloud providers offer Infrastructure as a Service (IaaS) , whereby virtual machines can be provisioned with specific compute resources and base operating systems. AWS also offers configuration management services like [OpsWorks][OpsWorks] and [Systems Manager][Systems Manager] GCP virtual machines are referred to as instances , and are available in three general machine family types: general-purpose, memory-optimized, and compute-optimized. Machine type describes the different packaged configurations representing allocated compute resources, or what is called a SKU in Azure. Containers Build and package container artifacts Private container registry Serverless Storage Archive Backups Azure Backup are integrated into Portal and clickable from the VM blade. You have to specify a Recovery Services vault and a Backup policy . The policy can specify frequency of backups, and other settings. Using Backup service costs $10 per VM plus the cost of used storage. 2 methods to restore data after backing up a VM to Azure Backup: Restore a recovery point as a new VM Restore access to files only Physical media Data Box Import/Export Service Snowball Transfer Appliance Uploading files to GCS \ud83c\udfe2 Networking All cloud providers offer an implementation of software-defined networking (SDN) that allows a logically isolated network to be defined as a block of IP addresses allocated from one of the private ranges (10.0.0.0/8, 192.168.0.0/16, or 172.16.0.0/12), what in AWS and GCP is referred to as a VPC and in Azure a VNet . In all providers, the network is confined to a single region and must have at least one IP segment called a subnet defined within it which must be a subset of the range used to define the virtual network itself. The smallest possible CIDR range for a subnet in Azure is 29, which provides 3 addresses for use (Azure reserves 5). In AWS, the smallest possible CIDR range is 28. In AWS, VPCs have a default range of 172.31.0.0/16 and subnets have a default subnet mask of /20. In Azure, subnets span Availability Zones, can only be deleted if empty, and their names, which are immutable, must be unique. In AWS, a subnet exists only within a single Availability Zone. VNet peering allows VMs in two separate virtual networks to communicate directly. In all cloud providers, this is a one-way process which must be repeated in both directions in order to have two-way communication. In Azure, before the introduction of peering, virtual networks were connected using S2S VPN or by connecting to the same ExpressRoute circuit. It is not required for the peered networks to be in the same region ( Global VNet peering ), subscription, or tenant, although cross-tenant peering is not available in the Portal but must be configured from the command-line or ARM templates. VNet peering has to be disabled before moving a VNet , and a VNet can only be moved within the same subscription. There is a maximum of 100 peering connections per VNet Peerings cannot be moved to another resource group or subscription, so they must be disabled before moving peered VNets. Service endpoints facilitate restricting traffic from Azure services. Service endpoint policies allow restricting traffic to the granularity of individual Azure service instances. An internet gateway is a VPC resource that allows EC2 instances to obtain a public IP address and access the Internet. In order to access the Internet, instances must be in a public subnet , one that contains a default route to the VPC 's internet gateway. ExpressRoute is the main service used to connect Azure to on-premises networks, although P2S and S2S VPNs are also options. Direct Connect provides dedicated network connectivity to an AWS VPC through links offered through APN partners. In GCP, in addition to peering, a shared VPC can be created that is associated with multiple projects. Resources: Migrating to GCP? First Things First: VPCs User-defined routes In Azure, a virtual appliance refers to a VM running a network application like a load-balancer, firewall, or router. Service chaining refers to the process of deploying a network virtual appliance (NVA) into a hub network to route traffic between spokes using user-defined routes (UDR) . This is a method of reducing the complexity of pairing between individual spoke networks in complex hub-and-spoke architectures. AZ-103: 309 In such a deployment, the peerings must be set to Allow Forwarded Traffic . Alternatively, two peered networks can share a single virtual network gateway, say to connect to an external network. The pairing connection to the network that contains the gateway must be set to Use Remote Gateways The pairing connection from the network containing the gateway must be set to Allow Gateway Transit Network security Network Security Group (NSG) Security Group Firewall Rules Azure Network Security Groups (NSGs) are associated with network interfaces and contain an arbitrary number of security rules . Each rule has the following properties: Name Priority : number between 100 and 4096, lower numbers indicate a higher priority Source or destination : IP address, CIDR block, service tag, or application security group Protocol : TCP , UDP , ICMP , or Any Direction : Inbound or outbound Port range ; Action : allow or deny Service tags represent a group of IP address prefixes managed by Microsoft available for use in NSG rules: VirtualNetwork : all CIDR ranges defined for the virtual network, all connected on-premises address spaces, peered VNets or VNets connected to a VNET gateway AzureLoadBalancer : Virtual IP address of the host where Azure's health probes originate Internet : IP address space that is outside the virtual network AzureCloud* : IP address space for Azure, including all datacenter public IP addresses AzureTrafficManager* : IP address space for the Azure Traffic Manager probe IP addresses Storage : NSG flow logging ,which saves the 5-tuple of all packets, is available as a low-cost way to monitor traffic. Flow logs record all IP flows going in and out of an NSG and are collected per NSG rule. They are charged per GB of logs collected and include a free tier of 5 GB/month. In AWS VPCs, Security Groups are similar to firewall rules that regulate inbound and outbound traffic of an instance. Outbound traffic is unrestricted by default, and every VPC contains a default security group. A network access control lists (NACLs) , also like a firewall, contains inbound and outbound rules but operates on the subnet. By default, a NACL allows all inbound and outbound traffic. In GCP, each VPC has a set of firewall rules that control traffic not only into and out of the VPC , but between instances in the same VPC . Each rule can be tagged, and individual instances with the same tags inherit those rules. Resources: Protect your Google Cloud Instances with Firewall Rules DNS Azure DNS Route 53 Cloud DNS CDN Users can use Azure CDN as a cache, reducing load from website. Content is cached by the CDN until its time-to-live (TTL) elapses, which can be controlled in the HTTP response from the origin server. Permanently removing content from the CDN requires it be first removed from the origin servers, meaning if the content is in a storage account it should be set to private or deleted from the storage, or the container itself should be deleted. Cached copies may remain in the CDN endpoint until the TTL has expired, unless it is purged . There are 4 pricing tiers available within Azure CDN: Azure CDN Standard from Microsoft does not offer dynamic site acceleration (DSA) (cf. Azure Front Door Service) Azure CDN Standard from Akamai Azure CDN Standard from Verizon Azure CDN Premium from Verizon , for which caching is configured using a rules engine. AWS CloudFront GCP CDN Load-balancing Azure Load Balancer Application Gateway AWS Elastic Load Balancer GCP Load balancing \ud83d\udc68\u200d\ud83d\udcbb Development NoSQL NoSQL databases differ from relational databases in that they do not obey the principle of data normalization . That is, the same data can be stored in more than one place. This is an advantage for databases that are optimized for reads as opposed to writes, because fewer queries are needed to retrieve information. However, when changing information that is duplicated in several places, write operations will be more laborious and prone to error. NoSQL databases are also horizontally scalable because the information can be sharded horizontally more easily than relational database, which are only vertically scalable (meaning scaling them requires larger and larger computers) and can only be sharded vertically. ( src ) Big Data History Beginning in 2000, Amazon began developing Merchant.com, a planned e-commerce service that was intended to be the base upon which other enterprises would develop online shopping sites. At the time, Amazon's development environment was a jumbled mess, and in the effort to consolidate and organize the enterprise into a set of well-documented APIs. Despite these changes, software development remained sluggish, and an investigation discovered that individual teams were procuring storage, compute, and database resources independently. AWS originated out of the effort to consolidate these resources across the enterprise and remove this bottleneck. Azure was announced in 2008 and publicly released in 2010 after earlier experiments in cloud computing like Whitehorse and RedDog. In fact, references to the \"classic\" model predating the Azure Resource Manager (ARM) actually refer to RedDog: the \"classic\" portal was also known as \"RedDog Front-End\". \ud83d\udcd8 Glossary Apigee The Apigee API platform is a management service that allows developers to deploy, monitor, and secure their APIs and generates API proxies. App Engine App Engine allows developers to deploy applications developed in popular programming languages to a serverless environment. It is available in two environment types: Standard and Flexible. Standard environment is the original App Engine environment, consisting of a preconfigured, language-specific runtime like Java, Python, PHP, Node.js, or Go. Flexible environment is similar to [ GKE ][ GKE ] in that it can run a customized container. App Engine is designed to support applications implemented as a microservices architecture. There are four components: The application is the top-level container that houses all other components. Services are versioned and provide a specific function. Versions are produced every time a service is updated. Every version runs on an instance . Each version of a service runs on its own instance, whose size can be determined by specifying the instance class . Instances can be dynamic or resident. Resident instances run continually and can be added or removed manually. Dynamic instances support autoscaling based on load. App Engine has three modes of scaling: Automatic scaling creates an instance with a specified request rate, response latency, and application metrics. Basic scaling creates instances only when requests are received Manual scaling supports operational continuity regardless of load level. App Engine Deployer Read-only access to all application configuration and settings. App Engine Service Admin Read-only access to all application configuration and settings. Write access to module-level and version-level settings. Cannot deploy a new version. App Service An App Service plan resource determines the billable compute resources available for the App Services applications managed by it. A plan acts as a container for multiple web applications sharing the same server farm (\"workers\"), and for this reason Windows and Linux apps can't be mixed in the same App Service plan. \"Web app\" is the legacy name for Azure App Service . App Service SSL certificates need to be deleted from each App Service before moving it to a new resource group. Application Gateway Azure Application Gateway is used to load balance a large-scale set using more than 100 instances in place of Azure Load Balancer . AZ-103: p. 223 Application Gateway supports session affinity to save user state using browser cookies. Unlike Azure Load Balancer, which operates at OSI layer 4 and has limited security capabilities, Application Gateway operates at OSI layer 7 and provides Web Application Firewall (WAF) functioanlity to block attacks like SQL injection, cross-site scripting, and header injection. HTTPS is also only available with layer 7 load balancers like Application Gateway. Athena Athena is a serverless AWS service that allows SQL queries to be run against data stored in a [ S3 ][ S3 ] bucket. Athena works closely with [AWS Glue][AWS Glue] to extract schema information and crawl data sources. Before running for the first time, you must provide a path to a S3 bucket to store query results. AWS CLI AWS CLI is version 1 is maintained for legacy compatibility purposes. AWS Developer Tools A collection of tools that provide CI services: CodeCommit CodeBuild CodeDeploy CodePipeline AWS Glue AzCopy AzCopy can be used to copy files to File storage. Azure Bastion Azure Bastion is a PaaS service deployed within a VNet that allows connectivity to a VM from the Portal. Once deployed in a VNet , RDP/SSH is available to all VMs in that VNet . This session is streamed to your local device over an HTMLS session using the browser. It is not deployed per VM, but once per VNet to its own dedicated subnet , at least /27 or larger No public IP is necessary on the VM, the connection from Bastion to the VM is to the private IP. However, the Bastion itself does require a public IP. Bastion can now span peered VNets IPv6 support is limited in Azure. IPv6 addresses are not added to VMs by default and must be explicitly defined by adding an endpoint to each VM to be using it. Routing by IPv6 is also not supported, so load balancers have to be deployed. Bicep Project Bicep is a domain-specific language and command-line utility that can be used to generate [ARM][ARM] templates. Project Bicep \u2013 Next generation ARM Templates Azure Container Instances Azure Container Instances ( ACI ) allows a simpler way of running isolated containers in smaller-scale deployments than Azure Kubernetes Service . The top-level resource in ACI is the container group , a collection of containers that get scheduled on the same host machine. These containers share a lifecycle, resources, local network, and storage volumes, and is equivalent to a Kubernetes pod. Container groups can be deployed to a subnet that already hosts a container group or an empty one, but it may not be deployed to a subnet that already has other resources like VMs. Azure Data Explorer Azure Data Explorer (ADX) has two architectural elements: Data Management Engine ADX does not hold large tables in a single table, rather it automatically shards them into Extents Azure DevOps Azure DevOps used to be known as Visual Studio Team Services and Team Foundation Server . Install DevOps CLI az extension add --name azure-devops Azure DNS Azure DNS supports private zones, which provide name resolution for VMs on a VNet and between VNets without having to create a custom DNS solution. Time-to-live for DNS record sets is provided in seconds. Azure DNS alias records allow other Azure resources to be referenced from the DNS zone, rather than static IP addresses or domain names. This allows these records to be automatically updated or deleted when the underlying Azure resource is changed. An A alias record set is a special type of record set that allows you to create an alternative name for a record set in your domain zone or for resources in your subscription. A CNAME alias record set can only point to another CNAME record set. Custom domains can be used by implementing CNAME DNS records, which are used in DNS to map alias domain names to the \"canonical\" name. Azure File Service Azure File Service allows you to create one or more file shares in the cloud (up to 5 TB per share), similar to a regular Windows File Server. It supports the SMB protocol, so you can connect directly to a file share from outside of Azure, if traffic to port 445 is allowed through the LAN and ISP. It can also be mapped within Windows. A clever use of a file share is as persistent storage for the Azure Cloud Shell. src Azure File Sync Azure File Sync extends Azure File Service to allow on-premises file services to be extended to Azure while maintaining performance and compatibility, communicating over TCP 443 over SSL, and not IPSec. Use cases include: Replace on-premises file servers Easily replicate data on-premises to make it available during lift-and-shift migrations Simply cloud development and management Azure File Sync works using an Azure File Sync agent , available as an MSI package for Windows Server 2012R2, 2016, and 2019, to register file servers as endpoints to an Azure File Sync Group . After installation, Azure credentials for a subscription must be provided. AZ-103: 153 In order to create an Azure File Sync, first a Storage Sync Service resource must be created, which works like a container to hold one or more sync groups . Every sync group has only a single cloud endpoint , referring to a storage account, but can have more than one server endpoint . Any server can only be registered to a single Storage Sync Service, and servers synced to different Storage Sync Service resources cannot sync with each other. Cloud tiering is an optional feature in Azure File Sync in which frequently accessed files are cached in the on-prem file servers, while less commonly accessed files are tiered to Azure Files. This is done by enabling Cloud Tiering, then selecting a free space policy , a percentage which indicates the amount of free space to maintain on the server endpoint's volume. When a user does access one of these tiered files, that file is downloaded to the on-prem cache and made available locally from that point on. This frees up local storage. Cloud tiering cannot be used with server endpoints on the system volume Although server endpoints can be configured with different free space policies, the most restrictive setting takes effect For tiered files, the file will be partially downloaded as needed Although a mount point can be a server endpoint, there can be no mount points inside a server endpoint When a filename collision occurs between the file share and file server, the file on the server has its filename appended with the server's name. Azure Policy Azure Policy is a service that can create, assign, and manage policies to enforce governance. Policy definitions, authored in JSON, implement policy by describing desired behavior for Azure resources when they are created or updated. AZ-103: p. 72 To implement policy, a policy definition is created first, then a policy assignment assigns it to a scope. Policy definitions can be packaged together using initiative definitions and applied to a scope using initiative assignments RBAC roles deny by default and allow explicitly . But Azure Policy allows by default and denies explicitly Policies can be applied at the management group , subscription , or resource group scope, with all child resources and resource groups being affected. Every policy definition has a single effect , which includes: Audit : create a warning event in the log Modify : used to add, update, or remove properties or tags on a resource during creation or update. Append AuditIfNotExists Deny DeployIfNotExists Disabled The order of evaluation of effects is: Disabled, Append, Deny, Audit (\"DADA\") Azure VMs Virtual Machines represent Azure's IaaS offering. A dedicated host group has to be created and placed in a resource group and associated with a location and availability zone and assigned a fault domain. A host then has to be created, a size specified, and associated with a host group. Any VM intended to run on the host has to be created in the same location and availability zone and associated with the host in the Advanced tab. Azure spot instances are available at deep discounts. 3 types of disk are available to Azure VMs: Operating System Disk (OS Disk) Temporary Disk Data Disk Azure VM image types include: Managed images (recommended), which remove the dependency of the VM to the image, at least within the same region. Copying a VM to another region still requires the managed image to be copied first. Unmanaged images, which required the VM to be created in the same storage account as that of the image. VM copies required the image to be copies first. VM images are captured from an existing VM that has been generalized (prepared), removing unique settings (hostname, security IDs, personal information, user accounts, domain join information, etc) but not customizations (software installations, patches, additional files, folders), using sysprep.exe for Windows machines or Microsoft Azure Linux Agent ( waagent ) for Linux machines. VM images in AWS are called Amazon Machine Images (AMI) . Azure VPN Virtual network gateways in Azure are of two types: VPN gateways and ExpressRoute gateways . Any virtual network can have only a single gateway of each type. VPN gateways send encrypted traffic between the virtual network and an on-premises location. VPN Gateways must be deployed into their own dedicated subnet (named \"GatewaySubnet\" ) with a minimum size of CIDR /29, although a CIDR /27 address block is recommended. VPN connections between an on-premises network and a VNet are only possible if the network ranges do not overlap. VPN gateways can be classified by the topology of the connection: Site-to-Site (S2S) connections require an on-premises VPN device associated with a public IP address. Multi-Site connections require a RouteBased VPN type. Point-to-Site (P2S) allows individual computers to securely connect to a VNet without need for a VPN device, which is useful for telecommuting, and can use SSTP, OpenVPN, or IKEv2. There are several authentication considerations. VNet -to- VNet connections are also possible, but VNet peering may be preferable if the virtual networks meet certain requirements. Site-to-Site Multi-Site Point-to-Site VNet -to- VNet VPN gateways can also be classified on VPN type . Route-based VPNs (previously called \"dynamic routing gateways\") require routes to be defined in a routing table to direct packets into tunnel interfaces. Policy-based VPNs (previously called \"static routing gateways\" in the classic deployment model) can only be used on the Basic gateway SKU and offer only a single S2S tunnel. Route-based Policy-based There is a profusion of Gateway SKUs that determine the maximum connections, throughput, and availability of other features like BGP and zone-redundancy available for each topology. Every Azure VPN gateway consists of two instances in an active-standby configuration. During failover, a brief interruption of 10-15 seconds for planned maintenance or up to 60-90 seconds in the case of unplanned disruption, may occur. But the gateway can be configured to be active-active , which will establish S2S VPN tunnels to both gateway instances with traffic being routed through both tunnels simultaneously. There will still be only a single connection resource, but the on-premises VPN device must be configured to establish both of these tunnels. The most highly available arrangement would use multiple VPN devices with the VPN gateway in active-active configuration, creating 4 IPsec tunnels that evenly carry Azure traffic. Active-Standby Active-Active Dual redundancy Bigquery Petabyte-scale analytics database service for data warehousing. BigQuery can be executed using the bq command-line utility. BigTable GCP realtime database used for Big Data. BigTable can be executed using the cbt command-line utility. BigTable evolved out of Google's need to ensure access to petabytes of data in its web search business line. It was described in a 2006 research paper that ended up launching the entire NoSQL industry. In 2015 it was made available as a service to cloud customers. src BigTable doesn't support secondary indexes. Billing Account Administrator GCP predefined role that grants permissions to manage self-service accounts but not to create new ones. Billing Account Creator Predefined GCP role that grants permissions to create new self-service accounts. Billing Account User GCP predefined role that enables user to link projects to a billing account. Billing Account Viewer GCP predefined role that grants permissions to view transactional and billing data associated to a GCP account. Cloud AutoML GCP service that allows developers without machine learning experience to develop machine learning models. Cloud Device Administrator : Azure built-in role that grants users full access to manage devices in Azure AD. Cloud Functions GCP serverless compute offering suited to running short-running logic, such as calling other APIs in response to an event. Cloud IAM GCP's identity and access management platform. Permissions include: roles/container.admin roles/container.clusterAdmin roles/container.clusterViewer roles/container.developer roles/container.hostServiceAgentUser Predefined roles include: App Engine Deployer App Engine Service Admin Billing Account Administrator Billing Account Creator Billing Account User Billing Account Viewer Compute Engine Admin Compute Engine Network Admin Compute Engine Security Admin Compute Engine Viewer Compute Service Agent Folder Admin Project Creator Shared VPC Admin Cloud Machine Learning Engine Platform for building and deploying scalable machine learning systems to production. Cloud Natural Language Processing GCP tool for analyzing human languages and extracting information from text. Cloud Run Google Cloud Run is built on a native open standard that will allow using the same container on other cloud providers. It bills down to the nearest 100 ms interval. Cloud Run provides an HTTPS endpoint to the container. Cloud Run can also run on your own K8S cluster running on [ GKE ][ GKE ], recommended for workloads that have a consistently high level of traffic, since you are billed for the provisioned cluster resources. However, resources like CPU, GPU, and other items can be customized. Cloud Vision Image analysis platform for annotating images with metadata, extracting text, or filtering content. CloudFormation AWS declarative automation service, which can use JSON or YAML-format templates. These resources are placed into a named stack , a container that organizes the resources described by the template, and the stack name must be unique to the account. This allows provisioned resources to be easily managed, since the stack contains a record of events, and to be quickly destroyed by deleting the stack. CloudFormation Designer allows templates to be viewed as a diagram of resources. CloudFront AWS CDN offering that helps deliver static and dynamic content worldwide. CloudFront caches content in edge locations , of which there are more than 150 spread out across 6 continents. Edge locations may not be chosen arbitrarily, rather there are three options: US, Canada, and Europe US, Canada, Europe, Asia, and Africa All edge locations In order to make content available on CloudFront, you must create a distribution , which defines the type and origin of the content to cache. There are two types of distribution: A Web distribution is used for static and dynamic content, including streaming video, accessible via HTTP or HTTPS. Its origin can be a web server or a public S3 bucket. Real-Time Messaging Protocol (RTMP) distribution delivers streaming audio or video. The media player and media files must be stored in S3 buckets. CloudTrail AWS service that logs actions against AWS resources. These events are divided into API and non-API actions. API actions include creating, modifying, or deleting resources. Non-API actions include everything else, like logging into the management console. Events are also classified as management events and data events Management events (also control plane operations ) are operations that a principal attempts to execute against an AWS resource. Data events are S3 object-level activity and Lambda function executions. These are treated separately from management events because they tend to be higher volume. CloudWatch Amazon CloudWatch collects logs, metrics, and events from AWS resources and non-AWS on-premises servers and presents a dashboard for visual analysis. All AWS resources automatically send their metrics to CloudWatch Metrics, which stores the data for up to 15 months. CloudWatch alarms can be configured for single metrics. Applications and AWS services have to be configured to send log events to CloudWatch Logs, and they are stored indefinitely by default although retention settings can be configured. Log events from the same source are organized into a log stream. Log streams are then organized into log groups. Metric filters extract metric data from log events. CloudWatch Events is a feature that monitors for changes in AWS resources as a result of API operations. Cloudyn Previously a standalone service available in Azure, now deprecated because its functionality has been incorporated natively into other sections of the Cost Management + Billing blade. CodeCommit AWS private git repo service. CodeDeploy AWS service for automatically deploying applications to AWS compute resources or on-prem servers. CodeDeploy can pull source code from [ S3 ][ S3 ] and repos from GitHub or Bitbucket but notably not CodeCommit (ref. CodePipeline ). CodePipeline AWS service for orchestrating and automating every stage of software development. It defines a series of stages, two of which are required - source and deployment - but other stages like testing or approval can be incorporated. Cognito AWS service that integrates with identity providers like Amazon, Google, Microsoft, and Facebook to add user access control to an application. Compute Engine Compute Engine is GCP's IaaS offering. An instance group is a collection of VM instances that you can manage as a single entity. Two types: Managed instance groups operate applications like web front-ends across a group of identical VMs created with a template. They provide high availability, healing, scaling, and automatic updates. Unmanaged instance groups allow you to manually load balance a group of VMs. VMs can be added or removed at will. Getting started with GCE Compute Engine Admin Predefined GCP role that grants full control of Compute Engine resources. Compute Engine Network Admin Predefined GCP role that grants full control of Compute Engine networking resources. Compute Engine Security Admin Predefined GCP role that grants full control of Compute Engine security resources. Compute Engine Viewer Predefined GCP role that grans read-only access to all Compute Engine resources, but exclusive of data stored on disks, images, and snapshots. Compute Service Agent Predefined GCP role that grants Compute Engine Service Account access to assert service account authority. Computer Vision subfield of artificial intelligence concerned with developing the capability of computers to recognize objects in images and to understand visual information. Container Instances Azure Container Instances ( ACI ) is a PaaS service that facilitates deployment of individual containers. CosmosDB Azure NoSQL offering. Cosmos DB started as Project Florence in 2010 to address shortcomings with SQL Server in supporting highly available services like Xbox. In 2015 the product was relaunched as Document DB, then renamed Cosmos DB in 2017. An emulator is available for Cosmos DB here . A Cosmos DB account can be used for free for 30 days, and does not require an Azure subscription. Throughput is measured and billed in Request Units (RU) per second. The minimum manually provisioned throughput level is 400 RU/sec. There are three throughput provisioning offers: Manual , where a static throughput level is provisioned. This is best for highly predictable workloads. Autoscaling , where Azure will automatically scale throughput based on usage, reducing it down to a minimum of 10% of the provisioned throughput. Serverless , where you pay for only the RUs you need. This throughput provisioning model is ideal for small demonstration projects. This feature is forthcoming. The cost of using a CosmosDB database can be approximated using the Capacity calculator . In general, these are reasonable back-of-hand estimates for common operations to estimate costs: Read item: 1 RU SQL query: ~2.8 RU Create item: 10 RU There are various choices of API for Cosmos DB accounts which affect the data model used for databases. SQL API is the Core API, and works off JSON documents and SQL query syntax MongoDB API uses BSON documents (binary encoded JSON) and MongoDB query syntax Table API and uses Key-Value database design reflects API of Azure Table Storage Gremlin API is a graph database using a flat store of vertices and edges Cassandra API is columnar, and unlike most NoSQL databases does specify a schema Consistency Uniformity of data across replicas in a distributed database. Consistency levels describe how and when data is replicated to provide varying consistency guarantees. Strong consistency is the strongest consistency model and requires synchronous replication after every change to database, increasing latency for each write. Session consistency is unique, in that it offers consistent prefix to databases that support a single session or an application with a single token. Eventual consistency is the weakest consistency model and provides no ordering guarantees. Consistent prefix offers read throughput, availability, and write latency comparable to eventual consistency while guaranteeing global order. Bounded staleness implies asynchronous replication and offers guarantees on the number of versions ( K ) or time interval ( T ) reads lag behind writes, referred to as the staleness window . As the staleness window approaches, Azure will delay writes by providing back pressure on writes. Outside the staleness window, data is guaranteed to be globally consistent. Outside the region in which the writes were made, Azure guarantees total global order or consistent prefix , which means, the global order is maintained. Strong Session Eventual Bounded staleness Horizontal partitioning is what allows Cosmos DB to scale-out massively to provide high availability and elasticity. Partitions can be thought of as physical fixed-capacity data buckets that back every container. A partition split occurs when a new physical partition is brought online, resulting in half of the documents existing on a previously existing partition being moved to the new one. Cosmos DB automatically and transparently splits horizontal partitions to achieve elasticity. Logical partitions , determined by the partition key which is set at container creation, group individual documents in ways that are kept on the same physical partition. It is recommended to have a high number of logical partitions, so that CosmosDB has greater flexibility partitioning documents. The partition key is immutable, so the correct choice of partition key is an important architectural consideration. Even distribution of documents is ideal to avoid hot partitions , where some partitions have much greater activity than others, due to uneven distribution of documents. Any partition may not be greater than 20 GB in size. Physical partitions have 4 replicas within a region. There are several common partitioning patterns: Partitioning on /id , which results in every document existing in its own logical partition. This pattern is write-optimized and ideal for IoT applications. Any SQL query for more than one document would be cross-partition by necessity, so direct reads using the /id value would be far more economical. Partitioning small lookup lists on a /type property. This will keep lists of related items used for lookups in the same partition. Optimizing for queries by organizing multiple types of document according to a key data-point. For example, customer data could be kept in the same partition as that customer's orders, avoiding cross-partition queries. Cost Management Contributor Azure built-in role that grants access to the Cost Management blade. Cost Management Reader Azure built-in role that grants access to the Cost Management blade. Data Box Microsoft-provided appliance that allows for the transfer of large volumes of data to Azure, available only to EA , CSP, and Microsoft Partner Network Sponsorship offer types. Workflow Order: Use Portal to order a data box by creating a Data Box resource Receive: Connect Data Box to network Copy data: Mount file shares and copy data to the device. Return: to Microsoft Upload: Microsoft will upload the data and securely erase it from the device Offering Capacity Storage saccounts Data Box Disk 35 TB 1 Data Box 100 TB 10 Data Box Heavy 1,000 TB 10 Dataflow GCP streaming data framework for defining both batch and stream processing pipelines. Dataprep GCP managed service that allows analysts to visually explore, clean, and prepare data for later analysis. Dataproc GCP service that manages the creation of data science clusters and data analysis jobs. DynamoDB : NoSQL database known for fast (1-9 ms) query times. DynamoDB measures capacity in Read Capacity Units (RCU) and Write Capacity Units (WCU) . 1 RCU = 1 record at most 4 KB in size 1 WCU = 1 record at most 1 KB in size DynamoDB offers the choice between strongly consistent and eventually consistent (half the cost) reads. DynamoDB offers two types of indexes: Global Secondary Index allows you to create a completely new aggregation of data. GSI updates are eventually consistent , with asynchronous updates populated after an update response is passed to the client. Local Secondary Index (LSI) alternate sort key attribute that allows only sorting DynamoDB Streams (changelog for the DynamoDB table) interfaces with AWS Lambda to implement complex queries , computed values like sum, average, maximum, etc. These are implemented in a different processing space than the DynamoDB table itself, so that it does not affect the table. AWS Lambda has an invocation role which defines what Lambda can see (triggered upon a change to the table as reported in DynamoDB Streams) and an execution role which defines what it can do . Elastic Container Service Elastic Kubernetes Service Elastic Beanstalk : AWS PaaS offering. Elastic File System : Scalable file system for AWS Linux instances that allows multiple instances to be attached to a single EFS volume to share files. EFS volumes are highly available, spanning multiple Availability Zones in a single VPC . Elastic MapReduce : AWS's managed Big Data analysis service, supporting Apache Hadoop, Apache Spark, HBase, Presto, and Flink. Enterprise Agreement Azure customers on an Enterprise Agreement can add up-front commitments to Azure then be billed annually. If the committed spend is exceeded, the overage is billed at the same EA rate. EA customers can create spending quotas and set notification thresholds through the EA Portal. 3 portals used to manage Azure subscriptions EA Portal (ea.azure.com) available only to customers with an Enterprise Agreement Account Portal Azure Portal, includes Azure Cost Management ExpressRoute There are four main architectures used with ExpressRoute Any-to-any connection is used to integrate on-premises WANs using IPVPN. Co-location with cloud exchange is used to order virtual cross-connections to the Azure cloud through the co-location provider's Ethernet exchange. Point-to-point Ethernet connection is used to configure on-premises data center connectivity to Azure through individual point-to-point links Firebase : GCP NoSQL database offering known for its client libraries. Firebase Auth offers a free user interface for applications, Firebase UI . Firestore was released from beta in early 2019 and combines and improves upon functionality of previous products named Cloud Datastore and Firebase Realtime Database . Firestore is organized into documents , which consist of key-value pairs and are similar to JSON objects, and collections . JSON-like objects are called maps and keys are called fields in Firestore. Collections can contain only documents, but documents can contain sub-collections. Root can only contain collections. So navigating deeper and deeper into the information store will involve alternating between collections and documents. Firestore features a compatibility mode that emulates the behavior of Datastore in accessing Firestore's storage layer while removing some of Datastore's limitations. Queries in Firestore can only be used to find documents stored in one specific collection or sub-collection. However a collection group query , meaning one that spans multiple collections, began to be supported in 2019. Complex relational queries are not possible (in a single query), and query results are usually returned based on equality or greater-than/less-than comparisons. The field has to be specified as having a scope of \"Collection group\" within GCP, and there is a limit of (about) 200 for these queries. An index is created for every field in every document added to a collection, which results in very fast query times that are proportional to the number of results , not records searched. This structure ensures that equality searches are highly performant, as are comparison searches using greater-than or less-than. But this implementation creates bizarre limitations to Firestore's querying capabilities: There is no native way to perform wildcard searches or OR queries. For common instances of such queries, Google recommends adding a field that contains the value for each record Inequality searches present a challenge for Firestore. For some queries that combine conditions on more than one field (i.e. restaurants within a certain range of a location), Firebase will create a \"composite index\" (only within the index, the document itself is not affected) automatically to facilitate searches on those fields. Unlike Firebase , which charges based on the volume of data stored, Firestore charges based on number of operations performed and records returned. Folder Admin : Predefined GCP role that allows folders to be created at an Organization. Front Door : Azure offering that works like Azure Load Balancer for web apps. Glacier : AWS storage service that offers long-term archival at low cost. One or more files are stored in an archive, typically a .zip or .tar file containing multiple files. Archives can range from 1 B to 40 TB in size. Archives are stored in a Glacier vault , a region-specific container analogous to S3 buckets. Vaults must have regionally unique names, but there is no need for a globally unique name. Glacier vaults can be created and deleted using the Glacier service console. But uploading, downloading, or deleting archives must be done through the AWS CLI or an application using the SDK. Some third-party applications can also interact with Glacier. Google Cloud Identity Google's IDaaS provider. GKE Clusters have two modes of operation, Standard and Autopilot , that offer a more configurable and a more managed experience respectively. In GKE , clusters are billed at a flat fee of $0.10 per cluster hour . The GKE free tier provides $74.40 in monthly credits per billing account, available to Autopilot and Standard zonal clusters but not Standard regional clusters. Note that there are 744 hours in a 31-day month, and the free tier monthly credit corresponds to exactly this amount of usage for a single cluster. Import/Export Service Azure service that allows the physical shipment of disks procured by the user to Azure for import into a storage account , which can be placed into blob or file storage. This service requires the use of a Windows computer with BitLocker and .NET Framework and is dependent on the WAImportExport.exe utility. Procure 2.5-inch or 3.5-inch SATA (not SAS) disks Connect the disks to a Windows machine. Create a volume and encrypt it using BitLocker Install the Azure Import/Export tool (WAImportExport.exe) on the disks. Copy files Create an import job in the Azure Portal Kinesis AWS service for ingestion and processing of streaming data, such as access logs, video, audio, and telemetry. Kubeflow Cloud-native platform for machine learning based on Google\u2019s internal machine learning pipelines. Kusto Case-sensitive query language developed by Microsoft and used in several Azure services: Azure Data Explorer Log Analytics Sentinel Application Insights Microsoft Defender ATP Lightsail offers blueprints that will automatically provision all compute, storage, database, and network resources needed for a deployment. Macie AWS service that automatically finds and classifies sensitive data stored in AWS using machine learning to recognize sensitive data such as PII or trade secrets. Microsoft Azure Recovery Services Azure agent for backing up Windows machines only, but can also be installed on instances of other cloud providers like AWS. MARS can be configured to protect the entire system, volumes, or individual files and folders. Monitor Neptune AWS graph database. Network Performance Monitor Azure Log Analytics network monitoring solution for hybrid networks, providing 3 services: - Performance Monitor monitors connectivity between various points in both Azure and on-prem networks - Service Connectivity Monitor monitors outbound connectivity from network nodes to external TCP services, monitoring performance metrics like latency, response time, and packet loss - ExpressRoute monitors end-to-end connectivity between on-prem network and Azure over ExpressRoute Network Watcher Network Watcher appears like a normal resource in a resource group, but it is deployed as a single instance per Azure region. Network Watcher monitoring and diagnostic tools: IP Flow Verify Next Hop Packet Captures link a Network Watcher resource, a target VM, a storage account, and a filter that specifies the characteristics of network traffic (source and destination IP addresses and ports as well as protocol) to capture, as well as a time limit. Network Topology OpsWorks OpsWorks is AWS's declarative configuration management service that uses the Chef and Puppet configuration management platforms and comes in three varieties: OpsWorks for Puppet Enterprise OpsWorks for Chef Automate OpsWorks Stacks Project direct parent of all other GCP resources, consisting of a project name, project ID, and project number. Project Creator Predefined GCP role given to all users currently assigned to a project. Pub/Sub GCP messaging service, allowing services and applications to communicate. Recovery Services Vault Azure resource used to centrally manage the backup and recovery. A Backup protection policy defines how a backup plan is implemented. These are most easily created through the Portal. A vault can only back up data from other resources that exist in its region. Rekognition AWS deep learning-based image recognition service. Resource Policy Contributor Azure built-in role that includes access to most Policy operations and should be considered privileged. Route 53 AWS managed DNS service. Like any other DNS system, it relies on resource records defined in a zone . Route 53 can also provide name resolution for private domain names , used on private networks. Private hosted zones provide DNS resolution for a single domain name within multiple VPCs. But when a resource record must be changed dynamically to work around failures or route users to an underutilized server, routing policies can be used. Simple policy is the default for new resource records and maps a domain name to a single value (i.e. an IP address). Weighted policy distributes traffic across multiple resources according to a predefined ratio. Latency policy sends users to resources in their closest Region. Failover policy allows a secondary resource to be marked for routing when the primary resource is unavailable. Geolocation policy routes users based on their specific continent, country, or state. Multivalue answer policy allows even distribution of traffic across multiple resources by randomizing the order of returned records. All routing policies except Simple can use health checks to modulate routing action. All health checks occur every 10 or 30 seconds and can check one of three resources: Endpoint makes a test connection to a TCP port CloudWatch alarm can be set off in case of high latency or other metrics. Calculated monitors the status of other health checks. Route 53 also offers the Route 53 Traffic Flow visual editor that allows you to create a diagram to represent the desired routing. The diagram isn't translated to individual resource records but rather represents a single policy record which costs 50 USD/month each. In addition to the routing policies above, Traffic Flow also offers the Geoproximity routing policy that directs users to a geographic location based on how close they are. Simple Storage Service AWS storage service. S3 stores objects in a container called a bucket . Each bucket must have a globally unique name and exposes a HTTP endpoint (at https://$BUCKET.s3.amazonaws.com/) Each object is associated with a key . Keys are equivalent to filenames, and the bucket is equivalent to a flat filesystem. However, directories can be simulated by placing slashes in the key. Bucket policies (applied to buckets) and user policies (applied to IAM principals) can be used to modulate accessibility. Public or anonymous access to an object can only be granted by bucket policies. Bucket and object ACLs are legacy access control methods that are still usable. S3 buckets store data unencrypted, although encryption at rest is available in two options: Server-side encryption: S3 encrypts uploaded objects before storing them, and decrypts it again before delivery. Client-side encryption: User must encrypt data prior to uploading and decrypt it after downloading. S3 offers various storage classes that differ in their availability and durability , the percent likelihood that an object within it will not be lost over the course of a year. Frequently accessed objects: STANDARD REDUCED_REDUNDANCY Infrequently accessed objects STANDARD_IA ONEZONE_IA GLACIER INTELLIGENT_TIERING automatically moves objects to the most cost-effective storage tier based on access patterns. Storage Gateway is an on-premises VM that provides a connection to S3 for on-premises infrastructure. File gateway lets you use NFS and SMB file shares to transfer data to S3 . Data is stored in S3 and cached locally. Volume gateway can be used as an iSCSI target by on-premises servers. Two configuration variants exist: Stored volumes : All data is stored locally and asynchronously backed up to S3 as EBS snapshots. A stored volume can range from 1 GB to 16 TB in size. Cached volumes : Data is stored in S3 and frequently used data is cached locally. A cached volume can range from 1 GB to 32 TB in size. Tape gateway is configured as an iSCSI target by a backup application. Virtual tapes range from 100 GB to 2.5 TB in size. These tapes are asynchronously transferred to a virtual tape library (VTL) backed by S3 and removed when the upload is complete. Recovery requires downloading the virtual tape again. Cloud Storage in Minutes with AWS Storage Gateway Sentinel Azure cloud-native SIEM and SOAR soluation that can collect data from many sources and present it to security analysts, who can run Kusto queries against the dataset. Azure Sentinel can ingest data from on-premises devices using one of several types of connector , categorized by the type of data ingestion: Native connectors integrate directly with other Microsoft security products, like Azure AD, M365, and Azure Security Center Direct connectors are configured from their source location, such as AWS CloudTrail, Azure Firewall, and Azure Front Door API connectors are implemented by security providers, like Azure Information Protection (AIP), Barracuda Web Application Firewall (WAF), and Microsoft WAF Agent-based connectors , using the Log Analytics agent, make it possible to ingest data from any source that can stream logs in Common Event Format (CEF), such as Windows and Linux machines. Analytic rules are rules that users create to help detect threats and anomalies in an environment: Scheduled rules run on a predetermined schedule Microsoft Security Machine learning behavior analytic rules can (currently) only be created from templates provided by Microsoft using proprietary ML algorithms Simple Queue Service AWS service that can broker messages between components of highly decoupled applications. Snowball Physical appliance designed to move large amounts of data to the cloud. The largest Snowball device can store 72 TB of information. Snowball Edge refers to a family of options similar to Snowball but with compute power to run EC2 instances and Lambda functions locally. All Snowball Edge options feature a QSFP+ network interface that is capable of speeds up to 100 Gbps. Snowball Edge devices can also be clustered together. Storage Optimized provides up to 80 TB of storage and 24 vCPUs. Compute Optimized provides up to 39.5 TB of storage and 52 vCPUs. Compute Optimized with GPU is similar to Compute Optimized but includes an NVIDIA GPU, making it useful for ML and HPC applications. Spanner GCP managed scaleable database service. Stackdriver GCP service that collects metrics, logs, and event data from applications and infrastructure and integrates the data so DevOps engineers can monitor, assess, and diagnose operational problems. Super administrator Unique GCP role associated with the root Organization which has powers that exceed that of other administrative users. Storage accounts Azure storage accounts are managed through [Azure Resource Manager][ARM] and management operations are authenticated and authorized using [Azure Active Directory][Azure AD]. There are four services provided within each storage account: Blobs provides a highly scalable service for storing arbitrary data objects, such as text or binary data. There can be multiple containers within a storage account, and a container can have its own folder structure. There are three types of blob: page , block , and append blobs. Tables provides a NoSQL-style store for storing structured data. Tables in Azure storage do not require a fixed schema, thus different entries in the same table can have different fields Queues provides reliable message queueing between application components Files provides managed file shares that can be used by VMs or on-premises servers Options that must be selected when creating a storage account: Performance tier Standard supports all storage services and uses magnetic disks to provide cost-efficient and reliable storage Premium only supports page blobs with the locally-redundant (LRS) replication option, uses high-performance SSD disks Account kind General-purpose V2: only kind to support ZRS General-purpose V1: does not support various access tiers. Blob storage: specialized storage account used to store block and append blobs Replication mode : Storage accounts can be freely moved between the following replication modes, except ZRS, in which case it is recommended to copy data to a new account. Locally-redundant storage (LRS): makes 3 local sychronous within the same Azure facility (zone) Zone-redundant storage (ZRS): makes 3 synchronous copies across multiple availability zones; available for general-purpose v2 storage accounts at Standard performance tier only. Geographically-redundant storage (GRS): makes 3 local synchronous copies plus 3 additional asynchronous copies (typically within 15 minutes, but no SLA) to a second data center far away from the primary region Read-access geographically redundant storage (RA-GRS): makes 3 local synchronous copies plus 3 additional asynchronous copies to a second data center far away from the primary region, which has only read-only access Access tier : Both Blob and StorageV1 can be upgraded to StorageV2, a process which is irreversible. Hot blob storage access tier optimized for the frequent access of objects in the storage account Cool blob storage access tier optimized for storing large amounts of data that is infrequently accessed and stored for at least 30 days Archive blob storage access tier designed for long-term storage of infrequently-used data that can tolerate several hours of retrieval latency, remaining in the Archive tier for at least 180 days. It is stored offline and can take up to 15 hours for it to be \"rehydrated\" to the Cool or Hot tier before it can be accessed. Premium providing high-performance access for frequently-used data on SSD, only available from the Block Blob storage account type. Every storage account service exposes its own Internet-facing endpoint, which must be secured in one of several ways. A firewall can be implemented by using network rules to limit traffic to particular networks. The storage firewall controls IP addresses and VNets can access the storage account and applies to all storage account services. Access can be restricted to specific VNets by creating a Virtual Network Service Endpoint , however this still uses the public IP address. Private Link allows similar functionality using private IPs. MS Docs Public access to blobs can be restricted at the container level on container creation. By default, no public read access is enabled for anonymous users, but users with RBAC rights or with the storage account name and key can have access. This can be done through ARM APIs, the Portal, or Azure Storage Explorer. Container access levels: No public read access: container and blobs can only be accessed by storage account owner (default for new containers) Public read-only access for blobs only (container data is not available, and anonymous clients cannot enumerate the blobs within the container) Full public read-only access: all container and blob data can be read by anonymous requests: Access can also be switched between Shared Key -based authentication (relying on storage account keys) and Azure AD authentication , where a RBAC role determines access to a Container. Authorize access to blobs and queues using Azure Active Directory Access keys grant full access to all data in all services of a storage account and represent the simplest and most powerful control over access. Access keys are typically used by applications for access to Azure storage, either through a Shared Access Signature (SAS) token or directly accessing the storage itself with the name and key. Storage account keys were implemented early in the history of Azure and grant full access to the entire storage account. However, it is considered an anti-pattern to distribute this key; a SAS token should be generated for every stored item to be distributed. Because storage account keys provide write access, a storage account with a ReadOnly resource lock will not enumerate its storage account keys, and users with Read permission will not be able to retrieve the keys either. Systems Manager AWS service for imperative configuration management. Systems Manager relies on several types of script: Command documents use normal shell commands and can be run periodically or on a trigger, so long as the instance to be managed has the required agent. Automation documents allow administration of AWS resources, similar in effect to using the Management Console or the AWS CLI . Trusted Advisor AWS service allows a visual check of resource configurations to ensure compliance with best practices, available only to Business and Enterprise Support plans. It offers several categories Cost optimization Performance Security Fault tolerance Service limits User Access Administrator Azure built-in role that grants the permissions necessary to assign a user administrative access at the subscription scope. Permissions: Microsoft.Authorization/roleAssignments/write Microsoft.Authorization/roleAssignments/delete User Administrator Azure built-in role that grants the power to manage all aspects of users and groups, including resetting passwords for limited admins. VM Agent Microsoft Azure Virtual Machine Agent (VM Agent) manages VM interaction with the Azure Fabric Controller and comes preinstalled with Windows images from the Marketplace. It can also be installed on a custom image. VM Agent supports the VMSnapshot extension, which is added when backups are enabled. This extension takes a snapshot of the storage at the block level and sends it to the RSV configured. For Windows VMs, this extension leverable the Volume Shadow Copy service. Microsoft Azure Linux Agent Azure agent that manages VM interaction with the Azure Fabric Controller on Linux VMs. WAImportExport.exe CLI tool associated with Azure Import/Export service that requires a Windows computer with .NET Framework and BitLocker. There are two versions: Version 1 is recommended for blob storage Version 2 is recommended for files storage . Check disks required for selected blobs WAImportExport.exe PreviewExport /sn:<Storage account name> /sk:<Storage account key> /ExportBlobListFile:<Path to XML blob list file> /DriveSize:<Size of drives used> Various flags in WAImportExport.exe allow an XML-format \"blob list\" file to be used to specify files, or as output. All Root Blob in root Containers Pattern Pattern in container Export all blobs in the storage account <?xml version=\"1.0\" encoding=\"utf-8\"?> <BlobList> <BlobPath> / </BlobPath> </BlobList> Export all blobs in the root container <?xml version=\"1.0\" encoding=\"utf-8\"?> <BlobList> <BlobPath> /$root </BlobPath> </BlobList> Export blob \"logo.bmp\" in the root container <?xml version=\"1.0\" encoding=\"utf-8\"?> <BlobList> <BlobPath> $root/logo.bmp </BlobPath> </BlobList> Export all blobs in container \"music\" <?xml version=\"1.0\" encoding=\"utf-8\"?> <BlobList> <BlobPath> /music/ </BlobPath> </BlobList> Export all blobs in any container that begins with \"book\" <?xml version=\"1.0\" encoding=\"utf-8\"?> <BlobList> <BlobPath> /book </BlobPath> </BlobList> Export all blobs in container \"music\" that begin with prefix \"love\" <?xml version=\"1.0\" encoding=\"utf-8\"?> <BlobList> <BlobPath> /music/love </BlobPath> </BlobList> WebJobs a feature of Azure App Service that enables you to run a program or script in the same instance as a web app, API app, or mobile app, at no additional cost and supported only on Windows App Service plans . There are two types: Continuous webjobs default to running on all instances of the linked web app (although it can be configured to run on only one) Triggered webjobs run only when triggered or on a schedule and on only a single instance of the linked web app selected by Azure.","title":"Cloud"},{"location":"Infrastructure/Cloud/#cloud","text":"Compute Containers :fontawesome-solid-save: Storage Network Development Big Data IaaS Azure VMs EC2 Compute Engine PaaS App Service Elastic Beanstalk App Engine Serverless Functions Lambda Cloud Functions Cloud Run Individual containers ACI ECS Kubernetes AKS EKS GKE Container registry Artifact Registry Archive Glacier Backups Recovery Services Vault Backup Physical media Data Box Import/Export Service Snowball Transfer Appliance Private networks VNets VPC VPC Security rules Network Security Group (NSG) Security Group Firewall Rules DNS Azure DNS Route 53 Cloud DNS NoSQL Cosmos DB DynamoDB DocumentDB Firestore Spanner CI/CD Azure Devops CodeBuild CodeCommit CodeDeploy CodePipeline Cloud Build Messaging SNS Pub/Sub Computer Vision Computer Vision Rekognition Cloud Vision Big Data Data Lake Store Redshift Athena BigQuery BigTable Dataprep Streaming data Event Hubs Service Bus Stream Analytics Kinesis Athena DataFlow Batch processing HDInsight Batch EMR Batch DataFlow Dataproc Links TODO: Cloud Storage CloudWatch Azure Functions Glacier Google Cloud Storage (GCS) gcloud gsutil Simple Notification Service","title":"\u2601&#xfe0f; Cloud"},{"location":"Infrastructure/Cloud/#administration","text":"","title":"\ud83d\udee0&#xfe0f; Administration"},{"location":"Infrastructure/Cloud/#cost-management","text":"Azure quotas apply to subscriptions and are implemented with tags . Resource quotas trigger alarms when resource creation and consumption hit a threshold. These are not to be confused with resource limits which can stop resources from being created, whereas quotas can not. Spending quotas trigger alarms when spending has reached a threshold. Azure budgets can be viewed and administered in the Cost Management + Billing blade. Users must have at least the Reader role at the subscription scope to view, and Contributor to create and manage, budgets. Resource locks are used to apply restrictions across all users and roles and can be applied at subscription, resource group, or resource scopes. CanNotDelete ReadOnly effectively restricts all authorized users to the permissions granted by the Reader role Storage account keys of a locked storage account cannot be listed because the list keys operation is handled through a POST request Visual Studio Server Explorer will not be able to display files for a locked App Service resource, because that interaction requires write access VMs in a locked resource group will not be able to be started or restarted, because those operations require a POST request All child resources of the scope at which a lock is applied inherit the lock. A CanNotDelete lock applied to a DNS A record would also prevent the deletion of the DNS zone that the record resides in, as well as the resource group the zone resides in. Of the builtin roles , only two have access to the Microsoft.Authorization/* or Microsoft.Authorization/locks/* actions required to create or delete locks: Owner User Access Administrator Resource locks apply to the management plane of Azure, specifically operations sent to https://management.azure.com Managed applications create two resource groups to implement locks: One resource group to contain an overview of the service, which isn't locked Another resource group containing the infrastructure for the service, which is locked Sources: Move resources to a new resource group or subscription Some services have limitations or requirements when moving resources between groups ( src ) Source and destination subscriptions must be within the same [AAD][Azure AD] tenant Destination subscription must be registered for the resource provider of the resource being moved Account moving the resources must have at least the following permissions: Microsoft.Resources/subscriptions/resourceGroups/moveResources/action Microsoft.Resources/subscriptions/resourceGroups/write","title":"\ud83d\udcb0 Cost management"},{"location":"Infrastructure/Cloud/#iam","text":"All cloud providers offer IAM systems that are used to control access to resources, all of which establish a similar taxonomy of concepts. The type of user that is granted access to resources is referred to variously as a member or a security principal . Bundles of specific permissions that can be assigned to users are called roles . All cloud providers offer the ability to define custom roles and come with many ready-to-use role definitions: predefined roles or built-in roles . Roles form the basis of RBAC , which is the recommended model used by all cloud providers. Roles are associated to users by policies and role assignments . gcloud gcloud projects get-iam-policy $project The Cloud providers also still support legacy IAM systems which are deprecated. Classic administrator roles included Account Administrator, Service Administrator and Co-Administrator Primitive roles included Owner, Editor, and Viewer can still be applied to most GCP resources.","title":"IAM"},{"location":"Infrastructure/Cloud/#infrastructure","text":"All cloud providers divide their global services into a hierarchy of geographically defined regions , each of which is in turn divided into availability zones (what AWS calls its Global Infrastructure ). Azure datacenters contain multiple availability zones, and every Azure region has at least three availability zones. Azure services are also divided into geographies , generally coterminous with countries. Azure geographies are further divided into regional pairs . Each regional pair receives rolling updates one member at a time. Most services are regionally based, meaning the underlying hardware of that service's instance will exist in only a single Region. Some regions, like AWS GovCloud , have restricted access. Some AWS resources, however, are technically running on hardware that exists in a single Region, but presented as global. GCP regions Resources Services available on Free Tier","title":"Infrastructure"},{"location":"Infrastructure/Cloud/#monitoring","text":"Azure Monitor Network Watcher CloudWatch Stackdriver Trace","title":"\ud83d\udc41&#xfe0f; Monitoring"},{"location":"Infrastructure/Cloud/#resources","text":"Cloud providers exhibit some variety in how resources can be organized. All cloud providers support key-value tags , many of which can be applied to the same resource. Any Azure resource can only exist in a single resource group , which can contain resources from any region or subscription. However, resource groups may not contain other resource groups. GCP projects are equivalent to Azure resource groups, in that they are containers for and direct parents to resources. However, projects can be placed within folders , which do support nested hierarchies. AWS does not have an equivalent method of organizing resources. Azure subscriptions can be organized into Management Groups , and they can be nested in a hierarchy of management groups up to a maximum depth of six levels. In AWS the Organizational Unit (OU) , which can organize user accounts (subscriptions) and the resources they contain in a nested hierarchy, appears to be equivalent. A pattern common to Azure is that of a service being implemented in two resource types, one of which determines important configuration settings shared by all instances of the service which are contained within it. This is the case for storage accounts , App Service , Azure Data Explorer clusters, etc. Description Tenant Organization Organization Corresponds to a company or organization Management group Organizational Unit Logical container for user accounts and the resources created by that user Subscription Member account ? Credential associated with an individual Folder Organize resources and their parents in a nested hierarchy Resource group Project Logical container that is the direct parent to any resource, tied to a Region Tag Tag Label Key-value pairs that are used to organize resources The resource hierarchy organizes GCP resources in 3 levels below Domain Domain Organization corresponds to a company or organization. A single cloud identity is associated with a single organization and can have super admins Billing Account tracks charges and billing account admins can set budgets. Payments Profile is a Google-level resource that is used to pay for all Google services.","title":"Resources"},{"location":"Infrastructure/Cloud/#support","text":"AWS offers various support plan tiers that provide 24/7 email, chat, and phone access to AWS cloud support engineers. Basic Support Plan Developer Support Plan (greater of $29 or 3% of monthly account usage) Business Support Plan Enterprise Support Plan (>$15,000/mo.) offers a Technical Account Manager (TAM) , a dedicated guide and advocate AWS documentation is available in several places: AWS documentation AWS Knowledge Center is a sprawling FAQ AWS security resources AWS forums Professional Services team makes white papers and webinars publicly available","title":"\ud83d\udee0&#xfe0f; Support"},{"location":"Infrastructure/Cloud/#tags","text":"Azure tags: Tag names have a limit of 512 characters (128 characters for storage accounts) Tag values have a limit of 256 characters. Resources and resource groups are limited to 15 tags. VMs cannot exceed 2048 characters for all tag names and values combined.","title":"Tags"},{"location":"Infrastructure/Cloud/#infrastructure-as-code","text":"All cloud providers support ways of provisioning resources declaratively. Azure [ARM][ARM] templates are JSON, but [Bicep][Bicep] is a domain-specific language and command-line utility that can be used to generate templates from simpler, YAML-like syntax.","title":"Infrastructure as Code"},{"location":"Infrastructure/Cloud/#compute","text":"","title":"\ud83d\udda5&#xfe0f; Compute"},{"location":"Infrastructure/Cloud/#iaas","text":"All cloud providers offer Infrastructure as a Service (IaaS) , whereby virtual machines can be provisioned with specific compute resources and base operating systems. AWS also offers configuration management services like [OpsWorks][OpsWorks] and [Systems Manager][Systems Manager] GCP virtual machines are referred to as instances , and are available in three general machine family types: general-purpose, memory-optimized, and compute-optimized. Machine type describes the different packaged configurations representing allocated compute resources, or what is called a SKU in Azure.","title":"IaaS"},{"location":"Infrastructure/Cloud/#containers","text":"Build and package container artifacts Private container registry","title":"Containers"},{"location":"Infrastructure/Cloud/#serverless","text":"","title":"Serverless"},{"location":"Infrastructure/Cloud/#storage","text":"","title":"Storage"},{"location":"Infrastructure/Cloud/#archive","text":"","title":"Archive"},{"location":"Infrastructure/Cloud/#backups","text":"Azure Backup are integrated into Portal and clickable from the VM blade. You have to specify a Recovery Services vault and a Backup policy . The policy can specify frequency of backups, and other settings. Using Backup service costs $10 per VM plus the cost of used storage. 2 methods to restore data after backing up a VM to Azure Backup: Restore a recovery point as a new VM Restore access to files only","title":"Backups"},{"location":"Infrastructure/Cloud/#physical-media","text":"Data Box Import/Export Service Snowball Transfer Appliance Uploading files to GCS","title":"Physical media"},{"location":"Infrastructure/Cloud/#networking","text":"All cloud providers offer an implementation of software-defined networking (SDN) that allows a logically isolated network to be defined as a block of IP addresses allocated from one of the private ranges (10.0.0.0/8, 192.168.0.0/16, or 172.16.0.0/12), what in AWS and GCP is referred to as a VPC and in Azure a VNet . In all providers, the network is confined to a single region and must have at least one IP segment called a subnet defined within it which must be a subset of the range used to define the virtual network itself. The smallest possible CIDR range for a subnet in Azure is 29, which provides 3 addresses for use (Azure reserves 5). In AWS, the smallest possible CIDR range is 28. In AWS, VPCs have a default range of 172.31.0.0/16 and subnets have a default subnet mask of /20. In Azure, subnets span Availability Zones, can only be deleted if empty, and their names, which are immutable, must be unique. In AWS, a subnet exists only within a single Availability Zone. VNet peering allows VMs in two separate virtual networks to communicate directly. In all cloud providers, this is a one-way process which must be repeated in both directions in order to have two-way communication. In Azure, before the introduction of peering, virtual networks were connected using S2S VPN or by connecting to the same ExpressRoute circuit. It is not required for the peered networks to be in the same region ( Global VNet peering ), subscription, or tenant, although cross-tenant peering is not available in the Portal but must be configured from the command-line or ARM templates. VNet peering has to be disabled before moving a VNet , and a VNet can only be moved within the same subscription. There is a maximum of 100 peering connections per VNet Peerings cannot be moved to another resource group or subscription, so they must be disabled before moving peered VNets. Service endpoints facilitate restricting traffic from Azure services. Service endpoint policies allow restricting traffic to the granularity of individual Azure service instances. An internet gateway is a VPC resource that allows EC2 instances to obtain a public IP address and access the Internet. In order to access the Internet, instances must be in a public subnet , one that contains a default route to the VPC 's internet gateway. ExpressRoute is the main service used to connect Azure to on-premises networks, although P2S and S2S VPNs are also options. Direct Connect provides dedicated network connectivity to an AWS VPC through links offered through APN partners. In GCP, in addition to peering, a shared VPC can be created that is associated with multiple projects. Resources: Migrating to GCP? First Things First: VPCs","title":"\ud83c\udfe2 Networking"},{"location":"Infrastructure/Cloud/#user-defined-routes","text":"In Azure, a virtual appliance refers to a VM running a network application like a load-balancer, firewall, or router. Service chaining refers to the process of deploying a network virtual appliance (NVA) into a hub network to route traffic between spokes using user-defined routes (UDR) . This is a method of reducing the complexity of pairing between individual spoke networks in complex hub-and-spoke architectures. AZ-103: 309 In such a deployment, the peerings must be set to Allow Forwarded Traffic . Alternatively, two peered networks can share a single virtual network gateway, say to connect to an external network. The pairing connection to the network that contains the gateway must be set to Use Remote Gateways The pairing connection from the network containing the gateway must be set to Allow Gateway Transit","title":"User-defined routes"},{"location":"Infrastructure/Cloud/#network-security","text":"Network Security Group (NSG) Security Group Firewall Rules Azure Network Security Groups (NSGs) are associated with network interfaces and contain an arbitrary number of security rules . Each rule has the following properties: Name Priority : number between 100 and 4096, lower numbers indicate a higher priority Source or destination : IP address, CIDR block, service tag, or application security group Protocol : TCP , UDP , ICMP , or Any Direction : Inbound or outbound Port range ; Action : allow or deny Service tags represent a group of IP address prefixes managed by Microsoft available for use in NSG rules: VirtualNetwork : all CIDR ranges defined for the virtual network, all connected on-premises address spaces, peered VNets or VNets connected to a VNET gateway AzureLoadBalancer : Virtual IP address of the host where Azure's health probes originate Internet : IP address space that is outside the virtual network AzureCloud* : IP address space for Azure, including all datacenter public IP addresses AzureTrafficManager* : IP address space for the Azure Traffic Manager probe IP addresses Storage : NSG flow logging ,which saves the 5-tuple of all packets, is available as a low-cost way to monitor traffic. Flow logs record all IP flows going in and out of an NSG and are collected per NSG rule. They are charged per GB of logs collected and include a free tier of 5 GB/month. In AWS VPCs, Security Groups are similar to firewall rules that regulate inbound and outbound traffic of an instance. Outbound traffic is unrestricted by default, and every VPC contains a default security group. A network access control lists (NACLs) , also like a firewall, contains inbound and outbound rules but operates on the subnet. By default, a NACL allows all inbound and outbound traffic. In GCP, each VPC has a set of firewall rules that control traffic not only into and out of the VPC , but between instances in the same VPC . Each rule can be tagged, and individual instances with the same tags inherit those rules. Resources: Protect your Google Cloud Instances with Firewall Rules","title":"Network security"},{"location":"Infrastructure/Cloud/#dns","text":"Azure DNS Route 53 Cloud DNS","title":"DNS"},{"location":"Infrastructure/Cloud/#cdn","text":"Users can use Azure CDN as a cache, reducing load from website. Content is cached by the CDN until its time-to-live (TTL) elapses, which can be controlled in the HTTP response from the origin server. Permanently removing content from the CDN requires it be first removed from the origin servers, meaning if the content is in a storage account it should be set to private or deleted from the storage, or the container itself should be deleted. Cached copies may remain in the CDN endpoint until the TTL has expired, unless it is purged . There are 4 pricing tiers available within Azure CDN: Azure CDN Standard from Microsoft does not offer dynamic site acceleration (DSA) (cf. Azure Front Door Service) Azure CDN Standard from Akamai Azure CDN Standard from Verizon Azure CDN Premium from Verizon , for which caching is configured using a rules engine. AWS CloudFront GCP CDN","title":"CDN"},{"location":"Infrastructure/Cloud/#load-balancing","text":"Azure Load Balancer Application Gateway AWS Elastic Load Balancer GCP Load balancing","title":"Load-balancing"},{"location":"Infrastructure/Cloud/#development","text":"","title":"\ud83d\udc68\u200d\ud83d\udcbb Development"},{"location":"Infrastructure/Cloud/#nosql","text":"NoSQL databases differ from relational databases in that they do not obey the principle of data normalization . That is, the same data can be stored in more than one place. This is an advantage for databases that are optimized for reads as opposed to writes, because fewer queries are needed to retrieve information. However, when changing information that is duplicated in several places, write operations will be more laborious and prone to error. NoSQL databases are also horizontally scalable because the information can be sharded horizontally more easily than relational database, which are only vertically scalable (meaning scaling them requires larger and larger computers) and can only be sharded vertically. ( src )","title":"NoSQL"},{"location":"Infrastructure/Cloud/#big-data","text":"","title":"Big Data"},{"location":"Infrastructure/Cloud/#history","text":"Beginning in 2000, Amazon began developing Merchant.com, a planned e-commerce service that was intended to be the base upon which other enterprises would develop online shopping sites. At the time, Amazon's development environment was a jumbled mess, and in the effort to consolidate and organize the enterprise into a set of well-documented APIs. Despite these changes, software development remained sluggish, and an investigation discovered that individual teams were procuring storage, compute, and database resources independently. AWS originated out of the effort to consolidate these resources across the enterprise and remove this bottleneck. Azure was announced in 2008 and publicly released in 2010 after earlier experiments in cloud computing like Whitehorse and RedDog. In fact, references to the \"classic\" model predating the Azure Resource Manager (ARM) actually refer to RedDog: the \"classic\" portal was also known as \"RedDog Front-End\".","title":"History"},{"location":"Infrastructure/Cloud/#glossary","text":"Apigee The Apigee API platform is a management service that allows developers to deploy, monitor, and secure their APIs and generates API proxies. App Engine App Engine allows developers to deploy applications developed in popular programming languages to a serverless environment. It is available in two environment types: Standard and Flexible. Standard environment is the original App Engine environment, consisting of a preconfigured, language-specific runtime like Java, Python, PHP, Node.js, or Go. Flexible environment is similar to [ GKE ][ GKE ] in that it can run a customized container. App Engine is designed to support applications implemented as a microservices architecture. There are four components: The application is the top-level container that houses all other components. Services are versioned and provide a specific function. Versions are produced every time a service is updated. Every version runs on an instance . Each version of a service runs on its own instance, whose size can be determined by specifying the instance class . Instances can be dynamic or resident. Resident instances run continually and can be added or removed manually. Dynamic instances support autoscaling based on load. App Engine has three modes of scaling: Automatic scaling creates an instance with a specified request rate, response latency, and application metrics. Basic scaling creates instances only when requests are received Manual scaling supports operational continuity regardless of load level. App Engine Deployer Read-only access to all application configuration and settings. App Engine Service Admin Read-only access to all application configuration and settings. Write access to module-level and version-level settings. Cannot deploy a new version. App Service An App Service plan resource determines the billable compute resources available for the App Services applications managed by it. A plan acts as a container for multiple web applications sharing the same server farm (\"workers\"), and for this reason Windows and Linux apps can't be mixed in the same App Service plan. \"Web app\" is the legacy name for Azure App Service . App Service SSL certificates need to be deleted from each App Service before moving it to a new resource group. Application Gateway Azure Application Gateway is used to load balance a large-scale set using more than 100 instances in place of Azure Load Balancer . AZ-103: p. 223 Application Gateway supports session affinity to save user state using browser cookies. Unlike Azure Load Balancer, which operates at OSI layer 4 and has limited security capabilities, Application Gateway operates at OSI layer 7 and provides Web Application Firewall (WAF) functioanlity to block attacks like SQL injection, cross-site scripting, and header injection. HTTPS is also only available with layer 7 load balancers like Application Gateway. Athena Athena is a serverless AWS service that allows SQL queries to be run against data stored in a [ S3 ][ S3 ] bucket. Athena works closely with [AWS Glue][AWS Glue] to extract schema information and crawl data sources. Before running for the first time, you must provide a path to a S3 bucket to store query results. AWS CLI AWS CLI is version 1 is maintained for legacy compatibility purposes. AWS Developer Tools A collection of tools that provide CI services: CodeCommit CodeBuild CodeDeploy CodePipeline AWS Glue AzCopy AzCopy can be used to copy files to File storage. Azure Bastion Azure Bastion is a PaaS service deployed within a VNet that allows connectivity to a VM from the Portal. Once deployed in a VNet , RDP/SSH is available to all VMs in that VNet . This session is streamed to your local device over an HTMLS session using the browser. It is not deployed per VM, but once per VNet to its own dedicated subnet , at least /27 or larger No public IP is necessary on the VM, the connection from Bastion to the VM is to the private IP. However, the Bastion itself does require a public IP. Bastion can now span peered VNets IPv6 support is limited in Azure. IPv6 addresses are not added to VMs by default and must be explicitly defined by adding an endpoint to each VM to be using it. Routing by IPv6 is also not supported, so load balancers have to be deployed. Bicep Project Bicep is a domain-specific language and command-line utility that can be used to generate [ARM][ARM] templates. Project Bicep \u2013 Next generation ARM Templates Azure Container Instances Azure Container Instances ( ACI ) allows a simpler way of running isolated containers in smaller-scale deployments than Azure Kubernetes Service . The top-level resource in ACI is the container group , a collection of containers that get scheduled on the same host machine. These containers share a lifecycle, resources, local network, and storage volumes, and is equivalent to a Kubernetes pod. Container groups can be deployed to a subnet that already hosts a container group or an empty one, but it may not be deployed to a subnet that already has other resources like VMs. Azure Data Explorer Azure Data Explorer (ADX) has two architectural elements: Data Management Engine ADX does not hold large tables in a single table, rather it automatically shards them into Extents Azure DevOps Azure DevOps used to be known as Visual Studio Team Services and Team Foundation Server . Install DevOps CLI az extension add --name azure-devops Azure DNS Azure DNS supports private zones, which provide name resolution for VMs on a VNet and between VNets without having to create a custom DNS solution. Time-to-live for DNS record sets is provided in seconds. Azure DNS alias records allow other Azure resources to be referenced from the DNS zone, rather than static IP addresses or domain names. This allows these records to be automatically updated or deleted when the underlying Azure resource is changed. An A alias record set is a special type of record set that allows you to create an alternative name for a record set in your domain zone or for resources in your subscription. A CNAME alias record set can only point to another CNAME record set. Custom domains can be used by implementing CNAME DNS records, which are used in DNS to map alias domain names to the \"canonical\" name. Azure File Service Azure File Service allows you to create one or more file shares in the cloud (up to 5 TB per share), similar to a regular Windows File Server. It supports the SMB protocol, so you can connect directly to a file share from outside of Azure, if traffic to port 445 is allowed through the LAN and ISP. It can also be mapped within Windows. A clever use of a file share is as persistent storage for the Azure Cloud Shell. src Azure File Sync Azure File Sync extends Azure File Service to allow on-premises file services to be extended to Azure while maintaining performance and compatibility, communicating over TCP 443 over SSL, and not IPSec. Use cases include: Replace on-premises file servers Easily replicate data on-premises to make it available during lift-and-shift migrations Simply cloud development and management Azure File Sync works using an Azure File Sync agent , available as an MSI package for Windows Server 2012R2, 2016, and 2019, to register file servers as endpoints to an Azure File Sync Group . After installation, Azure credentials for a subscription must be provided. AZ-103: 153 In order to create an Azure File Sync, first a Storage Sync Service resource must be created, which works like a container to hold one or more sync groups . Every sync group has only a single cloud endpoint , referring to a storage account, but can have more than one server endpoint . Any server can only be registered to a single Storage Sync Service, and servers synced to different Storage Sync Service resources cannot sync with each other. Cloud tiering is an optional feature in Azure File Sync in which frequently accessed files are cached in the on-prem file servers, while less commonly accessed files are tiered to Azure Files. This is done by enabling Cloud Tiering, then selecting a free space policy , a percentage which indicates the amount of free space to maintain on the server endpoint's volume. When a user does access one of these tiered files, that file is downloaded to the on-prem cache and made available locally from that point on. This frees up local storage. Cloud tiering cannot be used with server endpoints on the system volume Although server endpoints can be configured with different free space policies, the most restrictive setting takes effect For tiered files, the file will be partially downloaded as needed Although a mount point can be a server endpoint, there can be no mount points inside a server endpoint When a filename collision occurs between the file share and file server, the file on the server has its filename appended with the server's name. Azure Policy Azure Policy is a service that can create, assign, and manage policies to enforce governance. Policy definitions, authored in JSON, implement policy by describing desired behavior for Azure resources when they are created or updated. AZ-103: p. 72 To implement policy, a policy definition is created first, then a policy assignment assigns it to a scope. Policy definitions can be packaged together using initiative definitions and applied to a scope using initiative assignments RBAC roles deny by default and allow explicitly . But Azure Policy allows by default and denies explicitly Policies can be applied at the management group , subscription , or resource group scope, with all child resources and resource groups being affected. Every policy definition has a single effect , which includes: Audit : create a warning event in the log Modify : used to add, update, or remove properties or tags on a resource during creation or update. Append AuditIfNotExists Deny DeployIfNotExists Disabled The order of evaluation of effects is: Disabled, Append, Deny, Audit (\"DADA\") Azure VMs Virtual Machines represent Azure's IaaS offering. A dedicated host group has to be created and placed in a resource group and associated with a location and availability zone and assigned a fault domain. A host then has to be created, a size specified, and associated with a host group. Any VM intended to run on the host has to be created in the same location and availability zone and associated with the host in the Advanced tab. Azure spot instances are available at deep discounts. 3 types of disk are available to Azure VMs: Operating System Disk (OS Disk) Temporary Disk Data Disk Azure VM image types include: Managed images (recommended), which remove the dependency of the VM to the image, at least within the same region. Copying a VM to another region still requires the managed image to be copied first. Unmanaged images, which required the VM to be created in the same storage account as that of the image. VM copies required the image to be copies first. VM images are captured from an existing VM that has been generalized (prepared), removing unique settings (hostname, security IDs, personal information, user accounts, domain join information, etc) but not customizations (software installations, patches, additional files, folders), using sysprep.exe for Windows machines or Microsoft Azure Linux Agent ( waagent ) for Linux machines. VM images in AWS are called Amazon Machine Images (AMI) . Azure VPN Virtual network gateways in Azure are of two types: VPN gateways and ExpressRoute gateways . Any virtual network can have only a single gateway of each type. VPN gateways send encrypted traffic between the virtual network and an on-premises location. VPN Gateways must be deployed into their own dedicated subnet (named \"GatewaySubnet\" ) with a minimum size of CIDR /29, although a CIDR /27 address block is recommended. VPN connections between an on-premises network and a VNet are only possible if the network ranges do not overlap. VPN gateways can be classified by the topology of the connection: Site-to-Site (S2S) connections require an on-premises VPN device associated with a public IP address. Multi-Site connections require a RouteBased VPN type. Point-to-Site (P2S) allows individual computers to securely connect to a VNet without need for a VPN device, which is useful for telecommuting, and can use SSTP, OpenVPN, or IKEv2. There are several authentication considerations. VNet -to- VNet connections are also possible, but VNet peering may be preferable if the virtual networks meet certain requirements. Site-to-Site Multi-Site Point-to-Site VNet -to- VNet VPN gateways can also be classified on VPN type . Route-based VPNs (previously called \"dynamic routing gateways\") require routes to be defined in a routing table to direct packets into tunnel interfaces. Policy-based VPNs (previously called \"static routing gateways\" in the classic deployment model) can only be used on the Basic gateway SKU and offer only a single S2S tunnel. Route-based Policy-based There is a profusion of Gateway SKUs that determine the maximum connections, throughput, and availability of other features like BGP and zone-redundancy available for each topology. Every Azure VPN gateway consists of two instances in an active-standby configuration. During failover, a brief interruption of 10-15 seconds for planned maintenance or up to 60-90 seconds in the case of unplanned disruption, may occur. But the gateway can be configured to be active-active , which will establish S2S VPN tunnels to both gateway instances with traffic being routed through both tunnels simultaneously. There will still be only a single connection resource, but the on-premises VPN device must be configured to establish both of these tunnels. The most highly available arrangement would use multiple VPN devices with the VPN gateway in active-active configuration, creating 4 IPsec tunnels that evenly carry Azure traffic. Active-Standby Active-Active Dual redundancy Bigquery Petabyte-scale analytics database service for data warehousing. BigQuery can be executed using the bq command-line utility. BigTable GCP realtime database used for Big Data. BigTable can be executed using the cbt command-line utility. BigTable evolved out of Google's need to ensure access to petabytes of data in its web search business line. It was described in a 2006 research paper that ended up launching the entire NoSQL industry. In 2015 it was made available as a service to cloud customers. src BigTable doesn't support secondary indexes. Billing Account Administrator GCP predefined role that grants permissions to manage self-service accounts but not to create new ones. Billing Account Creator Predefined GCP role that grants permissions to create new self-service accounts. Billing Account User GCP predefined role that enables user to link projects to a billing account. Billing Account Viewer GCP predefined role that grants permissions to view transactional and billing data associated to a GCP account. Cloud AutoML GCP service that allows developers without machine learning experience to develop machine learning models. Cloud Device Administrator : Azure built-in role that grants users full access to manage devices in Azure AD. Cloud Functions GCP serverless compute offering suited to running short-running logic, such as calling other APIs in response to an event. Cloud IAM GCP's identity and access management platform. Permissions include: roles/container.admin roles/container.clusterAdmin roles/container.clusterViewer roles/container.developer roles/container.hostServiceAgentUser Predefined roles include: App Engine Deployer App Engine Service Admin Billing Account Administrator Billing Account Creator Billing Account User Billing Account Viewer Compute Engine Admin Compute Engine Network Admin Compute Engine Security Admin Compute Engine Viewer Compute Service Agent Folder Admin Project Creator Shared VPC Admin Cloud Machine Learning Engine Platform for building and deploying scalable machine learning systems to production. Cloud Natural Language Processing GCP tool for analyzing human languages and extracting information from text. Cloud Run Google Cloud Run is built on a native open standard that will allow using the same container on other cloud providers. It bills down to the nearest 100 ms interval. Cloud Run provides an HTTPS endpoint to the container. Cloud Run can also run on your own K8S cluster running on [ GKE ][ GKE ], recommended for workloads that have a consistently high level of traffic, since you are billed for the provisioned cluster resources. However, resources like CPU, GPU, and other items can be customized. Cloud Vision Image analysis platform for annotating images with metadata, extracting text, or filtering content. CloudFormation AWS declarative automation service, which can use JSON or YAML-format templates. These resources are placed into a named stack , a container that organizes the resources described by the template, and the stack name must be unique to the account. This allows provisioned resources to be easily managed, since the stack contains a record of events, and to be quickly destroyed by deleting the stack. CloudFormation Designer allows templates to be viewed as a diagram of resources. CloudFront AWS CDN offering that helps deliver static and dynamic content worldwide. CloudFront caches content in edge locations , of which there are more than 150 spread out across 6 continents. Edge locations may not be chosen arbitrarily, rather there are three options: US, Canada, and Europe US, Canada, Europe, Asia, and Africa All edge locations In order to make content available on CloudFront, you must create a distribution , which defines the type and origin of the content to cache. There are two types of distribution: A Web distribution is used for static and dynamic content, including streaming video, accessible via HTTP or HTTPS. Its origin can be a web server or a public S3 bucket. Real-Time Messaging Protocol (RTMP) distribution delivers streaming audio or video. The media player and media files must be stored in S3 buckets. CloudTrail AWS service that logs actions against AWS resources. These events are divided into API and non-API actions. API actions include creating, modifying, or deleting resources. Non-API actions include everything else, like logging into the management console. Events are also classified as management events and data events Management events (also control plane operations ) are operations that a principal attempts to execute against an AWS resource. Data events are S3 object-level activity and Lambda function executions. These are treated separately from management events because they tend to be higher volume. CloudWatch Amazon CloudWatch collects logs, metrics, and events from AWS resources and non-AWS on-premises servers and presents a dashboard for visual analysis. All AWS resources automatically send their metrics to CloudWatch Metrics, which stores the data for up to 15 months. CloudWatch alarms can be configured for single metrics. Applications and AWS services have to be configured to send log events to CloudWatch Logs, and they are stored indefinitely by default although retention settings can be configured. Log events from the same source are organized into a log stream. Log streams are then organized into log groups. Metric filters extract metric data from log events. CloudWatch Events is a feature that monitors for changes in AWS resources as a result of API operations. Cloudyn Previously a standalone service available in Azure, now deprecated because its functionality has been incorporated natively into other sections of the Cost Management + Billing blade. CodeCommit AWS private git repo service. CodeDeploy AWS service for automatically deploying applications to AWS compute resources or on-prem servers. CodeDeploy can pull source code from [ S3 ][ S3 ] and repos from GitHub or Bitbucket but notably not CodeCommit (ref. CodePipeline ). CodePipeline AWS service for orchestrating and automating every stage of software development. It defines a series of stages, two of which are required - source and deployment - but other stages like testing or approval can be incorporated. Cognito AWS service that integrates with identity providers like Amazon, Google, Microsoft, and Facebook to add user access control to an application. Compute Engine Compute Engine is GCP's IaaS offering. An instance group is a collection of VM instances that you can manage as a single entity. Two types: Managed instance groups operate applications like web front-ends across a group of identical VMs created with a template. They provide high availability, healing, scaling, and automatic updates. Unmanaged instance groups allow you to manually load balance a group of VMs. VMs can be added or removed at will. Getting started with GCE Compute Engine Admin Predefined GCP role that grants full control of Compute Engine resources. Compute Engine Network Admin Predefined GCP role that grants full control of Compute Engine networking resources. Compute Engine Security Admin Predefined GCP role that grants full control of Compute Engine security resources. Compute Engine Viewer Predefined GCP role that grans read-only access to all Compute Engine resources, but exclusive of data stored on disks, images, and snapshots. Compute Service Agent Predefined GCP role that grants Compute Engine Service Account access to assert service account authority. Computer Vision subfield of artificial intelligence concerned with developing the capability of computers to recognize objects in images and to understand visual information. Container Instances Azure Container Instances ( ACI ) is a PaaS service that facilitates deployment of individual containers. CosmosDB Azure NoSQL offering. Cosmos DB started as Project Florence in 2010 to address shortcomings with SQL Server in supporting highly available services like Xbox. In 2015 the product was relaunched as Document DB, then renamed Cosmos DB in 2017. An emulator is available for Cosmos DB here . A Cosmos DB account can be used for free for 30 days, and does not require an Azure subscription. Throughput is measured and billed in Request Units (RU) per second. The minimum manually provisioned throughput level is 400 RU/sec. There are three throughput provisioning offers: Manual , where a static throughput level is provisioned. This is best for highly predictable workloads. Autoscaling , where Azure will automatically scale throughput based on usage, reducing it down to a minimum of 10% of the provisioned throughput. Serverless , where you pay for only the RUs you need. This throughput provisioning model is ideal for small demonstration projects. This feature is forthcoming. The cost of using a CosmosDB database can be approximated using the Capacity calculator . In general, these are reasonable back-of-hand estimates for common operations to estimate costs: Read item: 1 RU SQL query: ~2.8 RU Create item: 10 RU There are various choices of API for Cosmos DB accounts which affect the data model used for databases. SQL API is the Core API, and works off JSON documents and SQL query syntax MongoDB API uses BSON documents (binary encoded JSON) and MongoDB query syntax Table API and uses Key-Value database design reflects API of Azure Table Storage Gremlin API is a graph database using a flat store of vertices and edges Cassandra API is columnar, and unlike most NoSQL databases does specify a schema Consistency Uniformity of data across replicas in a distributed database. Consistency levels describe how and when data is replicated to provide varying consistency guarantees. Strong consistency is the strongest consistency model and requires synchronous replication after every change to database, increasing latency for each write. Session consistency is unique, in that it offers consistent prefix to databases that support a single session or an application with a single token. Eventual consistency is the weakest consistency model and provides no ordering guarantees. Consistent prefix offers read throughput, availability, and write latency comparable to eventual consistency while guaranteeing global order. Bounded staleness implies asynchronous replication and offers guarantees on the number of versions ( K ) or time interval ( T ) reads lag behind writes, referred to as the staleness window . As the staleness window approaches, Azure will delay writes by providing back pressure on writes. Outside the staleness window, data is guaranteed to be globally consistent. Outside the region in which the writes were made, Azure guarantees total global order or consistent prefix , which means, the global order is maintained. Strong Session Eventual Bounded staleness Horizontal partitioning is what allows Cosmos DB to scale-out massively to provide high availability and elasticity. Partitions can be thought of as physical fixed-capacity data buckets that back every container. A partition split occurs when a new physical partition is brought online, resulting in half of the documents existing on a previously existing partition being moved to the new one. Cosmos DB automatically and transparently splits horizontal partitions to achieve elasticity. Logical partitions , determined by the partition key which is set at container creation, group individual documents in ways that are kept on the same physical partition. It is recommended to have a high number of logical partitions, so that CosmosDB has greater flexibility partitioning documents. The partition key is immutable, so the correct choice of partition key is an important architectural consideration. Even distribution of documents is ideal to avoid hot partitions , where some partitions have much greater activity than others, due to uneven distribution of documents. Any partition may not be greater than 20 GB in size. Physical partitions have 4 replicas within a region. There are several common partitioning patterns: Partitioning on /id , which results in every document existing in its own logical partition. This pattern is write-optimized and ideal for IoT applications. Any SQL query for more than one document would be cross-partition by necessity, so direct reads using the /id value would be far more economical. Partitioning small lookup lists on a /type property. This will keep lists of related items used for lookups in the same partition. Optimizing for queries by organizing multiple types of document according to a key data-point. For example, customer data could be kept in the same partition as that customer's orders, avoiding cross-partition queries. Cost Management Contributor Azure built-in role that grants access to the Cost Management blade. Cost Management Reader Azure built-in role that grants access to the Cost Management blade. Data Box Microsoft-provided appliance that allows for the transfer of large volumes of data to Azure, available only to EA , CSP, and Microsoft Partner Network Sponsorship offer types. Workflow Order: Use Portal to order a data box by creating a Data Box resource Receive: Connect Data Box to network Copy data: Mount file shares and copy data to the device. Return: to Microsoft Upload: Microsoft will upload the data and securely erase it from the device Offering Capacity Storage saccounts Data Box Disk 35 TB 1 Data Box 100 TB 10 Data Box Heavy 1,000 TB 10 Dataflow GCP streaming data framework for defining both batch and stream processing pipelines. Dataprep GCP managed service that allows analysts to visually explore, clean, and prepare data for later analysis. Dataproc GCP service that manages the creation of data science clusters and data analysis jobs. DynamoDB : NoSQL database known for fast (1-9 ms) query times. DynamoDB measures capacity in Read Capacity Units (RCU) and Write Capacity Units (WCU) . 1 RCU = 1 record at most 4 KB in size 1 WCU = 1 record at most 1 KB in size DynamoDB offers the choice between strongly consistent and eventually consistent (half the cost) reads. DynamoDB offers two types of indexes: Global Secondary Index allows you to create a completely new aggregation of data. GSI updates are eventually consistent , with asynchronous updates populated after an update response is passed to the client. Local Secondary Index (LSI) alternate sort key attribute that allows only sorting DynamoDB Streams (changelog for the DynamoDB table) interfaces with AWS Lambda to implement complex queries , computed values like sum, average, maximum, etc. These are implemented in a different processing space than the DynamoDB table itself, so that it does not affect the table. AWS Lambda has an invocation role which defines what Lambda can see (triggered upon a change to the table as reported in DynamoDB Streams) and an execution role which defines what it can do . Elastic Container Service Elastic Kubernetes Service Elastic Beanstalk : AWS PaaS offering. Elastic File System : Scalable file system for AWS Linux instances that allows multiple instances to be attached to a single EFS volume to share files. EFS volumes are highly available, spanning multiple Availability Zones in a single VPC . Elastic MapReduce : AWS's managed Big Data analysis service, supporting Apache Hadoop, Apache Spark, HBase, Presto, and Flink. Enterprise Agreement Azure customers on an Enterprise Agreement can add up-front commitments to Azure then be billed annually. If the committed spend is exceeded, the overage is billed at the same EA rate. EA customers can create spending quotas and set notification thresholds through the EA Portal. 3 portals used to manage Azure subscriptions EA Portal (ea.azure.com) available only to customers with an Enterprise Agreement Account Portal Azure Portal, includes Azure Cost Management ExpressRoute There are four main architectures used with ExpressRoute Any-to-any connection is used to integrate on-premises WANs using IPVPN. Co-location with cloud exchange is used to order virtual cross-connections to the Azure cloud through the co-location provider's Ethernet exchange. Point-to-point Ethernet connection is used to configure on-premises data center connectivity to Azure through individual point-to-point links Firebase : GCP NoSQL database offering known for its client libraries. Firebase Auth offers a free user interface for applications, Firebase UI . Firestore was released from beta in early 2019 and combines and improves upon functionality of previous products named Cloud Datastore and Firebase Realtime Database . Firestore is organized into documents , which consist of key-value pairs and are similar to JSON objects, and collections . JSON-like objects are called maps and keys are called fields in Firestore. Collections can contain only documents, but documents can contain sub-collections. Root can only contain collections. So navigating deeper and deeper into the information store will involve alternating between collections and documents. Firestore features a compatibility mode that emulates the behavior of Datastore in accessing Firestore's storage layer while removing some of Datastore's limitations. Queries in Firestore can only be used to find documents stored in one specific collection or sub-collection. However a collection group query , meaning one that spans multiple collections, began to be supported in 2019. Complex relational queries are not possible (in a single query), and query results are usually returned based on equality or greater-than/less-than comparisons. The field has to be specified as having a scope of \"Collection group\" within GCP, and there is a limit of (about) 200 for these queries. An index is created for every field in every document added to a collection, which results in very fast query times that are proportional to the number of results , not records searched. This structure ensures that equality searches are highly performant, as are comparison searches using greater-than or less-than. But this implementation creates bizarre limitations to Firestore's querying capabilities: There is no native way to perform wildcard searches or OR queries. For common instances of such queries, Google recommends adding a field that contains the value for each record Inequality searches present a challenge for Firestore. For some queries that combine conditions on more than one field (i.e. restaurants within a certain range of a location), Firebase will create a \"composite index\" (only within the index, the document itself is not affected) automatically to facilitate searches on those fields. Unlike Firebase , which charges based on the volume of data stored, Firestore charges based on number of operations performed and records returned. Folder Admin : Predefined GCP role that allows folders to be created at an Organization. Front Door : Azure offering that works like Azure Load Balancer for web apps. Glacier : AWS storage service that offers long-term archival at low cost. One or more files are stored in an archive, typically a .zip or .tar file containing multiple files. Archives can range from 1 B to 40 TB in size. Archives are stored in a Glacier vault , a region-specific container analogous to S3 buckets. Vaults must have regionally unique names, but there is no need for a globally unique name. Glacier vaults can be created and deleted using the Glacier service console. But uploading, downloading, or deleting archives must be done through the AWS CLI or an application using the SDK. Some third-party applications can also interact with Glacier. Google Cloud Identity Google's IDaaS provider.","title":"\ud83d\udcd8 Glossary"},{"location":"Infrastructure/Cloud/#gke","text":"Clusters have two modes of operation, Standard and Autopilot , that offer a more configurable and a more managed experience respectively. In GKE , clusters are billed at a flat fee of $0.10 per cluster hour . The GKE free tier provides $74.40 in monthly credits per billing account, available to Autopilot and Standard zonal clusters but not Standard regional clusters. Note that there are 744 hours in a 31-day month, and the free tier monthly credit corresponds to exactly this amount of usage for a single cluster. Import/Export Service Azure service that allows the physical shipment of disks procured by the user to Azure for import into a storage account , which can be placed into blob or file storage. This service requires the use of a Windows computer with BitLocker and .NET Framework and is dependent on the WAImportExport.exe utility. Procure 2.5-inch or 3.5-inch SATA (not SAS) disks Connect the disks to a Windows machine. Create a volume and encrypt it using BitLocker Install the Azure Import/Export tool (WAImportExport.exe) on the disks. Copy files Create an import job in the Azure Portal Kinesis AWS service for ingestion and processing of streaming data, such as access logs, video, audio, and telemetry. Kubeflow Cloud-native platform for machine learning based on Google\u2019s internal machine learning pipelines.","title":"GKE"},{"location":"Infrastructure/Cloud/#kusto","text":"Case-sensitive query language developed by Microsoft and used in several Azure services: Azure Data Explorer Log Analytics Sentinel Application Insights Microsoft Defender ATP Lightsail offers blueprints that will automatically provision all compute, storage, database, and network resources needed for a deployment. Macie AWS service that automatically finds and classifies sensitive data stored in AWS using machine learning to recognize sensitive data such as PII or trade secrets. Microsoft Azure Recovery Services Azure agent for backing up Windows machines only, but can also be installed on instances of other cloud providers like AWS. MARS can be configured to protect the entire system, volumes, or individual files and folders. Monitor Neptune AWS graph database. Network Performance Monitor Azure Log Analytics network monitoring solution for hybrid networks, providing 3 services: - Performance Monitor monitors connectivity between various points in both Azure and on-prem networks - Service Connectivity Monitor monitors outbound connectivity from network nodes to external TCP services, monitoring performance metrics like latency, response time, and packet loss - ExpressRoute monitors end-to-end connectivity between on-prem network and Azure over ExpressRoute Network Watcher Network Watcher appears like a normal resource in a resource group, but it is deployed as a single instance per Azure region. Network Watcher monitoring and diagnostic tools: IP Flow Verify Next Hop Packet Captures link a Network Watcher resource, a target VM, a storage account, and a filter that specifies the characteristics of network traffic (source and destination IP addresses and ports as well as protocol) to capture, as well as a time limit. Network Topology OpsWorks OpsWorks is AWS's declarative configuration management service that uses the Chef and Puppet configuration management platforms and comes in three varieties: OpsWorks for Puppet Enterprise OpsWorks for Chef Automate OpsWorks Stacks Project direct parent of all other GCP resources, consisting of a project name, project ID, and project number. Project Creator Predefined GCP role given to all users currently assigned to a project. Pub/Sub GCP messaging service, allowing services and applications to communicate. Recovery Services Vault Azure resource used to centrally manage the backup and recovery. A Backup protection policy defines how a backup plan is implemented. These are most easily created through the Portal. A vault can only back up data from other resources that exist in its region. Rekognition AWS deep learning-based image recognition service. Resource Policy Contributor Azure built-in role that includes access to most Policy operations and should be considered privileged. Route 53 AWS managed DNS service. Like any other DNS system, it relies on resource records defined in a zone . Route 53 can also provide name resolution for private domain names , used on private networks. Private hosted zones provide DNS resolution for a single domain name within multiple VPCs. But when a resource record must be changed dynamically to work around failures or route users to an underutilized server, routing policies can be used. Simple policy is the default for new resource records and maps a domain name to a single value (i.e. an IP address). Weighted policy distributes traffic across multiple resources according to a predefined ratio. Latency policy sends users to resources in their closest Region. Failover policy allows a secondary resource to be marked for routing when the primary resource is unavailable. Geolocation policy routes users based on their specific continent, country, or state. Multivalue answer policy allows even distribution of traffic across multiple resources by randomizing the order of returned records. All routing policies except Simple can use health checks to modulate routing action. All health checks occur every 10 or 30 seconds and can check one of three resources: Endpoint makes a test connection to a TCP port CloudWatch alarm can be set off in case of high latency or other metrics. Calculated monitors the status of other health checks. Route 53 also offers the Route 53 Traffic Flow visual editor that allows you to create a diagram to represent the desired routing. The diagram isn't translated to individual resource records but rather represents a single policy record which costs 50 USD/month each. In addition to the routing policies above, Traffic Flow also offers the Geoproximity routing policy that directs users to a geographic location based on how close they are. Simple Storage Service AWS storage service. S3 stores objects in a container called a bucket . Each bucket must have a globally unique name and exposes a HTTP endpoint (at https://$BUCKET.s3.amazonaws.com/) Each object is associated with a key . Keys are equivalent to filenames, and the bucket is equivalent to a flat filesystem. However, directories can be simulated by placing slashes in the key. Bucket policies (applied to buckets) and user policies (applied to IAM principals) can be used to modulate accessibility. Public or anonymous access to an object can only be granted by bucket policies. Bucket and object ACLs are legacy access control methods that are still usable. S3 buckets store data unencrypted, although encryption at rest is available in two options: Server-side encryption: S3 encrypts uploaded objects before storing them, and decrypts it again before delivery. Client-side encryption: User must encrypt data prior to uploading and decrypt it after downloading. S3 offers various storage classes that differ in their availability and durability , the percent likelihood that an object within it will not be lost over the course of a year. Frequently accessed objects: STANDARD REDUCED_REDUNDANCY Infrequently accessed objects STANDARD_IA ONEZONE_IA GLACIER INTELLIGENT_TIERING automatically moves objects to the most cost-effective storage tier based on access patterns. Storage Gateway is an on-premises VM that provides a connection to S3 for on-premises infrastructure. File gateway lets you use NFS and SMB file shares to transfer data to S3 . Data is stored in S3 and cached locally. Volume gateway can be used as an iSCSI target by on-premises servers. Two configuration variants exist: Stored volumes : All data is stored locally and asynchronously backed up to S3 as EBS snapshots. A stored volume can range from 1 GB to 16 TB in size. Cached volumes : Data is stored in S3 and frequently used data is cached locally. A cached volume can range from 1 GB to 32 TB in size. Tape gateway is configured as an iSCSI target by a backup application. Virtual tapes range from 100 GB to 2.5 TB in size. These tapes are asynchronously transferred to a virtual tape library (VTL) backed by S3 and removed when the upload is complete. Recovery requires downloading the virtual tape again. Cloud Storage in Minutes with AWS Storage Gateway Sentinel Azure cloud-native SIEM and SOAR soluation that can collect data from many sources and present it to security analysts, who can run Kusto queries against the dataset. Azure Sentinel can ingest data from on-premises devices using one of several types of connector , categorized by the type of data ingestion: Native connectors integrate directly with other Microsoft security products, like Azure AD, M365, and Azure Security Center Direct connectors are configured from their source location, such as AWS CloudTrail, Azure Firewall, and Azure Front Door API connectors are implemented by security providers, like Azure Information Protection (AIP), Barracuda Web Application Firewall (WAF), and Microsoft WAF Agent-based connectors , using the Log Analytics agent, make it possible to ingest data from any source that can stream logs in Common Event Format (CEF), such as Windows and Linux machines. Analytic rules are rules that users create to help detect threats and anomalies in an environment: Scheduled rules run on a predetermined schedule Microsoft Security Machine learning behavior analytic rules can (currently) only be created from templates provided by Microsoft using proprietary ML algorithms Simple Queue Service AWS service that can broker messages between components of highly decoupled applications. Snowball Physical appliance designed to move large amounts of data to the cloud. The largest Snowball device can store 72 TB of information. Snowball Edge refers to a family of options similar to Snowball but with compute power to run EC2 instances and Lambda functions locally. All Snowball Edge options feature a QSFP+ network interface that is capable of speeds up to 100 Gbps. Snowball Edge devices can also be clustered together. Storage Optimized provides up to 80 TB of storage and 24 vCPUs. Compute Optimized provides up to 39.5 TB of storage and 52 vCPUs. Compute Optimized with GPU is similar to Compute Optimized but includes an NVIDIA GPU, making it useful for ML and HPC applications. Spanner GCP managed scaleable database service. Stackdriver GCP service that collects metrics, logs, and event data from applications and infrastructure and integrates the data so DevOps engineers can monitor, assess, and diagnose operational problems. Super administrator Unique GCP role associated with the root Organization which has powers that exceed that of other administrative users. Storage accounts Azure storage accounts are managed through [Azure Resource Manager][ARM] and management operations are authenticated and authorized using [Azure Active Directory][Azure AD]. There are four services provided within each storage account: Blobs provides a highly scalable service for storing arbitrary data objects, such as text or binary data. There can be multiple containers within a storage account, and a container can have its own folder structure. There are three types of blob: page , block , and append blobs. Tables provides a NoSQL-style store for storing structured data. Tables in Azure storage do not require a fixed schema, thus different entries in the same table can have different fields Queues provides reliable message queueing between application components Files provides managed file shares that can be used by VMs or on-premises servers Options that must be selected when creating a storage account: Performance tier Standard supports all storage services and uses magnetic disks to provide cost-efficient and reliable storage Premium only supports page blobs with the locally-redundant (LRS) replication option, uses high-performance SSD disks Account kind General-purpose V2: only kind to support ZRS General-purpose V1: does not support various access tiers. Blob storage: specialized storage account used to store block and append blobs Replication mode : Storage accounts can be freely moved between the following replication modes, except ZRS, in which case it is recommended to copy data to a new account. Locally-redundant storage (LRS): makes 3 local sychronous within the same Azure facility (zone) Zone-redundant storage (ZRS): makes 3 synchronous copies across multiple availability zones; available for general-purpose v2 storage accounts at Standard performance tier only. Geographically-redundant storage (GRS): makes 3 local synchronous copies plus 3 additional asynchronous copies (typically within 15 minutes, but no SLA) to a second data center far away from the primary region Read-access geographically redundant storage (RA-GRS): makes 3 local synchronous copies plus 3 additional asynchronous copies to a second data center far away from the primary region, which has only read-only access Access tier : Both Blob and StorageV1 can be upgraded to StorageV2, a process which is irreversible. Hot blob storage access tier optimized for the frequent access of objects in the storage account Cool blob storage access tier optimized for storing large amounts of data that is infrequently accessed and stored for at least 30 days Archive blob storage access tier designed for long-term storage of infrequently-used data that can tolerate several hours of retrieval latency, remaining in the Archive tier for at least 180 days. It is stored offline and can take up to 15 hours for it to be \"rehydrated\" to the Cool or Hot tier before it can be accessed. Premium providing high-performance access for frequently-used data on SSD, only available from the Block Blob storage account type. Every storage account service exposes its own Internet-facing endpoint, which must be secured in one of several ways. A firewall can be implemented by using network rules to limit traffic to particular networks. The storage firewall controls IP addresses and VNets can access the storage account and applies to all storage account services. Access can be restricted to specific VNets by creating a Virtual Network Service Endpoint , however this still uses the public IP address. Private Link allows similar functionality using private IPs. MS Docs Public access to blobs can be restricted at the container level on container creation. By default, no public read access is enabled for anonymous users, but users with RBAC rights or with the storage account name and key can have access. This can be done through ARM APIs, the Portal, or Azure Storage Explorer. Container access levels: No public read access: container and blobs can only be accessed by storage account owner (default for new containers) Public read-only access for blobs only (container data is not available, and anonymous clients cannot enumerate the blobs within the container) Full public read-only access: all container and blob data can be read by anonymous requests: Access can also be switched between Shared Key -based authentication (relying on storage account keys) and Azure AD authentication , where a RBAC role determines access to a Container. Authorize access to blobs and queues using Azure Active Directory Access keys grant full access to all data in all services of a storage account and represent the simplest and most powerful control over access. Access keys are typically used by applications for access to Azure storage, either through a Shared Access Signature (SAS) token or directly accessing the storage itself with the name and key. Storage account keys were implemented early in the history of Azure and grant full access to the entire storage account. However, it is considered an anti-pattern to distribute this key; a SAS token should be generated for every stored item to be distributed. Because storage account keys provide write access, a storage account with a ReadOnly resource lock will not enumerate its storage account keys, and users with Read permission will not be able to retrieve the keys either. Systems Manager AWS service for imperative configuration management. Systems Manager relies on several types of script: Command documents use normal shell commands and can be run periodically or on a trigger, so long as the instance to be managed has the required agent. Automation documents allow administration of AWS resources, similar in effect to using the Management Console or the AWS CLI . Trusted Advisor AWS service allows a visual check of resource configurations to ensure compliance with best practices, available only to Business and Enterprise Support plans. It offers several categories Cost optimization Performance Security Fault tolerance Service limits User Access Administrator Azure built-in role that grants the permissions necessary to assign a user administrative access at the subscription scope. Permissions: Microsoft.Authorization/roleAssignments/write Microsoft.Authorization/roleAssignments/delete User Administrator Azure built-in role that grants the power to manage all aspects of users and groups, including resetting passwords for limited admins. VM Agent Microsoft Azure Virtual Machine Agent (VM Agent) manages VM interaction with the Azure Fabric Controller and comes preinstalled with Windows images from the Marketplace. It can also be installed on a custom image. VM Agent supports the VMSnapshot extension, which is added when backups are enabled. This extension takes a snapshot of the storage at the block level and sends it to the RSV configured. For Windows VMs, this extension leverable the Volume Shadow Copy service. Microsoft Azure Linux Agent Azure agent that manages VM interaction with the Azure Fabric Controller on Linux VMs. WAImportExport.exe CLI tool associated with Azure Import/Export service that requires a Windows computer with .NET Framework and BitLocker. There are two versions: Version 1 is recommended for blob storage Version 2 is recommended for files storage . Check disks required for selected blobs WAImportExport.exe PreviewExport /sn:<Storage account name> /sk:<Storage account key> /ExportBlobListFile:<Path to XML blob list file> /DriveSize:<Size of drives used> Various flags in WAImportExport.exe allow an XML-format \"blob list\" file to be used to specify files, or as output. All Root Blob in root Containers Pattern Pattern in container Export all blobs in the storage account <?xml version=\"1.0\" encoding=\"utf-8\"?> <BlobList> <BlobPath> / </BlobPath> </BlobList> Export all blobs in the root container <?xml version=\"1.0\" encoding=\"utf-8\"?> <BlobList> <BlobPath> /$root </BlobPath> </BlobList> Export blob \"logo.bmp\" in the root container <?xml version=\"1.0\" encoding=\"utf-8\"?> <BlobList> <BlobPath> $root/logo.bmp </BlobPath> </BlobList> Export all blobs in container \"music\" <?xml version=\"1.0\" encoding=\"utf-8\"?> <BlobList> <BlobPath> /music/ </BlobPath> </BlobList> Export all blobs in any container that begins with \"book\" <?xml version=\"1.0\" encoding=\"utf-8\"?> <BlobList> <BlobPath> /book </BlobPath> </BlobList> Export all blobs in container \"music\" that begin with prefix \"love\" <?xml version=\"1.0\" encoding=\"utf-8\"?> <BlobList> <BlobPath> /music/love </BlobPath> </BlobList> WebJobs a feature of Azure App Service that enables you to run a program or script in the same instance as a web app, API app, or mobile app, at no additional cost and supported only on Windows App Service plans . There are two types: Continuous webjobs default to running on all instances of the linked web app (although it can be configured to run on only one) Triggered webjobs run only when triggered or on a schedule and on only a single instance of the linked web app selected by Azure.","title":"Kusto "},{"location":"Infrastructure/Network/","text":"Networking IPv6 Various address types are associated with IPv6 ranges: Global Unicast : unique address that may be used on the public Internet Unique Local (FD00::/8): equivalent to RFC 1918 private ranges in IPv4, and not routable on the Internet Link Local (FE80::/10): equivalent to loopback addresses and automatically generated by IPv6 devices. This is enabled by the command ipv6 enable Multicast (FF00::/8): equivalent to broadcast, where a packet is sent to a group of subscribed devices FF02::1: all devices FF02::2: all routers Anycast : allows multiple devices to share the same IPv6 address EUI-64 Generates an IPv6 host address from the device's MAC address SLAAC IPv6 device learns its prefix information automatically over the local link from another device (i.e. router). The device can then generate its own host portion using EUI-64 Because SLAAC cannot provide additional information such as DNS addresses, it is typically used alongside stateless DHCP . Permit router to run IPv6-related routing protocols like EIGRP for IPv6 or OSPF version 3. ipv6 unicast-routing Switching When a switch receives an Ethernet frame, it examines the destination MAC address and compares it to its MAC address table . This table is continuously updated by the switch as it learns new addresses and discards or ages old ones. MAC address table S1>show mac address-table When it finds an unknown destination, it proceeds with frame flooding where a frame is sent out all ports for the unknown MAC address's VLAN. If it knows the destination MAC, it transmits the frame on the appropriate port. Collisions no longer occur in modern networks because switches create a separate collision domain for each connection with a host, a condition called microsegmentation . In older half duplex networks, technologies like CSMA/CD were used to negotiate the possibility of collisions. Modern Cisco devices perform autonegotiation to resolve a common duplex and speed. Ethernet frames have a common format: Preamble (7 bytes) is a pattern of alternating 1's and 0's for synchronization Start Frame Delimiter (SFD) (1 byte) Destination MAC (6 bytes) Source MAC (6 bytes) Type (2 bytes) identifies IPv4 or IPv6 Data and Pad ranges in size from 46 to 1500 bytes. Padding is necessary if the frame would otherwise be less than the minimum 46 bytes. Frame Check Sequence (FCS) (4 bytes) is for error-checking Larger frame sizes are possible with baby giants (1600 bytes) and jumbo frames (9216 bytes). Interfaces Display state Bring up Switch>show interfaces status Switch>enable Switch#configure terminal Switch(config)#interface fa0/1 Switch(config-if)#no shutdown Switch(config-if)#end Basic administration Change hostname Switch(config)#hostname SW2 Set banner message Switch(config)#banner motd #Hello, world!# Restart Switch#reload Display configuration Current Saved Switch#show running-config Switch#show startup-config Display contents of NVRAM Switch>show flash: Configure router IP IPv4 IPv6 EUI-64 SLAAC R1(config)#interface fa0/0 R1(config-if)#ip address 0.10.10.1 255.255.255.0 R1(config-if)#no shutdown R1(config-if)#end R1(config)#interface fa0/0 R1(config-if)#ipv6 address 2001:aaaa:bbbb::1/64 R1(config-if)#no shutdown R1(config-if)#end R1(config)#interface fa0/0 R1(config-if)#ipv6 address 2001:aaaa:bbbb::/64 eui-64 R1(config-if)#no shutdown R1(config-if)#end R1(config)#interface fa0/0 R1(config-if)#ipv6 address autoconfig R1(config-if)#no shutdown R1(config-if)#end VLAN A VLAN is a broadcast domain created on a switch that corresponds to a TCP /IP subnet All non-trunk ports on a Cisco switch are assigned to VLAN 1 by default. And in order to be part of a VLAN, an interface must be set to access mode . The Native VLAN or VLAN 1 is intended to ensure that management traffic (i.e. CDP ) can still flow between devices even if a link loses its status as a trunk. It is considered best practice to tag the Native VLAN or use an unused VLAN for this purpose, for security. Display VLAN assignments SW1#show vlan brief Configure VLAN settings Name a VLAN Data VLAN Voice VLAN SW1#configure terminal SW1(config)#vlan 30 SW1(config-vlan)#name WEST SW1(config-vlan)#do show vlan brief SW1#configure terminal SW1(config)#interface gi0/1 SW1(config-if)#switchport mode access SW1(config-if)#switchport access vlan 20 SW1(config-if)#end SW1#show vlan brief SW1#configure terminal SW1(config)#interface gi0/2 SW1(config-if)#switchport mode access SW1(config-if)#switchport access vlan 30 SW1(config-if)#switchport voice vlan 50 SW1(config-if)#end SW1#show vlan brief SW1#show interface gi0/2 switchport VTP VTP is a Cisco protocol that facilitates VLAN creation and management across many switches using interswitch links called trunks . There are three VTP Operating Modes : Server (default) permits you to create and modify VLANs on the local device. Transparent disables VTP . Client allows switches to inherit the VLAN information from a server. You cannot create VLANs locally on a VTP Client device. Display VTP mode SW1#show vtp status Trunking 802.1Q trunk links are the modern way of sharing traffic between switches by injecting a 4-byte tag value in the existing frame between the Source MAC address and Type fields rather than fully re-encapsulating the frame to add a VLAN marking as was the case in ISL . Without configuring trunking, the interfaces are in access mode. SW1 SW3 SW1#show interfaces gi0/1 switchport Name: GigabitEthernet0/1 Switchport: Enabled Administrative Mode: dynamic auto Operational Mode: static access Administrative Trunking Encapsulation: dot1q Operational Trunking Encapsulation: native Negotiation of Trunking: false Access Mode VLAN: 1 Trunking Native Mode VLAN: 1 (default) Administrative Native VLAN tagging: enabled Voice VLAN: none Administrative private-vlan host-association: none Administrative private-vlan mapping: none Administrative private-vlan trunk native VLAN: none Administrative private-vlan trunk Native VLAN tagging: enabled Administrative private-vlan trunk encapsulation: dot1q Administrative private-vlan trunk normal VLANs: none Administrative private-vlan trunk private VLANs: none Operational private-vlan: none Trunking VLANs Enabled: All Pruning VLANs Enabled: none Capture Mode Disabled Capture VLANs Allowed: ALL Protected: false Unknown unicast blocked: disabled Unknown multicast blocked: disabled Appliance trust: none SW3#show interfaces gi0/2 switchport Name: GigabitEthernet0/1 Switchport: Enabled Administrative Mode: dynamic auto Operational Mode: static access Administrative Trunking Encapsulation: dot1q Operational Trunking Encapsulation: native Negotiation of Trunking: false Access Mode VLAN: 1 Trunking Native Mode VLAN: 1 (default) Administrative Native VLAN tagging: enabled Voice VLAN: none Administrative private-vlan host-association: none Administrative private-vlan mapping: none Administrative private-vlan trunk native VLAN: none Administrative private-vlan trunk Native VLAN tagging: enabled Administrative private-vlan trunk encapsulation: dot1q Administrative private-vlan trunk normal VLANs: none Administrative private-vlan trunk private VLANs: none Operational private-vlan: none Trunking VLANs Enabled: All Pruning VLANs Enabled: none Capture Mode Disabled Capture VLANs Allowed: ALL Protected: false Unknown unicast blocked: disabled Unknown multicast blocked: disabled Appliance trust: none Now the interface of one switch is configured as a trunk, and because the adminstrative mode of either is \"dynamic\", the facing interface accepts the change. Configure interface as trunk SW1#configure terminal SW1(config)#interfaces gi0/2 SW1(config-if)#switchport mode trunk There are several ways of confirming the trunk status: status switchport trunk SW1#show interfaces status Interface Name Status Vlan Duplex Speed Type Fa0/1 connected 1 a-full a-100 media type is 10/100BaseTX Fa0/2 notconnect 1 auto auto media type is 10/100BaseTX Fa0/3 notconnect 1 auto auto media type is 10/100BaseTX Fa0/4 notconnect 1 auto auto media type is 10/100BaseTX Fa0/5 notconnect 1 auto auto media type is 10/100BaseTX Fa0/6 notconnect 1 auto auto media type is 10/100BaseTX Fa0/7 notconnect 1 auto auto media type is 10/100BaseTX Fa0/8 notconnect 1 auto auto media type is 10/100BaseTX Fa0/9 notconnect 1 auto auto media type is 10/100BaseTX Fa0/10 notconnect 1 auto auto media type is 10/100BaseTX Fa0/11 connected 1 a-full a-100 media type is 10/100BaseTX Fa0/12 notconnect 1 auto auto media type is 10/100BaseTX Fa0/13 notconnect 1 auto auto media type is 10/100BaseTX Fa0/14 notconnect 1 auto auto media type is 10/100BaseTX Fa0/15 notconnect 1 auto auto media type is 10/100BaseTX Fa0/16 notconnect 1 auto auto media type is 10/100BaseTX Fa0/17 notconnect 1 auto auto media type is 10/100BaseTX Fa0/18 notconnect 1 auto auto media type is 10/100BaseTX Fa0/19 notconnect 1 auto auto media type is 10/100BaseTX Fa0/20 notconnect 1 auto auto media type is 10/100BaseTX Fa0/21 notconnect 1 auto auto media type is 10/100BaseTX Fa0/22 notconnect 1 auto auto media type is 10/100BaseTX Fa0/23 notconnect 1 auto auto media type is 10/100BaseTX Fa0/24 notconnect 1 auto auto media type is 10/100BaseTX Gi0/1 connected 1 a-full a-1000 media type is 10/100/1000BaseTX Gi0/2 connected trunk a-full a-1000 media type is 10/100/1000BaseTX vlan1 connected 1 auto auto media type is 10/100/1000BaseTX SW1#show interfaces gi0/2 switchport Name: GigabitEthernet0/2 Switchport: Enabled Administrative Mode: dynamic auto Operational Mode: trunk Administrative Trunking Encapsulation: dot1q Operational Trunking Encapsulation: native Negotiation of Trunking: false Access Mode VLAN: 1 Trunking Native Mode VLAN: 1 (default) Administrative Native VLAN tagging: enabled Voice VLAN: none Administrative private-vlan host-association: none Administrative private-vlan mapping: none Administrative private-vlan trunk native VLAN: none Administrative private-vlan trunk Native VLAN tagging: enabled Administrative private-vlan trunk encapsulation: dot1q Administrative private-vlan trunk normal VLANs: none Administrative private-vlan trunk private VLANs: none Operational private-vlan: none Trunking VLANs Enabled: All Pruning VLANs Enabled: none Capture Mode Disabled Capture VLANs Allowed: ALL Protected: false Unknown unicast blocked: disabled Unknown multicast blocked: disabled Appliance trust: none SW1#show interfaces trunk Port Mode Encapsulation Status Native vlan Gi0/2 on 802.1q trunking 1 Port Vlans allowed on trunk Gi0/2 1-4094 Port Vlans allowed and active in management domain Gi0/2 1 Port Vlans in spanning tree forwarding state and not pruned Gi0/2 1 On SW3, the three commands produce similar output, except for the interface's Administrative Mode. status switchport trunk SW3#show interfaces status Interface Name Status Vlan Duplex Speed Type Fa0/1 disabled 1 auto auto media type is 10/100BaseTX Fa0/2 notconnect 1 auto auto media type is 10/100BaseTX Fa0/3 notconnect 1 auto auto media type is 10/100BaseTX Fa0/4 notconnect 1 auto auto media type is 10/100BaseTX Fa0/5 notconnect 1 auto auto media type is 10/100BaseTX Fa0/6 notconnect 1 auto auto media type is 10/100BaseTX Fa0/7 notconnect 1 auto auto media type is 10/100BaseTX Fa0/8 notconnect 1 auto auto media type is 10/100BaseTX Fa0/9 notconnect 1 auto auto media type is 10/100BaseTX Fa0/10 notconnect 1 auto auto media type is 10/100BaseTX Fa0/11 connected 1 a-full a-100 media type is 10/100BaseTX Fa0/12 notconnect 1 auto auto media type is 10/100BaseTX Fa0/13 notconnect 1 auto auto media type is 10/100BaseTX Fa0/14 notconnect 1 auto auto media type is 10/100BaseTX Fa0/15 notconnect 1 auto auto media type is 10/100BaseTX Fa0/16 notconnect 1 auto auto media type is 10/100BaseTX Fa0/17 notconnect 1 auto auto media type is 10/100BaseTX Fa0/18 notconnect 1 auto auto media type is 10/100BaseTX Fa0/19 notconnect 1 auto auto media type is 10/100BaseTX Fa0/20 notconnect 1 auto auto media type is 10/100BaseTX Fa0/21 notconnect 1 auto auto media type is 10/100BaseTX Fa0/22 notconnect 1 auto auto media type is 10/100BaseTX Fa0/23 notconnect 1 auto auto media type is 10/100BaseTX Fa0/24 notconnect 1 auto auto media type is 10/100BaseTX Gi0/1 connected trunk a-full a-1000 media type is 10/100/1000BaseTX Gi0/2 notconnect 1 auto auto media type is 10/100/1000BaseTX vlan1 connected 1 auto auto media type is 10/100/1000BaseTX SW3#show interfaces gi0/2 switchport Name: GigabitEthernet0/2 Switchport: Enabled Administrative Mode: trunk Operational Mode: trunk Administrative Trunking Encapsulation: dot1q Operational Trunking Encapsulation: native Negotiation of Trunking: false Access Mode VLAN: none Trunking Native Mode VLAN: 1 (default) Administrative Native VLAN tagging: enabled Voice VLAN: none Administrative private-vlan host-association: none Administrative private-vlan mapping: none Administrative private-vlan trunk native VLAN: none Administrative private-vlan trunk Native VLAN tagging: enabled Administrative private-vlan trunk encapsulation: dot1q Administrative private-vlan trunk normal VLANs: none Administrative private-vlan trunk private VLANs: none Operational private-vlan: none Trunking VLANs Enabled: Pruning VLANs Enabled: none Capture Mode Disabled Capture VLANs Allowed: ALL Protected: false Unknown unicast blocked: disabled Unknown multicast blocked: disabled Appliance trust: none SW3#show interfaces trunk Port Mode Encapsulation Status Native vlan Gi0/1 on 802.1q trunking 1 Port Vlans allowed on trunk Gi0/1 1-4094 Port Vlans allowed and active in management domain Gi0/1 1 Port Vlans in spanning tree forwarding state and not pruned Gi0/1 1 Encapsulation method is 802.1Q by default, although it can be explicitly specified. SW1(config-if)#switchport trunk encapsulation dot1q SW1(config-if)#switchport mode trunk STP The classic version of STP is PVST+, which is slower to converge than RSTP . Cisco's implementation of RSTP is called RPVST+. STP Switch#show spanning-tree PortFast is a STP feature that speeds up the process of moving a port from blocking to forwarding and is used exclusively on ports connected to servers and workstations. PortFast Switch(config-if)#spanning-tree portfast BPDU Guard is a related feature that can detect if a switch was mistakenly connect to a PortFast port and error disable the port for safety and security. BPDU Guard Switch(config-if)#spanning-tree bpduguard enable Confirming PortFast and BPDU Guard Switch#show spanning-tree interface gi0/3 detail CDP CDP allows Cisco devices to communicate about each other to directly connected neighbors. VoIP in particular is reliant on CDP , however CDP messages cannot be passed from router to router through a switch. It is also considered a security issue, since it is enabled by default on Cisco routers and switches on all interfaces and devices may share information with an unauthorized neighbor. Disable CDP on a device Switch(config)#no cdp run Enable CDP on an individual interface Switch(config)#cdp run Switch(config)#interface gi1/0 Switch(config-if)#cdp enable LLDP is an open standard similar to CDP , but it is not enabled by default Enable LLDP Switch(config)#lldp run Switch(config)#interface gi1/0 Switch(config-if)#lldp transmit Switch(config-if)#lldp receive EtherChannel EtherChannel aggregates multiple physical links to have them act as as a single one, providing redundancy and increased bandwidth. It is often brought up in the context of STP because although STP does not permit redundant links it will also not block any one of the links within an EtherChannel bundle or port-channel interface . Other aggregation technologies that make multiple switches act as one logical device: Switch stacking is used in the access layer and uses special stacking ports and cables. Chassis aggregation is used in the distribution and core layers to aggregate only two switches with Ethernet interfaces. It is more complex to setup but is also more functional. Configure EtherChannel EtherChannel can be configured using PAgP ( desirable ), LACP ( active ), statically ( on ), or at Layer 3. Because PAgP is the default, all other methods require the interfaces to be shutdown first. PAgP LACP Static SW1(config)#interface range gi0/2 , gi1/1 SW1(config-if-range)#channel-group 2 mode desirable SW2(config)#interface range gi0/1 , gi0/3 SW2(config-if-range)#channel-group 2 mode desirable SW1(config)#interface range gi0/2 , gi1/1 SW1(config-if-range)#shutdown SW1(config-if-range)#channel-group 3 mode active SW2(config)#interface range gi0/2 , gi1/0 SW2(config-if-range)#channel-group 3 mode active SW3(config)#interface range gi0/2 , gi1/0 SW3(config-if-range)#no shutdown SW1(config)#interface range gi0/1 , gi0/3 SW1(config-if-range)#shutdown SW1(config-if-range)#channel-group 1 mode on SW2(config)#inteface range gi0/1 , gi1/0 SW2(config-if-range)#channel-group 1 mode on SW1(config)#interface range gi0/1 , gi0/3 SW1(config-if-range)#no shutdown Verify EtherChannels SW1#show etherchannel summary Routing The routing table of a Cisco router has various sources, including many protocols of varying reliability. Each source is associated with: A letter code (i.e. R for RIP , etc) Administrative distance , a numeric value which reflects Cisco's measure of the source's trustworthiness. A lower value indicates higher trustworthiness, and the maximum value of 255 indicates an unusable route. The administrative values associated with each routing protocol must be memorized for the CCNA exam. For dynamic routing protocols a metric is also provided whose significance varies with protocol. For example, with RIP the metric signifies hop count . Administrative distance and metric are delimited by a forward slash and appear within square brackets following the prefix . Display routing table R1#show ip route Configure static route R1(config)#ip route 0.0.0.0 0.0.0.0 10.10.10.2 Set Gateway of Last Resort ip default-gateway ip default-network route ip default-gateway ip default-network ip route 0.0.0.0 0.0.0.0 Configure router IP IPv4 IPv6 EUI-64 SLAAC R1(config)#interface fa0/0 R1(config-if)#ip address 0.10.10.1 255.255.255.0 R1(config-if)#no shutdown R1(config-if)#end R1(config)#interface fa0/0 R1(config-if)#ipv6 address 2001:aaaa:bbbb::1/64 R1(config-if)#no shutdown R1(config-if)#end R1(config)#interface fa0/0 R1(config-if)#ipv6 address 2001:aaaa:bbbb::/64 eui-64 R1(config-if)#no shutdown R1(config-if)#end R1(config)#interface fa0/0 R1(config-if)#ipv6 address autoconfig R1(config-if)#no shutdown R1(config-if)#end Inter-VLAN routing A multilayer switch possesses a routing engine (RE) to route between VLANs. Without an RE, a Router on a stick configuration is necessary. Configure ROAS A subinterface can be configured by specifying a VLAN number after a period on the physical interface, then associating an IP range with it. Theese IP addresses can be used as default gateway addresses hosts will use in respective subnets. ROAS can be configured with or without setting an IP address on the interface. IP No IP R1(config-if)#ip address 10.1.0.1 255.255.255.0 R1(config-if)#interface gi0/1.10 R1(config-subif)#encapsulation dot1q 10 R1(config-subif)#ip address 10.1.10.1 255.255.255.0 R1(config-subif)#end R1(config)#inteface gi0/1.20 R1(config-subif)#encapsulation dot1q 20 R1(config-subif)#ip address 10.1.20.1 255.255.255.0 R1(config-subif)#end R1(config-if)#interface gi0/1.10 R1(config-subif)#encapsulation dot1q 10 R1(config-subif)#ip address 10.1.10.1 255.255.255.0 R1(config-subif)#end R1(config)#inteface gi0/1.20 R1(config-subif)#encapsulation dot1q 20 R1(config-subif)#ip address 10.1.20.1 255.255.255.0 R1(config-subif)#end SW1(config)#interface gi0/1 SW1(config-if)#switchport trunk encapsulation dot1q SW1(config-if)#switchport mode trunk Verify ROAS show ip interface brief R1#show ip interface brief === \"`show vlans``","title":"Network"},{"location":"Infrastructure/Network/#networking","text":"","title":"Networking"},{"location":"Infrastructure/Network/#ipv6","text":"Various address types are associated with IPv6 ranges: Global Unicast : unique address that may be used on the public Internet Unique Local (FD00::/8): equivalent to RFC 1918 private ranges in IPv4, and not routable on the Internet Link Local (FE80::/10): equivalent to loopback addresses and automatically generated by IPv6 devices. This is enabled by the command ipv6 enable Multicast (FF00::/8): equivalent to broadcast, where a packet is sent to a group of subscribed devices FF02::1: all devices FF02::2: all routers Anycast : allows multiple devices to share the same IPv6 address EUI-64 Generates an IPv6 host address from the device's MAC address SLAAC IPv6 device learns its prefix information automatically over the local link from another device (i.e. router). The device can then generate its own host portion using EUI-64 Because SLAAC cannot provide additional information such as DNS addresses, it is typically used alongside stateless DHCP . Permit router to run IPv6-related routing protocols like EIGRP for IPv6 or OSPF version 3. ipv6 unicast-routing","title":"IPv6"},{"location":"Infrastructure/Network/#switching","text":"When a switch receives an Ethernet frame, it examines the destination MAC address and compares it to its MAC address table . This table is continuously updated by the switch as it learns new addresses and discards or ages old ones. MAC address table S1>show mac address-table When it finds an unknown destination, it proceeds with frame flooding where a frame is sent out all ports for the unknown MAC address's VLAN. If it knows the destination MAC, it transmits the frame on the appropriate port. Collisions no longer occur in modern networks because switches create a separate collision domain for each connection with a host, a condition called microsegmentation . In older half duplex networks, technologies like CSMA/CD were used to negotiate the possibility of collisions. Modern Cisco devices perform autonegotiation to resolve a common duplex and speed. Ethernet frames have a common format: Preamble (7 bytes) is a pattern of alternating 1's and 0's for synchronization Start Frame Delimiter (SFD) (1 byte) Destination MAC (6 bytes) Source MAC (6 bytes) Type (2 bytes) identifies IPv4 or IPv6 Data and Pad ranges in size from 46 to 1500 bytes. Padding is necessary if the frame would otherwise be less than the minimum 46 bytes. Frame Check Sequence (FCS) (4 bytes) is for error-checking Larger frame sizes are possible with baby giants (1600 bytes) and jumbo frames (9216 bytes). Interfaces Display state Bring up Switch>show interfaces status Switch>enable Switch#configure terminal Switch(config)#interface fa0/1 Switch(config-if)#no shutdown Switch(config-if)#end Basic administration Change hostname Switch(config)#hostname SW2 Set banner message Switch(config)#banner motd #Hello, world!# Restart Switch#reload Display configuration Current Saved Switch#show running-config Switch#show startup-config Display contents of NVRAM Switch>show flash: Configure router IP IPv4 IPv6 EUI-64 SLAAC R1(config)#interface fa0/0 R1(config-if)#ip address 0.10.10.1 255.255.255.0 R1(config-if)#no shutdown R1(config-if)#end R1(config)#interface fa0/0 R1(config-if)#ipv6 address 2001:aaaa:bbbb::1/64 R1(config-if)#no shutdown R1(config-if)#end R1(config)#interface fa0/0 R1(config-if)#ipv6 address 2001:aaaa:bbbb::/64 eui-64 R1(config-if)#no shutdown R1(config-if)#end R1(config)#interface fa0/0 R1(config-if)#ipv6 address autoconfig R1(config-if)#no shutdown R1(config-if)#end","title":"Switching"},{"location":"Infrastructure/Network/#vlan","text":"A VLAN is a broadcast domain created on a switch that corresponds to a TCP /IP subnet All non-trunk ports on a Cisco switch are assigned to VLAN 1 by default. And in order to be part of a VLAN, an interface must be set to access mode . The Native VLAN or VLAN 1 is intended to ensure that management traffic (i.e. CDP ) can still flow between devices even if a link loses its status as a trunk. It is considered best practice to tag the Native VLAN or use an unused VLAN for this purpose, for security. Display VLAN assignments SW1#show vlan brief Configure VLAN settings Name a VLAN Data VLAN Voice VLAN SW1#configure terminal SW1(config)#vlan 30 SW1(config-vlan)#name WEST SW1(config-vlan)#do show vlan brief SW1#configure terminal SW1(config)#interface gi0/1 SW1(config-if)#switchport mode access SW1(config-if)#switchport access vlan 20 SW1(config-if)#end SW1#show vlan brief SW1#configure terminal SW1(config)#interface gi0/2 SW1(config-if)#switchport mode access SW1(config-if)#switchport access vlan 30 SW1(config-if)#switchport voice vlan 50 SW1(config-if)#end SW1#show vlan brief SW1#show interface gi0/2 switchport","title":"VLAN"},{"location":"Infrastructure/Network/#vtp","text":"VTP is a Cisco protocol that facilitates VLAN creation and management across many switches using interswitch links called trunks . There are three VTP Operating Modes : Server (default) permits you to create and modify VLANs on the local device. Transparent disables VTP . Client allows switches to inherit the VLAN information from a server. You cannot create VLANs locally on a VTP Client device. Display VTP mode SW1#show vtp status","title":"VTP"},{"location":"Infrastructure/Network/#trunking","text":"802.1Q trunk links are the modern way of sharing traffic between switches by injecting a 4-byte tag value in the existing frame between the Source MAC address and Type fields rather than fully re-encapsulating the frame to add a VLAN marking as was the case in ISL . Without configuring trunking, the interfaces are in access mode. SW1 SW3 SW1#show interfaces gi0/1 switchport Name: GigabitEthernet0/1 Switchport: Enabled Administrative Mode: dynamic auto Operational Mode: static access Administrative Trunking Encapsulation: dot1q Operational Trunking Encapsulation: native Negotiation of Trunking: false Access Mode VLAN: 1 Trunking Native Mode VLAN: 1 (default) Administrative Native VLAN tagging: enabled Voice VLAN: none Administrative private-vlan host-association: none Administrative private-vlan mapping: none Administrative private-vlan trunk native VLAN: none Administrative private-vlan trunk Native VLAN tagging: enabled Administrative private-vlan trunk encapsulation: dot1q Administrative private-vlan trunk normal VLANs: none Administrative private-vlan trunk private VLANs: none Operational private-vlan: none Trunking VLANs Enabled: All Pruning VLANs Enabled: none Capture Mode Disabled Capture VLANs Allowed: ALL Protected: false Unknown unicast blocked: disabled Unknown multicast blocked: disabled Appliance trust: none SW3#show interfaces gi0/2 switchport Name: GigabitEthernet0/1 Switchport: Enabled Administrative Mode: dynamic auto Operational Mode: static access Administrative Trunking Encapsulation: dot1q Operational Trunking Encapsulation: native Negotiation of Trunking: false Access Mode VLAN: 1 Trunking Native Mode VLAN: 1 (default) Administrative Native VLAN tagging: enabled Voice VLAN: none Administrative private-vlan host-association: none Administrative private-vlan mapping: none Administrative private-vlan trunk native VLAN: none Administrative private-vlan trunk Native VLAN tagging: enabled Administrative private-vlan trunk encapsulation: dot1q Administrative private-vlan trunk normal VLANs: none Administrative private-vlan trunk private VLANs: none Operational private-vlan: none Trunking VLANs Enabled: All Pruning VLANs Enabled: none Capture Mode Disabled Capture VLANs Allowed: ALL Protected: false Unknown unicast blocked: disabled Unknown multicast blocked: disabled Appliance trust: none Now the interface of one switch is configured as a trunk, and because the adminstrative mode of either is \"dynamic\", the facing interface accepts the change. Configure interface as trunk SW1#configure terminal SW1(config)#interfaces gi0/2 SW1(config-if)#switchport mode trunk There are several ways of confirming the trunk status: status switchport trunk SW1#show interfaces status Interface Name Status Vlan Duplex Speed Type Fa0/1 connected 1 a-full a-100 media type is 10/100BaseTX Fa0/2 notconnect 1 auto auto media type is 10/100BaseTX Fa0/3 notconnect 1 auto auto media type is 10/100BaseTX Fa0/4 notconnect 1 auto auto media type is 10/100BaseTX Fa0/5 notconnect 1 auto auto media type is 10/100BaseTX Fa0/6 notconnect 1 auto auto media type is 10/100BaseTX Fa0/7 notconnect 1 auto auto media type is 10/100BaseTX Fa0/8 notconnect 1 auto auto media type is 10/100BaseTX Fa0/9 notconnect 1 auto auto media type is 10/100BaseTX Fa0/10 notconnect 1 auto auto media type is 10/100BaseTX Fa0/11 connected 1 a-full a-100 media type is 10/100BaseTX Fa0/12 notconnect 1 auto auto media type is 10/100BaseTX Fa0/13 notconnect 1 auto auto media type is 10/100BaseTX Fa0/14 notconnect 1 auto auto media type is 10/100BaseTX Fa0/15 notconnect 1 auto auto media type is 10/100BaseTX Fa0/16 notconnect 1 auto auto media type is 10/100BaseTX Fa0/17 notconnect 1 auto auto media type is 10/100BaseTX Fa0/18 notconnect 1 auto auto media type is 10/100BaseTX Fa0/19 notconnect 1 auto auto media type is 10/100BaseTX Fa0/20 notconnect 1 auto auto media type is 10/100BaseTX Fa0/21 notconnect 1 auto auto media type is 10/100BaseTX Fa0/22 notconnect 1 auto auto media type is 10/100BaseTX Fa0/23 notconnect 1 auto auto media type is 10/100BaseTX Fa0/24 notconnect 1 auto auto media type is 10/100BaseTX Gi0/1 connected 1 a-full a-1000 media type is 10/100/1000BaseTX Gi0/2 connected trunk a-full a-1000 media type is 10/100/1000BaseTX vlan1 connected 1 auto auto media type is 10/100/1000BaseTX SW1#show interfaces gi0/2 switchport Name: GigabitEthernet0/2 Switchport: Enabled Administrative Mode: dynamic auto Operational Mode: trunk Administrative Trunking Encapsulation: dot1q Operational Trunking Encapsulation: native Negotiation of Trunking: false Access Mode VLAN: 1 Trunking Native Mode VLAN: 1 (default) Administrative Native VLAN tagging: enabled Voice VLAN: none Administrative private-vlan host-association: none Administrative private-vlan mapping: none Administrative private-vlan trunk native VLAN: none Administrative private-vlan trunk Native VLAN tagging: enabled Administrative private-vlan trunk encapsulation: dot1q Administrative private-vlan trunk normal VLANs: none Administrative private-vlan trunk private VLANs: none Operational private-vlan: none Trunking VLANs Enabled: All Pruning VLANs Enabled: none Capture Mode Disabled Capture VLANs Allowed: ALL Protected: false Unknown unicast blocked: disabled Unknown multicast blocked: disabled Appliance trust: none SW1#show interfaces trunk Port Mode Encapsulation Status Native vlan Gi0/2 on 802.1q trunking 1 Port Vlans allowed on trunk Gi0/2 1-4094 Port Vlans allowed and active in management domain Gi0/2 1 Port Vlans in spanning tree forwarding state and not pruned Gi0/2 1 On SW3, the three commands produce similar output, except for the interface's Administrative Mode. status switchport trunk SW3#show interfaces status Interface Name Status Vlan Duplex Speed Type Fa0/1 disabled 1 auto auto media type is 10/100BaseTX Fa0/2 notconnect 1 auto auto media type is 10/100BaseTX Fa0/3 notconnect 1 auto auto media type is 10/100BaseTX Fa0/4 notconnect 1 auto auto media type is 10/100BaseTX Fa0/5 notconnect 1 auto auto media type is 10/100BaseTX Fa0/6 notconnect 1 auto auto media type is 10/100BaseTX Fa0/7 notconnect 1 auto auto media type is 10/100BaseTX Fa0/8 notconnect 1 auto auto media type is 10/100BaseTX Fa0/9 notconnect 1 auto auto media type is 10/100BaseTX Fa0/10 notconnect 1 auto auto media type is 10/100BaseTX Fa0/11 connected 1 a-full a-100 media type is 10/100BaseTX Fa0/12 notconnect 1 auto auto media type is 10/100BaseTX Fa0/13 notconnect 1 auto auto media type is 10/100BaseTX Fa0/14 notconnect 1 auto auto media type is 10/100BaseTX Fa0/15 notconnect 1 auto auto media type is 10/100BaseTX Fa0/16 notconnect 1 auto auto media type is 10/100BaseTX Fa0/17 notconnect 1 auto auto media type is 10/100BaseTX Fa0/18 notconnect 1 auto auto media type is 10/100BaseTX Fa0/19 notconnect 1 auto auto media type is 10/100BaseTX Fa0/20 notconnect 1 auto auto media type is 10/100BaseTX Fa0/21 notconnect 1 auto auto media type is 10/100BaseTX Fa0/22 notconnect 1 auto auto media type is 10/100BaseTX Fa0/23 notconnect 1 auto auto media type is 10/100BaseTX Fa0/24 notconnect 1 auto auto media type is 10/100BaseTX Gi0/1 connected trunk a-full a-1000 media type is 10/100/1000BaseTX Gi0/2 notconnect 1 auto auto media type is 10/100/1000BaseTX vlan1 connected 1 auto auto media type is 10/100/1000BaseTX SW3#show interfaces gi0/2 switchport Name: GigabitEthernet0/2 Switchport: Enabled Administrative Mode: trunk Operational Mode: trunk Administrative Trunking Encapsulation: dot1q Operational Trunking Encapsulation: native Negotiation of Trunking: false Access Mode VLAN: none Trunking Native Mode VLAN: 1 (default) Administrative Native VLAN tagging: enabled Voice VLAN: none Administrative private-vlan host-association: none Administrative private-vlan mapping: none Administrative private-vlan trunk native VLAN: none Administrative private-vlan trunk Native VLAN tagging: enabled Administrative private-vlan trunk encapsulation: dot1q Administrative private-vlan trunk normal VLANs: none Administrative private-vlan trunk private VLANs: none Operational private-vlan: none Trunking VLANs Enabled: Pruning VLANs Enabled: none Capture Mode Disabled Capture VLANs Allowed: ALL Protected: false Unknown unicast blocked: disabled Unknown multicast blocked: disabled Appliance trust: none SW3#show interfaces trunk Port Mode Encapsulation Status Native vlan Gi0/1 on 802.1q trunking 1 Port Vlans allowed on trunk Gi0/1 1-4094 Port Vlans allowed and active in management domain Gi0/1 1 Port Vlans in spanning tree forwarding state and not pruned Gi0/1 1 Encapsulation method is 802.1Q by default, although it can be explicitly specified. SW1(config-if)#switchport trunk encapsulation dot1q SW1(config-if)#switchport mode trunk","title":"Trunking"},{"location":"Infrastructure/Network/#stp","text":"The classic version of STP is PVST+, which is slower to converge than RSTP . Cisco's implementation of RSTP is called RPVST+. STP Switch#show spanning-tree PortFast is a STP feature that speeds up the process of moving a port from blocking to forwarding and is used exclusively on ports connected to servers and workstations. PortFast Switch(config-if)#spanning-tree portfast BPDU Guard is a related feature that can detect if a switch was mistakenly connect to a PortFast port and error disable the port for safety and security. BPDU Guard Switch(config-if)#spanning-tree bpduguard enable Confirming PortFast and BPDU Guard Switch#show spanning-tree interface gi0/3 detail","title":"STP"},{"location":"Infrastructure/Network/#cdp","text":"CDP allows Cisco devices to communicate about each other to directly connected neighbors. VoIP in particular is reliant on CDP , however CDP messages cannot be passed from router to router through a switch. It is also considered a security issue, since it is enabled by default on Cisco routers and switches on all interfaces and devices may share information with an unauthorized neighbor. Disable CDP on a device Switch(config)#no cdp run Enable CDP on an individual interface Switch(config)#cdp run Switch(config)#interface gi1/0 Switch(config-if)#cdp enable LLDP is an open standard similar to CDP , but it is not enabled by default Enable LLDP Switch(config)#lldp run Switch(config)#interface gi1/0 Switch(config-if)#lldp transmit Switch(config-if)#lldp receive","title":"CDP"},{"location":"Infrastructure/Network/#etherchannel","text":"EtherChannel aggregates multiple physical links to have them act as as a single one, providing redundancy and increased bandwidth. It is often brought up in the context of STP because although STP does not permit redundant links it will also not block any one of the links within an EtherChannel bundle or port-channel interface . Other aggregation technologies that make multiple switches act as one logical device: Switch stacking is used in the access layer and uses special stacking ports and cables. Chassis aggregation is used in the distribution and core layers to aggregate only two switches with Ethernet interfaces. It is more complex to setup but is also more functional. Configure EtherChannel EtherChannel can be configured using PAgP ( desirable ), LACP ( active ), statically ( on ), or at Layer 3. Because PAgP is the default, all other methods require the interfaces to be shutdown first. PAgP LACP Static SW1(config)#interface range gi0/2 , gi1/1 SW1(config-if-range)#channel-group 2 mode desirable SW2(config)#interface range gi0/1 , gi0/3 SW2(config-if-range)#channel-group 2 mode desirable SW1(config)#interface range gi0/2 , gi1/1 SW1(config-if-range)#shutdown SW1(config-if-range)#channel-group 3 mode active SW2(config)#interface range gi0/2 , gi1/0 SW2(config-if-range)#channel-group 3 mode active SW3(config)#interface range gi0/2 , gi1/0 SW3(config-if-range)#no shutdown SW1(config)#interface range gi0/1 , gi0/3 SW1(config-if-range)#shutdown SW1(config-if-range)#channel-group 1 mode on SW2(config)#inteface range gi0/1 , gi1/0 SW2(config-if-range)#channel-group 1 mode on SW1(config)#interface range gi0/1 , gi0/3 SW1(config-if-range)#no shutdown Verify EtherChannels SW1#show etherchannel summary","title":"EtherChannel"},{"location":"Infrastructure/Network/#routing","text":"The routing table of a Cisco router has various sources, including many protocols of varying reliability. Each source is associated with: A letter code (i.e. R for RIP , etc) Administrative distance , a numeric value which reflects Cisco's measure of the source's trustworthiness. A lower value indicates higher trustworthiness, and the maximum value of 255 indicates an unusable route. The administrative values associated with each routing protocol must be memorized for the CCNA exam. For dynamic routing protocols a metric is also provided whose significance varies with protocol. For example, with RIP the metric signifies hop count . Administrative distance and metric are delimited by a forward slash and appear within square brackets following the prefix . Display routing table R1#show ip route Configure static route R1(config)#ip route 0.0.0.0 0.0.0.0 10.10.10.2 Set Gateway of Last Resort ip default-gateway ip default-network route ip default-gateway ip default-network ip route 0.0.0.0 0.0.0.0 Configure router IP IPv4 IPv6 EUI-64 SLAAC R1(config)#interface fa0/0 R1(config-if)#ip address 0.10.10.1 255.255.255.0 R1(config-if)#no shutdown R1(config-if)#end R1(config)#interface fa0/0 R1(config-if)#ipv6 address 2001:aaaa:bbbb::1/64 R1(config-if)#no shutdown R1(config-if)#end R1(config)#interface fa0/0 R1(config-if)#ipv6 address 2001:aaaa:bbbb::/64 eui-64 R1(config-if)#no shutdown R1(config-if)#end R1(config)#interface fa0/0 R1(config-if)#ipv6 address autoconfig R1(config-if)#no shutdown R1(config-if)#end","title":"Routing"},{"location":"Infrastructure/Network/#inter-vlan-routing","text":"A multilayer switch possesses a routing engine (RE) to route between VLANs. Without an RE, a Router on a stick configuration is necessary. Configure ROAS A subinterface can be configured by specifying a VLAN number after a period on the physical interface, then associating an IP range with it. Theese IP addresses can be used as default gateway addresses hosts will use in respective subnets. ROAS can be configured with or without setting an IP address on the interface. IP No IP R1(config-if)#ip address 10.1.0.1 255.255.255.0 R1(config-if)#interface gi0/1.10 R1(config-subif)#encapsulation dot1q 10 R1(config-subif)#ip address 10.1.10.1 255.255.255.0 R1(config-subif)#end R1(config)#inteface gi0/1.20 R1(config-subif)#encapsulation dot1q 20 R1(config-subif)#ip address 10.1.20.1 255.255.255.0 R1(config-subif)#end R1(config-if)#interface gi0/1.10 R1(config-subif)#encapsulation dot1q 10 R1(config-subif)#ip address 10.1.10.1 255.255.255.0 R1(config-subif)#end R1(config)#inteface gi0/1.20 R1(config-subif)#encapsulation dot1q 20 R1(config-subif)#ip address 10.1.20.1 255.255.255.0 R1(config-subif)#end SW1(config)#interface gi0/1 SW1(config-if)#switchport trunk encapsulation dot1q SW1(config-if)#switchport mode trunk Verify ROAS show ip interface brief R1#show ip interface brief === \"`show vlans``","title":"Inter-VLAN routing"},{"location":"Infrastructure/ansible-xfer/","text":"Ansible The architecture of an environment managed by Ansible features several concepts: Controller host which contains playbooks which are executed Managed hosts which are controlled using SSH Tasks Hello, World! --- - name : Hello, World! playbook hosts : all tasks : - name : Hello, World! play command : echo Hello, World! ... # (1) Ad-hoc ansible all -m command -a \"echo Hello, World!\" Configuration ansible.cfg [privilege_escalation] become = yes become_method = sudo become_user = root become_ask_pass = no remote_user = ansible [defaults] interpreter_python = auto_silent deprecation_warnings = False Display non-default settings ansible-config dump --only-changed Documentation ansible-doc # List currently installed modules ansible-doc -l # Get module-specific information ansible-doc $MODULE # Get example code ansible-doc -s $MODULE Verify YAML syntax ansible-playbook --syntax-check $FILE Files Create file --- - name : Create file hosts : all tasks : - name : Create file copy : dest : /etc/motd content : \"Hello, World!\\n\" # (1) ... Alma Linux 9 requires an additional package to be installed to handle SELinux contexts: # This package is incorrectly identified as \"libselinux-python\" in the Ansible error displayed on the controller host dnf install python3-libselinux Delete file --- - name : Delete file hosts : all tasks : - name : Delete file file : path : /etc/motd state : absent ...","title":"Ansible"},{"location":"Infrastructure/ansible-xfer/#ansible","text":"The architecture of an environment managed by Ansible features several concepts: Controller host which contains playbooks which are executed Managed hosts which are controlled using SSH","title":"Ansible"},{"location":"Infrastructure/ansible-xfer/#tasks","text":"Hello, World! --- - name : Hello, World! playbook hosts : all tasks : - name : Hello, World! play command : echo Hello, World! ... # (1) Ad-hoc ansible all -m command -a \"echo Hello, World!\"","title":"Tasks"},{"location":"Infrastructure/ansible-xfer/#configuration","text":"ansible.cfg [privilege_escalation] become = yes become_method = sudo become_user = root become_ask_pass = no remote_user = ansible [defaults] interpreter_python = auto_silent deprecation_warnings = False Display non-default settings ansible-config dump --only-changed","title":"Configuration"},{"location":"Infrastructure/ansible-xfer/#documentation","text":"ansible-doc # List currently installed modules ansible-doc -l # Get module-specific information ansible-doc $MODULE # Get example code ansible-doc -s $MODULE Verify YAML syntax ansible-playbook --syntax-check $FILE","title":"Documentation"},{"location":"Infrastructure/ansible-xfer/#files","text":"Create file --- - name : Create file hosts : all tasks : - name : Create file copy : dest : /etc/motd content : \"Hello, World!\\n\" # (1) ... Alma Linux 9 requires an additional package to be installed to handle SELinux contexts: # This package is incorrectly identified as \"libselinux-python\" in the Ansible error displayed on the controller host dnf install python3-libselinux Delete file --- - name : Delete file hosts : all tasks : - name : Delete file file : path : /etc/motd state : absent ...","title":"Files"},{"location":"Infrastructure/labbing/","text":"Labbing Hyper-V A NAT network can be configured in Hyper-V for use with VMs. NAT network limit There is only a single NAT network allowed per host. A new internal virtual switch is created, with a gateway explicitly configured. New-VMSwitch -Name Ansible -Type Internal New-NetIPAddress -IPAddress 192 . 168 . 2 . 1 -PrefixLength 24 -InterfaceIndex 14 New-NetNat -Name Ansible -InternalIPInterfaceAddressPrefix 192 . 168 . 2 . 0 / 24 WinNAT does not allocate IP addresses to endpoints, so IP addresses must be statically configured within each individual guest. ip address add 192 .168.2.100 dev eth0 ip route add 192 .168.2.0/24 dev eth0 ip route add default via 192 .168.2.1 dev eth0 WinNAT also does not provide DNS services, so the host's DNS will have to be inspected and used. WiFi complications Documentation for VirtualBox states that WLAN adapters do not support promiscuous mode and therefore the Bridge networking option operates differently when using the computer's WiFi adapter. This potentially may be the cause of odd behavior when using External networking in Hyper-V... Static IP configuration varies by the network management toolset and backend presenton a system. Ubuntu systems use Netplan whereas other distributions most commonly use Network Manager . Netplan Network Manager network : version : 2 ethernets : eth0 : addresses : - 192.168.2.100/24 gateway4 : 192.168.2.1 nameservers : addresses : - 192.168.1.1 search : [] [connection] id = Ethernet uuid = abcdef01-2345-6789-0abc-def012345678 type = ethernet interface-name = eth0 [ethernet] [ipv4] address1 = 192.168.2.100/24,192.168.2.1 dns = 10.40.7.2 method = manual [ipv6] addr-gen-mode = stable-privacy method = auto WSLv2 distros will not be able to access Hyper-V guests unless forwarding is enabled on both IP interfaces for the Hyper-V NAT as well as WSL Set-NetIpInterface -ifIndex $NAT -Forwarding Enabled Set-NetIpInterface -ifIndex $WSL -Forwarding Enabled # (1) The setting for the WSL interface appears not to be persistent. History Traditionally, network interfaces on Linux were configured in /etc/network/interfaces","title":"Labbing"},{"location":"Infrastructure/labbing/#labbing","text":"","title":"Labbing"},{"location":"Infrastructure/labbing/#hyper-v","text":"A NAT network can be configured in Hyper-V for use with VMs. NAT network limit There is only a single NAT network allowed per host. A new internal virtual switch is created, with a gateway explicitly configured. New-VMSwitch -Name Ansible -Type Internal New-NetIPAddress -IPAddress 192 . 168 . 2 . 1 -PrefixLength 24 -InterfaceIndex 14 New-NetNat -Name Ansible -InternalIPInterfaceAddressPrefix 192 . 168 . 2 . 0 / 24 WinNAT does not allocate IP addresses to endpoints, so IP addresses must be statically configured within each individual guest. ip address add 192 .168.2.100 dev eth0 ip route add 192 .168.2.0/24 dev eth0 ip route add default via 192 .168.2.1 dev eth0 WinNAT also does not provide DNS services, so the host's DNS will have to be inspected and used. WiFi complications Documentation for VirtualBox states that WLAN adapters do not support promiscuous mode and therefore the Bridge networking option operates differently when using the computer's WiFi adapter. This potentially may be the cause of odd behavior when using External networking in Hyper-V... Static IP configuration varies by the network management toolset and backend presenton a system. Ubuntu systems use Netplan whereas other distributions most commonly use Network Manager . Netplan Network Manager network : version : 2 ethernets : eth0 : addresses : - 192.168.2.100/24 gateway4 : 192.168.2.1 nameservers : addresses : - 192.168.1.1 search : [] [connection] id = Ethernet uuid = abcdef01-2345-6789-0abc-def012345678 type = ethernet interface-name = eth0 [ethernet] [ipv4] address1 = 192.168.2.100/24,192.168.2.1 dns = 10.40.7.2 method = manual [ipv6] addr-gen-mode = stable-privacy method = auto WSLv2 distros will not be able to access Hyper-V guests unless forwarding is enabled on both IP interfaces for the Hyper-V NAT as well as WSL Set-NetIpInterface -ifIndex $NAT -Forwarding Enabled Set-NetIpInterface -ifIndex $WSL -Forwarding Enabled # (1) The setting for the WSL interface appears not to be persistent. History Traditionally, network interfaces on Linux were configured in /etc/network/interfaces","title":"Hyper-V"},{"location":"Infrastructure/vmware/","text":"VMware vSphere is a suite of core infrastructure solutions for managing and monitoring a virtual infrastructure. The term was coined in 2009 with the release of VMware Virtual Infrastructure 4. VMware vCenter Server server is the management layer of multiple ESXi hosts, allowing clusters to be created and enabling features like vSphere HA and vSphere DRS. It comes as either a Windows application or a virtual appliance running on a stripped-down version of SUSE Linux. vCenter includes the Platform Services Controller (PSC) which consolidates several previously separate components, such as the SSO, Inventory Service, and Certificate Management. This can be installed on the same host as vCenter, or separately as either a Windows application or Linux appliance. Processes - **hostd** ESXi host agent - **vpxa** vCenter agent PowerCLI Setup Install-Module -Name VMware . PowerCLI # (1) Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser # (2) Set-PowerCLIConfiguration -InvalidCertificateAction Fail -Scope AllUsers If the Hyper-V module is installed, the -AllowClobber option must be provided. Allow local scripts to be run, but not those downloaded from the web or email. Decommission servers Connect-VIServer -Server $viserver # (1) $servers = Import-CSV $args [ 0 ] foreach ( $server in $servers ) { $VM = Get-VM -Name $server . Name Stop-VM $VM -Confirm -ErrorAction SilentlyContinue New-TagAssignment -Entity $VM -Tag ( Get-Tag -Name TPA_Decomm ) # (3) Set-NetworkAdapter -NetworkAdapter ( Get-NetworkAdapter -VM $VM ) -StartConnected $false -Confirm Move-VM -VM $VM -Destination DECOMMISSION # (2) } Connect-VIServer establishes a connection to a vCenter Server system, checking certificates according to the policy set by Set-PowerCLIConfiguration . The connection can be destroyed with Disconnect-VIServer If the destination folder is under a parent, then apparently the best that can be done is to retrieve the Folder object first, specifying the parent folder to -Location : Move-VM -VM $VM -Destination ( Get-Folder Linux -Location PROD ) Remove tag assignment Remove-TagAssignment ( Get-TagAssignment -Entity $VM -Tag ( Get-Tag -Name TPA_Decomm ))","title":"VMware"},{"location":"Infrastructure/vmware/#vmware","text":"vSphere is a suite of core infrastructure solutions for managing and monitoring a virtual infrastructure. The term was coined in 2009 with the release of VMware Virtual Infrastructure 4. VMware vCenter Server server is the management layer of multiple ESXi hosts, allowing clusters to be created and enabling features like vSphere HA and vSphere DRS. It comes as either a Windows application or a virtual appliance running on a stripped-down version of SUSE Linux. vCenter includes the Platform Services Controller (PSC) which consolidates several previously separate components, such as the SSO, Inventory Service, and Certificate Management. This can be installed on the same host as vCenter, or separately as either a Windows application or Linux appliance.","title":"VMware"},{"location":"Infrastructure/vmware/#processes","text":"- **hostd** ESXi host agent - **vpxa** vCenter agent","title":"Processes"},{"location":"Infrastructure/vmware/#powercli","text":"Setup Install-Module -Name VMware . PowerCLI # (1) Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser # (2) Set-PowerCLIConfiguration -InvalidCertificateAction Fail -Scope AllUsers If the Hyper-V module is installed, the -AllowClobber option must be provided. Allow local scripts to be run, but not those downloaded from the web or email. Decommission servers Connect-VIServer -Server $viserver # (1) $servers = Import-CSV $args [ 0 ] foreach ( $server in $servers ) { $VM = Get-VM -Name $server . Name Stop-VM $VM -Confirm -ErrorAction SilentlyContinue New-TagAssignment -Entity $VM -Tag ( Get-Tag -Name TPA_Decomm ) # (3) Set-NetworkAdapter -NetworkAdapter ( Get-NetworkAdapter -VM $VM ) -StartConnected $false -Confirm Move-VM -VM $VM -Destination DECOMMISSION # (2) } Connect-VIServer establishes a connection to a vCenter Server system, checking certificates according to the policy set by Set-PowerCLIConfiguration . The connection can be destroyed with Disconnect-VIServer If the destination folder is under a parent, then apparently the best that can be done is to retrieve the Folder object first, specifying the parent folder to -Location : Move-VM -VM $VM -Destination ( Get-Folder Linux -Location PROD ) Remove tag assignment Remove-TagAssignment ( Get-TagAssignment -Entity $VM -Tag ( Get-Tag -Name TPA_Decomm ))","title":"PowerCLI"},{"location":"Infrastructure/Cloud/GCP/","text":"Google Cloud Platform gcloud Adding repo Ubuntu Fedora deb [ signed-by = /usr/share/keyrings/cloud.google.gpg ] http://packages.cloud.google.com/apt cloud-sdk main To be saved in /etc/yum.repos.d/google-cloud-sdk.repo [google-cloud-sdk] name = Google Cloud SDK baseurl = https://packages.cloud.google.com/yum/repos/cloud-sdk-el7-x86_64 enabled = 1 gpgcheck = 1 repo_gpgcheck = 0 gpgkey = https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg Initialize gcloud init In GCP, APIs are enabled per project. List all APIs # Enabled APIs only gcloud services list # Including disabled APIs gcloud services list --available Enable API gcloud services enable compute.googleapis.com container.googleapis.com Display all available regions and zones gcloud compute regions list gcloud compute zones list Set default region and zone gcloud config set compute/region us-east1 # Moncks Corner, South Carolina gcloud config set compute/zone us-east1-c Storage Create disk gcloud compute disks create my-disk --size = 10GB --zone = us-east1-b","title":"Google Cloud Platform"},{"location":"Infrastructure/Cloud/GCP/#google-cloud-platform","text":"","title":"Google Cloud Platform"},{"location":"Infrastructure/Cloud/GCP/#gcloud","text":"Adding repo Ubuntu Fedora deb [ signed-by = /usr/share/keyrings/cloud.google.gpg ] http://packages.cloud.google.com/apt cloud-sdk main To be saved in /etc/yum.repos.d/google-cloud-sdk.repo [google-cloud-sdk] name = Google Cloud SDK baseurl = https://packages.cloud.google.com/yum/repos/cloud-sdk-el7-x86_64 enabled = 1 gpgcheck = 1 repo_gpgcheck = 0 gpgkey = https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg Initialize gcloud init In GCP, APIs are enabled per project. List all APIs # Enabled APIs only gcloud services list # Including disabled APIs gcloud services list --available Enable API gcloud services enable compute.googleapis.com container.googleapis.com Display all available regions and zones gcloud compute regions list gcloud compute zones list Set default region and zone gcloud config set compute/region us-east1 # Moncks Corner, South Carolina gcloud config set compute/zone us-east1-c","title":"gcloud"},{"location":"Infrastructure/Cloud/GCP/#storage","text":"Create disk gcloud compute disks create my-disk --size = 10GB --zone = us-east1-b","title":"Storage"},{"location":"Infrastructure/Cloud/Tasks/","text":"Tasks Cloud APIs are equivalent to Azure resource providers . Unlike Azure, which automatically registers resource providers on use, Cloud APIs must be enabled per project. GCP gcloud services enable container.googleapis.com Azure az provider register -n Microsoft.ContainerService Display all available regions GCP gcloud compute regions list Install CLI utility Prerequisites Signing key Repo Install GCP apt install apt-transport-https ca-certificates gnupg Azure apt install apt-transport-https ca-certificates curl lsb-release gnupg GCP curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add - Azure curl -sL https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor | sudo tee /etc/apt/trusted.gpg.d/microsoft.gpg > /dev/null GCP echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main\" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list Azure echo \"deb [arch=amd64] https://packages.microsoft.com/repos/azure-cli/ $( lsb_release -cs ) main\" | sudo tee /etc/apt/sources.list.d/azure-cli.list GCP sudo apt-get update && sudo apt-get install google-cloud-sdk Azure sudo apt-get update && sudo apt-get install azure-cli Kubernetes Clusters Create Read List Update Delete GCP gcloud container clusters create $name --num-nodes = 1 # Standard mode gcloud container clusters create-auto $name # Autopilot mode Azure az aks create -g $group -n $name --node-count 1 --enable-addons monitoring --generate-ssh-keys GCP gcloud container clusters describe $name Azure GCP gcloud container clusters list Azure az aks list GCP Azure GCP gcloud container clusters delete $name Azure az aks delete -g $group -n $name Add context to kubeconfig GCP gcloud container clusters get-credential $name Azure az aks get-credentials -g $group -n $name Get-AzAKSCredentials Storage Create a 10GB disk GCP gcloud compute disks create my-disk --size = 10GB --zone = us-east1-a Azure AWS \ud83d\udee0\ufe0f Administration Display subscription ID Get-AzSubscription az account show \ud83d\udda5\ufe0f CLI Initialize CLI utility gcloud init IAM Add guest user New-AzureADMSInvitation -InvitedUserEmailAddress $EMAIL -SendInvitationMessage $True -InviteRedirectUrl \"http://myapps.onmicrosoft.com\" Assign a role # At the organization level gcloud organizations add-iam-policy-binding $ORG_ID --member = \"user: $EMAIL \" --role = \"roles/compute.xpnAdmin\" # At the folder level gcloud beta resource-manager-folders add-iam-policy-binding $FOLDER_ID --member = \"user: $EMAIL \" --role = \"roles/compute.xpnAdmin\" \ud83d\udcb0 Cost management To view resource quotas for a subscription, go to the subscription in Azure Portal and open the Usage + quotas blade. From there you can select resources and then click the Request Increase button. View current usage of vCPU quotas Get-AzVMUsage View current usage of storage service Get-AzStorageUsage Create a budget To create a budget, open Cost Management + Billing , then Subscriptions , select a subscription, then click Budgets . Then click + Add , which produces a Create budget blade. The created budget can be seen in the Budgets blade. PowerShell commands used with budgets: Get-AzResourceGroup retrieve Resource Group object Set-AzResourceGroup apply a tag to a resource group with no preexisting tags .Tags method that retrieves Tag collection from a resource group .Add() method used to add tags to a resource group that already has tags. Monitoring VM extension Set-AzVMExtension -ResourceGroupName ExamRefRG -Location \"West Europe\" -VMName VM1 -Name networkWatcherAgent -Publisher Microsoft . Azure . NetworkWatcher -Type NetworkWatcherAgentWindows -TypeHandlerVersion 1 . 4 az vm extension set --vm-name VM1 --resource-group ExamRefRG --publisher Microsoft.Azure.NetworkWatcher --version 1 .4 --name NetworkWatcherAgentWindows --extension-instance-name NetworkWatcherAgent Start packet capture $nw = Get-AzResource | Where ResourceType -eq \"Microsoft.Network/networkWatchers\" -and Location -eq \"WestEurope\" $networkWatcher = Get-AzNetworkWatcher -Name $nw . Name -ResourceGroupName $nw . ResourceGroupName $storageAccount = Get-AzStorageAccount -Name examref-storage -ResourceGroupName ExamRefRG $filter1 = New-AzPacketCaptureFilterConfig -Protocol TCP -RemoteIPAddress \"1.1.1.1-255.255.255.255\" -LocalIPAddress \"10.0.0.3\" -LocalPort \"1-65535\" -RemotePort \"20;80;443\" $filter2 = New-AzPacketCaptureFilterConfig -Protocol UDP $vm = Get-AzVM ` -Name VM1 -ResourceGroupName ExamRefRG New-AzNetworkWatcherPacketCapture -NetworkWatcher $networkWatcher -TargetVirtualMachineId $vm . Id -PacketCaptureName \"PacketCaptureTest\" -StorageAccountId $storageAccount . id -TimeLimitInSeconds 60 -Filter $filter1 , $filter2 filter = '[ { \"protocol\": \"TCP\", \"remoteIPAddress\": \"1.1.1.1-255.255.255.255\", \"localIPAddress\":\"10.0.0.3\", \"remotePort\":\"20\" } ]' az network watcher packet-capture create --name PacketCaptureTest2 --resource-group ExamRefRG --vm VM1 --time-limit 300 --storage-account examref-storage --filters $filter Check status of packet capture Get-AzNetworkWatcherPacketCapture -NetworkWatcher $networkWatcher -PacketCaptureName \"PacketCaptureTest\" az network watcher packet-capture show-status --name PacketCaptureTest --location WestEurope Stop packet capture Stop-AzNetworkWatcherPacketCapture -NetworkWatcher $networkWatcher -PacketCaptureName \"PacketCaptureTest\" az network watcher packet-capture stop --name PacketCaptureTest --location WestEurope Use IP Flow Verify to test outbound connectivity from source VM and port to destination. If any configured filtering rules block traffic between the endpoints, it will return the name of the offending NSG. Test-AzNetworkWatcherIPFlow az network watcher test-ip-flow Next Hop Get-AzNetworkWatcherNextHop az network watcher show-next-hop Use Network Topology Get-AzNetworkWatcherTopology az network watcher show-topology Capture SFTP traffic $r = Get-AzResource | where ResourceType -eq \"Microsoft.Network/networkWatchers\" -and Location -eq \"EastUS\" $nw = Get-AzNetworkWatcher -Name $r . Name -ResourceGroupName $r . ResourceGroupName $s = Get-AzStorageAccount -ResourceGroupName \"Diagnostics-RG\" -Name \"Diagnostics-Storage\" $filter = New-AzPacketCaptureFilterConfig -Protocol TCP -RemoteIPAddress \"1.1.1.1-255.255.255.255\" -LocalIPAddress \"10.0.0.4\" -LocalPort \"1-65535\" -RemotePort \"22\" New-AzNetworkWatcherPacketCapture -NetworkWatcher $nw -TargetVirtualMachineId $vm . ID -PacketCaptureName \"Capture SFTP traffic\" -StorageAccountId $s . Id -TimeLimitInSeconds 60 -Filter $filter Policy Assign a policy $scope = '/subscriptions/$subscriptionID' $policyparam = '{ \"tagName\" : { \"value\": \"Environment\" }, \"tagValue\": { \"value\" : \"Production\" } }' $assignment = New-AzPolicyAssignment -Name 'append-environment-tag' -DisplayName 'Append Environment Tag' -Scope $scope -PolicyDefinition $definition -PolicyParameter $policyparam Remove policy assignment and definition Remove-AzPolicyAssignment -Id $assignment . ResourceId Remove-AzPolicyDefinition -Id $definition . ResourceId Create a policy definition Azure Portal (All Services) > Policy > Definitions: Both builtin and custom policies can be managed here. New-AzPolicyDefinition -Name 'appendEnvironmentTag' -DisplayName 'Append Environment Tag' -Policy 'AppendDefaultTag.json' -Parameter 'AppendDefaultTagParams.json' az policy definition create --name 'allowedVMs' --description 'Only allow virtual machines in the defined SKUs' --mode ALL --rules '{...}' --params '{...}' Apply policy to a scope az policy assignment create --policy allowedVMs --name 'deny-non-compliant-vms' --scope '/subscriptions/<Subscription ID>' -p Delete policy assignment az policy assignment delete --name deny-non-compliant-vms Resources Create resource group New-AzGroup -Location $location -Name $rgName az group create -l $location -n $rgName Register resource provider in subscription az provider register --namespace 'Microsoft.PolicyInsights' Move resources $webapp = Get-AzResource -ResourceGroupName OldRG -ResourceName ExampleSite $plan = Get-AzResource -ResourceGroupName OldRG -ResourceName ExamplePlan Move-AzResource -DestinationResourceGroupName NewRG -ResourceId $webapp . ResourceId , $plan . ResourceId webapp = $( az resource show -g OldRG -n ExampleSite --resource-type \"Microsoft.Web/sites\" --query id --output tsv ) plan = $( az resource show -g OldRG -n ExamplePlan --resource-type \"Microsoft.Web/serverfarms\" --query id --output tsv ) az resource move --destination-group newgroup --ids $webapp $plan Create lock on a resource New-AzResourceLock -LockName LockSite -LockLevel CanNotDelete -ResourceGroupName $rg -ResourceName $r -ResourceType Microsoft . Web / sites az lock create --name LockSite --lock-type CanNotDelete --resource-group $rg --resource-name $r --resource-type Microsoft.Web/sites Create lock on a resource group New-AzResourceLock -LockName LockGroup -LockLevel CanNotDelete -ResourceGroupName $rg az lock create --name LockGroup --lock-type CanNotDelete --resource-group $rg Display resource lock Get-AzResourceLock -ResourceName $r -ResourceType Microsoft . Web / sites -ResourceGroupName $rg az lock list --resource-group $rg --resource-name $r --namespace Microsoft.Web --resource-type sites --parent \"\" Delete resource lock $lockId = ( Get-AzResourceLock -ResourceGroupName $rg -ResourceName $r -ResourceType Microsoft . Web / sites ). LockId Remove-AzResourceLock -LockId $lockId lockid = $( az lock show --name LockSite --resource-group $rg --resource-type Microsoft.Web/sites --resource-name $r --output tsv --query id ) az lock delete --ids $lockid Sources Manage Azure Resource Manager resource groups by using Azure PowerShell Manage Azure Resource Manager resource groups by using Azure CLI Resource providers Lock resources to prevent unexpected changes AZ-103: 1.3 , p. 76 Tags List all resources by tag ( Get-AzResource -Tag @{ CostCode = \"1001\" }). Name # List all resources by tag name, with no value ( Get-AzResource -TagName CostCode ). Name az resource list --tag Dept = Finance List resource groups by tag ( Get-AzResourceGroup -Tag @{ CostCode = \"1001\" }). ResourceGroupName az group list --tag CostCode = 1001 Enumerate a resource's tags $r = Get-AzResource -Name $resourceName -ResourceGroup rg Get-AzTag -ResourceId $r . id # Resource group $rg = Get-AzResourceGroup -Name $rgName Get-AzTag -ResourceId $rg . ResourceId # Subscription $s = ( Get-AzSubscription -SubscriptionName \"Example Subscription\" ). Id Get-AzTag -ResourceId \"/subscriptions/$s\" az resource show -n $resourceName -g $rgName --query tags # Resource group az group show -n $rgName --query tags Tag resource $r = Get-AzResource -ResourceName hrvm1 -ResourceGroupName rg $r . Tags . Add ( \"Owner\" , \"user@contoso.com\" ) Set-AzResource -Tag $r . Tags -ResourceId $r . ResourceId -Force Resource group $tags = @{ \"Dept\" = \"Finance\" ; \"Status\" = \"Normal\" } $rg = Get-AzResourceGroup -Name demoGroup New-AzTag -ResourceId $rg . ResourceId -tag $tags $tags = ( Get-AzResourceGroup -Name rg ). Tags $tags . Add ( \"Owner\" , \"user@contoso.com\" ) Set-AzResourceGroup -Tag $tags -Name rg jsonrtag = $( az group show -n rg --query tags ) rt = $( echo $jsonrtag | tr -d '\"{},' | sed 's/: /=/g' ) az group update -n rg --tags $rt Owner = user@contoso.com Remove specific tags $tags = @{ \"Project\" = \"ECommerce\" ; \"Team\" = \"Web\" } Update-AzTag -ResourceId $resource . id -Tag $tags -Operation Delete Remove all tags $s = ( Get-AzSubscription -SubscriptionName \"Example Subscription\" ). Id Remove-AzTag -ResourceId \"/subscriptions/$s\" # Alternatively Set-AzResourceGroup -Tag @{} -Name rg Apply tags to resource, overwriting $tags = @{ \"Dept\" = \"Finance\" ; \"Status\" = \"Normal\" } New-AzTag -ResourceId $resource . id -Tag $tags Set-AzResource -ResourceId $r . ResourceId -Tag @{ CostCode = \"1001\" ; Environment = \"Production\" } -Force az resource tag --tags 'Dept=IT' 'Environment=Test' -g $rgName -n examplevnet --resource-type \"Microsoft.Network/virtualNetworks\" Apply tags to resource group Set-AzResourceGroup -Name rg -Tag @{ CostCode = 1001 ; Environment = Production } az group update -n $rgName --tags 'Environment=Test' 'Dept=IT' # Alternatively az group update -n $rgName --set tags.Environment = Production tags.CostCode = 1001 Compute IaaS Create a VM ( src ) gcloud compute instances create instance-1 --zone-uscentral1-a PaaS Deploy app in current working directory. gcloud app deploy View the deployed app gcloud app browse app.yaml allows configuration of the app in several ways runtime : python37 In Azure, multiple web applications are organized under an App Service Plan resource. So if no such app service plan exists, it must be created. $p = New-AzAppServicePlan -Name $n -ResourceGroupName $g -Location $l -Tier \"Basic\" -NumberofWorkers 2 -WorkerSize \"Small\" New-AzWebApp -Name $n -Location $l -ResourceGroupName $g -AppServicePlan $p az appservice plan create -g $g -n $p --is-linux az webapp create -n $n -g $g --plan $p Containers Create a new source repository These steps require: Cloud SDK and Git to be installed A GCP project with billing and the Cloud Source Repositories API enabled #Create a new repository gcloud source repos create hello-world #Clone it locally gcloud source repos clone hello-world # Create scripts, then add, commit and push them as usual. git commit -am \"Initial\" git push origin master Create container registry New-AzContainerRegistry -ResourceGroupName $rg -Name $registry -Sku \"Basic\" -EnableAdminUser az acr create --name $registry --resource-group $rg --sku Basic --admin-enabled true \u2693 Kubernetes Create Kubernetes cluster New-AzAKS -ResourceGroupName $g -Name $n -NodeCount 2 -NetworkPlugin azure -NodeVmSetType VirtualMachineScaleSets -WindowsProfileAdminUserName azureuser -WindowsProfileAdminUserPassword $Password -KubernetesVersion 1 . 16 . 7 # PowerShell does not offer an option to generate SSH keys for access to the cluster; `ssh-keygen` must be used. - Create a Windows Server container on an AKS cluster az aks create -g $g -n $n --node-count 2 --network-plugin azure --vm-set-type VirtualMachineScaleSets --windows-admin-username azureuser --windows-admin-password $PASSWORD --generate-ssh-keys --enable-addons monitoring - Create a Windows Server container on an AKS cluster Add a pool of nodes New-AzAksNodePool -ResourceGroupName $rgName -Name npwin -ClusterName $clusterName -OsType Windows -KubernetesVersion 1 . 16 . 7 az aks nodepool add -g $g -n $n --cluster-name $clusterName --os-type Windows --node-count 1 Persistent volume claim apiVersion : v1 kind : PersistentVolumeClaim metadata : name : azure-managed-disk spec : accessModes : - ReadWriteOnce storageClassName : managed-premium resources : requests : storage : 5Gi - Source Provision Azure Disk Standard Premium kind : StorageClass apiVersion : storage.k8s.io/v1 metadata : name : managed-disk-forapp provisioner : kubernetes.io/azure-disk reclaimPolicy : Retain parameters : storageaccounttype : default kind : Managed kind : StorageClass apiVersion : storage.k8s.io/v1 metadata : name : managed-disk-forapp provisioner : kubernetes.io/azure-disk reclaimPolicy : Retain parameters : storageaccounttype : Premium_LRS kind : Managed Functions Deploy gcloud functions deploy hello_get --runtime python37 --trigger-http Test gcloud functions describe hello_get Storage Create storage account Azure Portal Click Create a resouce , then Storage , then Storage account . Choose a globally unique name for the account, containing lower-case characters and digits only. New-AzStorageAccount -ResourceGroupName ExamRefRG -Name mystorage112300 -SkuName Standard_LRS -Location WestUS -Kind StorageV2 -AccessTier Hot az storage account create --name $accountName --resource-group $resourceGroup -location $location --sku $sku Change access tier of storage account === \"Azure PowerShell ```powershell Set-AzStorageAccount -ResourceGroupName RG -Name $accountName -AccessTier Cool -Force ``` Change replication mode of storage account Set-AzStorageAccount -ResourceGroupName $resourceGroup -Name $accountName -SkuName $type Renew storage account keys === \"Azure ```powershell New-AzStorageAccountKey ``` az storage account keys renew Create Azure Key Vault New-AzKeyVault -VaultName $vaultName -ResourceGroupName $g -Location $location $key = Add-AzKeyVaultKey -VaultName $vaultName -Name $keyName -Destination 'Software' $storageKey = Get-AzStorageAccountKey -ResourceGroupName $g -Name $storageAccount $secretvalue = ConvertTo-SecureString $storageKey [ 0 ]. Value -AsPlainText -Force $secret = Set-AzKeyVaultSecret -VaultName $vaultName -Name $secretName -SecretValue $secretvalue az keyvault create --name $vaultName --resource-group $g --location $location az keyvault key create --vault-name \" $vaultName \" --name $keyName --protection \"software\" az keyvault secret set --vault-name \" $vaultName \" --name \" $secretName \" --value \" $secretValue \" Create key in Azure Key Vault $key = Add-AzKeyVaultKey -VaultName $vaultName -Name $keyName -Destination 'Software' $storageKey = Get-AzStorageAccountKey -ResourceGroupName $g -Name $storageAccount $secretvalue = ConvertTo-SecureString $storageKey [ 0 ]. Value -AsPlainText -Force $secret = Set-AzKeyVaultSecret -VaultName $vaultName -Name $secretName -SecretValue $secretvalue az keyvault key create --vault-name $vaultName --name $keyName --protection \"software\" az keyvault secret set --vault-name $vaultName --name $secretName --value $secretValue Create Azure sync group Specify name of sync group in dialog after creating an Azure File Sync Change storage class $STORAGE_CLASS can be multi_regional , regional , nearline , or coldline gsutil rewrite -s $STORAGE_CLASS gs:// $PATH_TO_OBJECT File shares Deploy Azure File Sync # Create Storage Sync Service $storageSync = New-AzStorageSyncService -ResourceGroupName $g -Name $storageSyncName -Location $l # Create Azure File Share $storageKey = Get-AzStorageAccountKey -ResourceGroupName $g -Name $storageAccount $context = New-AzStorageContext -StorageAccountName $storageAccount -StorageAccountKey $storageKey . Value [ 0 ] New-AzStorageShare -Name $shareName -Context $context # Creating a Storage Sync Service resource is only possible in PowerShell or Portal constring = $( az storage account show-connection-string -n $storageAccountName ) az storage share create --name $shareName --quota 2048 --connection-string $constring Create sync group $syncgroup = New-AzStorageSyncGroup -Name $syncgroupname -ParentObject $storageSync Create cloud endpoint New-AzStorageSyncCloudEndpoint -Name $shareName -ParentObject $syncgroup -StorageAccountResourceId $storageAccount . Id -AzureFileShareName $shareName Network access Display the status of the default NetworkRule for a storage account Get-AzStorageAccountNetworkRuleSet -ResourceGroupName $rgName -AccountName $n | Select-Object DefaultAction az storage account show - $rgName -n $n --query networkRuleSet.defaultAction Set default rule Update-AzStorageAccountNetworkRuleSet -ResourceGroupName $g -Name $n -DefaultAction Deny Update-AzStorageAccountNetworkRuleSet -ResourceGroupName $g -Name $n -DefaultAction Allow az storage account update -g $g -n $n --default-action Deny az storage account update -g $g -n $n --default-action Allow Networking Create virtual network with a specific prefix and subnet $subnet = New-AzVirtualNetworkSubnetConfig -Name $subnetName -AddressPrefix \"10.0.0.0/24\" $vnet = New-AzVirtualNetwork -Name $name -ResourceGroupName $rgName -Location $l -AddressPrefix \"10.0.0.0/16\" -Subnet $subnet az network vnet create -g $rgName -n $name --address-prefix \"10.0.0.0/16\" --subnet-name $subnetName --subnet-prefix \"10.0.0.0/24\" gcloud networks create $name --subnet-mode = custom gcloud beta compute networks subnets create $subnetName --network = $name --region = $l --range = \"10.0.0.0/16\" --enable-private-ip-google-access --enable-flow-logs Create peering Add-AzVirtualNetworkPeering -Name 'peering1' -VirtualNetwork $net1 -RemoteVirtualNetworkId $net2 . Id Add-AzVirtualNetworkPeering -Name 'peering2' -VirtualNetwork $net2 -RemoteVirtualNetworkId $net1 . Id az network vnet peering create -n 'peering1' -g $g --vnet-name net1 --allow-vnet-access --remote-vnet net2 az network vnet peering create -n 'peering2' -g $g --vnet-name net2 --allow-vnet-access --remote-vnet net1 gcloud compute networks peerings create \"peering1\" --network net1 --peer-project $p --peer-network net2 --auto-create-routes gcloud compute networks peerings create \"peering2\" --network net1 --peer-project $p --peer-network net1 --auto-create-routes Check peering Get-AzVirtualNetworkPeering -ResourceGroupName $rg -VirtualNetworkName $vnetName az network vnet peering list --resource-group $rg --vnet-name VNet1 az network vnet peering list --resource-group $rg --vnet-name VNet2 User-defined routes # Create the route table resource $routeTable = New-AzRouteTable -Name $routeTableName -ResourceGroupName ExamRefRG # Add a route to route table object Add-AzRouteConfig -RouteTable $routeTable -Name $routeName -AddressPrefix 10 . 3 . 0 . 0 / 16 -NextHopType VirtualAppliance -NextHopIpAddress 10 . 2 . 20 . 4 Set-AzRouteTable -RouteTable $routeTable # Associate route table with subnet Set-AzVirtualNetworkSubnetConfig -VirtualNetwork $vnet -Name Default -AddressPrefix $subnet . AddressPrefix -RouteTable $routeTable # Commit changes Set-AzVirtualNetwork -VirtualNetwork $vnet # Get effective routes for a NIC Get-AzEffectiveRouteTable -NetworkInterfaceName $nicName -ResourceGroupName $rgName # Create route table resource az network route-table create --name $routeTableName --resource-group $rgName # Add route to route table az network route-table route create --resource-group $rgName --route-table-name $routeTableName --name $routeName --address-prefix 10 .3.0.0/16 --next-hop-type VirtualAppliance --next-hop-ip-address 10 .2.20.4 # Associate route table with subnet az network vnet subnet update --name defualt --vnet-name Vnet1 --resource-group $rgName --route-table rt # Get effective routes for NIC az network nic show-effective-route-table --name $nicName --resource-group $rgName Create NSG $nsgRules = @() $nsgRules += New-AzNetworkSecurityRuleConfig -Name \"AllowingWinRMHTTP\" -Description \"To Enable PowerShell Remote Access\" -Access Allow -Protocol Tcp -Direction Inbound -Priority 103 -SourceAddressPrefix Internet -SourcePortRange * -DestinationAddressPrefix * -DestinationPortRange 5985 $nsgRules += New-AzNetworkSecurityRuleConfig -Name \"AllowingWinRMHTTPS\" -Description \"To Enable PowerShell Remote Access\" -Access Allow -Protocol Tcp -Direction Inbound -Priority 104 -SourceAddressPrefix Internet -SourcePortRange * -DestinationAddressPrefix * -DestinationPortRange 5986 $nsg = New-AzNetworkSecurityGroup -Name \"wscore-nsg\" -ResourceGroupName \"RG\" -Location \"East US\" -SecurityRules $nsgRules View rules Get-AzEffectiveNetworkSecurityGroup -NetworkInterfaceName $nicName -ResourceGroupName $rgName az network nic list-effective-nsg --name $nicName --resource-group $rgName Create Bastion Connecting to a VM requires at least Reader role privileges on the VM, its NIC, and on the Bastion itself. New-AzBastion -ResourceGroupName $rgName -Name $n -PublicIpAddress $pip -VirtualNetwork $vnet az network bastion create -g $rgName -n $n -l $l --public-ip-address $pip --vnet-name $vnetName Create virtual appliance IP forwarding must be enabled on the VM's NIC, then applications installed on the VM can begin accepting packets destined for other IP addresses. CDN Create new profile Azure Portal Click Create a resource Click Web Click CDN , opening the CDN profile blade Specify name for the profile, name of the resource group, region, and pricing tier. Click Create AZ-103: p. 140 Create endpoint Azure Portal Add an endpoint to a CDN profile (Portal) 1. Open the CDN Profile 2. Click + Endpoint button 3. Specify unique name, configuration for origin settings such as type, host header, and origin port for HTTP and HTTPS. 4. Click Add button AZ-103: p. 141 Publish content in a CDN endpoint Azure Portal Create a new CDN profile Add an endpoint to the profile DNS Create DNS zone New-AzDnsZone -Name examref . com -ResourceGroupName ExamRefRG az network dns zone create --name examref.com --resource-group ExamRefRG Create empty A record New-AzDnsRecordSet -Name www -RecordType A -ZoneName examref . com -ResourceGroupName ExamRefRG -Ttl 3600 -DnsRecords ( New-AzDnsRecordConfig -IPv4Address \"1.2.3.4\" ) az network dns record-set a create --name www --zone-name examref.com --resource-group ExamRefRG --ttl 3600 Create multiple records $records = @() $records += New-AzDnsRecordConfig -IPv4Address \"1.2.3.4\" $records += New-AzDnsRecordConfig -IPv4Address \"5.6.7.8\" New-AzDnsRecordSet -Name \"@\" -RecordType A -ZoneName examref . com -ResourceGroupName ExamRefRG -Ttl 3600 -DnsRecords $records az network dns record-set a add-record --record-set-name www --zone-name examref.com --resource-group ExamRefRG --ipv4-address 1 .2.3.4 az network dns record-set a add-record --record-set-name www --zone-name examref.com --resource-group ExamRefRG --ipv4-address 5 .6.7.8 Remove record $recordset = Get-AzDnsRecordSet -Name www -RecordType A -ZoneName examref . com -ResourceGroupName ExamRefRG Add-AzdnsRecordConfig -RecordSet $recordset -IPv4Address \"5.6.7.8\" Remove-AzDnsRecordConfig -RecordSet $recordset -IPv4Address \"1.2.3.4\" Set-AzDnsRecordSet -RecordSet $recordset az network dns record-set a remove-record --record-set-name www --zone-name examref.com --resource-group ExamRefRG --ipv4-address 1 .2.3.4 Read records Get-AzDnsRecordSet -ZoneName examref . com -ResourceGroupName ExamRefRG az network dns record-set list --zone-name examref.com --resource-group ExamRefRG -o table Create a virtual network with custom DNS settings New-AzVirtualNetwork -Name VNet1 -ResourceGroupName $rgName -Location $location -AddressPrefix 10 . 1 . 0 . 0 / 16 -Subnet ( New-AzVirtualNetworkSubnetConfig -Name Default -AddressPrefix 10 . 1 . 0 . 0 / 24 ) -DNSServer 10 . 0 . 0 . 4 , 10 . 0 . 0 . 5 az network vnet create --name VNet1 --resource-group $rgName --address-prefixes 10 .0.0.0/16 --dns-servers 10 .0.0.4 10 .0.0.5 Modify the DNS server configuration of an existing VNET $vnet = Get-AzVirtualNetwork -Name $vnetName -ResourceGroupName $rgName $vnet . DhcpOptions . DnsServers . Clear () $vnet . DhcpOptions . DnsServers . Add ( \"10.10.200.1\" ) $vnet . DhcpOptions . DnsServers . Add ( \"10.10.200.2\" ) Set-AzVirtualNetwork -VirtualNetwork $vnet az network vnet update --name $vnetName --resource-group $rgName --dns-servers 10 .10.200.1 10 .10.200.2 Restart the VMs in the VNet to pick up the DNS change $vm = Get-AzVM -Name VNet1-VM -ResourceGroupName ExamRefRG Restart-AzVM -ID $vm . Id Update the DNS settings on a NIC $nic = Get-AzNetworkInterface -Name VM1-NIC -ResourceGroupName ExamRefRG $nic . DnsSettings . DnsServers . Clear () $nic . DnsSettings . DnsServers . Add ( \"8.8.8.8\" ) $nic . DnsSettings . DnsServers . Add ( \"8.8.4.4\" ) Commit the DNS change, causing the VM to restart Set-AzNetworkInterface -NetworkInterface $nic Remove custom DNS servers from a VNET az network vnet update --name VNet1 --resource-group ExamRefRG --remove DHCPOptions.DNSServers Set custom DNS servers on a NIC az network nic update --name VM1-NIC --resource-group ExamRefRG --dns-servers 8 .8.8.8 8 .8.4.4 Load balancing Create public load balancer Creating a load balancer in PowerShell requires defining objects which are all passed to New-AzLoadBalancer as objects: - Frontend IP - Public Ip Address resource (if public) - Private IP address specified as a string (if internal) - Backend address pool - Health probe - Load balancing rule By contrast, in Azure CLI, the load balancer can be defined first with az network lb create before adding a probe and rule, passing the name of the load balancer to --lb-name . $publicIP = New-AzPublicIpAddress -Name ExamRefLB-IP -ResourceGroupName $g -Location $location -AllocationMethod Static $frontendIP = New-AzLoadBalancerFrontendIpConfig -Name frontend -PublicIpAddress $publicIP $beAddressPool = New-AzLoadBalancerBackendAddressPoolConfig -Name backend $healthProbe = New-AzLoadBalancerProbeConfig -Name -RequestPath '/' -Protocol http -Port 80 $lbrule = New-AzLoadBalancerRuleConfig -Name -FrontendIpConfiguration $frontendIP -BackendAddressPool $beAddressPool -Probe $healthProbe -Protocol Tcp -FrontendPort 80 -BackendPort 80 $lb = New-AzLoadBalancer -ResourceGroupName -Name -Location -FrontendIpConfiguration $frontendIP -LoadBalancingRule $lbrule -BackendAddressPool $beAddressPool -Probe $healthProbe az network public-ip create --name ExamRefLB-IP --resource-group ExamRefRG --location --allocation-method Static az network lb create --name ExamRefLB --resource-group ExamRefRG --location --backend-pool-name backend --frontend-ip-name frontend --public-ip-address ExamRefLB-IP az network lb probe create --resource-group ExamRefRG --name HealthProbe --lb-name ExamRefLB --protocol http --port 80 --path / --interval 5 --threshold az network lb rule create --name ExamRefRule --lb-name ExamRefLB --resource-group ExamRefRG --protocol Tcp --frontend-port 80 --backend-port 80 --frontend-ip-name ExamRefFrontEnd --backend-pool-name backend --probe-name HealthProbe","title":"Tasks"},{"location":"Infrastructure/Cloud/Tasks/#tasks","text":"Cloud APIs are equivalent to Azure resource providers . Unlike Azure, which automatically registers resource providers on use, Cloud APIs must be enabled per project. GCP gcloud services enable container.googleapis.com Azure az provider register -n Microsoft.ContainerService","title":"Tasks"},{"location":"Infrastructure/Cloud/Tasks/#display-all-available-regions","text":"GCP gcloud compute regions list","title":"Display all available regions"},{"location":"Infrastructure/Cloud/Tasks/#install-cli-utility","text":"Prerequisites Signing key Repo Install GCP apt install apt-transport-https ca-certificates gnupg Azure apt install apt-transport-https ca-certificates curl lsb-release gnupg GCP curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add - Azure curl -sL https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor | sudo tee /etc/apt/trusted.gpg.d/microsoft.gpg > /dev/null GCP echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main\" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list Azure echo \"deb [arch=amd64] https://packages.microsoft.com/repos/azure-cli/ $( lsb_release -cs ) main\" | sudo tee /etc/apt/sources.list.d/azure-cli.list GCP sudo apt-get update && sudo apt-get install google-cloud-sdk Azure sudo apt-get update && sudo apt-get install azure-cli","title":"Install CLI utility"},{"location":"Infrastructure/Cloud/Tasks/#kubernetes","text":"","title":"Kubernetes"},{"location":"Infrastructure/Cloud/Tasks/#clusters","text":"Create Read List Update Delete GCP gcloud container clusters create $name --num-nodes = 1 # Standard mode gcloud container clusters create-auto $name # Autopilot mode Azure az aks create -g $group -n $name --node-count 1 --enable-addons monitoring --generate-ssh-keys GCP gcloud container clusters describe $name Azure GCP gcloud container clusters list Azure az aks list GCP Azure GCP gcloud container clusters delete $name Azure az aks delete -g $group -n $name","title":"Clusters"},{"location":"Infrastructure/Cloud/Tasks/#add-context-to-kubeconfig","text":"GCP gcloud container clusters get-credential $name Azure az aks get-credentials -g $group -n $name Get-AzAKSCredentials","title":"Add context to kubeconfig"},{"location":"Infrastructure/Cloud/Tasks/#storage","text":"","title":" Storage"},{"location":"Infrastructure/Cloud/Tasks/#create-a-10gb-disk","text":"GCP gcloud compute disks create my-disk --size = 10GB --zone = us-east1-a Azure AWS","title":"Create a 10GB disk"},{"location":"Infrastructure/Cloud/Tasks/#administration","text":"Display subscription ID Get-AzSubscription az account show","title":"\ud83d\udee0&#xfe0f; Administration"},{"location":"Infrastructure/Cloud/Tasks/#cli","text":"Initialize CLI utility gcloud init","title":"\ud83d\udda5&#xfe0f; CLI"},{"location":"Infrastructure/Cloud/Tasks/#iam","text":"Add guest user New-AzureADMSInvitation -InvitedUserEmailAddress $EMAIL -SendInvitationMessage $True -InviteRedirectUrl \"http://myapps.onmicrosoft.com\" Assign a role # At the organization level gcloud organizations add-iam-policy-binding $ORG_ID --member = \"user: $EMAIL \" --role = \"roles/compute.xpnAdmin\" # At the folder level gcloud beta resource-manager-folders add-iam-policy-binding $FOLDER_ID --member = \"user: $EMAIL \" --role = \"roles/compute.xpnAdmin\"","title":"IAM"},{"location":"Infrastructure/Cloud/Tasks/#cost-management","text":"To view resource quotas for a subscription, go to the subscription in Azure Portal and open the Usage + quotas blade. From there you can select resources and then click the Request Increase button. View current usage of vCPU quotas Get-AzVMUsage View current usage of storage service Get-AzStorageUsage Create a budget To create a budget, open Cost Management + Billing , then Subscriptions , select a subscription, then click Budgets . Then click + Add , which produces a Create budget blade. The created budget can be seen in the Budgets blade. PowerShell commands used with budgets: Get-AzResourceGroup retrieve Resource Group object Set-AzResourceGroup apply a tag to a resource group with no preexisting tags .Tags method that retrieves Tag collection from a resource group .Add() method used to add tags to a resource group that already has tags.","title":"\ud83d\udcb0 Cost management"},{"location":"Infrastructure/Cloud/Tasks/#monitoring","text":"VM extension Set-AzVMExtension -ResourceGroupName ExamRefRG -Location \"West Europe\" -VMName VM1 -Name networkWatcherAgent -Publisher Microsoft . Azure . NetworkWatcher -Type NetworkWatcherAgentWindows -TypeHandlerVersion 1 . 4 az vm extension set --vm-name VM1 --resource-group ExamRefRG --publisher Microsoft.Azure.NetworkWatcher --version 1 .4 --name NetworkWatcherAgentWindows --extension-instance-name NetworkWatcherAgent Start packet capture $nw = Get-AzResource | Where ResourceType -eq \"Microsoft.Network/networkWatchers\" -and Location -eq \"WestEurope\" $networkWatcher = Get-AzNetworkWatcher -Name $nw . Name -ResourceGroupName $nw . ResourceGroupName $storageAccount = Get-AzStorageAccount -Name examref-storage -ResourceGroupName ExamRefRG $filter1 = New-AzPacketCaptureFilterConfig -Protocol TCP -RemoteIPAddress \"1.1.1.1-255.255.255.255\" -LocalIPAddress \"10.0.0.3\" -LocalPort \"1-65535\" -RemotePort \"20;80;443\" $filter2 = New-AzPacketCaptureFilterConfig -Protocol UDP $vm = Get-AzVM ` -Name VM1 -ResourceGroupName ExamRefRG New-AzNetworkWatcherPacketCapture -NetworkWatcher $networkWatcher -TargetVirtualMachineId $vm . Id -PacketCaptureName \"PacketCaptureTest\" -StorageAccountId $storageAccount . id -TimeLimitInSeconds 60 -Filter $filter1 , $filter2 filter = '[ { \"protocol\": \"TCP\", \"remoteIPAddress\": \"1.1.1.1-255.255.255.255\", \"localIPAddress\":\"10.0.0.3\", \"remotePort\":\"20\" } ]' az network watcher packet-capture create --name PacketCaptureTest2 --resource-group ExamRefRG --vm VM1 --time-limit 300 --storage-account examref-storage --filters $filter Check status of packet capture Get-AzNetworkWatcherPacketCapture -NetworkWatcher $networkWatcher -PacketCaptureName \"PacketCaptureTest\" az network watcher packet-capture show-status --name PacketCaptureTest --location WestEurope Stop packet capture Stop-AzNetworkWatcherPacketCapture -NetworkWatcher $networkWatcher -PacketCaptureName \"PacketCaptureTest\" az network watcher packet-capture stop --name PacketCaptureTest --location WestEurope Use IP Flow Verify to test outbound connectivity from source VM and port to destination. If any configured filtering rules block traffic between the endpoints, it will return the name of the offending NSG. Test-AzNetworkWatcherIPFlow az network watcher test-ip-flow Next Hop Get-AzNetworkWatcherNextHop az network watcher show-next-hop Use Network Topology Get-AzNetworkWatcherTopology az network watcher show-topology Capture SFTP traffic $r = Get-AzResource | where ResourceType -eq \"Microsoft.Network/networkWatchers\" -and Location -eq \"EastUS\" $nw = Get-AzNetworkWatcher -Name $r . Name -ResourceGroupName $r . ResourceGroupName $s = Get-AzStorageAccount -ResourceGroupName \"Diagnostics-RG\" -Name \"Diagnostics-Storage\" $filter = New-AzPacketCaptureFilterConfig -Protocol TCP -RemoteIPAddress \"1.1.1.1-255.255.255.255\" -LocalIPAddress \"10.0.0.4\" -LocalPort \"1-65535\" -RemotePort \"22\" New-AzNetworkWatcherPacketCapture -NetworkWatcher $nw -TargetVirtualMachineId $vm . ID -PacketCaptureName \"Capture SFTP traffic\" -StorageAccountId $s . Id -TimeLimitInSeconds 60 -Filter $filter","title":"Monitoring"},{"location":"Infrastructure/Cloud/Tasks/#policy","text":"Assign a policy $scope = '/subscriptions/$subscriptionID' $policyparam = '{ \"tagName\" : { \"value\": \"Environment\" }, \"tagValue\": { \"value\" : \"Production\" } }' $assignment = New-AzPolicyAssignment -Name 'append-environment-tag' -DisplayName 'Append Environment Tag' -Scope $scope -PolicyDefinition $definition -PolicyParameter $policyparam Remove policy assignment and definition Remove-AzPolicyAssignment -Id $assignment . ResourceId Remove-AzPolicyDefinition -Id $definition . ResourceId Create a policy definition Azure Portal (All Services) > Policy > Definitions: Both builtin and custom policies can be managed here. New-AzPolicyDefinition -Name 'appendEnvironmentTag' -DisplayName 'Append Environment Tag' -Policy 'AppendDefaultTag.json' -Parameter 'AppendDefaultTagParams.json' az policy definition create --name 'allowedVMs' --description 'Only allow virtual machines in the defined SKUs' --mode ALL --rules '{...}' --params '{...}' Apply policy to a scope az policy assignment create --policy allowedVMs --name 'deny-non-compliant-vms' --scope '/subscriptions/<Subscription ID>' -p Delete policy assignment az policy assignment delete --name deny-non-compliant-vms","title":"Policy"},{"location":"Infrastructure/Cloud/Tasks/#resources","text":"Create resource group New-AzGroup -Location $location -Name $rgName az group create -l $location -n $rgName Register resource provider in subscription az provider register --namespace 'Microsoft.PolicyInsights' Move resources $webapp = Get-AzResource -ResourceGroupName OldRG -ResourceName ExampleSite $plan = Get-AzResource -ResourceGroupName OldRG -ResourceName ExamplePlan Move-AzResource -DestinationResourceGroupName NewRG -ResourceId $webapp . ResourceId , $plan . ResourceId webapp = $( az resource show -g OldRG -n ExampleSite --resource-type \"Microsoft.Web/sites\" --query id --output tsv ) plan = $( az resource show -g OldRG -n ExamplePlan --resource-type \"Microsoft.Web/serverfarms\" --query id --output tsv ) az resource move --destination-group newgroup --ids $webapp $plan Create lock on a resource New-AzResourceLock -LockName LockSite -LockLevel CanNotDelete -ResourceGroupName $rg -ResourceName $r -ResourceType Microsoft . Web / sites az lock create --name LockSite --lock-type CanNotDelete --resource-group $rg --resource-name $r --resource-type Microsoft.Web/sites Create lock on a resource group New-AzResourceLock -LockName LockGroup -LockLevel CanNotDelete -ResourceGroupName $rg az lock create --name LockGroup --lock-type CanNotDelete --resource-group $rg Display resource lock Get-AzResourceLock -ResourceName $r -ResourceType Microsoft . Web / sites -ResourceGroupName $rg az lock list --resource-group $rg --resource-name $r --namespace Microsoft.Web --resource-type sites --parent \"\" Delete resource lock $lockId = ( Get-AzResourceLock -ResourceGroupName $rg -ResourceName $r -ResourceType Microsoft . Web / sites ). LockId Remove-AzResourceLock -LockId $lockId lockid = $( az lock show --name LockSite --resource-group $rg --resource-type Microsoft.Web/sites --resource-name $r --output tsv --query id ) az lock delete --ids $lockid Sources Manage Azure Resource Manager resource groups by using Azure PowerShell Manage Azure Resource Manager resource groups by using Azure CLI Resource providers Lock resources to prevent unexpected changes AZ-103: 1.3 , p. 76","title":"Resources"},{"location":"Infrastructure/Cloud/Tasks/#tags","text":"List all resources by tag ( Get-AzResource -Tag @{ CostCode = \"1001\" }). Name # List all resources by tag name, with no value ( Get-AzResource -TagName CostCode ). Name az resource list --tag Dept = Finance List resource groups by tag ( Get-AzResourceGroup -Tag @{ CostCode = \"1001\" }). ResourceGroupName az group list --tag CostCode = 1001 Enumerate a resource's tags $r = Get-AzResource -Name $resourceName -ResourceGroup rg Get-AzTag -ResourceId $r . id # Resource group $rg = Get-AzResourceGroup -Name $rgName Get-AzTag -ResourceId $rg . ResourceId # Subscription $s = ( Get-AzSubscription -SubscriptionName \"Example Subscription\" ). Id Get-AzTag -ResourceId \"/subscriptions/$s\" az resource show -n $resourceName -g $rgName --query tags # Resource group az group show -n $rgName --query tags Tag resource $r = Get-AzResource -ResourceName hrvm1 -ResourceGroupName rg $r . Tags . Add ( \"Owner\" , \"user@contoso.com\" ) Set-AzResource -Tag $r . Tags -ResourceId $r . ResourceId -Force Resource group $tags = @{ \"Dept\" = \"Finance\" ; \"Status\" = \"Normal\" } $rg = Get-AzResourceGroup -Name demoGroup New-AzTag -ResourceId $rg . ResourceId -tag $tags $tags = ( Get-AzResourceGroup -Name rg ). Tags $tags . Add ( \"Owner\" , \"user@contoso.com\" ) Set-AzResourceGroup -Tag $tags -Name rg jsonrtag = $( az group show -n rg --query tags ) rt = $( echo $jsonrtag | tr -d '\"{},' | sed 's/: /=/g' ) az group update -n rg --tags $rt Owner = user@contoso.com Remove specific tags $tags = @{ \"Project\" = \"ECommerce\" ; \"Team\" = \"Web\" } Update-AzTag -ResourceId $resource . id -Tag $tags -Operation Delete Remove all tags $s = ( Get-AzSubscription -SubscriptionName \"Example Subscription\" ). Id Remove-AzTag -ResourceId \"/subscriptions/$s\" # Alternatively Set-AzResourceGroup -Tag @{} -Name rg Apply tags to resource, overwriting $tags = @{ \"Dept\" = \"Finance\" ; \"Status\" = \"Normal\" } New-AzTag -ResourceId $resource . id -Tag $tags Set-AzResource -ResourceId $r . ResourceId -Tag @{ CostCode = \"1001\" ; Environment = \"Production\" } -Force az resource tag --tags 'Dept=IT' 'Environment=Test' -g $rgName -n examplevnet --resource-type \"Microsoft.Network/virtualNetworks\" Apply tags to resource group Set-AzResourceGroup -Name rg -Tag @{ CostCode = 1001 ; Environment = Production } az group update -n $rgName --tags 'Environment=Test' 'Dept=IT' # Alternatively az group update -n $rgName --set tags.Environment = Production tags.CostCode = 1001","title":"Tags"},{"location":"Infrastructure/Cloud/Tasks/#compute","text":"","title":"Compute"},{"location":"Infrastructure/Cloud/Tasks/#iaas","text":"Create a VM ( src ) gcloud compute instances create instance-1 --zone-uscentral1-a","title":"IaaS"},{"location":"Infrastructure/Cloud/Tasks/#paas","text":"Deploy app in current working directory. gcloud app deploy View the deployed app gcloud app browse app.yaml allows configuration of the app in several ways runtime : python37 In Azure, multiple web applications are organized under an App Service Plan resource. So if no such app service plan exists, it must be created. $p = New-AzAppServicePlan -Name $n -ResourceGroupName $g -Location $l -Tier \"Basic\" -NumberofWorkers 2 -WorkerSize \"Small\" New-AzWebApp -Name $n -Location $l -ResourceGroupName $g -AppServicePlan $p az appservice plan create -g $g -n $p --is-linux az webapp create -n $n -g $g --plan $p","title":"PaaS"},{"location":"Infrastructure/Cloud/Tasks/#containers","text":"Create a new source repository These steps require: Cloud SDK and Git to be installed A GCP project with billing and the Cloud Source Repositories API enabled #Create a new repository gcloud source repos create hello-world #Clone it locally gcloud source repos clone hello-world # Create scripts, then add, commit and push them as usual. git commit -am \"Initial\" git push origin master Create container registry New-AzContainerRegistry -ResourceGroupName $rg -Name $registry -Sku \"Basic\" -EnableAdminUser az acr create --name $registry --resource-group $rg --sku Basic --admin-enabled true","title":"Containers"},{"location":"Infrastructure/Cloud/Tasks/#kubernetes_1","text":"Create Kubernetes cluster New-AzAKS -ResourceGroupName $g -Name $n -NodeCount 2 -NetworkPlugin azure -NodeVmSetType VirtualMachineScaleSets -WindowsProfileAdminUserName azureuser -WindowsProfileAdminUserPassword $Password -KubernetesVersion 1 . 16 . 7 # PowerShell does not offer an option to generate SSH keys for access to the cluster; `ssh-keygen` must be used. - Create a Windows Server container on an AKS cluster az aks create -g $g -n $n --node-count 2 --network-plugin azure --vm-set-type VirtualMachineScaleSets --windows-admin-username azureuser --windows-admin-password $PASSWORD --generate-ssh-keys --enable-addons monitoring - Create a Windows Server container on an AKS cluster Add a pool of nodes New-AzAksNodePool -ResourceGroupName $rgName -Name npwin -ClusterName $clusterName -OsType Windows -KubernetesVersion 1 . 16 . 7 az aks nodepool add -g $g -n $n --cluster-name $clusterName --os-type Windows --node-count 1 Persistent volume claim apiVersion : v1 kind : PersistentVolumeClaim metadata : name : azure-managed-disk spec : accessModes : - ReadWriteOnce storageClassName : managed-premium resources : requests : storage : 5Gi - Source Provision Azure Disk Standard Premium kind : StorageClass apiVersion : storage.k8s.io/v1 metadata : name : managed-disk-forapp provisioner : kubernetes.io/azure-disk reclaimPolicy : Retain parameters : storageaccounttype : default kind : Managed kind : StorageClass apiVersion : storage.k8s.io/v1 metadata : name : managed-disk-forapp provisioner : kubernetes.io/azure-disk reclaimPolicy : Retain parameters : storageaccounttype : Premium_LRS kind : Managed","title":"\u2693 Kubernetes"},{"location":"Infrastructure/Cloud/Tasks/#functions","text":"Deploy gcloud functions deploy hello_get --runtime python37 --trigger-http Test gcloud functions describe hello_get","title":"Functions"},{"location":"Infrastructure/Cloud/Tasks/#storage_1","text":"Create storage account Azure Portal Click Create a resouce , then Storage , then Storage account . Choose a globally unique name for the account, containing lower-case characters and digits only. New-AzStorageAccount -ResourceGroupName ExamRefRG -Name mystorage112300 -SkuName Standard_LRS -Location WestUS -Kind StorageV2 -AccessTier Hot az storage account create --name $accountName --resource-group $resourceGroup -location $location --sku $sku Change access tier of storage account === \"Azure PowerShell ```powershell Set-AzStorageAccount -ResourceGroupName RG -Name $accountName -AccessTier Cool -Force ``` Change replication mode of storage account Set-AzStorageAccount -ResourceGroupName $resourceGroup -Name $accountName -SkuName $type Renew storage account keys === \"Azure ```powershell New-AzStorageAccountKey ``` az storage account keys renew Create Azure Key Vault New-AzKeyVault -VaultName $vaultName -ResourceGroupName $g -Location $location $key = Add-AzKeyVaultKey -VaultName $vaultName -Name $keyName -Destination 'Software' $storageKey = Get-AzStorageAccountKey -ResourceGroupName $g -Name $storageAccount $secretvalue = ConvertTo-SecureString $storageKey [ 0 ]. Value -AsPlainText -Force $secret = Set-AzKeyVaultSecret -VaultName $vaultName -Name $secretName -SecretValue $secretvalue az keyvault create --name $vaultName --resource-group $g --location $location az keyvault key create --vault-name \" $vaultName \" --name $keyName --protection \"software\" az keyvault secret set --vault-name \" $vaultName \" --name \" $secretName \" --value \" $secretValue \" Create key in Azure Key Vault $key = Add-AzKeyVaultKey -VaultName $vaultName -Name $keyName -Destination 'Software' $storageKey = Get-AzStorageAccountKey -ResourceGroupName $g -Name $storageAccount $secretvalue = ConvertTo-SecureString $storageKey [ 0 ]. Value -AsPlainText -Force $secret = Set-AzKeyVaultSecret -VaultName $vaultName -Name $secretName -SecretValue $secretvalue az keyvault key create --vault-name $vaultName --name $keyName --protection \"software\" az keyvault secret set --vault-name $vaultName --name $secretName --value $secretValue Create Azure sync group Specify name of sync group in dialog after creating an Azure File Sync Change storage class $STORAGE_CLASS can be multi_regional , regional , nearline , or coldline gsutil rewrite -s $STORAGE_CLASS gs:// $PATH_TO_OBJECT","title":"Storage"},{"location":"Infrastructure/Cloud/Tasks/#file-shares","text":"Deploy Azure File Sync # Create Storage Sync Service $storageSync = New-AzStorageSyncService -ResourceGroupName $g -Name $storageSyncName -Location $l # Create Azure File Share $storageKey = Get-AzStorageAccountKey -ResourceGroupName $g -Name $storageAccount $context = New-AzStorageContext -StorageAccountName $storageAccount -StorageAccountKey $storageKey . Value [ 0 ] New-AzStorageShare -Name $shareName -Context $context # Creating a Storage Sync Service resource is only possible in PowerShell or Portal constring = $( az storage account show-connection-string -n $storageAccountName ) az storage share create --name $shareName --quota 2048 --connection-string $constring Create sync group $syncgroup = New-AzStorageSyncGroup -Name $syncgroupname -ParentObject $storageSync Create cloud endpoint New-AzStorageSyncCloudEndpoint -Name $shareName -ParentObject $syncgroup -StorageAccountResourceId $storageAccount . Id -AzureFileShareName $shareName","title":"File shares"},{"location":"Infrastructure/Cloud/Tasks/#network-access","text":"Display the status of the default NetworkRule for a storage account Get-AzStorageAccountNetworkRuleSet -ResourceGroupName $rgName -AccountName $n | Select-Object DefaultAction az storage account show - $rgName -n $n --query networkRuleSet.defaultAction Set default rule Update-AzStorageAccountNetworkRuleSet -ResourceGroupName $g -Name $n -DefaultAction Deny Update-AzStorageAccountNetworkRuleSet -ResourceGroupName $g -Name $n -DefaultAction Allow az storage account update -g $g -n $n --default-action Deny az storage account update -g $g -n $n --default-action Allow","title":"Network access"},{"location":"Infrastructure/Cloud/Tasks/#networking","text":"Create virtual network with a specific prefix and subnet $subnet = New-AzVirtualNetworkSubnetConfig -Name $subnetName -AddressPrefix \"10.0.0.0/24\" $vnet = New-AzVirtualNetwork -Name $name -ResourceGroupName $rgName -Location $l -AddressPrefix \"10.0.0.0/16\" -Subnet $subnet az network vnet create -g $rgName -n $name --address-prefix \"10.0.0.0/16\" --subnet-name $subnetName --subnet-prefix \"10.0.0.0/24\" gcloud networks create $name --subnet-mode = custom gcloud beta compute networks subnets create $subnetName --network = $name --region = $l --range = \"10.0.0.0/16\" --enable-private-ip-google-access --enable-flow-logs Create peering Add-AzVirtualNetworkPeering -Name 'peering1' -VirtualNetwork $net1 -RemoteVirtualNetworkId $net2 . Id Add-AzVirtualNetworkPeering -Name 'peering2' -VirtualNetwork $net2 -RemoteVirtualNetworkId $net1 . Id az network vnet peering create -n 'peering1' -g $g --vnet-name net1 --allow-vnet-access --remote-vnet net2 az network vnet peering create -n 'peering2' -g $g --vnet-name net2 --allow-vnet-access --remote-vnet net1 gcloud compute networks peerings create \"peering1\" --network net1 --peer-project $p --peer-network net2 --auto-create-routes gcloud compute networks peerings create \"peering2\" --network net1 --peer-project $p --peer-network net1 --auto-create-routes Check peering Get-AzVirtualNetworkPeering -ResourceGroupName $rg -VirtualNetworkName $vnetName az network vnet peering list --resource-group $rg --vnet-name VNet1 az network vnet peering list --resource-group $rg --vnet-name VNet2 User-defined routes # Create the route table resource $routeTable = New-AzRouteTable -Name $routeTableName -ResourceGroupName ExamRefRG # Add a route to route table object Add-AzRouteConfig -RouteTable $routeTable -Name $routeName -AddressPrefix 10 . 3 . 0 . 0 / 16 -NextHopType VirtualAppliance -NextHopIpAddress 10 . 2 . 20 . 4 Set-AzRouteTable -RouteTable $routeTable # Associate route table with subnet Set-AzVirtualNetworkSubnetConfig -VirtualNetwork $vnet -Name Default -AddressPrefix $subnet . AddressPrefix -RouteTable $routeTable # Commit changes Set-AzVirtualNetwork -VirtualNetwork $vnet # Get effective routes for a NIC Get-AzEffectiveRouteTable -NetworkInterfaceName $nicName -ResourceGroupName $rgName # Create route table resource az network route-table create --name $routeTableName --resource-group $rgName # Add route to route table az network route-table route create --resource-group $rgName --route-table-name $routeTableName --name $routeName --address-prefix 10 .3.0.0/16 --next-hop-type VirtualAppliance --next-hop-ip-address 10 .2.20.4 # Associate route table with subnet az network vnet subnet update --name defualt --vnet-name Vnet1 --resource-group $rgName --route-table rt # Get effective routes for NIC az network nic show-effective-route-table --name $nicName --resource-group $rgName Create NSG $nsgRules = @() $nsgRules += New-AzNetworkSecurityRuleConfig -Name \"AllowingWinRMHTTP\" -Description \"To Enable PowerShell Remote Access\" -Access Allow -Protocol Tcp -Direction Inbound -Priority 103 -SourceAddressPrefix Internet -SourcePortRange * -DestinationAddressPrefix * -DestinationPortRange 5985 $nsgRules += New-AzNetworkSecurityRuleConfig -Name \"AllowingWinRMHTTPS\" -Description \"To Enable PowerShell Remote Access\" -Access Allow -Protocol Tcp -Direction Inbound -Priority 104 -SourceAddressPrefix Internet -SourcePortRange * -DestinationAddressPrefix * -DestinationPortRange 5986 $nsg = New-AzNetworkSecurityGroup -Name \"wscore-nsg\" -ResourceGroupName \"RG\" -Location \"East US\" -SecurityRules $nsgRules View rules Get-AzEffectiveNetworkSecurityGroup -NetworkInterfaceName $nicName -ResourceGroupName $rgName az network nic list-effective-nsg --name $nicName --resource-group $rgName Create Bastion Connecting to a VM requires at least Reader role privileges on the VM, its NIC, and on the Bastion itself. New-AzBastion -ResourceGroupName $rgName -Name $n -PublicIpAddress $pip -VirtualNetwork $vnet az network bastion create -g $rgName -n $n -l $l --public-ip-address $pip --vnet-name $vnetName Create virtual appliance IP forwarding must be enabled on the VM's NIC, then applications installed on the VM can begin accepting packets destined for other IP addresses.","title":"Networking"},{"location":"Infrastructure/Cloud/Tasks/#cdn","text":"Create new profile Azure Portal Click Create a resource Click Web Click CDN , opening the CDN profile blade Specify name for the profile, name of the resource group, region, and pricing tier. Click Create AZ-103: p. 140 Create endpoint Azure Portal Add an endpoint to a CDN profile (Portal) 1. Open the CDN Profile 2. Click + Endpoint button 3. Specify unique name, configuration for origin settings such as type, host header, and origin port for HTTP and HTTPS. 4. Click Add button AZ-103: p. 141 Publish content in a CDN endpoint Azure Portal Create a new CDN profile Add an endpoint to the profile","title":"CDN"},{"location":"Infrastructure/Cloud/Tasks/#dns","text":"Create DNS zone New-AzDnsZone -Name examref . com -ResourceGroupName ExamRefRG az network dns zone create --name examref.com --resource-group ExamRefRG Create empty A record New-AzDnsRecordSet -Name www -RecordType A -ZoneName examref . com -ResourceGroupName ExamRefRG -Ttl 3600 -DnsRecords ( New-AzDnsRecordConfig -IPv4Address \"1.2.3.4\" ) az network dns record-set a create --name www --zone-name examref.com --resource-group ExamRefRG --ttl 3600 Create multiple records $records = @() $records += New-AzDnsRecordConfig -IPv4Address \"1.2.3.4\" $records += New-AzDnsRecordConfig -IPv4Address \"5.6.7.8\" New-AzDnsRecordSet -Name \"@\" -RecordType A -ZoneName examref . com -ResourceGroupName ExamRefRG -Ttl 3600 -DnsRecords $records az network dns record-set a add-record --record-set-name www --zone-name examref.com --resource-group ExamRefRG --ipv4-address 1 .2.3.4 az network dns record-set a add-record --record-set-name www --zone-name examref.com --resource-group ExamRefRG --ipv4-address 5 .6.7.8 Remove record $recordset = Get-AzDnsRecordSet -Name www -RecordType A -ZoneName examref . com -ResourceGroupName ExamRefRG Add-AzdnsRecordConfig -RecordSet $recordset -IPv4Address \"5.6.7.8\" Remove-AzDnsRecordConfig -RecordSet $recordset -IPv4Address \"1.2.3.4\" Set-AzDnsRecordSet -RecordSet $recordset az network dns record-set a remove-record --record-set-name www --zone-name examref.com --resource-group ExamRefRG --ipv4-address 1 .2.3.4 Read records Get-AzDnsRecordSet -ZoneName examref . com -ResourceGroupName ExamRefRG az network dns record-set list --zone-name examref.com --resource-group ExamRefRG -o table Create a virtual network with custom DNS settings New-AzVirtualNetwork -Name VNet1 -ResourceGroupName $rgName -Location $location -AddressPrefix 10 . 1 . 0 . 0 / 16 -Subnet ( New-AzVirtualNetworkSubnetConfig -Name Default -AddressPrefix 10 . 1 . 0 . 0 / 24 ) -DNSServer 10 . 0 . 0 . 4 , 10 . 0 . 0 . 5 az network vnet create --name VNet1 --resource-group $rgName --address-prefixes 10 .0.0.0/16 --dns-servers 10 .0.0.4 10 .0.0.5 Modify the DNS server configuration of an existing VNET $vnet = Get-AzVirtualNetwork -Name $vnetName -ResourceGroupName $rgName $vnet . DhcpOptions . DnsServers . Clear () $vnet . DhcpOptions . DnsServers . Add ( \"10.10.200.1\" ) $vnet . DhcpOptions . DnsServers . Add ( \"10.10.200.2\" ) Set-AzVirtualNetwork -VirtualNetwork $vnet az network vnet update --name $vnetName --resource-group $rgName --dns-servers 10 .10.200.1 10 .10.200.2 Restart the VMs in the VNet to pick up the DNS change $vm = Get-AzVM -Name VNet1-VM -ResourceGroupName ExamRefRG Restart-AzVM -ID $vm . Id Update the DNS settings on a NIC $nic = Get-AzNetworkInterface -Name VM1-NIC -ResourceGroupName ExamRefRG $nic . DnsSettings . DnsServers . Clear () $nic . DnsSettings . DnsServers . Add ( \"8.8.8.8\" ) $nic . DnsSettings . DnsServers . Add ( \"8.8.4.4\" ) Commit the DNS change, causing the VM to restart Set-AzNetworkInterface -NetworkInterface $nic Remove custom DNS servers from a VNET az network vnet update --name VNet1 --resource-group ExamRefRG --remove DHCPOptions.DNSServers Set custom DNS servers on a NIC az network nic update --name VM1-NIC --resource-group ExamRefRG --dns-servers 8 .8.8.8 8 .8.4.4","title":"DNS"},{"location":"Infrastructure/Cloud/Tasks/#load-balancing","text":"Create public load balancer Creating a load balancer in PowerShell requires defining objects which are all passed to New-AzLoadBalancer as objects: - Frontend IP - Public Ip Address resource (if public) - Private IP address specified as a string (if internal) - Backend address pool - Health probe - Load balancing rule By contrast, in Azure CLI, the load balancer can be defined first with az network lb create before adding a probe and rule, passing the name of the load balancer to --lb-name . $publicIP = New-AzPublicIpAddress -Name ExamRefLB-IP -ResourceGroupName $g -Location $location -AllocationMethod Static $frontendIP = New-AzLoadBalancerFrontendIpConfig -Name frontend -PublicIpAddress $publicIP $beAddressPool = New-AzLoadBalancerBackendAddressPoolConfig -Name backend $healthProbe = New-AzLoadBalancerProbeConfig -Name -RequestPath '/' -Protocol http -Port 80 $lbrule = New-AzLoadBalancerRuleConfig -Name -FrontendIpConfiguration $frontendIP -BackendAddressPool $beAddressPool -Probe $healthProbe -Protocol Tcp -FrontendPort 80 -BackendPort 80 $lb = New-AzLoadBalancer -ResourceGroupName -Name -Location -FrontendIpConfiguration $frontendIP -LoadBalancingRule $lbrule -BackendAddressPool $beAddressPool -Probe $healthProbe az network public-ip create --name ExamRefLB-IP --resource-group ExamRefRG --location --allocation-method Static az network lb create --name ExamRefLB --resource-group ExamRefRG --location --backend-pool-name backend --frontend-ip-name frontend --public-ip-address ExamRefLB-IP az network lb probe create --resource-group ExamRefRG --name HealthProbe --lb-name ExamRefLB --protocol http --port 80 --path / --interval 5 --threshold az network lb rule create --name ExamRefRule --lb-name ExamRefLB --resource-group ExamRefRG --protocol Tcp --frontend-port 80 --backend-port 80 --frontend-ip-name ExamRefFrontEnd --backend-pool-name backend --probe-name HealthProbe","title":"Load balancing"},{"location":"Infrastructure/Cloud/Azure/ARM/","text":"Azure Resource Manager (ARM) is the interface for managing and organizing cloud resources. ? An ARM template is a JSON file that precisely defines all ARM resources in a deployment. An ARM template can be deployed into a resource group as a single operation. ARM templates are typically adapted from existing Azure Quickstart templates, which are contributed by the community and hosted on a gallery . The Azure Resource Manager Visualizer assists users in seeing what the template will do before actually deploying. The Custom Script Extension is a way to run scripts on Azure VMs and represents one of the ways to automate configuration of new deployments. ? If you explort a deployment to a template, only the resources deployed in that deployment will be templatized. In the case of a complex deployment that had several phases, the ultimate result of the deployment can be obtained by exporting the template from the resource group. Structure A template must have at least the following sections. $schema contentVersion resources A template may have the following optional sections parameters variables functions outputs { \"$schema\" : \"http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\" , \"contentVersion\" : \"\" , \"parameters\" : { }, \"variables\" : { }, \"functions\" : [ ], \"resources\" : [ ], \"outputs\" : { } } Resources Create a public IP address. { \"type\" : \"Microsoft.Network/publicIPAddresses\" , \"apiVersion\" : \"2018-08-01\" , \"name\" : \"[variables('publicIPAddressName')]\" , \"location\" : \"[parameters('location')]\" , \"properties\" : { \"publicIPAllocationMethod\" : \"Dynamic\" , \"dnsSettings\" : { \"domainNameLabel\" : \"[parameters('dnsLabelPrefix')]\" } } } Create a storage account { \"type\" : \"Microsoft.Storage/storageAccounts\" , \"apiVersion\" : \"2019-06-01\" , \"name\" : \"[variables('storageAccountName')]\" , \"location\" : \"[parameters('location')]\" , \"sku\" : { \"name\" : \"[parameters('storageAccountType')]\" }, \"kind\" : \"StorageV2\" , \"properties\" : {} } Create a Azure Data Explorer cluster Parameters \"adminUsername\" : { \"type\" : \"string\" , \"metadata\" : { \"description\" : \"Username for the Virtual Machine.\" }}, \"adminPassword\" : { \"type\" : \"securestring\" , \"metadata\" : { \"description\" : \"Password for the Virtual Machine.\" }} Simple storage account { \"storageAccountType\" : { \"type\" : \"string\" , \"defaultValue\" : \"Standard_LRS\" , \"allowedValues\" : [ \"Standard_LRS\" , \"Standard_GRS\" , \"Standard_ZRS\" , \"Premium_LRS\" ], \"metadata\" : { \"description\" : \"Storage Account type\" }}, \"location\" : { \"type\" : \"string\" , \"defaultValue\" : \"[resourceGroup().location]\" , \"metadata\" : { \"description\" : \"Location for all resources.\" }}} Variables { \"nicName\" : \"myVMNic\" , \"addressPrefix\" : \"10.0.0.0/16\" , \"subnetName\" : \"Subnet\" , \"subnetPrefix\" : \"10.0.0.0/24\" , \"publicIPAddressName\" : \"myPublicIP\" , \"virtualNetworkName\" : \"MyVNET\" } Simple storage account { \"storageAccountName\" : \"[concat('store', uniquestring(resourceGroup().id))]\" }, Functions Create a globally unique name, useful for some resources that require it. concat is a built-in function. [ { \"namespace\" : \"contoso\" , \"members\" : { \"uniqueName\" : { \"parameters\" : [ { \"name\" : \"namePrefix\" , \"type\" : \"string\" } ], \"output\" : { \"type\" : \"string\" , \"value\" : \"[concat(toLower(parameters('namePrefix')), uniqueString(resourceGroup().id))]\" }}}} ] Outputs \"outputs\" : { \"hostname\" : { \"type\" : \"string\" , \"value\" : \"[reference(variables('publicIPAddressName')).dnsSettings.fqdn]\" } } Standard storage account { \"$schema\" : \"http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\" , \"contentVersion\" : \"\" , \"parameters\" : { }, \"variables\" : { }, \"functions\" : [ ], \"resources\" : [ ], \"outputs\" : { \"storageAccountName\" : { \"type\" : \"string\" , \"value\" : \"[variables('storageAccountName')]\" } } } Tasks Deploy a VM quickstart template Create a resource group Create a resource group RESOURCEGROUP = learn-quickstart-vm-rg LOCATION = eastus az group create --name $RESOURCEGROUP --location $LOCATION Create template parameters Validate template USERNAME = azureuser PASSWORD = $( openssl rand -base64 32 ) DNS_LABEL_PREFIX = mydeployment- $RANDOM az deployment group validate --resource-group $RESOURCEGROUP --template-uri \"https://raw.githubusercontent.com/Azure/azure-quickstart-templates/master/101-vm-simple-windows/azuredeploy.json\" --parameters adminUsername = $USERNAME --parameters adminPassword = $PASSWORD --parameters dnsLabelPrefix = $DNS_LABEL_PREFIX Deploy template USERNAME = azureuser PASSWORD = $( openssl rand -base64 32 ) DNS_LABEL_PREFIX = mydeployment- $RANDOM az deployment group create --name MyDeployment --resource-group $RESOURCEGROUP --template-uri \"https://raw.githubusercontent.com/Azure/azure-quickstart-templates/master/101-vm-simple-windows/azuredeploy.json\" --parameters adminUsername = $USERNAME --parameters adminPassword = $PASSWORD --parameters dnsLabelPrefix = $DNS_LABEL_PREFIX Verify deployment az deployment group show --name MyDeployment --resource-group $RESOURCEGROUP ARM ? { \"name\" : \"[concat(variables('vmName'), '/', 'ConfigureIIS')]\" , \"type\" : \"Microsoft.Compute/virtualMachines/extensions\" , \"apiVersion\" : \"2018-06-01\" , \"location\" : \"[parameters('location')]\" , \"properties\" : { \"publisher\" : \"Microsoft.Compute\" , \"type\" : \"CustomScriptExtension\" , \"typeHandlerVersion\" : \"1.9\" , \"autoUpgradeMinorVersion\" : true , \"settings\" : { \"fileUris\" : [ \"https://raw.githubusercontent.com/MicrosoftDocs/mslearn-welcome-to-azure/master/configure-iis.ps1\" ] }, \"protectedSettings\" : { \"commandToExecute\" : \"powershell -ExecutionPolicy Unrestricted -File configure-iis.ps1\" } }, \"dependsOn\" : [ \"[resourceId('Microsoft.Compute/virtualMachines/', variables('vmName'))]\" ] }","title":"ARM"},{"location":"Infrastructure/Cloud/Azure/ARM/#structure","text":"A template must have at least the following sections. $schema contentVersion resources A template may have the following optional sections parameters variables functions outputs { \"$schema\" : \"http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\" , \"contentVersion\" : \"\" , \"parameters\" : { }, \"variables\" : { }, \"functions\" : [ ], \"resources\" : [ ], \"outputs\" : { } }","title":"Structure"},{"location":"Infrastructure/Cloud/Azure/ARM/#resources","text":"Create a public IP address. { \"type\" : \"Microsoft.Network/publicIPAddresses\" , \"apiVersion\" : \"2018-08-01\" , \"name\" : \"[variables('publicIPAddressName')]\" , \"location\" : \"[parameters('location')]\" , \"properties\" : { \"publicIPAllocationMethod\" : \"Dynamic\" , \"dnsSettings\" : { \"domainNameLabel\" : \"[parameters('dnsLabelPrefix')]\" } } } Create a storage account { \"type\" : \"Microsoft.Storage/storageAccounts\" , \"apiVersion\" : \"2019-06-01\" , \"name\" : \"[variables('storageAccountName')]\" , \"location\" : \"[parameters('location')]\" , \"sku\" : { \"name\" : \"[parameters('storageAccountType')]\" }, \"kind\" : \"StorageV2\" , \"properties\" : {} } Create a Azure Data Explorer cluster","title":"Resources"},{"location":"Infrastructure/Cloud/Azure/ARM/#parameters","text":"\"adminUsername\" : { \"type\" : \"string\" , \"metadata\" : { \"description\" : \"Username for the Virtual Machine.\" }}, \"adminPassword\" : { \"type\" : \"securestring\" , \"metadata\" : { \"description\" : \"Password for the Virtual Machine.\" }} Simple storage account { \"storageAccountType\" : { \"type\" : \"string\" , \"defaultValue\" : \"Standard_LRS\" , \"allowedValues\" : [ \"Standard_LRS\" , \"Standard_GRS\" , \"Standard_ZRS\" , \"Premium_LRS\" ], \"metadata\" : { \"description\" : \"Storage Account type\" }}, \"location\" : { \"type\" : \"string\" , \"defaultValue\" : \"[resourceGroup().location]\" , \"metadata\" : { \"description\" : \"Location for all resources.\" }}}","title":"Parameters"},{"location":"Infrastructure/Cloud/Azure/ARM/#variables","text":"{ \"nicName\" : \"myVMNic\" , \"addressPrefix\" : \"10.0.0.0/16\" , \"subnetName\" : \"Subnet\" , \"subnetPrefix\" : \"10.0.0.0/24\" , \"publicIPAddressName\" : \"myPublicIP\" , \"virtualNetworkName\" : \"MyVNET\" } Simple storage account { \"storageAccountName\" : \"[concat('store', uniquestring(resourceGroup().id))]\" },","title":"Variables"},{"location":"Infrastructure/Cloud/Azure/ARM/#functions","text":"Create a globally unique name, useful for some resources that require it. concat is a built-in function. [ { \"namespace\" : \"contoso\" , \"members\" : { \"uniqueName\" : { \"parameters\" : [ { \"name\" : \"namePrefix\" , \"type\" : \"string\" } ], \"output\" : { \"type\" : \"string\" , \"value\" : \"[concat(toLower(parameters('namePrefix')), uniqueString(resourceGroup().id))]\" }}}} ]","title":"Functions"},{"location":"Infrastructure/Cloud/Azure/ARM/#outputs","text":"\"outputs\" : { \"hostname\" : { \"type\" : \"string\" , \"value\" : \"[reference(variables('publicIPAddressName')).dnsSettings.fqdn]\" } } Standard storage account { \"$schema\" : \"http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\" , \"contentVersion\" : \"\" , \"parameters\" : { }, \"variables\" : { }, \"functions\" : [ ], \"resources\" : [ ], \"outputs\" : { \"storageAccountName\" : { \"type\" : \"string\" , \"value\" : \"[variables('storageAccountName')]\" } } }","title":"Outputs"},{"location":"Infrastructure/Cloud/Azure/ARM/#tasks","text":"Deploy a VM quickstart template","title":"Tasks"},{"location":"Infrastructure/Cloud/Azure/ARM/#create-a-resource-group","text":"Create a resource group RESOURCEGROUP = learn-quickstart-vm-rg LOCATION = eastus az group create --name $RESOURCEGROUP --location $LOCATION Create template parameters","title":"Create a resource group"},{"location":"Infrastructure/Cloud/Azure/ARM/#validate-template","text":"USERNAME = azureuser PASSWORD = $( openssl rand -base64 32 ) DNS_LABEL_PREFIX = mydeployment- $RANDOM az deployment group validate --resource-group $RESOURCEGROUP --template-uri \"https://raw.githubusercontent.com/Azure/azure-quickstart-templates/master/101-vm-simple-windows/azuredeploy.json\" --parameters adminUsername = $USERNAME --parameters adminPassword = $PASSWORD --parameters dnsLabelPrefix = $DNS_LABEL_PREFIX","title":"Validate template"},{"location":"Infrastructure/Cloud/Azure/ARM/#deploy-template","text":"USERNAME = azureuser PASSWORD = $( openssl rand -base64 32 ) DNS_LABEL_PREFIX = mydeployment- $RANDOM az deployment group create --name MyDeployment --resource-group $RESOURCEGROUP --template-uri \"https://raw.githubusercontent.com/Azure/azure-quickstart-templates/master/101-vm-simple-windows/azuredeploy.json\" --parameters adminUsername = $USERNAME --parameters adminPassword = $PASSWORD --parameters dnsLabelPrefix = $DNS_LABEL_PREFIX Verify deployment az deployment group show --name MyDeployment --resource-group $RESOURCEGROUP","title":"Deploy template"},{"location":"Infrastructure/Cloud/Azure/ARM/#arm","text":"? { \"name\" : \"[concat(variables('vmName'), '/', 'ConfigureIIS')]\" , \"type\" : \"Microsoft.Compute/virtualMachines/extensions\" , \"apiVersion\" : \"2018-06-01\" , \"location\" : \"[parameters('location')]\" , \"properties\" : { \"publisher\" : \"Microsoft.Compute\" , \"type\" : \"CustomScriptExtension\" , \"typeHandlerVersion\" : \"1.9\" , \"autoUpgradeMinorVersion\" : true , \"settings\" : { \"fileUris\" : [ \"https://raw.githubusercontent.com/MicrosoftDocs/mslearn-welcome-to-azure/master/configure-iis.ps1\" ] }, \"protectedSettings\" : { \"commandToExecute\" : \"powershell -ExecutionPolicy Unrestricted -File configure-iis.ps1\" } }, \"dependsOn\" : [ \"[resourceId('Microsoft.Compute/virtualMachines/', variables('vmName'))]\" ] }","title":"ARM"},{"location":"Infrastructure/Cloud/Azure/Azure-AD/","text":"Azure AD Azure AD has its own set of roles which apply to Azure AD resources and which are distinct from those of Azure RBAC . The terms tenant and directory are deeply connected and often confused with one another. A tenant refers to an instance of Azure AD that is tied to a subscription, and refers to the organization. Each tenant is associated with a dedicated and trusted directory that includes the tenant's users, groups, and apps. Roles: Global Administrator can manage access to administrative features in AAD and can grant administrator roles to other users. An AAD Global Administrator can also temporarily elevate their own access to the Azure RBAC role of User Access Administrator in order to manage all Azure subscriptions and management groups . Whoever signs up for the directory is automatically assigned this role. Device administrator In order to make sure AD users can change their password either locally or in the cloud, Azure AD has to be upgraded to Premium . Enterprise State Roaming allows users to securely synchronize user settings and application settings to Azure. Self-Service Password Reset (SSPR) is supported for all users. SSPR registration can be configured by group or for all domain users, but not individual users. B2B Business-to-business (B2B) collaboration allows you to invite guest users into your own ( What is guest user access in Azure Active Directory B2B? ) Joining a device When you join a device to an Azure AD tenant's domain, Azure AD creates local administrator accounts on the device for: - The user joining the device - The Azure AD global administrator - The Azure AD device administrator SSPR Tasks Sources: - Portal - PowerShell Add users in bulk Import members by first navigating to the group to which they will be added, then importing from a CSV. A template is available. Azure Portal Sources: Bulk create users in Azure Active Directory Bulk add group members in Azure Active Directory Licenses Note: The user to be licensed must first have a Usage location set. Azure Portal Use the ISO 3166-1 A2 two-letter country or region code to set this value in PowerShell Set-AzureADUser -UsageLocation 'US' Azure Portal Sources Assign or remove licenses in the Azure Active Directory Portal Configure Microsoft 365 user account properties with PowerShell Enable MFA Create a Conditional Access policy to enforce MFA with specified users. Azure Portal Enable [SSPR][SSPR] Azure Portal Add custom domain name AZ-103: 410 Sources Tutorial: Secure user sign-in events with Azure Multi-Factor Authentication How to manage the local administrators group on Azure AD joined devices Password policies and account restrictions in Azure AD","title":"Azure AD"},{"location":"Infrastructure/Cloud/Azure/Azure-AD/#azure-ad","text":"Azure AD has its own set of roles which apply to Azure AD resources and which are distinct from those of Azure RBAC . The terms tenant and directory are deeply connected and often confused with one another. A tenant refers to an instance of Azure AD that is tied to a subscription, and refers to the organization. Each tenant is associated with a dedicated and trusted directory that includes the tenant's users, groups, and apps. Roles: Global Administrator can manage access to administrative features in AAD and can grant administrator roles to other users. An AAD Global Administrator can also temporarily elevate their own access to the Azure RBAC role of User Access Administrator in order to manage all Azure subscriptions and management groups . Whoever signs up for the directory is automatically assigned this role. Device administrator In order to make sure AD users can change their password either locally or in the cloud, Azure AD has to be upgraded to Premium . Enterprise State Roaming allows users to securely synchronize user settings and application settings to Azure. Self-Service Password Reset (SSPR) is supported for all users. SSPR registration can be configured by group or for all domain users, but not individual users.","title":"Azure AD"},{"location":"Infrastructure/Cloud/Azure/Azure-AD/#b2b","text":"Business-to-business (B2B) collaboration allows you to invite guest users into your own ( What is guest user access in Azure Active Directory B2B? )","title":"B2B"},{"location":"Infrastructure/Cloud/Azure/Azure-AD/#joining-a-device","text":"When you join a device to an Azure AD tenant's domain, Azure AD creates local administrator accounts on the device for: - The user joining the device - The Azure AD global administrator - The Azure AD device administrator","title":"Joining a device"},{"location":"Infrastructure/Cloud/Azure/Azure-AD/#sspr","text":"","title":"SSPR"},{"location":"Infrastructure/Cloud/Azure/Azure-AD/#tasks","text":"Sources: - Portal - PowerShell","title":"Tasks"},{"location":"Infrastructure/Cloud/Azure/Azure-AD/#add-users-in-bulk","text":"Import members by first navigating to the group to which they will be added, then importing from a CSV. A template is available. Azure Portal Sources: Bulk create users in Azure Active Directory Bulk add group members in Azure Active Directory","title":"Add users in bulk"},{"location":"Infrastructure/Cloud/Azure/Azure-AD/#licenses","text":"Note: The user to be licensed must first have a Usage location set. Azure Portal Use the ISO 3166-1 A2 two-letter country or region code to set this value in PowerShell Set-AzureADUser -UsageLocation 'US' Azure Portal Sources Assign or remove licenses in the Azure Active Directory Portal Configure Microsoft 365 user account properties with PowerShell","title":"Licenses"},{"location":"Infrastructure/Cloud/Azure/Azure-AD/#enable-mfa","text":"Create a Conditional Access policy to enforce MFA with specified users. Azure Portal Enable [SSPR][SSPR] Azure Portal","title":"Enable MFA"},{"location":"Infrastructure/Cloud/Azure/Azure-AD/#add-custom-domain-name","text":"AZ-103: 410","title":"Add custom domain name"},{"location":"Infrastructure/Cloud/Azure/Azure-AD/#sources","text":"Tutorial: Secure user sign-in events with Azure Multi-Factor Authentication How to manage the local administrators group on Azure AD joined devices Password policies and account restrictions in Azure AD","title":"Sources"},{"location":"Infrastructure/Cloud/Azure/Azure-App-Service/","text":"Pricing tiers: - Free/Shared : uses a shared infrastructure with minimal storage. No options for deploying different staged versions, routing of traffic, or backups - Basic : Dedicated compute for app, including avaiilability of SSL and manual scaling of app instance number. - Standard : Daily backups, automatic scaling of app instances, deployment slots, and user routing with Traffic Manager - Premium : more frequent backups, increased storage, and greater number of deployment slots and instance scaling options. Tasks List available runtimes az webapp list-runtimes --linux Deploy image from ACR Sources az webapp create --name $n --resource-group $g --plan $p --deployment-container-image-name $registry .azurecr.io/ $image :latest az webapp config appsettings set -n $n -g $g --settings \"WEBSITES_PORT=8000\" # Service principal ID $p = az webapp identity assign -n $n -g $g --query principalId -o tsv $s = az account show --query id --output tsv # Grant web app permission to access the container registry az role assignment create --assignee $p --scope /subscriptions/ $s /resourceGroups/ $g /provides/Microsoft.ContainerRegistry/registries/ $registry # Deploy image (note that `$registry` is specified twice in this command) az webapp config container set -n $n -g $g --docker-custom-image-name $registry .azurecr.io/ $image :latest --docker-registry-server-url https:// $registry .azurecr.io # Restart web app after making changes az webapp restart -n $n -g $g Deploy from GitHub #!/bin/bash # Replace the following URL with a public GitHub repo URL gitrepo = https://github.com/Azure-Samples/php-docs-hello-world webappname = mywebapp $RANDOM # Create a resource group. az group create --location westeurope --name myResourceGroup # Create an App Service plan in `FREE` tier. az appservice plan create --name $webappname --resource-group myResourceGroup --sku FREE # Create a web app. az webapp create --name $webappname --resource-group myResourceGroup --plan $webappname # Deploy code from a public GitHub repository. az webapp deployment source config --name $webappname --resource-group myResourceGroup \\ --repo-url $gitrepo --branch master --manual-integration # Copy the result of the following command into a browser to see the web app. echo http:// $webappname .azurewebsites.net $gitrepo = \"<replace-with-URL-of-your-own-GitHub-repo>\" $gittoken = \"<replace-with-a-GitHub-access-token>\" $webappname = \"mywebapp $( Get-Random ) \" $location = \"West Europe\" # Create a resource group. New-AzResourceGroup -Name myResourceGroup -Location $location # Create an App Service plan in Free tier. New-AzAppServicePlan -Name $webappname -Location $location ` -ResourceGroupName myResourceGroup -Tier Free # Create a web app. New-AzWebApp -Name $webappname -Location $location -AppServicePlan $webappname ` -ResourceGroupName myResourceGroup # SET GitHub $PropertiesObject = @{ token = $gittoken ; } Set-AzResource -PropertyObject $PropertiesObject ` -ResourceId / providers / Microsoft . Web / sourcecontrols / GitHub -ApiVersion 2015 - 08 - 01 -Force # Configure GitHub deployment from your GitHub repo and deploy once. $PropertiesObject = @{ repoUrl = \"$gitrepo\" ; branch = \"master\" ; } Set-AzResource -PropertyObject $PropertiesObject -ResourceGroupName myResourceGroup ` -ResourceType Microsoft . Web / sites / sourcecontrols -ResourceName $webappname / web ` -ApiVersion 2015 - 08 - 01 -Force Create Front Door Add the front-door extension if using Azure CLI az extension add --name front-door Create web apps in two different regions. $webapp1 = New-AzWebApp -Name \"WebAppContoso- $( Get-Random ) \" -Location centralus -ResourceGroupName myResourceGroupFD -AppServicePlan myAppServicePlanCentralUS $webapp2 = New-AzWebApp -Name \"WebAppContoso- $( Get-Random ) \" -Location southcentralus -ResourceGroupName myResourceGroupFD -AppServicePlan myAppServicePlanSouthCentralUS az appservice plan create --name myAppServicePlanCentralUS --resource-group myRGFDCentral az webapp create --name WebAppContoso1 --resource-group myRGFDCentral --plan myAppServicePlanCentralUS az appservice plan create --name myAppServicePlanSouthCentralUS -resource-groupg myRGFDSouthCentral az webapp create --name WebAppContoso2 --resource-group myRGFDSouthCentral --plan myAppServicePlanSouthCentralUS Create a frontend object # Create a unique name $fdname = \"contoso-frontend- $( Get-Random ) \" #Create the frontend object $FrontendEndObject = New-AzFrontDoorFrontendEndpointObject ` -Name \"frontendEndpoint1\" ` -HostName $fdname \".azurefd.net\" Create the backend pool # Create backend objects that points to the hostname of the web apps $backendObject1 = New-AzFrontDoorBackendObject ` -Address $webapp1 . DefaultHostName $backendObject2 = New-AzFrontDoorBackendObject ` -Address $webapp2 . DefaultHostName # Create a health probe object $HealthProbeObject = New-AzFrontDoorHealthProbeSettingObject ` -Name \"HealthProbeSetting\" # Create the load balancing setting object $LoadBalancingSettingObject = New-AzFrontDoorLoadBalancingSettingObject ` -Name \"Loadbalancingsetting\" ` -SampleSize \"4\" ` -SuccessfulSamplesRequired \"2\" ` -AdditionalLatencyInMilliseconds \"0\" # Create a backend pool using the backend objects, health probe, and load balancing settings $BackendPoolObject = New-AzFrontDoorBackendPoolObject ` -Name \"myBackendPool\" ` -FrontDoorName $fdname ` -ResourceGroupName myResourceGroupFD ` -Backend $backendObject1 , $backendObject2 ` -HealthProbeSettingsName \"HealthProbeSetting\" ` -LoadBalancingSettingsName \"Loadbalancingsetting\" New-AzFrontDoor ` -Name $fdname ` -ResourceGroupName myResourceGroupFD ` -RoutingRule $RoutingRuleObject ` -BackendPool $BackendPoolObject ` -FrontendEndpoint $FrontendEndObject ` -LoadBalancingSetting $LoadBalancingSettingObject ` -HealthProbeSetting $HealthProbeObject az network front-door create \\ --resource-group myRGFDCentral \\ --name contoso-frontend \\ --accepted-protocols http https \\ --backend-address webappcontoso1.azurewebsites.net webappcontoso2.azurewebsites.net","title":"Azure App Service"},{"location":"Infrastructure/Cloud/Azure/Azure-App-Service/#tasks","text":"","title":"Tasks"},{"location":"Infrastructure/Cloud/Azure/Azure-App-Service/#list-available-runtimes","text":"az webapp list-runtimes --linux","title":"List available runtimes"},{"location":"Infrastructure/Cloud/Azure/Azure-App-Service/#deploy-image-from-acr","text":"Sources az webapp create --name $n --resource-group $g --plan $p --deployment-container-image-name $registry .azurecr.io/ $image :latest az webapp config appsettings set -n $n -g $g --settings \"WEBSITES_PORT=8000\" # Service principal ID $p = az webapp identity assign -n $n -g $g --query principalId -o tsv $s = az account show --query id --output tsv # Grant web app permission to access the container registry az role assignment create --assignee $p --scope /subscriptions/ $s /resourceGroups/ $g /provides/Microsoft.ContainerRegistry/registries/ $registry # Deploy image (note that `$registry` is specified twice in this command) az webapp config container set -n $n -g $g --docker-custom-image-name $registry .azurecr.io/ $image :latest --docker-registry-server-url https:// $registry .azurecr.io # Restart web app after making changes az webapp restart -n $n -g $g","title":"Deploy image from ACR"},{"location":"Infrastructure/Cloud/Azure/Azure-App-Service/#deploy-from-github","text":"#!/bin/bash # Replace the following URL with a public GitHub repo URL gitrepo = https://github.com/Azure-Samples/php-docs-hello-world webappname = mywebapp $RANDOM # Create a resource group. az group create --location westeurope --name myResourceGroup # Create an App Service plan in `FREE` tier. az appservice plan create --name $webappname --resource-group myResourceGroup --sku FREE # Create a web app. az webapp create --name $webappname --resource-group myResourceGroup --plan $webappname # Deploy code from a public GitHub repository. az webapp deployment source config --name $webappname --resource-group myResourceGroup \\ --repo-url $gitrepo --branch master --manual-integration # Copy the result of the following command into a browser to see the web app. echo http:// $webappname .azurewebsites.net $gitrepo = \"<replace-with-URL-of-your-own-GitHub-repo>\" $gittoken = \"<replace-with-a-GitHub-access-token>\" $webappname = \"mywebapp $( Get-Random ) \" $location = \"West Europe\" # Create a resource group. New-AzResourceGroup -Name myResourceGroup -Location $location # Create an App Service plan in Free tier. New-AzAppServicePlan -Name $webappname -Location $location ` -ResourceGroupName myResourceGroup -Tier Free # Create a web app. New-AzWebApp -Name $webappname -Location $location -AppServicePlan $webappname ` -ResourceGroupName myResourceGroup # SET GitHub $PropertiesObject = @{ token = $gittoken ; } Set-AzResource -PropertyObject $PropertiesObject ` -ResourceId / providers / Microsoft . Web / sourcecontrols / GitHub -ApiVersion 2015 - 08 - 01 -Force # Configure GitHub deployment from your GitHub repo and deploy once. $PropertiesObject = @{ repoUrl = \"$gitrepo\" ; branch = \"master\" ; } Set-AzResource -PropertyObject $PropertiesObject -ResourceGroupName myResourceGroup ` -ResourceType Microsoft . Web / sites / sourcecontrols -ResourceName $webappname / web ` -ApiVersion 2015 - 08 - 01 -Force","title":"Deploy from GitHub"},{"location":"Infrastructure/Cloud/Azure/Azure-App-Service/#create-front-door","text":"Add the front-door extension if using Azure CLI az extension add --name front-door Create web apps in two different regions. $webapp1 = New-AzWebApp -Name \"WebAppContoso- $( Get-Random ) \" -Location centralus -ResourceGroupName myResourceGroupFD -AppServicePlan myAppServicePlanCentralUS $webapp2 = New-AzWebApp -Name \"WebAppContoso- $( Get-Random ) \" -Location southcentralus -ResourceGroupName myResourceGroupFD -AppServicePlan myAppServicePlanSouthCentralUS az appservice plan create --name myAppServicePlanCentralUS --resource-group myRGFDCentral az webapp create --name WebAppContoso1 --resource-group myRGFDCentral --plan myAppServicePlanCentralUS az appservice plan create --name myAppServicePlanSouthCentralUS -resource-groupg myRGFDSouthCentral az webapp create --name WebAppContoso2 --resource-group myRGFDSouthCentral --plan myAppServicePlanSouthCentralUS Create a frontend object # Create a unique name $fdname = \"contoso-frontend- $( Get-Random ) \" #Create the frontend object $FrontendEndObject = New-AzFrontDoorFrontendEndpointObject ` -Name \"frontendEndpoint1\" ` -HostName $fdname \".azurefd.net\" Create the backend pool # Create backend objects that points to the hostname of the web apps $backendObject1 = New-AzFrontDoorBackendObject ` -Address $webapp1 . DefaultHostName $backendObject2 = New-AzFrontDoorBackendObject ` -Address $webapp2 . DefaultHostName # Create a health probe object $HealthProbeObject = New-AzFrontDoorHealthProbeSettingObject ` -Name \"HealthProbeSetting\" # Create the load balancing setting object $LoadBalancingSettingObject = New-AzFrontDoorLoadBalancingSettingObject ` -Name \"Loadbalancingsetting\" ` -SampleSize \"4\" ` -SuccessfulSamplesRequired \"2\" ` -AdditionalLatencyInMilliseconds \"0\" # Create a backend pool using the backend objects, health probe, and load balancing settings $BackendPoolObject = New-AzFrontDoorBackendPoolObject ` -Name \"myBackendPool\" ` -FrontDoorName $fdname ` -ResourceGroupName myResourceGroupFD ` -Backend $backendObject1 , $backendObject2 ` -HealthProbeSettingsName \"HealthProbeSetting\" ` -LoadBalancingSettingsName \"Loadbalancingsetting\" New-AzFrontDoor ` -Name $fdname ` -ResourceGroupName myResourceGroupFD ` -RoutingRule $RoutingRuleObject ` -BackendPool $BackendPoolObject ` -FrontendEndpoint $FrontendEndObject ` -LoadBalancingSetting $LoadBalancingSettingObject ` -HealthProbeSetting $HealthProbeObject az network front-door create \\ --resource-group myRGFDCentral \\ --name contoso-frontend \\ --accepted-protocols http https \\ --backend-address webappcontoso1.azurewebsites.net webappcontoso2.azurewebsites.net","title":"Create Front Door"},{"location":"Infrastructure/Cloud/Azure/Azure-Backup/","text":"Azure Backup can backup on-prem servers, cloud-based VMs, and virtualized workloads like SQL Server and Sharepoint. However Azure SQL databases are already backed up by an automatic service by default. AZ-103 p. 159 On-prem machines can be backed up using several agents AZ-103 p. 162 MARS Agent System Center Data Protection Manager (DPM) or Microsoft Azure Backup Server (MABS) can be used as backup servers. The backup server can then be backed up to a Recovery Services vault Azure VMs can be backed up Directly using an extension on the Azure VM Agent , which comes preinstalled on Marketplace images Specific files and folders on a VM can be backed up by running the MARS agent To the MABS running in Azure, which can then be backed up to a Recovery Services vault Storage accounts can be backed up, but not blob storage. Blob storage is already replicated locally, which provides fault-tolerance. Instead, you can use snapshots. When installed, the Get-AzVM command exposes a ProvisionVMAgent property with a boolean value under OSProfile.WindowsConfiguration . Containers There appear to be resources that house items to be protected that can be enumerated . Reports Log Analytics workspaces must be located in the same region as the Recovery Services vault in order to store Backup reports. Pre-Checks Azure Backup pre-checks complete with various statuses that indicate potential problems Passed : VM configuration is conducive for successful backups Warning : Issues that might lead to backup failures Critical : Issues that will lead to backup failures Tasks Create Recovery Services Vault Azure Portal Azure PowerShell Azure CLI New-AzRecoveryServicesVault -Name $n -ResourceGroupName $rgName -Location $l az backup vault create --name $n --resource-group $rgName --Location $l Enable MFA This requires MFA to be enabled. Azure Portal Enable multi-factor authentication for the Recovery services vault by going to the vault in the Portal, then Properties > Security settings: Update > Choose Yes in the dropdown. An option to generate a security PIN will appear in this same blade. Recover files Azure Portal Download the executable (for Windows VMs) or PowerShell script (for Linux VMs). A Python script is generated when downloading to a Linux machine. Configure Backup reports Sources - Configure Azure Backup reports A Log Analytics workspace must exist. Turn on diagnostics in the Recovery Services vault Select Archive to a storage account ( NOT Send to Log Analytics), providing a storage account to store information needed for report. Select AzureBackupReport under log section, which will collect all needed data models and information for the backup report. Connect to Azure Backup in PowerBI using a service content pack. Define new backup protection policy Azure PowerShell $SchPol = Get-AzRecoveryServicesBackupSchedulePolicyObject -WorkloadType \"AzureVM\" $SchPol . ScheduleRunTimes . Clear () $Dt = Get-Date $SchPol . ScheduleRunTimes . Add ( $Dt . ToUniversalTime ()) $RetPol = Get-AzRecoveryServicesBackupRetentionPolicyObject -WorkloadType \"AzureVM\" $RetPol . DailySchedule . DurationCountInDays = 365 New-AzRecoveryServicesBackupProtectionPolicy -Name \"NewPolicy\" -WorkloadType AzureVM -RetentionPolicy $RetPol -SchedulePolicy $SchPol Configure VM backup Azure PowerShell Azure CLI $policy = Get-AzRecoveryServicesBackupProtectionPolicy -Name \"DefaultPolicy\" Enable-AzRecoveryServicesBackupProtection -ResourceGroupName $g -Name $n -Policy $policy # GRS by default az backup protection enable-for-vm -g $g -v $v --vm vm --policy-name DefaultPolicy # LRS az backup vault backup-properties set -n $v -g $g --backup-storage-redundancy \"LocallyRedundant\" Initiate VM backup Azure PowerShell $backupcontainer = Get-AzRecoveryServicesBackupContainer -ContainerType \"AzureVM\" -FriendlyName \"myVM\" $item = Get-AzRecoveryServicesBackupItem -Container $backupcontainer -WorkloadType \"AzureVM\" Backup-AzRecoveryServicesBackupItem -Item $item --container-name / -c appears to accept the name of the VM itself. Azure CLI az backup protection backup-now -g myResourceGroup -n myRecoveryServicesVault --container-name myVM --item-name myVM --retain-until 18 -10-2017 --backup-management-type AzureIaasVM List containers -BackupManagementType accepts the following values - AzureVM - MARS - AzureWorkload - AzureStorage -ContainerType accepts: - AzureVM - Windows - AzureSQL - AzureStorage - AzureVMAppContainer $v = Get-AzRecoveryServicesVault -ResourceGroupName $rg -Name vault Get-AzRecoveryServicesBackupContainer -ContainerType Windows -BackupManagementType MARS -VaultId $v . ID This returns a list of JSON objects. --backup-management-type accepts the following values: - AzureIaasVM - AzureStorage - AzureWorkload az backup container list -g $g -v $v --backup-management-type AzureIaasVM Preserve only the \"name\" attribute of the first item, which itself is a semicolon-delimited string of values. ( Start backup now ) az backup container list -g $g -v $v --backup-management-type AzureIaasVM --query [ 0 ] .name Sources AZ-103: 2.4 AZ-104: 5.2 Azure Backup architecture and components Azure Virtual Machine Agent overview Understanding and using the Azure Linux Agent Restore files from VM Back up a VM - Azure CLI , PowerShell Get-AzRecoveryServicesBackupContainer New-AzRecoveryServicesBackupProtectionPolicy az backup container list","title":"Azure Backup"},{"location":"Infrastructure/Cloud/Azure/Azure-Backup/#containers","text":"There appear to be resources that house items to be protected that can be enumerated .","title":"Containers"},{"location":"Infrastructure/Cloud/Azure/Azure-Backup/#reports","text":"Log Analytics workspaces must be located in the same region as the Recovery Services vault in order to store Backup reports.","title":"Reports"},{"location":"Infrastructure/Cloud/Azure/Azure-Backup/#pre-checks","text":"Azure Backup pre-checks complete with various statuses that indicate potential problems Passed : VM configuration is conducive for successful backups Warning : Issues that might lead to backup failures Critical : Issues that will lead to backup failures","title":"Pre-Checks"},{"location":"Infrastructure/Cloud/Azure/Azure-Backup/#tasks","text":"Create Recovery Services Vault Azure Portal Azure PowerShell Azure CLI New-AzRecoveryServicesVault -Name $n -ResourceGroupName $rgName -Location $l az backup vault create --name $n --resource-group $rgName --Location $l Enable MFA This requires MFA to be enabled. Azure Portal Enable multi-factor authentication for the Recovery services vault by going to the vault in the Portal, then Properties > Security settings: Update > Choose Yes in the dropdown. An option to generate a security PIN will appear in this same blade. Recover files Azure Portal Download the executable (for Windows VMs) or PowerShell script (for Linux VMs). A Python script is generated when downloading to a Linux machine.","title":"Tasks"},{"location":"Infrastructure/Cloud/Azure/Azure-Backup/#configure-backup-reports","text":"Sources - Configure Azure Backup reports A Log Analytics workspace must exist. Turn on diagnostics in the Recovery Services vault Select Archive to a storage account ( NOT Send to Log Analytics), providing a storage account to store information needed for report. Select AzureBackupReport under log section, which will collect all needed data models and information for the backup report. Connect to Azure Backup in PowerBI using a service content pack. Define new backup protection policy Azure PowerShell $SchPol = Get-AzRecoveryServicesBackupSchedulePolicyObject -WorkloadType \"AzureVM\" $SchPol . ScheduleRunTimes . Clear () $Dt = Get-Date $SchPol . ScheduleRunTimes . Add ( $Dt . ToUniversalTime ()) $RetPol = Get-AzRecoveryServicesBackupRetentionPolicyObject -WorkloadType \"AzureVM\" $RetPol . DailySchedule . DurationCountInDays = 365 New-AzRecoveryServicesBackupProtectionPolicy -Name \"NewPolicy\" -WorkloadType AzureVM -RetentionPolicy $RetPol -SchedulePolicy $SchPol Configure VM backup Azure PowerShell Azure CLI $policy = Get-AzRecoveryServicesBackupProtectionPolicy -Name \"DefaultPolicy\" Enable-AzRecoveryServicesBackupProtection -ResourceGroupName $g -Name $n -Policy $policy # GRS by default az backup protection enable-for-vm -g $g -v $v --vm vm --policy-name DefaultPolicy # LRS az backup vault backup-properties set -n $v -g $g --backup-storage-redundancy \"LocallyRedundant\" Initiate VM backup Azure PowerShell $backupcontainer = Get-AzRecoveryServicesBackupContainer -ContainerType \"AzureVM\" -FriendlyName \"myVM\" $item = Get-AzRecoveryServicesBackupItem -Container $backupcontainer -WorkloadType \"AzureVM\" Backup-AzRecoveryServicesBackupItem -Item $item --container-name / -c appears to accept the name of the VM itself. Azure CLI az backup protection backup-now -g myResourceGroup -n myRecoveryServicesVault --container-name myVM --item-name myVM --retain-until 18 -10-2017 --backup-management-type AzureIaasVM","title":"Configure Backup reports"},{"location":"Infrastructure/Cloud/Azure/Azure-Backup/#list-containers","text":"-BackupManagementType accepts the following values - AzureVM - MARS - AzureWorkload - AzureStorage -ContainerType accepts: - AzureVM - Windows - AzureSQL - AzureStorage - AzureVMAppContainer $v = Get-AzRecoveryServicesVault -ResourceGroupName $rg -Name vault Get-AzRecoveryServicesBackupContainer -ContainerType Windows -BackupManagementType MARS -VaultId $v . ID This returns a list of JSON objects. --backup-management-type accepts the following values: - AzureIaasVM - AzureStorage - AzureWorkload az backup container list -g $g -v $v --backup-management-type AzureIaasVM Preserve only the \"name\" attribute of the first item, which itself is a semicolon-delimited string of values. ( Start backup now ) az backup container list -g $g -v $v --backup-management-type AzureIaasVM --query [ 0 ] .name","title":"List containers"},{"location":"Infrastructure/Cloud/Azure/Azure-Backup/#sources","text":"AZ-103: 2.4 AZ-104: 5.2 Azure Backup architecture and components Azure Virtual Machine Agent overview Understanding and using the Azure Linux Agent Restore files from VM Back up a VM - Azure CLI , PowerShell Get-AzRecoveryServicesBackupContainer New-AzRecoveryServicesBackupProtectionPolicy az backup container list","title":"Sources"},{"location":"Infrastructure/Cloud/Azure/Azure-File-Service/","text":"Azure File Service Mount Azure File Share Windows Connect to and mount an Azure File Share (Windows File Explorer) Right-click on This PC Click Map Network Drive option Specify drive letter to be used Specify folder: \\\\<storageAccount>.files.core.windows.net\\<shareName> Click Finish In the dialog box that opens login with the username: AZURE\\<storageName> Password should be access key for the storage account net use x \\\\erstandard01.file.core.windows.net\\logs /u:AZURE\\erstandard01 <accessKey> Automatically reconnect after reboot in Windows cmdkey /add:storageAccountName.file.core.windows.net /user:AZURE\\storageAccountName /pass:storageAccountKey $storageKey = ( Get-AzStorageAccountKey -ResourceGroupName $g -Name $storageNAme ). Value [ 0 ] $acctKey = ConvertTo-SecureString -String $storageKey -AsPlainText -Force $credential = New-Object System . Management . Automation . PSCredential -ArgumentList \"Azure\\$storageName\" , $acctKey New-PSDrive -Name \"Z\" -PSProvider FileSystem -Root \"\\\\$storageName.file.core.windows.net\\$shareName\" -Credential $credential Linux Mounting to /logs sudo mount -t cifs // $storageAccount .file.core.windows.net/logs /logs -o \"vers=3.0,username= $storageAccount ,password= $storageAccountKey ,dir_mode=0777,file_mode=0777,sec=ntlmssp\" Sources: Deploy Azure File Sync AZ-103: p. 148","title":"Azure File Service"},{"location":"Infrastructure/Cloud/Azure/Azure-File-Service/#azure-file-service","text":"","title":"Azure File Service"},{"location":"Infrastructure/Cloud/Azure/Azure-File-Service/#mount-azure-file-share","text":"","title":"Mount Azure File Share"},{"location":"Infrastructure/Cloud/Azure/Azure-File-Service/#windows","text":"Connect to and mount an Azure File Share (Windows File Explorer) Right-click on This PC Click Map Network Drive option Specify drive letter to be used Specify folder: \\\\<storageAccount>.files.core.windows.net\\<shareName> Click Finish In the dialog box that opens login with the username: AZURE\\<storageName> Password should be access key for the storage account net use x \\\\erstandard01.file.core.windows.net\\logs /u:AZURE\\erstandard01 <accessKey> Automatically reconnect after reboot in Windows cmdkey /add:storageAccountName.file.core.windows.net /user:AZURE\\storageAccountName /pass:storageAccountKey $storageKey = ( Get-AzStorageAccountKey -ResourceGroupName $g -Name $storageNAme ). Value [ 0 ] $acctKey = ConvertTo-SecureString -String $storageKey -AsPlainText -Force $credential = New-Object System . Management . Automation . PSCredential -ArgumentList \"Azure\\$storageName\" , $acctKey New-PSDrive -Name \"Z\" -PSProvider FileSystem -Root \"\\\\$storageName.file.core.windows.net\\$shareName\" -Credential $credential","title":"Windows"},{"location":"Infrastructure/Cloud/Azure/Azure-File-Service/#linux","text":"Mounting to /logs sudo mount -t cifs // $storageAccount .file.core.windows.net/logs /logs -o \"vers=3.0,username= $storageAccount ,password= $storageAccountKey ,dir_mode=0777,file_mode=0777,sec=ntlmssp\"","title":"Linux"},{"location":"Infrastructure/Cloud/Azure/Azure-File-Service/#sources","text":"Deploy Azure File Sync AZ-103: p. 148","title":"Sources:"},{"location":"Infrastructure/Cloud/Azure/Azure-IAM/","text":"Azure IAM Azure methods of administering access to resources can be divided into two groups Role-Based Access Controls (RBAC) are supported only by Azure Portal and the ARM APIs. RBAC is configured by selecting a role and associating it with a security principal , such as a user, group, or service identity. Child reosurces inherit the roles of their parents (\"role inheritance\"). Classic subscription administrators Classic subscription administrators Classic subscription administrators have full access to a subcription. They can access resources through Azure Portal, ARM APIs (PowerShell and CLI), and classic deployment model APIs. By default, the account that is used to sign up for a subscription is automatically set as both Account Administrator and Service Administrator . There can only be one Account Administrator per account and only 1 Service Administrator per subscription. Co-Administrators have the same access as Service Administrators, and there can be 200 of them per subscription, but cannot change the association of subscriptions to directories. Roles Components of a role assignment include: Security principal : objects associated with a role definition and a scope to apply RBAC to azure resources (i.e. a user, group, service principal, or managed identity which is an application registration that is managed automatically by Azure and an Azure service) User principal : identity associated with a user or group of users. Service principal : identity associated with an application. Role definition : list of permissions which define what actions can or cannot be performed against a resource. In addition to the 4 foundational built-in roles , there are many other built-in roles and custom roles can be defined using a JSON file. Scope Scopes There are four scopes at which RBAC can be applied: Management group Subscriptions Resource groups Resources Azure RBAC roles can be used to grant rights to 2 types of principals: User principal : identity associated with a user or group of users. Service principal : identity associated with an application. RBAC roles can also be applied to a subscription through Management Groups , which represent the recommended practice for ensuring consistent application of tenant-wide security. Management groups form a hierarchy where each child inherits policy from its single parent while having additional controls. There is a single Management Group at the root of the hierarchy, associated with the Azure AD tenant (which is associated, in turn, with a subscription) that cannot be moved or deleted. Role assignments Current assignments for classic admins can be seen in the Properties blade of a subscription in Azure Portal. Co-Administrator assignments can be added by opening the Access Control (IAM) blade of a subscription, then clicking the Add co-administrator button. RBAC roles are supported only by Azure Portal and the ARM APIs. Access policy is applied to a scope , which includes subscriptions, resource groups, or resources: a policy applied to a subscription is said to be at the \"subscription scope\". Policy can also be applied to Management Groups, which is an additional scope above subscription. In this way, several subscriptions can inherit a single policy through a Management Group. RBAC roles can also be applied to a subscription through Management Groups , which represent the recommended practice for ensuring consistent application of tenant-wide security. Management groups form a hierarchy where each child inherits policy from its single parent while having additional controls. There is a single Management Group at the root of the hierarchy, associated with the Azure AD tenant (which is associated, in turn, with a subscription) that cannot be moved or deleted. Role definitions Custom roles configure two types of privileges and are specified by two different properties of the definition JSON file: Management and Data. This provides safety from allowing unrestricted access to data. The values of these properties is an array of strings, each of which follows the format Company.ProviderName/ResourceType/Action where action can be of values read , write , action , delete , or * . Privilege Property that defines allowed permissions Property that defines denied permissions Management Actions NotActions Data DataActions NotDataActions Unrestricted Network resources (read only) \"Actions\" : [ \"*\" ] \"Actions\" : [ \"Microsoft.Network/*/read\" ] Example role definitions: Contributor { \"Name\" : \"Contributor\" , \"Id\" : \"b24988ac-6180-42a0-ab88-20f7382dd24c\" , \"IsCustom\" : false , \"Description\" : \"Lets you manage everything except access to resources.\" , \"Actions\" : [ \"*\" ], \"NotActions\" : [ \"Microsoft.Authorization/*/Delete\" , \"Microsoft.Authorization/*/Write\" , \"Microsoft.Authorization/elevateAccess/Action\" , \"Microsoft.Blueprint/blueprintAssignments/write\" , \"Microsoft.Blueprint/blueprintAssignments/delete\" ], \"DataActions\" : [], \"NotDataActions\" : [], \"AssignableScopes\" : [ \"/\" ] } Some built-in roles: Owner has full access to all resources and can delegate access. Service Administrator and Co-Administrators are assigned this role at the subscription scope. Contributor can create and manage all resources (full read/write privileges), but cannot delegate access. Reader can view resources. [Cost Management Contributor][Cost Management Contributor] [Cost Management Reader][Cost Management Reader] [Resource Policy Contributor][Resource Policy Contributor] [User Administrator][User Administrator] [User Access Administrator][User Access Administrator] Tasks Create assignment Assign the Owner role to a user at the subscription scope Navigate to resource group > Access Control (IAM) > Role Assignments tab > Add > Add Role Assignment Open Subscription > Access Control (IAM) > Add Role Assignment> select a Role > Select target principal Access control (AIM) pane > Add > Add role assignment Select a role in the Role dropdown and a user in the Select field. Then Save Azure Portal PowerShell Azure CLI # Resource group scope New-AzRoleAssignment -SignInName \"rbacuser@example.com\" -RoleDefinitionName \"Virtual Machine Contributor\" -ResourceGroupName ExamRefRG # Subscription scope New-AzRoleAssignment -SignInName \"rbacuser@example.com\" -RoleDefinitionName \"Owner\" -Scope \"/subscriptions/$subId\" # Resource group scope az role assignment create --assignee \"rbacuser@example.com\" --role \"Virtual Machine Contributor\" --resource-group ExamRefRG # Subscription scope az role assignment create --assignee \"rbacuser@example.com\" --role \"Owner\" --subscription $subId Delete assignment Navigate to resource group > Access Control (IAM) > Role Assignments tab > Select one or more security principals > Remove Remove RBAC assignments from a user Azure PowerShell Remove-AzRoleAssignment -SignInName \"cloudadmin@opsgility.onmicrosoft.com\" -RoleDefinitionName \"Virtual Machine Contributor\" -ResourceGroupName ExamRefRG Remove-AzRoleAssignment -SignInName $u -ResourceGroupName $rgName -RoleDefinitionName \"Virtual Machine Contributor\" Azure AD group $g = Get-AzADGroup -SearchString \"Cloud Admins\" Remove-AzRoleAssignment -ObjectId $g . Id -ResourceGroupName $rg -RoleDefinitionName \"Virtual Machine Contributor\" az role assignment delete --assignee $u --resource-group $rg --role \"Virtual Machine Contributor\" Azure AD group g = $( az ad group list --query \"[?displayName=='Cloud Admins'].objectId\" -o tsv ) az role assignment delete --role \"Virtual Machine Contributor\" -\u2013assignee-object-id $g --resource-group $rg Read assignment Azure PowerShell Azure CLI Get-AzRoleDefinition -Name \"Virtual Machine Contributor\" | ConvertTo-Json az role definition list -n \"Virtual Machine Contributor\" List custom roles available for assignment Azure PowerShell Azure CLI Get-AzRoleDefinition | Where-Object { $_ . IsCustom -eq $true } az role definition list --custom-role-only -o table View all role assignments in a subscription az role assignment list --all Create role definition Azure PowerShell New-AzRoleDefinition -InputFile \"C:\\ARM_templates\\customrole1.json\" Configure cost center quotas and tagging Grant an AD group RBAC rights Azure PowerShell $group = Get-AzADGroup -SearchString \"Cloud Admins\" New-AzRoleAssignment -ObjectId $group . Id -RoleDefinitionName \"Virtual Machine Contributor\" -ResourceGroupName ExamRefRG Remove RBAC assignments from a group Azure PowerShell $adGroup = Get-AzADGRoup -SearchString \"Cloud Admins\" Remove-AzRoleAssignment -ResourceGroupName $rgName -ObjectId $adGroup . Id -RoleDefinitionName \"Virtual Machine Contributor\" Elevate permissions For Azure AD Global Administrators who want to temporarily elevate permissions Sign into Azure portal as an Azure AD Global Administrator. ? Navigate to Azure Active Directory > Properties . At the bottom of the page, under \"Access management for Azure resources\" click Yes then Save . Sign out and sign in again. Assign roles Revoke elevated access by returning to Azure Active Directory > Properties and selecting No under \"Access management for Azure resources\". Sources Elevating global administrator access Understand Azure role definitions SSPR Administrator accounts are treated differently from other user accounts for SSPR and have a \"strong default two-gate password reset policy\", which requires two pieces of authentication data and foregoes the use of security questions.","title":"Azure IAM"},{"location":"Infrastructure/Cloud/Azure/Azure-IAM/#azure-iam","text":"Azure methods of administering access to resources can be divided into two groups Role-Based Access Controls (RBAC) are supported only by Azure Portal and the ARM APIs. RBAC is configured by selecting a role and associating it with a security principal , such as a user, group, or service identity. Child reosurces inherit the roles of their parents (\"role inheritance\"). Classic subscription administrators","title":"Azure IAM"},{"location":"Infrastructure/Cloud/Azure/Azure-IAM/#classic-subscription-administrators","text":"Classic subscription administrators have full access to a subcription. They can access resources through Azure Portal, ARM APIs (PowerShell and CLI), and classic deployment model APIs. By default, the account that is used to sign up for a subscription is automatically set as both Account Administrator and Service Administrator . There can only be one Account Administrator per account and only 1 Service Administrator per subscription. Co-Administrators have the same access as Service Administrators, and there can be 200 of them per subscription, but cannot change the association of subscriptions to directories.","title":"Classic subscription administrators"},{"location":"Infrastructure/Cloud/Azure/Azure-IAM/#roles","text":"Components of a role assignment include: Security principal : objects associated with a role definition and a scope to apply RBAC to azure resources (i.e. a user, group, service principal, or managed identity which is an application registration that is managed automatically by Azure and an Azure service) User principal : identity associated with a user or group of users. Service principal : identity associated with an application. Role definition : list of permissions which define what actions can or cannot be performed against a resource. In addition to the 4 foundational built-in roles , there are many other built-in roles and custom roles can be defined using a JSON file. Scope","title":"Roles"},{"location":"Infrastructure/Cloud/Azure/Azure-IAM/#scopes","text":"There are four scopes at which RBAC can be applied: Management group Subscriptions Resource groups Resources Azure RBAC roles can be used to grant rights to 2 types of principals: User principal : identity associated with a user or group of users. Service principal : identity associated with an application. RBAC roles can also be applied to a subscription through Management Groups , which represent the recommended practice for ensuring consistent application of tenant-wide security. Management groups form a hierarchy where each child inherits policy from its single parent while having additional controls. There is a single Management Group at the root of the hierarchy, associated with the Azure AD tenant (which is associated, in turn, with a subscription) that cannot be moved or deleted.","title":"Scopes"},{"location":"Infrastructure/Cloud/Azure/Azure-IAM/#role-assignments","text":"Current assignments for classic admins can be seen in the Properties blade of a subscription in Azure Portal. Co-Administrator assignments can be added by opening the Access Control (IAM) blade of a subscription, then clicking the Add co-administrator button. RBAC roles are supported only by Azure Portal and the ARM APIs. Access policy is applied to a scope , which includes subscriptions, resource groups, or resources: a policy applied to a subscription is said to be at the \"subscription scope\". Policy can also be applied to Management Groups, which is an additional scope above subscription. In this way, several subscriptions can inherit a single policy through a Management Group. RBAC roles can also be applied to a subscription through Management Groups , which represent the recommended practice for ensuring consistent application of tenant-wide security. Management groups form a hierarchy where each child inherits policy from its single parent while having additional controls. There is a single Management Group at the root of the hierarchy, associated with the Azure AD tenant (which is associated, in turn, with a subscription) that cannot be moved or deleted.","title":"Role assignments"},{"location":"Infrastructure/Cloud/Azure/Azure-IAM/#role-definitions","text":"Custom roles configure two types of privileges and are specified by two different properties of the definition JSON file: Management and Data. This provides safety from allowing unrestricted access to data. The values of these properties is an array of strings, each of which follows the format Company.ProviderName/ResourceType/Action where action can be of values read , write , action , delete , or * . Privilege Property that defines allowed permissions Property that defines denied permissions Management Actions NotActions Data DataActions NotDataActions Unrestricted Network resources (read only) \"Actions\" : [ \"*\" ] \"Actions\" : [ \"Microsoft.Network/*/read\" ] Example role definitions: Contributor { \"Name\" : \"Contributor\" , \"Id\" : \"b24988ac-6180-42a0-ab88-20f7382dd24c\" , \"IsCustom\" : false , \"Description\" : \"Lets you manage everything except access to resources.\" , \"Actions\" : [ \"*\" ], \"NotActions\" : [ \"Microsoft.Authorization/*/Delete\" , \"Microsoft.Authorization/*/Write\" , \"Microsoft.Authorization/elevateAccess/Action\" , \"Microsoft.Blueprint/blueprintAssignments/write\" , \"Microsoft.Blueprint/blueprintAssignments/delete\" ], \"DataActions\" : [], \"NotDataActions\" : [], \"AssignableScopes\" : [ \"/\" ] } Some built-in roles: Owner has full access to all resources and can delegate access. Service Administrator and Co-Administrators are assigned this role at the subscription scope. Contributor can create and manage all resources (full read/write privileges), but cannot delegate access. Reader can view resources. [Cost Management Contributor][Cost Management Contributor] [Cost Management Reader][Cost Management Reader] [Resource Policy Contributor][Resource Policy Contributor] [User Administrator][User Administrator] [User Access Administrator][User Access Administrator]","title":"Role definitions"},{"location":"Infrastructure/Cloud/Azure/Azure-IAM/#tasks","text":"","title":"Tasks"},{"location":"Infrastructure/Cloud/Azure/Azure-IAM/#create-assignment","text":"Assign the Owner role to a user at the subscription scope Navigate to resource group > Access Control (IAM) > Role Assignments tab > Add > Add Role Assignment Open Subscription > Access Control (IAM) > Add Role Assignment> select a Role > Select target principal Access control (AIM) pane > Add > Add role assignment Select a role in the Role dropdown and a user in the Select field. Then Save Azure Portal PowerShell Azure CLI # Resource group scope New-AzRoleAssignment -SignInName \"rbacuser@example.com\" -RoleDefinitionName \"Virtual Machine Contributor\" -ResourceGroupName ExamRefRG # Subscription scope New-AzRoleAssignment -SignInName \"rbacuser@example.com\" -RoleDefinitionName \"Owner\" -Scope \"/subscriptions/$subId\" # Resource group scope az role assignment create --assignee \"rbacuser@example.com\" --role \"Virtual Machine Contributor\" --resource-group ExamRefRG # Subscription scope az role assignment create --assignee \"rbacuser@example.com\" --role \"Owner\" --subscription $subId","title":"Create assignment"},{"location":"Infrastructure/Cloud/Azure/Azure-IAM/#delete-assignment","text":"Navigate to resource group > Access Control (IAM) > Role Assignments tab > Select one or more security principals > Remove Remove RBAC assignments from a user Azure PowerShell Remove-AzRoleAssignment -SignInName \"cloudadmin@opsgility.onmicrosoft.com\" -RoleDefinitionName \"Virtual Machine Contributor\" -ResourceGroupName ExamRefRG Remove-AzRoleAssignment -SignInName $u -ResourceGroupName $rgName -RoleDefinitionName \"Virtual Machine Contributor\" Azure AD group $g = Get-AzADGroup -SearchString \"Cloud Admins\" Remove-AzRoleAssignment -ObjectId $g . Id -ResourceGroupName $rg -RoleDefinitionName \"Virtual Machine Contributor\" az role assignment delete --assignee $u --resource-group $rg --role \"Virtual Machine Contributor\" Azure AD group g = $( az ad group list --query \"[?displayName=='Cloud Admins'].objectId\" -o tsv ) az role assignment delete --role \"Virtual Machine Contributor\" -\u2013assignee-object-id $g --resource-group $rg Read assignment Azure PowerShell Azure CLI Get-AzRoleDefinition -Name \"Virtual Machine Contributor\" | ConvertTo-Json az role definition list -n \"Virtual Machine Contributor\" List custom roles available for assignment Azure PowerShell Azure CLI Get-AzRoleDefinition | Where-Object { $_ . IsCustom -eq $true } az role definition list --custom-role-only -o table View all role assignments in a subscription az role assignment list --all Create role definition Azure PowerShell New-AzRoleDefinition -InputFile \"C:\\ARM_templates\\customrole1.json\"","title":"Delete assignment"},{"location":"Infrastructure/Cloud/Azure/Azure-IAM/#configure-cost-center-quotas-and-tagging","text":"Grant an AD group RBAC rights Azure PowerShell $group = Get-AzADGroup -SearchString \"Cloud Admins\" New-AzRoleAssignment -ObjectId $group . Id -RoleDefinitionName \"Virtual Machine Contributor\" -ResourceGroupName ExamRefRG Remove RBAC assignments from a group Azure PowerShell $adGroup = Get-AzADGRoup -SearchString \"Cloud Admins\" Remove-AzRoleAssignment -ResourceGroupName $rgName -ObjectId $adGroup . Id -RoleDefinitionName \"Virtual Machine Contributor\"","title":"Configure cost center quotas and tagging"},{"location":"Infrastructure/Cloud/Azure/Azure-IAM/#elevate-permissions","text":"For Azure AD Global Administrators who want to temporarily elevate permissions Sign into Azure portal as an Azure AD Global Administrator. ? Navigate to Azure Active Directory > Properties . At the bottom of the page, under \"Access management for Azure resources\" click Yes then Save . Sign out and sign in again. Assign roles Revoke elevated access by returning to Azure Active Directory > Properties and selecting No under \"Access management for Azure resources\". Sources Elevating global administrator access Understand Azure role definitions","title":"Elevate permissions"},{"location":"Infrastructure/Cloud/Azure/Azure-IAM/#sspr","text":"Administrator accounts are treated differently from other user accounts for SSPR and have a \"strong default two-gate password reset policy\", which requires two pieces of authentication data and foregoes the use of security questions.","title":"SSPR"},{"location":"Infrastructure/Cloud/Azure/Azure-IaaS/","text":"High availability These high-availability options are mutually exclusive: Availability zones protect from datacenter-level failures by providing physical and logical separation between VM instances. Zones have independent power sources, network, and cooling, and there are at least 3 zones in every region. Availability sets offer redundancy for VMs in the same application tier, ensuring at least one VM is available in the case of a host update or problem. Each VM is assigned a fault domain (representing separate physical racks in the datacenter) and an update domain (representing groups of VMs and underlying physical hardware that can be rebooted at the same time for host updates). Availability sets have a maximum of 20 update domains and 3 fault domains. VM scale sets (VMSS) support the ability to dynamically add and remove instances. By default, a VMSS supports up to 100 instances or up to 1000 instances if deployed with the property singlePlacementGroup set to false (called a large-scale set ). A placement group is a concept similar to an availability set in that it assigns fault and upgrade domains. By default, a scale set consists of only a single placement group, but disabling this setting allows the scale set to be composed of multiple placement groups. If a custom image is used instead of one in the gallery, the limit is actually 300 instances. Load balancers Load balancers redistribute traffic from a frontend pool to a backend pool using rules. Health probes determine the health of the VMs in the backend pool: if they don't respond then new connections won't be sent. By default, Azure Load Balancer stores rules in a 5-tuple: Source IP address Source port Destination IP address Destination ports IP Protocol number Azure Load Balancers come in 2 pricing tiers: Basic which is free Standard which is charged based on the number of rules and the data that is processed. Both boot and guest OS diagnostics can be enabled on or after VM creation. Azure VMs have built-in extensions that enable configuration management. 2 most common extensions for configuration management: Windows PowerShell Desired State Configuration (DSC) allows you to define the state of a VM using PowerShell Desired State Configuration language A VM may have more than one Network Interface Card (NIC) , but they must belong to the same region as the VM itself. All NICs on a VM must also be attached to the same VNet. Tasks Find Marketplace image Produce the publisher (e.g. \"MicrosoftWindowsServer\") from Location Get-AzVMImagePublisher Produce the offer (e.g. \"WindowsServer\") from Location and PublisherName Get-AzVMImageOffer Produce the Sku from Location and PublisherName and Offer Get-AzVMImageSku Requires PublisherName, Offer, Location, Skus, and Version ( -Version * will produce a list of available version numbers) Get-AzVMImage Deploy from image Azure PowerShell Azure CLI Specify a managed image for use in a new virtual machine $image = Get-AzImage -ImageName $imageName -ResourceGroupName $g $vmConfig = Set-AzVMSourceImage -VM $vmConfig -Id $image . Id Specify a legacy unmanaged image for use in a new virtual machine $osDiskUri = \"https://examrefstorage.blob.core.windows.net/vhd/os-disk\" $imageUri = \"https://examrefstorage.blob.core.windows.net/images/legacy-image.vhd\" $vm = Set-AzVMOSDisk -VM $vm -Name $osDiskName -VhdUri $osDiskUri -CreateOption fromImage -SourceImageUri $imageUri -Windows Specify a managed image for use in a new virtual machine az vm create -g $g -n $vmName --image $imageName Specify a legacy unmanaged image for use in a new virtual machine az vm create -g $g -n $vmName --image $osDiskUri --generate-ssh-keys Access Enable-PSRemoting # If a network connection is Public, this command will not work Enable-PSRemoting -SkipNetworkProfileCheck -Force The remote computer must also have WinRM up and running. Azure can enable PowerShell on the target machine Invoke-AzVMRunCommand -AsJob -ResourceGroupName \"RG\" -VMName \"Socrates\" -CommandId EnableRemotePS Using Azure Cloud PowerShell Enable-AzVMPSRemoting -Name Socrates -ResourceGroupName \"RG\" Add the VM's public IP address $IP to the trusted hosts of the local workstation (must be run as administrator): Set-Item WSMan :\\ localhost \\ Client \\ TrustedHosts -Value $IP Traffic to ports 5985 and 5986 must be allowed through Windows firewall and, if the computer is an Azure VM, the Network Security Group. Enter-PSSession -ComputerName 123 . 47 . 78 . 90 -Credential $cred etsn 123 . 45 . 67 . 89 -Credential ( Get-Credential ) Invoke-AzVMRunCommand -ResourceGroupName RG -VMName VM -CommandId 'RunPowerShellScript' -ScriptPath C :\\ injectedscript . ps1 Open firewall WinRM ports 5985 and 5986 PowerShell netsh New-NetFirewallRule -DisplayName \"WinRMHTTP\" -Direction Inbound -LocalPort 5985 -Protocol TCP -Action Allow New-NetFirewallRule -DisplayName \"WinRMHTTPS\" -Direction Inbound -LocalPort 5986 -Protocol TCP -Action Allow netsh advfirewall firewall add rule name=WinRMHTTP dir=in action=allow protocol=TCP localport=5985 netsh advfirewall firewall add rule name=WinRMHTTPS dir=in action=allow protocol=TCP localport=5986 RDP Saving the .rdp file for later use $g = \"ExamRefRG\" $vmName = \"ExamRefVM\" $Path = \"C:\\path\\to\\ExamRefVM.rdp\" Get-AzRemoteDesktopFile -ResourceGroupName $g -Name $vmName -LocalPath $path Backup $t = Get-AzRecoveryServicesVault -Name t1 Set-AzRecoveryServicesBackupProperties -Vault $t -BackupStorageRedundancy GeoRedundant SSH $VirtualMachine = Get-AzVM -ResourceGroupName \"ResourceGroup11\" -Name \"VirtualMachine07\" Add-AzVMSshPublicKey -VM $VirtualMachine -KeyData \"MIIDszCCApugAwIBAgIJALBV9YJCF/tAMA0GCSq12Ib3DQEB21QUAMEUxCzAJBgNV\" -Path \"/home/admin/.ssh/authorized_keys\" Resize VM PowerShell Azure CLI $vm = Get-AzVM -ResourceGroupName $g -VMName $n $vm . HardwareProfile . VMSize = \"Standard_DS2_V2\" Update-AzVM -VM $vm -ResourceGroupName $g az vm list-vm-resize-options --resource-group $g --name $vmName --output table az vm resize --resource-group $g --name $vmName --size Standard_DS3_v2 Windows Server Core PowerShell Azure CLI Simply: New-AzVM -ResourceGroupName \"RG\" -Name \"VM\" -Location \"EastUS\" -Size \"Standard-B2s\" -Credential ( Get-Credential ) New-AzVM Greeks Socrates $vm The VM's NIC has to be linked to an NSG , a Subnet , and a Public IP Address . # Create a VNet with a subnet $subnet = New-AzVirtualNetworkSubnetConfig -Name \"subnet1\" -AddressPrefix \"10.0.0.0/24\" $vnet = New-AzVirtualNetwork -Name \"vnet\" -ResourceGroupName \"RG\" -Location \"East US\" -AddressPrefix \"10.0.0.0/16\" -Subnet $subnet # Create a Network Interface from the IP and VNet $ip = New-AzPublicIpAddress -Name \"wscore-ip\" -ResourceGroupName \"RG\" -Location \"East US\" -AllocationMethod Dynamic $nic = New-AzNetworkInterface -Name \"wscore-nic\" -ResourceGroupName \"RG\" -Location \"East US\" -SubnetId $vnet . Subnets [ 0 ]. Id -PublicIpAddressId $pip . Id -NetworkSecurityGroupId $nsg . Id $vm = New-AzVMConfig -VMName \"Socrates\" -VMSize \"Standard_B1s\" Set-AzVMOperatingSystem -VM $vm -Windows -ComputerName Socrates -Credential $aztestadmin Set-AzVMSourceImage -VM $vm -PublisherName \"MicrosoftWindowsServer\" -Offer \"WindowsServer\" -Skus \"2016-Datacenter-Server-Core\" -Version 2016 . 127 . 20190603 Set-AzVMOSDisk -VM $vm -CreateOption fromImage Add-AzVMNetworkInterface -VM $vm -NetworkInterface $nic # No `-Name`, since we set `-VMName` when initializing the PSVirtualMachine object with `New-AzVMConfig` New-AzVM -AsJob -VM $vm -Location \"East US\" -ResourceGroupName \"RG\" az vm create -n Socrates -g RG -l \"East US\" --image \"MicrosoftWindowsServer:WindowsServer:2016-Datacenter-Server-Core:2016.127.20190603\" --admin-username aztestadmin --admin-password $password --nics socrates-nic Display status of VMs Azure CLI az vm list --resource-group $RESOURCEGROUP --output table Enable IIS using the Custom Script Extension to run a PowerShell script on the VM. Azure CLI az vm extension set --resource-group $RESOURCEGROUP --vm-name SimpleWinVM --name CustomScriptExtension --publisher Microsoft.Compute --version 1 .9 --settings '{\"fileUris\":[\"https://raw.githubusercontent.com/MicrosoftDocs/mslearn-welcome-to-azure/master/configure-iis.ps1\"]}' --protected-settings '{\"commandToExecute\": \"powershell -ExecutionPolicy Unrestricted -File configure-iis.ps1\"}' Source Add NIC PowerShell Azure CLI Stop-AzVM -Name $vmName -ResourceGroupName $g Add-AzVMNetworkInterface -VM $vm -Id $nicId -Primary Update-AzVm -ResourceGroupName $g -VM $vm az network nic create --resource-group $g --name $nicName --vnet-name $ExamRefVNET --subnet $subnetName az vm nic add -g $g --vm-name $vmName --nics $nicName --primary-nic Redeploy PowerShell Azure CLI Set-AzVM -Redeploy -ResourceGroupName ExamRefRG -Name ExamRefVM az vm redeploy --resource-group ExamRefRG --name ExamRefVM Create managed VM PowerShell Azure CLI Set-AzVM -ResourceGroupName $g -Name $vmName -Generalized $vm = Get-AzVM -ResourceGroupName $g -Name $vmName $image = New-AzImageConfig -Location $location -SourceVirtualMachineId $vm . ID New-AzImage -Image $image -ImageName $imageName -ResourceGroupName $g az vm deallocate --resource-group $g --name $vmName az vm generalize --resource-group $g --name $vmName az image create --resource-group $g --name $imageName --source $vmName Create $subnets = @() $subnet1Name = \"Subnet1\" $subnet2Name = \"Subnet2\" $subnet1AddressPrefix = \"10.0.0.0/24\" $subnet2AddressPrefix = \"10.0.1.0/24\" $vnetAddressSpace = \"10.0.0.0/16\" $vnetName = \"ExamRefVNET\" New-AzResourceGroup -Name $resourceGroupName -Location $location Create a virtual network $subnets = @() $subnets += New-AzVirtualNetworkSubnetConfig -Name $subnet1Name -AddressPrefix $subnet1AddressPrefix $subnets += New-AzVirtualNetworkSubnetConfig -Name $subnet2Name -AddressPrefix $subnet2AddressPrefix $vnet = New-AzVirtualNetwork -Name $vnetName -Location $location -AddressPrefix $vnetAddressSpace -Subnet $subnets $pip = New-AzPublicIpAddress -Name $ipName -ResourceGroupName $g -Location $location -AllocationMethod Dynamic -DomainNameLabel $dnsName $nsgRules = @() $nsgRules += New-AzNetworkSecurityRuleConfig -Name \"RDP\" -Description \"RemoteDesktop\" -Protocol Tcp -SourcePortRange \"*\" -DestinationPortRange \"3389\" -SourceAddressPrefix \"*\" -DestinationAddressPrefix \"*\" -Access Allow -Priority 110 -Direction Inbound $nsg = New-AzNetworkSecurityGroup -ResourceGroupName $resourceGroupName -Name \"ExamREfNSG\" -SecurityRules $nsgRules -Location $location $nic = New-AzNetworkInterface -Name $nicNAme -ResourceGroupName $resourceGroupName -Location $location -SubnetId $vnet . Subnets [ 0 ]. Id -PublicIpAddressId $pip . Id -NetworkSecurityGroupId $nsg . Id Add-AzVMNetworkInterface -VM $vm -NetworkInterface $nic $vm = New-AzVMConfig -VMName $vmName -VMSize $vmSize Set-AzVMOperatingSystem -Windows -ComputerName $vmName -Credential $cred -ProvisionVMAgent -VM $vm Set-AzVMSourceImage -PublisherName $pubName -Offer $offerName -Skus $skuName -Version \"latest\" -VM $vm Set-AzVMOSDisk -CreateOption fromImage -VM $vm New-AzVM -ResourceGroupName $resourceGroupName -Location $location -VM $vm az group create --name $g --location $location vmName = \"myUbuntuVM\" imageName = \"UbuntuLTS\" az vm create --resource-group $g --name $vmName --image $imageName --generate-ssh-keys Create a virtual network vnetName = \"ExamRefVNET\" vnetAddressPrefix = \"10.0.0.0/16\" az network vnet create --resource-group $g -n ExamRefVNET --address-prefixes $vnetAddressPrefix -l $location dnsRecord = \"examrefdns123123\" ipName = \"ExamRefIP\" az network public-ip create -n $ipName -g $g --allocation-method Dynamic --dns-name $dnsRecord -l $location nsgName = \"webnsg\" az network nsg create -n $nsgName -g $g -l $location Create a NSG rules to allow inbound SSH and HTTP az network nsg rule create -n SSH --nsg-name ... --priority 100 -g ... --access Allow --description \"SSH Access\" --direction Inbound --protocol Tcp --destination-address-prefix \"*\" --destination-port-range 22 --source-address-prefix \"*\" --source-port-range \"*\" az network nsg rule create -n HTTP --nsg-name ... --priority 101 -g ... --access Allow --description \"Web Access\" --direction Inbound --protocol Tcp --destination-address-prefix \"*\" --destination-port-range 80 --source-address-prefix \"*\" --source-port-range \"* # Create a network interface nicname = \"WebVMNic1\" az network nic create -n $nicname -g $g --subnet $Subnet1Name --network-security-group $nsgName --vnet-name $vnetName --public-ip-address $ipName -l $location # Retrieve a list of marketplace images az vm image list --all # Retrieve form factors available in each region az vm list-sizes --location ... # Create the VM imageName = \"Canonical:UbuntuServer:16.04-LTS:latest\" vmSize = \"Standard_DS1_V2\" user = demouser vmName = \"WebVM\" az vm create -n $vmName -g $g -l $location --size $vmSize --nics $nicname --image $imageName --generate-ssh-keys VHD Add a new disk to a VM PowerShell Azure CLI $dataDiskName = \"MyDataDisk\" $location = \"WestUS\" $diskConfig = New-AzDiskConfig -SkuName Premium_LRS -Location $location -CreateOption Empty -DiskSizeGB 128 $dataDisk1 = New-AzDisk -DiskName $dataDiskName -Disk $diskConfig -ResourceGroupNAme ExamRefRG $vm = Get-AzVM -Name ExamRefVM -ResourceGroupName ExamRefRG $vm = Add-AzVMDataDisk -VM $vm -Name $dataDiskName -CreateOption Attach -ManagedDiskId $dataDisk1 . Id -Lun 1 Update-AzVM -VM $vm -ResourceGroupName ExamRefRG az vm disk attach -g ExamRefRG --vm-name ExamRefVM --name myDataDisk --new --size-gb 128 --sku Premium_LRS Modify host cache setting PowerShell Azure CLI $vm = Get-AzVM -ResourceGroupName $g -Name $vmName Set-AzVMDataDisk -VM $vm -Lun 0 -Caching ReadOnly Update-AzVM -ResourceGroupName $g -VM $vm az vm disk attach --vm-name $vmName --resource-group $g --size-gb 128 --disk $diskName --caching ReadWrite -new az vm unmanaged-disk attach Diagnostics Enable on deployment Enable after deployment Set-AzVmDiagnosticsExtension Enable diagnostics using a storage account specified in a XML configuration file Set-AzVMDiagnosticsExtension -ResourceGroupName \"ResourceGroup01\" -VMName \"VirtualMachine02\" -DiagnosticsConfigurationPath \"diagnostics_publicconfig.xml\" Providing storage account name absent in config, or overriding it if present Set-AzVMDiagnosticsExtension -ResourceGroupName \"ResourceGroup1\" -VMName \"VirtualMachine2\" -DiagnosticsConfigurationPath diagnostics_publicconfig . xml -StorageAccountName \"MyStorageAccount\" Explicitly providing storage account name and key Set-AzVMDiagnosticsExtension -ResourceGroupName \"ResourceGroup01\" -VMName \"VirtualMachine02\" -DiagnosticsConfigurationPath \"diagnostics_publicconfig.xml\" -StorageAccountName \"MyStorageAccount\" -StorageAccountKey $storage_key ARM templates Deploy a named ARM template PowerShell Azure CLI New-AzResourceGroupDeployment -Mode Complete -Name simpleVMDeployment -ResourceGroupName ExamRefRG az group deployment create --name simpleVMDeployment --mode Complete --resource-group ExamRefRG Export a resource group to an ARM template PowerShell Azure CLI Save-AzResourceGroupDeploymentTemplate -ResourceGroupName ExamRefRG -DeploymentName simpleVMDeployment az group deployment export --name simpleVMDeployment --resource-group ExamRefRG Create from existing resource group Export-AzResourceGroup -ResourceGroupName ExamRefRG Pass a template file during deployment New-AzResourceGroupDeployment -Name MyDeployment -ResourceGroupName ExamRefRG -TemplateFile C :\\ MyTemplates \\ AppTemplate . json az group export --name ExamRefRG az group deployment create --name MyDeployment --resource-group ExamRefRG --template-file AppTemplate.json --parameters @dev-env.json View all available sizes in a location Move a resource to another resource group or subscription (PowerShell) $resourceID = Get-AzResource -ResourceGroupName ExamRefRG | Format-Table -Property ResourceId Move-AzResource -DestinationResourceGroupName ExamRefDestRG -ResourceId $resourceID Move-AzResource -DestinationSubscriptionId $subscriptionID -DestinationResourceGroupName ExamRefDestRG -ResourceId $resourceID Move a resource to another resource group or subscription (Azure CLI) az resource list -g ExamRefRG az resource move --destination-group ExamRefDestRG --ids $resourceID az resource move --destination-group ExamrefDestRG --destination-subscription-id $subscriptionID --ids $resourceID DSC Package Package a DSC script into a zip file Publish-AzVMDscConfiguration -ConfigurationPath .\\ ContosoWeb . ps1 -OutputArchivePath .\\ ContosoWeb . zip Apply Publish a packaged DSC script to a storage account $g = \"ExamRefRG\" $location =- \"WestUS\" $vmName = \"ExamRefVM\" $storageName = \"dscstorageer1\" $configurationName = \"Main\" $archiveBlob = \"ContosoWeb.ps1.zip\" $configurationPath = \".\\ContosoWeb.ps1\" Publish-AzVMDscConfiguration -ConfigurationPath $configurationPath -ResourceGroupName $g -StorageAccountName $storageName Set-AzVmDscExtension -Version 2 . 76 -ResourceGroupName $g -VMName $vmName -ArchiveStorageAccountNAme $storageName -ArchiveBlobName $archiveBNlob -AutoUpdate : $false -ConfigurationName $configurationName VMSS Create a VMSS with IIS installed from a custom script extension $g = \"ExamRefRG\" $location = \"WestUS\" $vmSize = \"Standard_DS2_V2\" $capacity = 2 New-AzResourceGroup -Name $g -Location $location $vmssConfig = New-AzVmssConfig -Location $location -SkuCapacity $capacity -SkuName $vmSize -UpgradePolicyMode Automatic $publicIP = New-AzPublicIpAddress -ResourceGroupName $g -Location $locaiton -AllocationMethod Static -Name $publicIPName $frontendIP = New-AzLoadBalancerFrontendIpConfig -Name \"lbFrontEndPool\" -PublicIpAddress $publicIP $backendPool = New-AzLoadBalancerBackendAddressPoolConfig -Name \"lbBackEndPool\" $lb = New-AzLoadBalancer -ResourceGroupName $g -Name \"lbrule\" -Location $location -FrontendIPConfiguration $frontendIP -BackendAddressPool $backendPool Add-AzLoadBalancerProbeConfig -Name \"lbrule\" -LoadBalancer $lb -Protocol http -Port 80 -IntervalInSeconds 15 -ProbeCount 2 -RequestPath \"/\" Set-AzLoadBalancer -LoadBalancer $lb Reference a VM image from the gallery Set-AzVmssStorageProfile $vmssConfig -ImageReferencePublisher MicrosoftWindowsServer -ImageReferenceOffer WindowsServer -ImageReferenceSku 2016-Datacenter -ImageReferenceVersion latest -OsDiskCreateOption FromImage Set up information for authenticating with the VM Set-AzVmssOsProfile $vmssConfig -AdminUsername \"azureuser\" -AdminPassword \"P@ssword!\" -ComputerNamePrefix \"ssVM\" Create VNet resources $subnet = New-AzVirtualNetworkSubnetConfig -Name \"web\" -AddressPrefix 10 . 0 . 0 . 0 / 24 $vnet = New-AzVirtualNetwork -ResourceGroupName $g -Name $ssName -Location $location -AddressPrefix 10 . 0 . 0 . 0 / 16 -Subnet $subnet $ipConfig = New-AzVmssIpConfig -Name \"vmssIPConfig\" -LoadBalancerBackendAddressPoolsId $lb . BackendAddressPools [ 0 ]. Id -SubnetId $vnet . Subnets [ 0 ]. Id Attach the VNet to the config object Add-AzVmssNetworkInterfaceConfiguration -VirtualMachineScaleSet $vmssConfig -Name \"network-config\" -Primary $true -IPConfiguration $ipConfig Create the scale set with the config object New-AzVmss -ResourceGroupName $g -Name $scaleSetName -VirtualMachineScaleSet $vmssConfig g = \"ExamRefRG\" ssName = \"erscaleset\" userName = \"azureuser\" password = \"P@ssword!\" vmPrefix = \"ssVM\" location = \"WestUS\" az group create --name $g --location $location az vmss create --resource-group $g --name $ssName --image UbuntuLTS --authentication-type password --admin-username $userName --admin-password $password Custom script extension Use the custom script extension to install packages and Windows features and roles to VMs # Deploy the Active Directory Domain Services role Install-WindowsFeature -Name \"AD-Domain-Services\" -IncludeManagementTools -IncludeAllSubFeature Install-ADDSForest -DomainName $domain -DomainMode Win2012 -ForestMode Win2012 -Force -SafeModeAdministratorPassword $smPassword # Use `Set-AzVMCustomScriptExtension` to run script on a VM $vmName = \"ExamRefVM\" $scriptName = \"deploy-ad.ps1\" $domain = \"contoso.com\" $extensionName = \"installAD\" $location = \"WestUS\" $scriptUri = \"https://raw.githubusercontent.com/opsgility/lab-support-public/master/script-extensions/deploy-ad.ps1\" $scriptArgument = \"contoso.com $password\" Set-AzVMCustomScriptExtension -ResourceGroupName $g -VMName $vmName -FileUri $scriptUri -Argument \"$domain $password\" -Run $scriptName -Name $extensionName -Location $location vmName = \"LinuxVM\" extensionName = \"InstallApache\" az vm extension set --resource-group $g --vm-name $vmName --name customScript --publisher Microsoft.Azure.Extensions --protected-settings ./cseconfig.json Enable IIS using the Custom Script Extension to run a PowerShell script on the VM. ? az vm extension set --resource-group $RESOURCEGROUP --vm-name SimpleWinVM --name CustomScriptExtension --publisher Microsoft.Compute --version 1 .9 --settings '{\"fileUris\":[\"https://raw.githubusercontent.com/MicrosoftDocs/mslearn-welcome-to-azure/master/configure-iis.ps1\"]}' --protected-settings '{\"commandToExecute\": \"powershell -ExecutionPolicy Unrestricted -File configure-iis.ps1\"}' Create availability set Azure PowerShell Azure CLI New-AzavailabilitySet -ResourceGroupName $g -Name $n -Location $l -PlatformUpdateDomainCount 10 -PlatformFaultDomainCount 3 -Sku \"Aligned\" az vm availability-set create -n $n -g $g --platform-fault-domain-count 3 --platform-update-domain-count 10 Invoke a command on a VM Run a shell script, $script can be supplied inline az vm run-command invoke --command-id RunShellScript --scripts $script Parameters can be passed to the script argument az vm run-command invoke --command-id RunShellScript --scripts 'echo $1 $2' --parameters hello world Run a PowerShell script az vm run-command invoke --command-id RunPowerShellScript -n Socrates -g RG Run a script file az vm run-command invoke --command-id RunPowerShellScript -n Socrates -g RG --scripts @script.ps1 --parameters 'arg1=somefoo' 'arg2=somebar' Available values for command-id can be enumerated: az vm run-command list Dedicated host","title":"Azure IaaS"},{"location":"Infrastructure/Cloud/Azure/Azure-IaaS/#high-availability","text":"These high-availability options are mutually exclusive: Availability zones protect from datacenter-level failures by providing physical and logical separation between VM instances. Zones have independent power sources, network, and cooling, and there are at least 3 zones in every region. Availability sets offer redundancy for VMs in the same application tier, ensuring at least one VM is available in the case of a host update or problem. Each VM is assigned a fault domain (representing separate physical racks in the datacenter) and an update domain (representing groups of VMs and underlying physical hardware that can be rebooted at the same time for host updates). Availability sets have a maximum of 20 update domains and 3 fault domains. VM scale sets (VMSS) support the ability to dynamically add and remove instances. By default, a VMSS supports up to 100 instances or up to 1000 instances if deployed with the property singlePlacementGroup set to false (called a large-scale set ). A placement group is a concept similar to an availability set in that it assigns fault and upgrade domains. By default, a scale set consists of only a single placement group, but disabling this setting allows the scale set to be composed of multiple placement groups. If a custom image is used instead of one in the gallery, the limit is actually 300 instances.","title":"High availability"},{"location":"Infrastructure/Cloud/Azure/Azure-IaaS/#load-balancers","text":"Load balancers redistribute traffic from a frontend pool to a backend pool using rules. Health probes determine the health of the VMs in the backend pool: if they don't respond then new connections won't be sent. By default, Azure Load Balancer stores rules in a 5-tuple: Source IP address Source port Destination IP address Destination ports IP Protocol number Azure Load Balancers come in 2 pricing tiers: Basic which is free Standard which is charged based on the number of rules and the data that is processed. Both boot and guest OS diagnostics can be enabled on or after VM creation. Azure VMs have built-in extensions that enable configuration management. 2 most common extensions for configuration management: Windows PowerShell Desired State Configuration (DSC) allows you to define the state of a VM using PowerShell Desired State Configuration language A VM may have more than one Network Interface Card (NIC) , but they must belong to the same region as the VM itself. All NICs on a VM must also be attached to the same VNet.","title":"Load balancers"},{"location":"Infrastructure/Cloud/Azure/Azure-IaaS/#tasks","text":"","title":"Tasks"},{"location":"Infrastructure/Cloud/Azure/Azure-IaaS/#find-marketplace-image","text":"Produce the publisher (e.g. \"MicrosoftWindowsServer\") from Location Get-AzVMImagePublisher Produce the offer (e.g. \"WindowsServer\") from Location and PublisherName Get-AzVMImageOffer Produce the Sku from Location and PublisherName and Offer Get-AzVMImageSku Requires PublisherName, Offer, Location, Skus, and Version ( -Version * will produce a list of available version numbers) Get-AzVMImage Deploy from image Azure PowerShell Azure CLI Specify a managed image for use in a new virtual machine $image = Get-AzImage -ImageName $imageName -ResourceGroupName $g $vmConfig = Set-AzVMSourceImage -VM $vmConfig -Id $image . Id Specify a legacy unmanaged image for use in a new virtual machine $osDiskUri = \"https://examrefstorage.blob.core.windows.net/vhd/os-disk\" $imageUri = \"https://examrefstorage.blob.core.windows.net/images/legacy-image.vhd\" $vm = Set-AzVMOSDisk -VM $vm -Name $osDiskName -VhdUri $osDiskUri -CreateOption fromImage -SourceImageUri $imageUri -Windows Specify a managed image for use in a new virtual machine az vm create -g $g -n $vmName --image $imageName Specify a legacy unmanaged image for use in a new virtual machine az vm create -g $g -n $vmName --image $osDiskUri --generate-ssh-keys","title":"Find Marketplace image"},{"location":"Infrastructure/Cloud/Azure/Azure-IaaS/#access","text":"Enable-PSRemoting # If a network connection is Public, this command will not work Enable-PSRemoting -SkipNetworkProfileCheck -Force The remote computer must also have WinRM up and running. Azure can enable PowerShell on the target machine Invoke-AzVMRunCommand -AsJob -ResourceGroupName \"RG\" -VMName \"Socrates\" -CommandId EnableRemotePS Using Azure Cloud PowerShell Enable-AzVMPSRemoting -Name Socrates -ResourceGroupName \"RG\" Add the VM's public IP address $IP to the trusted hosts of the local workstation (must be run as administrator): Set-Item WSMan :\\ localhost \\ Client \\ TrustedHosts -Value $IP Traffic to ports 5985 and 5986 must be allowed through Windows firewall and, if the computer is an Azure VM, the Network Security Group. Enter-PSSession -ComputerName 123 . 47 . 78 . 90 -Credential $cred etsn 123 . 45 . 67 . 89 -Credential ( Get-Credential ) Invoke-AzVMRunCommand -ResourceGroupName RG -VMName VM -CommandId 'RunPowerShellScript' -ScriptPath C :\\ injectedscript . ps1 Open firewall WinRM ports 5985 and 5986 PowerShell netsh New-NetFirewallRule -DisplayName \"WinRMHTTP\" -Direction Inbound -LocalPort 5985 -Protocol TCP -Action Allow New-NetFirewallRule -DisplayName \"WinRMHTTPS\" -Direction Inbound -LocalPort 5986 -Protocol TCP -Action Allow netsh advfirewall firewall add rule name=WinRMHTTP dir=in action=allow protocol=TCP localport=5985 netsh advfirewall firewall add rule name=WinRMHTTPS dir=in action=allow protocol=TCP localport=5986","title":"Access"},{"location":"Infrastructure/Cloud/Azure/Azure-IaaS/#rdp","text":"Saving the .rdp file for later use $g = \"ExamRefRG\" $vmName = \"ExamRefVM\" $Path = \"C:\\path\\to\\ExamRefVM.rdp\" Get-AzRemoteDesktopFile -ResourceGroupName $g -Name $vmName -LocalPath $path","title":"RDP"},{"location":"Infrastructure/Cloud/Azure/Azure-IaaS/#backup","text":"$t = Get-AzRecoveryServicesVault -Name t1 Set-AzRecoveryServicesBackupProperties -Vault $t -BackupStorageRedundancy GeoRedundant","title":"Backup"},{"location":"Infrastructure/Cloud/Azure/Azure-IaaS/#ssh","text":"$VirtualMachine = Get-AzVM -ResourceGroupName \"ResourceGroup11\" -Name \"VirtualMachine07\" Add-AzVMSshPublicKey -VM $VirtualMachine -KeyData \"MIIDszCCApugAwIBAgIJALBV9YJCF/tAMA0GCSq12Ib3DQEB21QUAMEUxCzAJBgNV\" -Path \"/home/admin/.ssh/authorized_keys\" Resize VM PowerShell Azure CLI $vm = Get-AzVM -ResourceGroupName $g -VMName $n $vm . HardwareProfile . VMSize = \"Standard_DS2_V2\" Update-AzVM -VM $vm -ResourceGroupName $g az vm list-vm-resize-options --resource-group $g --name $vmName --output table az vm resize --resource-group $g --name $vmName --size Standard_DS3_v2","title":"SSH"},{"location":"Infrastructure/Cloud/Azure/Azure-IaaS/#windows-server-core","text":"PowerShell Azure CLI Simply: New-AzVM -ResourceGroupName \"RG\" -Name \"VM\" -Location \"EastUS\" -Size \"Standard-B2s\" -Credential ( Get-Credential ) New-AzVM Greeks Socrates $vm The VM's NIC has to be linked to an NSG , a Subnet , and a Public IP Address . # Create a VNet with a subnet $subnet = New-AzVirtualNetworkSubnetConfig -Name \"subnet1\" -AddressPrefix \"10.0.0.0/24\" $vnet = New-AzVirtualNetwork -Name \"vnet\" -ResourceGroupName \"RG\" -Location \"East US\" -AddressPrefix \"10.0.0.0/16\" -Subnet $subnet # Create a Network Interface from the IP and VNet $ip = New-AzPublicIpAddress -Name \"wscore-ip\" -ResourceGroupName \"RG\" -Location \"East US\" -AllocationMethod Dynamic $nic = New-AzNetworkInterface -Name \"wscore-nic\" -ResourceGroupName \"RG\" -Location \"East US\" -SubnetId $vnet . Subnets [ 0 ]. Id -PublicIpAddressId $pip . Id -NetworkSecurityGroupId $nsg . Id $vm = New-AzVMConfig -VMName \"Socrates\" -VMSize \"Standard_B1s\" Set-AzVMOperatingSystem -VM $vm -Windows -ComputerName Socrates -Credential $aztestadmin Set-AzVMSourceImage -VM $vm -PublisherName \"MicrosoftWindowsServer\" -Offer \"WindowsServer\" -Skus \"2016-Datacenter-Server-Core\" -Version 2016 . 127 . 20190603 Set-AzVMOSDisk -VM $vm -CreateOption fromImage Add-AzVMNetworkInterface -VM $vm -NetworkInterface $nic # No `-Name`, since we set `-VMName` when initializing the PSVirtualMachine object with `New-AzVMConfig` New-AzVM -AsJob -VM $vm -Location \"East US\" -ResourceGroupName \"RG\" az vm create -n Socrates -g RG -l \"East US\" --image \"MicrosoftWindowsServer:WindowsServer:2016-Datacenter-Server-Core:2016.127.20190603\" --admin-username aztestadmin --admin-password $password --nics socrates-nic Display status of VMs Azure CLI az vm list --resource-group $RESOURCEGROUP --output table Enable IIS using the Custom Script Extension to run a PowerShell script on the VM. Azure CLI az vm extension set --resource-group $RESOURCEGROUP --vm-name SimpleWinVM --name CustomScriptExtension --publisher Microsoft.Compute --version 1 .9 --settings '{\"fileUris\":[\"https://raw.githubusercontent.com/MicrosoftDocs/mslearn-welcome-to-azure/master/configure-iis.ps1\"]}' --protected-settings '{\"commandToExecute\": \"powershell -ExecutionPolicy Unrestricted -File configure-iis.ps1\"}' Source","title":"Windows Server Core"},{"location":"Infrastructure/Cloud/Azure/Azure-IaaS/#add-nic","text":"PowerShell Azure CLI Stop-AzVM -Name $vmName -ResourceGroupName $g Add-AzVMNetworkInterface -VM $vm -Id $nicId -Primary Update-AzVm -ResourceGroupName $g -VM $vm az network nic create --resource-group $g --name $nicName --vnet-name $ExamRefVNET --subnet $subnetName az vm nic add -g $g --vm-name $vmName --nics $nicName --primary-nic Redeploy PowerShell Azure CLI Set-AzVM -Redeploy -ResourceGroupName ExamRefRG -Name ExamRefVM az vm redeploy --resource-group ExamRefRG --name ExamRefVM Create managed VM PowerShell Azure CLI Set-AzVM -ResourceGroupName $g -Name $vmName -Generalized $vm = Get-AzVM -ResourceGroupName $g -Name $vmName $image = New-AzImageConfig -Location $location -SourceVirtualMachineId $vm . ID New-AzImage -Image $image -ImageName $imageName -ResourceGroupName $g az vm deallocate --resource-group $g --name $vmName az vm generalize --resource-group $g --name $vmName az image create --resource-group $g --name $imageName --source $vmName","title":"Add NIC"},{"location":"Infrastructure/Cloud/Azure/Azure-IaaS/#create","text":"$subnets = @() $subnet1Name = \"Subnet1\" $subnet2Name = \"Subnet2\" $subnet1AddressPrefix = \"10.0.0.0/24\" $subnet2AddressPrefix = \"10.0.1.0/24\" $vnetAddressSpace = \"10.0.0.0/16\" $vnetName = \"ExamRefVNET\" New-AzResourceGroup -Name $resourceGroupName -Location $location Create a virtual network $subnets = @() $subnets += New-AzVirtualNetworkSubnetConfig -Name $subnet1Name -AddressPrefix $subnet1AddressPrefix $subnets += New-AzVirtualNetworkSubnetConfig -Name $subnet2Name -AddressPrefix $subnet2AddressPrefix $vnet = New-AzVirtualNetwork -Name $vnetName -Location $location -AddressPrefix $vnetAddressSpace -Subnet $subnets $pip = New-AzPublicIpAddress -Name $ipName -ResourceGroupName $g -Location $location -AllocationMethod Dynamic -DomainNameLabel $dnsName $nsgRules = @() $nsgRules += New-AzNetworkSecurityRuleConfig -Name \"RDP\" -Description \"RemoteDesktop\" -Protocol Tcp -SourcePortRange \"*\" -DestinationPortRange \"3389\" -SourceAddressPrefix \"*\" -DestinationAddressPrefix \"*\" -Access Allow -Priority 110 -Direction Inbound $nsg = New-AzNetworkSecurityGroup -ResourceGroupName $resourceGroupName -Name \"ExamREfNSG\" -SecurityRules $nsgRules -Location $location $nic = New-AzNetworkInterface -Name $nicNAme -ResourceGroupName $resourceGroupName -Location $location -SubnetId $vnet . Subnets [ 0 ]. Id -PublicIpAddressId $pip . Id -NetworkSecurityGroupId $nsg . Id Add-AzVMNetworkInterface -VM $vm -NetworkInterface $nic $vm = New-AzVMConfig -VMName $vmName -VMSize $vmSize Set-AzVMOperatingSystem -Windows -ComputerName $vmName -Credential $cred -ProvisionVMAgent -VM $vm Set-AzVMSourceImage -PublisherName $pubName -Offer $offerName -Skus $skuName -Version \"latest\" -VM $vm Set-AzVMOSDisk -CreateOption fromImage -VM $vm New-AzVM -ResourceGroupName $resourceGroupName -Location $location -VM $vm az group create --name $g --location $location vmName = \"myUbuntuVM\" imageName = \"UbuntuLTS\" az vm create --resource-group $g --name $vmName --image $imageName --generate-ssh-keys Create a virtual network vnetName = \"ExamRefVNET\" vnetAddressPrefix = \"10.0.0.0/16\" az network vnet create --resource-group $g -n ExamRefVNET --address-prefixes $vnetAddressPrefix -l $location dnsRecord = \"examrefdns123123\" ipName = \"ExamRefIP\" az network public-ip create -n $ipName -g $g --allocation-method Dynamic --dns-name $dnsRecord -l $location nsgName = \"webnsg\" az network nsg create -n $nsgName -g $g -l $location Create a NSG rules to allow inbound SSH and HTTP az network nsg rule create -n SSH --nsg-name ... --priority 100 -g ... --access Allow --description \"SSH Access\" --direction Inbound --protocol Tcp --destination-address-prefix \"*\" --destination-port-range 22 --source-address-prefix \"*\" --source-port-range \"*\" az network nsg rule create -n HTTP --nsg-name ... --priority 101 -g ... --access Allow --description \"Web Access\" --direction Inbound --protocol Tcp --destination-address-prefix \"*\" --destination-port-range 80 --source-address-prefix \"*\" --source-port-range \"* # Create a network interface nicname = \"WebVMNic1\" az network nic create -n $nicname -g $g --subnet $Subnet1Name --network-security-group $nsgName --vnet-name $vnetName --public-ip-address $ipName -l $location # Retrieve a list of marketplace images az vm image list --all # Retrieve form factors available in each region az vm list-sizes --location ... # Create the VM imageName = \"Canonical:UbuntuServer:16.04-LTS:latest\" vmSize = \"Standard_DS1_V2\" user = demouser vmName = \"WebVM\" az vm create -n $vmName -g $g -l $location --size $vmSize --nics $nicname --image $imageName --generate-ssh-keys","title":"Create"},{"location":"Infrastructure/Cloud/Azure/Azure-IaaS/#vhd","text":"Add a new disk to a VM PowerShell Azure CLI $dataDiskName = \"MyDataDisk\" $location = \"WestUS\" $diskConfig = New-AzDiskConfig -SkuName Premium_LRS -Location $location -CreateOption Empty -DiskSizeGB 128 $dataDisk1 = New-AzDisk -DiskName $dataDiskName -Disk $diskConfig -ResourceGroupNAme ExamRefRG $vm = Get-AzVM -Name ExamRefVM -ResourceGroupName ExamRefRG $vm = Add-AzVMDataDisk -VM $vm -Name $dataDiskName -CreateOption Attach -ManagedDiskId $dataDisk1 . Id -Lun 1 Update-AzVM -VM $vm -ResourceGroupName ExamRefRG az vm disk attach -g ExamRefRG --vm-name ExamRefVM --name myDataDisk --new --size-gb 128 --sku Premium_LRS Modify host cache setting PowerShell Azure CLI $vm = Get-AzVM -ResourceGroupName $g -Name $vmName Set-AzVMDataDisk -VM $vm -Lun 0 -Caching ReadOnly Update-AzVM -ResourceGroupName $g -VM $vm az vm disk attach --vm-name $vmName --resource-group $g --size-gb 128 --disk $diskName --caching ReadWrite -new az vm unmanaged-disk attach","title":"VHD"},{"location":"Infrastructure/Cloud/Azure/Azure-IaaS/#diagnostics","text":"","title":"Diagnostics"},{"location":"Infrastructure/Cloud/Azure/Azure-IaaS/#enable-on-deployment","text":"","title":"Enable on deployment"},{"location":"Infrastructure/Cloud/Azure/Azure-IaaS/#enable-after-deployment","text":"Set-AzVmDiagnosticsExtension Enable diagnostics using a storage account specified in a XML configuration file Set-AzVMDiagnosticsExtension -ResourceGroupName \"ResourceGroup01\" -VMName \"VirtualMachine02\" -DiagnosticsConfigurationPath \"diagnostics_publicconfig.xml\" Providing storage account name absent in config, or overriding it if present Set-AzVMDiagnosticsExtension -ResourceGroupName \"ResourceGroup1\" -VMName \"VirtualMachine2\" -DiagnosticsConfigurationPath diagnostics_publicconfig . xml -StorageAccountName \"MyStorageAccount\" Explicitly providing storage account name and key Set-AzVMDiagnosticsExtension -ResourceGroupName \"ResourceGroup01\" -VMName \"VirtualMachine02\" -DiagnosticsConfigurationPath \"diagnostics_publicconfig.xml\" -StorageAccountName \"MyStorageAccount\" -StorageAccountKey $storage_key","title":"Enable after deployment"},{"location":"Infrastructure/Cloud/Azure/Azure-IaaS/#arm-templates","text":"Deploy a named ARM template PowerShell Azure CLI New-AzResourceGroupDeployment -Mode Complete -Name simpleVMDeployment -ResourceGroupName ExamRefRG az group deployment create --name simpleVMDeployment --mode Complete --resource-group ExamRefRG Export a resource group to an ARM template PowerShell Azure CLI Save-AzResourceGroupDeploymentTemplate -ResourceGroupName ExamRefRG -DeploymentName simpleVMDeployment az group deployment export --name simpleVMDeployment --resource-group ExamRefRG Create from existing resource group Export-AzResourceGroup -ResourceGroupName ExamRefRG Pass a template file during deployment New-AzResourceGroupDeployment -Name MyDeployment -ResourceGroupName ExamRefRG -TemplateFile C :\\ MyTemplates \\ AppTemplate . json az group export --name ExamRefRG az group deployment create --name MyDeployment --resource-group ExamRefRG --template-file AppTemplate.json --parameters @dev-env.json","title":"ARM templates"},{"location":"Infrastructure/Cloud/Azure/Azure-IaaS/#view-all-available-sizes-in-a-location","text":"Move a resource to another resource group or subscription (PowerShell) $resourceID = Get-AzResource -ResourceGroupName ExamRefRG | Format-Table -Property ResourceId Move-AzResource -DestinationResourceGroupName ExamRefDestRG -ResourceId $resourceID Move-AzResource -DestinationSubscriptionId $subscriptionID -DestinationResourceGroupName ExamRefDestRG -ResourceId $resourceID Move a resource to another resource group or subscription (Azure CLI) az resource list -g ExamRefRG az resource move --destination-group ExamRefDestRG --ids $resourceID az resource move --destination-group ExamrefDestRG --destination-subscription-id $subscriptionID --ids $resourceID","title":"View all available sizes in a location"},{"location":"Infrastructure/Cloud/Azure/Azure-IaaS/#dsc","text":"","title":"DSC"},{"location":"Infrastructure/Cloud/Azure/Azure-IaaS/#package","text":"Package a DSC script into a zip file Publish-AzVMDscConfiguration -ConfigurationPath .\\ ContosoWeb . ps1 -OutputArchivePath .\\ ContosoWeb . zip","title":"Package"},{"location":"Infrastructure/Cloud/Azure/Azure-IaaS/#apply","text":"Publish a packaged DSC script to a storage account $g = \"ExamRefRG\" $location =- \"WestUS\" $vmName = \"ExamRefVM\" $storageName = \"dscstorageer1\" $configurationName = \"Main\" $archiveBlob = \"ContosoWeb.ps1.zip\" $configurationPath = \".\\ContosoWeb.ps1\" Publish-AzVMDscConfiguration -ConfigurationPath $configurationPath -ResourceGroupName $g -StorageAccountName $storageName Set-AzVmDscExtension -Version 2 . 76 -ResourceGroupName $g -VMName $vmName -ArchiveStorageAccountNAme $storageName -ArchiveBlobName $archiveBNlob -AutoUpdate : $false -ConfigurationName $configurationName","title":"Apply"},{"location":"Infrastructure/Cloud/Azure/Azure-IaaS/#vmss","text":"Create a VMSS with IIS installed from a custom script extension $g = \"ExamRefRG\" $location = \"WestUS\" $vmSize = \"Standard_DS2_V2\" $capacity = 2 New-AzResourceGroup -Name $g -Location $location $vmssConfig = New-AzVmssConfig -Location $location -SkuCapacity $capacity -SkuName $vmSize -UpgradePolicyMode Automatic $publicIP = New-AzPublicIpAddress -ResourceGroupName $g -Location $locaiton -AllocationMethod Static -Name $publicIPName $frontendIP = New-AzLoadBalancerFrontendIpConfig -Name \"lbFrontEndPool\" -PublicIpAddress $publicIP $backendPool = New-AzLoadBalancerBackendAddressPoolConfig -Name \"lbBackEndPool\" $lb = New-AzLoadBalancer -ResourceGroupName $g -Name \"lbrule\" -Location $location -FrontendIPConfiguration $frontendIP -BackendAddressPool $backendPool Add-AzLoadBalancerProbeConfig -Name \"lbrule\" -LoadBalancer $lb -Protocol http -Port 80 -IntervalInSeconds 15 -ProbeCount 2 -RequestPath \"/\" Set-AzLoadBalancer -LoadBalancer $lb Reference a VM image from the gallery Set-AzVmssStorageProfile $vmssConfig -ImageReferencePublisher MicrosoftWindowsServer -ImageReferenceOffer WindowsServer -ImageReferenceSku 2016-Datacenter -ImageReferenceVersion latest -OsDiskCreateOption FromImage Set up information for authenticating with the VM Set-AzVmssOsProfile $vmssConfig -AdminUsername \"azureuser\" -AdminPassword \"P@ssword!\" -ComputerNamePrefix \"ssVM\" Create VNet resources $subnet = New-AzVirtualNetworkSubnetConfig -Name \"web\" -AddressPrefix 10 . 0 . 0 . 0 / 24 $vnet = New-AzVirtualNetwork -ResourceGroupName $g -Name $ssName -Location $location -AddressPrefix 10 . 0 . 0 . 0 / 16 -Subnet $subnet $ipConfig = New-AzVmssIpConfig -Name \"vmssIPConfig\" -LoadBalancerBackendAddressPoolsId $lb . BackendAddressPools [ 0 ]. Id -SubnetId $vnet . Subnets [ 0 ]. Id Attach the VNet to the config object Add-AzVmssNetworkInterfaceConfiguration -VirtualMachineScaleSet $vmssConfig -Name \"network-config\" -Primary $true -IPConfiguration $ipConfig Create the scale set with the config object New-AzVmss -ResourceGroupName $g -Name $scaleSetName -VirtualMachineScaleSet $vmssConfig g = \"ExamRefRG\" ssName = \"erscaleset\" userName = \"azureuser\" password = \"P@ssword!\" vmPrefix = \"ssVM\" location = \"WestUS\" az group create --name $g --location $location az vmss create --resource-group $g --name $ssName --image UbuntuLTS --authentication-type password --admin-username $userName --admin-password $password","title":"VMSS"},{"location":"Infrastructure/Cloud/Azure/Azure-IaaS/#custom-script-extension","text":"Use the custom script extension to install packages and Windows features and roles to VMs # Deploy the Active Directory Domain Services role Install-WindowsFeature -Name \"AD-Domain-Services\" -IncludeManagementTools -IncludeAllSubFeature Install-ADDSForest -DomainName $domain -DomainMode Win2012 -ForestMode Win2012 -Force -SafeModeAdministratorPassword $smPassword # Use `Set-AzVMCustomScriptExtension` to run script on a VM $vmName = \"ExamRefVM\" $scriptName = \"deploy-ad.ps1\" $domain = \"contoso.com\" $extensionName = \"installAD\" $location = \"WestUS\" $scriptUri = \"https://raw.githubusercontent.com/opsgility/lab-support-public/master/script-extensions/deploy-ad.ps1\" $scriptArgument = \"contoso.com $password\" Set-AzVMCustomScriptExtension -ResourceGroupName $g -VMName $vmName -FileUri $scriptUri -Argument \"$domain $password\" -Run $scriptName -Name $extensionName -Location $location vmName = \"LinuxVM\" extensionName = \"InstallApache\" az vm extension set --resource-group $g --vm-name $vmName --name customScript --publisher Microsoft.Azure.Extensions --protected-settings ./cseconfig.json Enable IIS using the Custom Script Extension to run a PowerShell script on the VM. ? az vm extension set --resource-group $RESOURCEGROUP --vm-name SimpleWinVM --name CustomScriptExtension --publisher Microsoft.Compute --version 1 .9 --settings '{\"fileUris\":[\"https://raw.githubusercontent.com/MicrosoftDocs/mslearn-welcome-to-azure/master/configure-iis.ps1\"]}' --protected-settings '{\"commandToExecute\": \"powershell -ExecutionPolicy Unrestricted -File configure-iis.ps1\"}' Create availability set Azure PowerShell Azure CLI New-AzavailabilitySet -ResourceGroupName $g -Name $n -Location $l -PlatformUpdateDomainCount 10 -PlatformFaultDomainCount 3 -Sku \"Aligned\" az vm availability-set create -n $n -g $g --platform-fault-domain-count 3 --platform-update-domain-count 10","title":"Custom script extension"},{"location":"Infrastructure/Cloud/Azure/Azure-IaaS/#invoke-a-command-on-a-vm","text":"Run a shell script, $script can be supplied inline az vm run-command invoke --command-id RunShellScript --scripts $script Parameters can be passed to the script argument az vm run-command invoke --command-id RunShellScript --scripts 'echo $1 $2' --parameters hello world Run a PowerShell script az vm run-command invoke --command-id RunPowerShellScript -n Socrates -g RG Run a script file az vm run-command invoke --command-id RunPowerShellScript -n Socrates -g RG --scripts @script.ps1 --parameters 'arg1=somefoo' 'arg2=somebar' Available values for command-id can be enumerated: az vm run-command list","title":"Invoke a command on a VM"},{"location":"Infrastructure/Cloud/Azure/Azure-IaaS/#dedicated-host","text":"","title":"Dedicated host"},{"location":"Infrastructure/Cloud/Azure/Azure-Load-Balancer/","text":"Azure Load Balancers are used to distribute inbound traffic across a pool of backend servers running in a VNet. They are defined by connecting a frontend and backend pool configurations with rules . - Backend Address Pool - Health Probe , specifying port and interval, can be of types TCP, HTTP, and (Standard SKU only) HTTPS - HTTP and HTTPS probes will return an unhealthy status if a HTTP status code other than 200 is received. - Load Balancer Rule defines how a frontend address and port is mapped to the destination port and address on the backend Load Balancers are available in Basic and Standard SKUs. Standard load balancers require a Standard Public IP unless they are intended for internal use only. Standard Public IPs do not allow any inbound communication by default and must have network security rules configured. Only Standard Health Probes support HTTPS. Basic backend SKUs must be comprised of either a single VM, VMs in the same availability set, or a VM scale set. Basic By default, Azure Load Balancer is set to timeout idle TCP connections after 4 minutes, but this can be configured . Azure load balancing rules route based on a 5-tuple hash , calculated from source and destination IP addresses and ports, as well as protocol. This means that traffic from any IP address will typically go to the same backend node, resulting in a modicum of affinity that can be configured . Frontend A frontend is defined by a 3-tuple composed of an IP address, a transport protocol, and a port number. Multiple frontends can be assigned to a load balancer to serve multiple websites or services. There are two modes: Internal, where the frontend references a subnet and an IP address from that subnet is allocated statically or dynamically Public, where a Public IP Address resource is used to receive inbound traffic. If the LB is at Standard SKU, then the IP must also be at Standard. If the backend resources of a load balancer don't have instance-level public IP (ILPIP) addresses, they establish outbound connectivity via the frontend IP of the public load balancer. Backend Any VM can only be a member of the backend pool of a single internal load balancer and simultaneously a single external load balancer. But a VM may not be a member of more than one external load balancer, nor more than one internal load balancer. Basic SKU Backend pools must comprise either a single VM or VMs in the same availability set or scale set . Only Standard SKU backends can accept VMs in a single VNet that are not explicitly assigned to an availability set. Outbound connections Floating IP Floating IP is Azure's term for Direct Server Return (DSR) , which refers to the ability of nodes normally behind a load balancer to respond directly to client requests without overburdening the load balancer with return traffic. This prevents the load balancer from becoming a bottleneck. Technically, Azure Load Balancer always operates in a DSR flow topology even if Floating IP is not enabled, using the VMs' own IP addresses. When enabled, Floating IP changes the IP address mapping to the Frontend IP address of the load balancer. Tasks Create internal load balancer $ip = \"10.0.0.20\" $frontendIP = New-AzLoadBalancerFrontendIpConfig -Name frontend -PrivateIpAddress $ip Add to backend address pool The process in PowerShell, counterintuitively, is actually to modify the VM's NIC to add a reference to the backend pool. AZ-103: p. 365 $vm = Get-AzVM -Name VM1 -ResourceGroupName $g $vmnic = Get-AzNetworkInterface -ResourceGroupName $g | where { $_ . VirtualMachine . Id -eq $vm . Id } $lb = Get-AzLoadBalancer -Name ExamRefLB -ResourceGroupName $g $backend = Get-AzLoadBalancerBackendAddressPoolConfig -Name ExamRefBackEndPool -LoadBalancer $lb # All IP configuration settings of the NIC have to be reapplied, there is no support for incremental changes $ipconfig = Get-AzNetworkInterfaceIpConfig -Name ipconfig1 -NetworkInterface vm1nic Set-AzNetworkInterfaceIpConfig -Name ipconfig1 -NetworkInterface $vm1nic -SubnetId $ipconfig . Subnet . Id -LoadBalancerBackendAddressPoolId $backend . Id Set-AzNetworkInterface -NetworkInterface $vm1nic Azure CLI supports incremental update of the NIC, which makes this command simpler than its PowerShell equivalent. az network nic ip-config address-pool add --resource-group ExamRefRG --address-pool ExamRefBackEndPool --lb-name ExamRefLB --nic-name vm1-nic --ip-config-name ipconfig1 Configure TCP reset TCP timeout values should be greater than that used for TCP keepalives. A new load balancing rule config object can have idle timeout set on declaration. New-AzLoadBalancerRuleConfig -Name \"MyLBRule\" -FrontendIpConfiguration $fe -BackendAddressPool $be -Probe $hp -Protocol TCP -FrontendPort 80 -BackendPort 80 ` -IdleTimeoutInMinutes 15 -EnableTcpReset These can be manually changed on the load balancing object as well $lb = Get-AzLoadBalancer -Name \"myLoadBalancer\" -ResourceGroup \"myResourceGroup\" $lb . LoadBalancingRules [ 0 ]. IdleTimeoutInMinutes = '15' $lb . LoadBalancingRules [ 0 ]. EnableTcpReset = 'true' Set-AzLoadBalancer -LoadBalancer $lb az network lb rule update -g $g -n MyLBRule --lb-name myLoadBalancer --idle-timeout 15 --enable-tcp-reset true Specify affinity In the Azure Portal, affinity is specified in the Session persistence dropdown. In Azure PowerShell and CLI, the option is the load distribution named parameter - New-AzLoadBalancerRuleConfig -LoadDistribution - az network lb rule create --load-distribution Sources: Multiple frontends for Azure Load Balancer SNAT for outbound connections Azure Load Balancer Floating IP configuration AZ-103: p. 358","title":"Azure Load Balancer"},{"location":"Infrastructure/Cloud/Azure/Azure-Load-Balancer/#frontend","text":"A frontend is defined by a 3-tuple composed of an IP address, a transport protocol, and a port number. Multiple frontends can be assigned to a load balancer to serve multiple websites or services. There are two modes: Internal, where the frontend references a subnet and an IP address from that subnet is allocated statically or dynamically Public, where a Public IP Address resource is used to receive inbound traffic. If the LB is at Standard SKU, then the IP must also be at Standard. If the backend resources of a load balancer don't have instance-level public IP (ILPIP) addresses, they establish outbound connectivity via the frontend IP of the public load balancer.","title":"Frontend"},{"location":"Infrastructure/Cloud/Azure/Azure-Load-Balancer/#backend","text":"Any VM can only be a member of the backend pool of a single internal load balancer and simultaneously a single external load balancer. But a VM may not be a member of more than one external load balancer, nor more than one internal load balancer. Basic SKU Backend pools must comprise either a single VM or VMs in the same availability set or scale set . Only Standard SKU backends can accept VMs in a single VNet that are not explicitly assigned to an availability set.","title":"Backend"},{"location":"Infrastructure/Cloud/Azure/Azure-Load-Balancer/#outbound-connections","text":"","title":"Outbound connections"},{"location":"Infrastructure/Cloud/Azure/Azure-Load-Balancer/#floating-ip","text":"Floating IP is Azure's term for Direct Server Return (DSR) , which refers to the ability of nodes normally behind a load balancer to respond directly to client requests without overburdening the load balancer with return traffic. This prevents the load balancer from becoming a bottleneck. Technically, Azure Load Balancer always operates in a DSR flow topology even if Floating IP is not enabled, using the VMs' own IP addresses. When enabled, Floating IP changes the IP address mapping to the Frontend IP address of the load balancer.","title":"Floating IP"},{"location":"Infrastructure/Cloud/Azure/Azure-Load-Balancer/#tasks","text":"","title":"Tasks"},{"location":"Infrastructure/Cloud/Azure/Azure-Load-Balancer/#create-internal-load-balancer","text":"$ip = \"10.0.0.20\" $frontendIP = New-AzLoadBalancerFrontendIpConfig -Name frontend -PrivateIpAddress $ip","title":"Create internal load balancer"},{"location":"Infrastructure/Cloud/Azure/Azure-Load-Balancer/#add-to-backend-address-pool","text":"The process in PowerShell, counterintuitively, is actually to modify the VM's NIC to add a reference to the backend pool. AZ-103: p. 365 $vm = Get-AzVM -Name VM1 -ResourceGroupName $g $vmnic = Get-AzNetworkInterface -ResourceGroupName $g | where { $_ . VirtualMachine . Id -eq $vm . Id } $lb = Get-AzLoadBalancer -Name ExamRefLB -ResourceGroupName $g $backend = Get-AzLoadBalancerBackendAddressPoolConfig -Name ExamRefBackEndPool -LoadBalancer $lb # All IP configuration settings of the NIC have to be reapplied, there is no support for incremental changes $ipconfig = Get-AzNetworkInterfaceIpConfig -Name ipconfig1 -NetworkInterface vm1nic Set-AzNetworkInterfaceIpConfig -Name ipconfig1 -NetworkInterface $vm1nic -SubnetId $ipconfig . Subnet . Id -LoadBalancerBackendAddressPoolId $backend . Id Set-AzNetworkInterface -NetworkInterface $vm1nic Azure CLI supports incremental update of the NIC, which makes this command simpler than its PowerShell equivalent. az network nic ip-config address-pool add --resource-group ExamRefRG --address-pool ExamRefBackEndPool --lb-name ExamRefLB --nic-name vm1-nic --ip-config-name ipconfig1","title":"Add to backend address pool"},{"location":"Infrastructure/Cloud/Azure/Azure-Load-Balancer/#configure-tcp-reset","text":"TCP timeout values should be greater than that used for TCP keepalives. A new load balancing rule config object can have idle timeout set on declaration. New-AzLoadBalancerRuleConfig -Name \"MyLBRule\" -FrontendIpConfiguration $fe -BackendAddressPool $be -Probe $hp -Protocol TCP -FrontendPort 80 -BackendPort 80 ` -IdleTimeoutInMinutes 15 -EnableTcpReset These can be manually changed on the load balancing object as well $lb = Get-AzLoadBalancer -Name \"myLoadBalancer\" -ResourceGroup \"myResourceGroup\" $lb . LoadBalancingRules [ 0 ]. IdleTimeoutInMinutes = '15' $lb . LoadBalancingRules [ 0 ]. EnableTcpReset = 'true' Set-AzLoadBalancer -LoadBalancer $lb az network lb rule update -g $g -n MyLBRule --lb-name myLoadBalancer --idle-timeout 15 --enable-tcp-reset true","title":"Configure TCP reset"},{"location":"Infrastructure/Cloud/Azure/Azure-Load-Balancer/#specify-affinity","text":"In the Azure Portal, affinity is specified in the Session persistence dropdown. In Azure PowerShell and CLI, the option is the load distribution named parameter - New-AzLoadBalancerRuleConfig -LoadDistribution - az network lb rule create --load-distribution Sources: Multiple frontends for Azure Load Balancer SNAT for outbound connections Azure Load Balancer Floating IP configuration AZ-103: p. 358","title":"Specify affinity"},{"location":"Infrastructure/Cloud/Azure/Azure-Monitoring/","text":"A robust monitoring strategy implementing proactive notifications helps to increase uptime and optimize performance. Azure offers Azure Monitor and Azure Advisor . Azure Monitor brings a unified alerting experience to Azure, with a single pane of glass for interacting with metrics, the Activity Log , Log Analytics , service and resource health and service-specific insights. Alerts can be filtered by subscription (maximum of 5), resource group (maximum of 1), resource type (available selections depend on resources deployed to selected group) time range (past hour, past day, past week, and past 30 days), and other criteria. Azure Monitor can create alert rules that are built on target resources or resource type and that proactively notify you of the health of resources and can also leverage action groups that automate actions to take in certain conditions. Azure Advisor is a free, personalized guide to Azure best practices that provides recommendations to help you optimize resources. Azure Advisor offers personalized recommendations across 4 domains: High availability, Security, Performance, and Cost Feature Logs Metrics Retention Stored in Log Analytics (2 years) Stored in Monitor for 93 days, but metrics can be sent to Log Analytics and Storage accounts as well Properties Varying properties for each log, with support for rich data types such as date and time Fixed set of properties (or attributes): time, type, resource, value, and (optionally) dimensions. Data availability Triggered by an event, requiring time to process before they are available for querying Gathered at intervals and available for immediate querying. Virtual machines can be one of the most expensive resources in a cloud implementation, and there are several ways to reduce their cost Deallocate compute when not needed Delete unused virtual machines and allocate them only on demand Right-size VMs so that you don't overuse resources Advisor can also identify ExpressRoute circuits that have been \"Not Provisioned\" for more than 30 days Gateways that have been idle for more than 90 days There are two monitoring data streams: Metrics are the numerical time series data produced by resources and services within Azure. They are collected at 1-minute intervals, identified by a metric name and namespace (category). Metrics can be one-dimensional or have up to 10 dimensions. Metrics have the following properties: Time the value was collected Type of measurement made Resource associated with value Value Metrics can be stored in: Azure Monitor for 93 days Log Analytics for 2 years Storage account, where they are treated according to the retention policy and storage limits of the account. Logs come in various types Diagnostics logs (including resource logs and tenant logs ) are a type of log data that can be configured to send data to other locations, such as a Storage account or Log Analytics workspace . Diagnostics logs have to be enabled for each resource to be monitored through the Portal by enabling Diagnostics Settings, and not all resource types support diagnostic logs. Of those that do, not all resources support a retention policy or sending metric data. Azure Activity logs is a subscription level log that captures events that range from operational data (i.e. resource creation, deletion) to service health events for a subscription, but lacks resource-level detail.. Guest telemetry can relay logs from VMs with the use of diagnostics agents Log Analytics A Log Analytics workspace is a form of abstracting the process of log collection and is used to collect and aggregate logs. Like any other resource, it must be associated with a location and a resource group. Any Azure resource can only report logs to a single workspace, but Azure Monitor allows multiple workspaces to be queried simultaneously. The logs can be queried through Log Analytics or Monitor. Because a workspace is a resource, RBAC can be applied to control access to it. Log Analytics is based on Azure Data Explorer and uses Kusto . Log Analytics pricing is divided into data ingestion and data retention : - Under Pay-As-You-Go data ingestion the first 5 GB per month are free and further data is charged at a rate of $2.76/GB/month - Capacity Reservations offer a discount on Pay-as-you-go by charging a fixed amount per day ($219.52/day for 100 GB), with further discounts at higher tiers - Data retention is free for any amount of data up to 31 days, and 90 days for Azure Sentinel -enabled workspaces Operational Insights Log Analytics was previously named Operational Insights , which was named System Center Advisor prior to 2014. Application Insights Application Insights is a platfrom separate from Log Analytics which is intended to monitor web applications. Alerts Alerts can be created from the Alerts pane in the Monitor blade: Most resource blades also have Alerts in the resource menu under Monitoring. Alert rules , which are used to generate alerts, contain - Target resource , any Azure resource that generates signals, defines the scope and signals available for the alert. - Signal (i.e. metric or Activity Log) emitted by target resource. Signals are of 3 types: 1. Metrics 2. Log search queries 3. Activity logs - Conditional logic for alert combines the signal and a logical test to trigger alert. - Action Group determines what will happen when the alert is trigged. Action groups are themselves resources, and thus located in a subscription and resource group, and have: - Name - Short name is used to identify the Action Group in emails and notifications and is limited to 12 characters - Actions define the configuration for a specific action type. - Severity (0-4) Alerts can have 3 states: New and not reviewed Acknowledged issue is being actioned by an admin Closed issue that generated the alerts has been resolved and the alert has been marked as closedAlerts have many notification options, including email, SMS, mobile app, voice, and integration with automation. Actions A single action group can trigger multiple actions. Available types include: Email/SMS/Push/Voice, Azure Function, Logic App, Webhook, ITSM, and Automation Runbook IT Service Management Connector up to 10 ITSM actions can be configured with an ITSM connection Supported providers include ServiceNow, System Center Service Manager, Provance, and Cherwell Connect Azure to ITSM tools using ITSMC Webhooks Runbook runs in Azure Automation Service Runbooks The maximum number of alert notifications per hour: - Email: 60 - Voice: 12 - SMS: 12 VMs \"Virtual Machine Insights \" (or \"Azure Monitor for VMs\") is the successor to older monitoring workflow that used \"guest OS diagnostics\" in conjunction with Metrics Explorer. It requires a log analytics workspace . Diagnostic settings , conventionally, was the feature that would be enabled to allow Azure to collect metrics and logs from VMs, including event logs and performance counters. Two primary views: - Performance is a successor to the old Metrics Explorer - Map (originally \"Service Map\") Tasks Enable diagnostics on a VM Sources: - az vm diagnostics set Diagnostics log collection with a storage account Browse to the resource itself. Alternatively, open Azure Monitor and then the Diagnostics Settings blade. From there you can view all eligible resouce types and view status of log collection. $resource = Get-AzResource -Name $resourceName -ResourceGroupName $resourceGroupName $storage = Get-AzResource -Name $resourceName -ResourceGroupName $resourceGroupName Set-AzDiagnosticSetting -ResourceId $resource . ResourceId -StorageAccountId $storage . ResourceId -Enabled $true resourceId = $( az resource show -resource-group $resourceGroupName -name $resourceName --resource-type $resourceType --query id --output tsv ) az monitor diagnostic-settings create --name $diagnosticName --storage-account $storageId --resource $resouceId --resource-group $resourceGroup \\ --logs '[ { \"category\": <categoryName>, \"enabled\": true, \"retentionPolicy\": { \"days\": <numberOfDays>, \"enabled\": true } } ] ' Diagnostics log streaming to an Event Hub $rule = Get-AzServiceBusRule -ResourceGroup $resourceGroupName -Namespace $namespace -Topic $topic -Subscription $subscription -Name $ruleName Set-AzDiagnosticSetting -ResourceId $resource . ResourceId -ServiceBusRuleId $rule . Id -Enabled $true resourceId = $( az resource show -resource-group $resourceGroupName -name $resourceName --resource-type $resourceType --query id --output tsv ) az monitor diagnostic-settings create --name $diagnosticName --event-hub $eventHubName --event-hub-rule $eventHubRuleId --resource $resourceId \\ --logs '[{ \"category\": <categoryName>, \"enabled\": true }]' Diagnostics log collection in a Log Analytics workspace The PowerShell module that allows interaction with Log Analytics still refers to the service's old name . $workspace = Get-AzOperationalInsightsWorkspace -Name $logAnalyticsName -ResourceGroupName $g Set-AzDiagnosticSetting -ResourceId $r . ResourceId -WorkspaceId $workspace . ResourceId -Enabled $true az monitor diagnostic-settings create --name $diagnosticName --workspace $logAnalyticsName --resource $rid --resouce-group $g \\ --logs '[{ \"category\": <categoryName>, \"enabled\": true }]' Create an alert rule Sources: - Create, view, and manage activity log alerts by using Azure Monitor Create Log Analytics workspace Sources AZ-103: 1.2 AZ-104: 5.1 Azure Monitor for VMs","title":"Azure Monitoring"},{"location":"Infrastructure/Cloud/Azure/Azure-Monitoring/#log-analytics","text":"A Log Analytics workspace is a form of abstracting the process of log collection and is used to collect and aggregate logs. Like any other resource, it must be associated with a location and a resource group. Any Azure resource can only report logs to a single workspace, but Azure Monitor allows multiple workspaces to be queried simultaneously. The logs can be queried through Log Analytics or Monitor. Because a workspace is a resource, RBAC can be applied to control access to it. Log Analytics is based on Azure Data Explorer and uses Kusto . Log Analytics pricing is divided into data ingestion and data retention : - Under Pay-As-You-Go data ingestion the first 5 GB per month are free and further data is charged at a rate of $2.76/GB/month - Capacity Reservations offer a discount on Pay-as-you-go by charging a fixed amount per day ($219.52/day for 100 GB), with further discounts at higher tiers - Data retention is free for any amount of data up to 31 days, and 90 days for Azure Sentinel -enabled workspaces","title":"Log Analytics"},{"location":"Infrastructure/Cloud/Azure/Azure-Monitoring/#operational-insights","text":"Log Analytics was previously named Operational Insights , which was named System Center Advisor prior to 2014.","title":"Operational Insights"},{"location":"Infrastructure/Cloud/Azure/Azure-Monitoring/#application-insights","text":"Application Insights is a platfrom separate from Log Analytics which is intended to monitor web applications.","title":"Application Insights"},{"location":"Infrastructure/Cloud/Azure/Azure-Monitoring/#alerts","text":"Alerts can be created from the Alerts pane in the Monitor blade: Most resource blades also have Alerts in the resource menu under Monitoring. Alert rules , which are used to generate alerts, contain - Target resource , any Azure resource that generates signals, defines the scope and signals available for the alert. - Signal (i.e. metric or Activity Log) emitted by target resource. Signals are of 3 types: 1. Metrics 2. Log search queries 3. Activity logs - Conditional logic for alert combines the signal and a logical test to trigger alert. - Action Group determines what will happen when the alert is trigged. Action groups are themselves resources, and thus located in a subscription and resource group, and have: - Name - Short name is used to identify the Action Group in emails and notifications and is limited to 12 characters - Actions define the configuration for a specific action type. - Severity (0-4) Alerts can have 3 states: New and not reviewed Acknowledged issue is being actioned by an admin Closed issue that generated the alerts has been resolved and the alert has been marked as closedAlerts have many notification options, including email, SMS, mobile app, voice, and integration with automation.","title":"Alerts"},{"location":"Infrastructure/Cloud/Azure/Azure-Monitoring/#actions","text":"A single action group can trigger multiple actions. Available types include: Email/SMS/Push/Voice, Azure Function, Logic App, Webhook, ITSM, and Automation Runbook IT Service Management Connector up to 10 ITSM actions can be configured with an ITSM connection Supported providers include ServiceNow, System Center Service Manager, Provance, and Cherwell Connect Azure to ITSM tools using ITSMC Webhooks Runbook runs in Azure Automation Service Runbooks The maximum number of alert notifications per hour: - Email: 60 - Voice: 12 - SMS: 12","title":"Actions"},{"location":"Infrastructure/Cloud/Azure/Azure-Monitoring/#vms","text":"\"Virtual Machine Insights \" (or \"Azure Monitor for VMs\") is the successor to older monitoring workflow that used \"guest OS diagnostics\" in conjunction with Metrics Explorer. It requires a log analytics workspace . Diagnostic settings , conventionally, was the feature that would be enabled to allow Azure to collect metrics and logs from VMs, including event logs and performance counters. Two primary views: - Performance is a successor to the old Metrics Explorer - Map (originally \"Service Map\")","title":"VMs"},{"location":"Infrastructure/Cloud/Azure/Azure-Monitoring/#tasks","text":"","title":"Tasks"},{"location":"Infrastructure/Cloud/Azure/Azure-Monitoring/#enable-diagnostics-on-a-vm","text":"Sources: - az vm diagnostics set","title":"Enable diagnostics on a VM"},{"location":"Infrastructure/Cloud/Azure/Azure-Monitoring/#diagnostics-log-collection-with-a-storage-account","text":"Browse to the resource itself. Alternatively, open Azure Monitor and then the Diagnostics Settings blade. From there you can view all eligible resouce types and view status of log collection. $resource = Get-AzResource -Name $resourceName -ResourceGroupName $resourceGroupName $storage = Get-AzResource -Name $resourceName -ResourceGroupName $resourceGroupName Set-AzDiagnosticSetting -ResourceId $resource . ResourceId -StorageAccountId $storage . ResourceId -Enabled $true resourceId = $( az resource show -resource-group $resourceGroupName -name $resourceName --resource-type $resourceType --query id --output tsv ) az monitor diagnostic-settings create --name $diagnosticName --storage-account $storageId --resource $resouceId --resource-group $resourceGroup \\ --logs '[ { \"category\": <categoryName>, \"enabled\": true, \"retentionPolicy\": { \"days\": <numberOfDays>, \"enabled\": true } } ] '","title":"Diagnostics log collection with a storage account"},{"location":"Infrastructure/Cloud/Azure/Azure-Monitoring/#diagnostics-log-streaming-to-an-event-hub","text":"$rule = Get-AzServiceBusRule -ResourceGroup $resourceGroupName -Namespace $namespace -Topic $topic -Subscription $subscription -Name $ruleName Set-AzDiagnosticSetting -ResourceId $resource . ResourceId -ServiceBusRuleId $rule . Id -Enabled $true resourceId = $( az resource show -resource-group $resourceGroupName -name $resourceName --resource-type $resourceType --query id --output tsv ) az monitor diagnostic-settings create --name $diagnosticName --event-hub $eventHubName --event-hub-rule $eventHubRuleId --resource $resourceId \\ --logs '[{ \"category\": <categoryName>, \"enabled\": true }]'","title":"Diagnostics log streaming to an Event Hub"},{"location":"Infrastructure/Cloud/Azure/Azure-Monitoring/#diagnostics-log-collection-in-a-log-analytics-workspace","text":"The PowerShell module that allows interaction with Log Analytics still refers to the service's old name . $workspace = Get-AzOperationalInsightsWorkspace -Name $logAnalyticsName -ResourceGroupName $g Set-AzDiagnosticSetting -ResourceId $r . ResourceId -WorkspaceId $workspace . ResourceId -Enabled $true az monitor diagnostic-settings create --name $diagnosticName --workspace $logAnalyticsName --resource $rid --resouce-group $g \\ --logs '[{ \"category\": <categoryName>, \"enabled\": true }]'","title":"Diagnostics log collection in a Log Analytics workspace"},{"location":"Infrastructure/Cloud/Azure/Azure-Monitoring/#create-an-alert-rule","text":"Sources: - Create, view, and manage activity log alerts by using Azure Monitor","title":"Create an alert rule"},{"location":"Infrastructure/Cloud/Azure/Azure-Monitoring/#create-log-analytics-workspace","text":"","title":"Create Log Analytics workspace"},{"location":"Infrastructure/Cloud/Azure/Azure-Monitoring/#sources","text":"AZ-103: 1.2 AZ-104: 5.1 Azure Monitor for VMs","title":"Sources"},{"location":"Infrastructure/Cloud/Azure/Azure-Policy/","text":"Sources - What is Azure Policy? - Azure Policy Samples Azure Policy is a service that can create, assign, and manage policies to enforce governance. Policy definitions, authored in JSON, implement policy by describing desired behavior for Azure resources when they are created or updated. AZ-103: p. 72 To implement policy, a policy definition is created first, then a policy assignment assigns it to a scope. Policy definitions can be packaged together using initiative definitions and applied to a scope using initiative assignments RBAC roles deny by default and allow explicitly . But Azure Policy allows by default and denies explicitly Policies can be applied at the management group , subscription , or resource group scope, with all child resources and resource groups being affected. Effects Sources: - Understand Azure Policy effects Every policy definition has a single effect , which includes: Audit : create a warning event in the log Modify : used to add, update, or remove properties or tags on a resource during creation or update. Append AuditIfNotExists Deny DeployIfNotExists Disabled The order of evaluation of effects is: Disabled, Append, Deny, Audit (\"DADA\") Schema Sources: - AZ-103: p.17 { \"mode\" : \"all\" , \"policyRule\" : { \"if\" : { \"allOf\" : [ { \"field\" : \"type\" , \"equals\" : \"Microsoft.Compute/virtualMachines\" }, { \"not\" : { \"field\" : \"Microsoft.Compute/virtualMachines/sku.name\" , \"in\" : \"[parameters('listOfAllowedSKUs')]\" } } ] }, \"then\" : { \"effect\" : \"deny\" } }, \"parameters\" : { \"listOfAllowedSKUs\" : { \"type\" : \"array\" , \"metadata\" : { \"displayName\" : \"Allowed VM SKUs\" , \"description\" : \"The list of allowed SKUs for virtual machines.\" , \"strongType\" : \"vmSKUs\" } } } } Schema { \"id\" : \"https://schema.management.azure.com/schemas/2018-05-01/policyDefinition.json#\" , \"$schema\" : \"http://json-schema.org/draft-04/schema#\" , \"title\" : \"Policy Definition\" , \"description\" : \"This schema defines Azure resource policy definition, please see https://azure.microsoft.com/en-us/documentation/articles/resource-manager-policy/ for more details.\" , \"type\" : \"object\" , \"properties\" : { \"if\" : { \"oneOf\" : [ { \"$ref\" : \"#/definitions/condition\" }, { \"$ref\" : \"#/definitions/operatorNot\" }, { \"$ref\" : \"#/definitions/operatorAnyOf\" }, { \"$ref\" : \"#/definitions/operatorAllOf\" } ] }, \"then\" : { \"type\" : \"object\" , \"properties\" : { \"effect\" : { \"type\" : \"string\" , \"enum\" : [ \"append\" , \"audit\" , \"auditIfNotExists\" , \"deny\" , \"deployIfNotExists\" ] }, \"details\" : { \"oneOf\" : [ { \"$ref\" : \"#/definitions/ifNotExistsDetails\" }, { \"$ref\" : \"#/definitions/appendDetails\" } ] } }, \"required\" : [ \"effect\" ], \"additionalProperties\" : false } }, \"required\" : [ \"if\" , \"then\" ], \"additionalProperties\" : false , \"definitions\" : { \"appendDetails\" : { \"type\" : \"array\" , \"items\" : { \"properties\" : { \"field\" : { \"type\" : \"string\" }, \"value\" : {} }, \"required\" : [ \"field\" , \"value\" ], \"additionalProperties\" : false }, \"minItems\" : 1 , \"additionalItems\" : false }, \"ifNotExistsDetails\" : { \"type\" : \"object\" , \"properties\" : { \"type\" : { \"type\" : \"string\" }, \"name\" : { \"type\" : \"string\" }, \"resourceGroupName\" : { \"type\" : \"string\" }, \"existenceScope\" : { \"type\" : \"string\" , \"enum\" : [ \"resourceGroup\" , \"subscription\" ] }, \"roleDefinitionIds\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" } }, \"existenceCondition\" : { \"oneOf\" : [ { \"$ref\" : \"#/definitions/condition\" }, { \"$ref\" : \"#/definitions/operatorNot\" }, { \"$ref\" : \"#/definitions/operatorAnyOf\" }, { \"$ref\" : \"#/definitions/operatorAllOf\" } ] }, \"deployment\" : { \"type\" : \"object\" , \"properties\" : { \"properties\" : { \"$ref\" : \"https://schema.management.azure.com/schemas/2018-05-01/Microsoft.Resources.json#/definitions/DeploymentProperties\" } } } }, \"required\" : [ \"type\" ], \"additionalProperties\" : false }, \"condition\" : { \"allOf\" : [ { \"oneOf\" : [ { \"properties\" : { \"source\" : { \"type\" : \"string\" } }, \"required\" : [ \"source\" ] }, { \"properties\" : { \"field\" : { \"type\" : \"string\" } }, \"required\" : [ \"field\" ] } ] }, { \"oneOf\" : [ { \"properties\" : { \"equals\" : { \"type\" : \"string\" } }, \"required\" : [ \"equals\" ] }, { \"properties\" : { \"notEquals\" : { \"type\" : \"string\" } }, \"required\" : [ \"notEquals\" ] }, { \"properties\" : { \"like\" : { \"type\" : \"string\" } }, \"required\" : [ \"like\" ] }, { \"properties\" : { \"notLike\" : { \"type\" : \"string\" } }, \"required\" : [ \"notLike\" ] }, { \"properties\" : { \"contains\" : { \"type\" : \"string\" } }, \"required\" : [ \"contains\" ] }, { \"properties\" : { \"notContains\" : { \"type\" : \"string\" } }, \"required\" : [ \"notContains\" ] }, { \"properties\" : { \"in\" : { \"oneOf\" : [ { \"type\" : \"array\" }, { \"type\" : \"string\" } ] } }, \"required\" : [ \"in\" ] }, { \"properties\" : { \"notIn\" : { \"oneOf\" : [ { \"type\" : \"array\" }, { \"type\" : \"string\" } ] } }, \"required\" : [ \"notIn\" ] }, { \"properties\" : { \"containsKey\" : { \"type\" : \"string\" } }, \"required\" : [ \"containsKey\" ] }, { \"properties\" : { \"notContainsKey\" : { \"type\" : \"string\" } }, \"required\" : [ \"notContainsKey\" ] }, { \"properties\" : { \"match\" : { \"type\" : \"string\" } }, \"required\" : [ \"match\" ] }, { \"properties\" : { \"notMatch\" : { \"type\" : \"string\" } }, \"required\" : [ \"notMatch\" ] }, { \"properties\" : { \"exists\" : { \"type\" : \"string\" } }, \"required\" : [ \"exists\" ] } ] } ] }, \"operatorNot\" : { \"properties\" : { \"not\" : { \"oneOf\" : [ { \"$ref\" : \"#/definitions/condition\" }, { \"$ref\" : \"#/definitions/operatorNot\" }, { \"$ref\" : \"#/definitions/operatorAnyOf\" }, { \"$ref\" : \"#/definitions/operatorAllOf\" } ] } }, \"required\" : [ \"not\" ], \"additionalProperties\" : false }, \"operatorAnyOf\" : { \"properties\" : { \"anyOf\" : { \"type\" : \"array\" , \"minItems\" : 1 , \"items\" : { \"oneOf\" : [ { \"$ref\" : \"#/definitions/condition\" }, { \"$ref\" : \"#/definitions/operatorNot\" }, { \"$ref\" : \"#/definitions/operatorAnyOf\" }, { \"$ref\" : \"#/definitions/operatorAllOf\" } ] } } }, \"required\" : [ \"anyOf\" ], \"additionalProperties\" : false }, \"operatorAllOf\" : { \"properties\" : { \"allOf\" : { \"type\" : \"array\" , \"minItems\" : 1 , \"items\" : { \"oneOf\" : [ { \"$ref\" : \"#/definitions/condition\" }, { \"$ref\" : \"#/definitions/operatorNot\" }, { \"$ref\" : \"#/definitions/operatorAnyOf\" }, { \"$ref\" : \"#/definitions/operatorAllOf\" } ] } } }, \"required\" : [ \"allOf\" ], \"additionalProperties\" : false } } }","title":"Azure Policy"},{"location":"Infrastructure/Cloud/Azure/Azure-Policy/#effects","text":"Sources: - Understand Azure Policy effects Every policy definition has a single effect , which includes: Audit : create a warning event in the log Modify : used to add, update, or remove properties or tags on a resource during creation or update. Append AuditIfNotExists Deny DeployIfNotExists Disabled The order of evaluation of effects is: Disabled, Append, Deny, Audit (\"DADA\")","title":"Effects"},{"location":"Infrastructure/Cloud/Azure/Azure-Policy/#schema","text":"Sources: - AZ-103: p.17 { \"mode\" : \"all\" , \"policyRule\" : { \"if\" : { \"allOf\" : [ { \"field\" : \"type\" , \"equals\" : \"Microsoft.Compute/virtualMachines\" }, { \"not\" : { \"field\" : \"Microsoft.Compute/virtualMachines/sku.name\" , \"in\" : \"[parameters('listOfAllowedSKUs')]\" } } ] }, \"then\" : { \"effect\" : \"deny\" } }, \"parameters\" : { \"listOfAllowedSKUs\" : { \"type\" : \"array\" , \"metadata\" : { \"displayName\" : \"Allowed VM SKUs\" , \"description\" : \"The list of allowed SKUs for virtual machines.\" , \"strongType\" : \"vmSKUs\" } } } } Schema { \"id\" : \"https://schema.management.azure.com/schemas/2018-05-01/policyDefinition.json#\" , \"$schema\" : \"http://json-schema.org/draft-04/schema#\" , \"title\" : \"Policy Definition\" , \"description\" : \"This schema defines Azure resource policy definition, please see https://azure.microsoft.com/en-us/documentation/articles/resource-manager-policy/ for more details.\" , \"type\" : \"object\" , \"properties\" : { \"if\" : { \"oneOf\" : [ { \"$ref\" : \"#/definitions/condition\" }, { \"$ref\" : \"#/definitions/operatorNot\" }, { \"$ref\" : \"#/definitions/operatorAnyOf\" }, { \"$ref\" : \"#/definitions/operatorAllOf\" } ] }, \"then\" : { \"type\" : \"object\" , \"properties\" : { \"effect\" : { \"type\" : \"string\" , \"enum\" : [ \"append\" , \"audit\" , \"auditIfNotExists\" , \"deny\" , \"deployIfNotExists\" ] }, \"details\" : { \"oneOf\" : [ { \"$ref\" : \"#/definitions/ifNotExistsDetails\" }, { \"$ref\" : \"#/definitions/appendDetails\" } ] } }, \"required\" : [ \"effect\" ], \"additionalProperties\" : false } }, \"required\" : [ \"if\" , \"then\" ], \"additionalProperties\" : false , \"definitions\" : { \"appendDetails\" : { \"type\" : \"array\" , \"items\" : { \"properties\" : { \"field\" : { \"type\" : \"string\" }, \"value\" : {} }, \"required\" : [ \"field\" , \"value\" ], \"additionalProperties\" : false }, \"minItems\" : 1 , \"additionalItems\" : false }, \"ifNotExistsDetails\" : { \"type\" : \"object\" , \"properties\" : { \"type\" : { \"type\" : \"string\" }, \"name\" : { \"type\" : \"string\" }, \"resourceGroupName\" : { \"type\" : \"string\" }, \"existenceScope\" : { \"type\" : \"string\" , \"enum\" : [ \"resourceGroup\" , \"subscription\" ] }, \"roleDefinitionIds\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" } }, \"existenceCondition\" : { \"oneOf\" : [ { \"$ref\" : \"#/definitions/condition\" }, { \"$ref\" : \"#/definitions/operatorNot\" }, { \"$ref\" : \"#/definitions/operatorAnyOf\" }, { \"$ref\" : \"#/definitions/operatorAllOf\" } ] }, \"deployment\" : { \"type\" : \"object\" , \"properties\" : { \"properties\" : { \"$ref\" : \"https://schema.management.azure.com/schemas/2018-05-01/Microsoft.Resources.json#/definitions/DeploymentProperties\" } } } }, \"required\" : [ \"type\" ], \"additionalProperties\" : false }, \"condition\" : { \"allOf\" : [ { \"oneOf\" : [ { \"properties\" : { \"source\" : { \"type\" : \"string\" } }, \"required\" : [ \"source\" ] }, { \"properties\" : { \"field\" : { \"type\" : \"string\" } }, \"required\" : [ \"field\" ] } ] }, { \"oneOf\" : [ { \"properties\" : { \"equals\" : { \"type\" : \"string\" } }, \"required\" : [ \"equals\" ] }, { \"properties\" : { \"notEquals\" : { \"type\" : \"string\" } }, \"required\" : [ \"notEquals\" ] }, { \"properties\" : { \"like\" : { \"type\" : \"string\" } }, \"required\" : [ \"like\" ] }, { \"properties\" : { \"notLike\" : { \"type\" : \"string\" } }, \"required\" : [ \"notLike\" ] }, { \"properties\" : { \"contains\" : { \"type\" : \"string\" } }, \"required\" : [ \"contains\" ] }, { \"properties\" : { \"notContains\" : { \"type\" : \"string\" } }, \"required\" : [ \"notContains\" ] }, { \"properties\" : { \"in\" : { \"oneOf\" : [ { \"type\" : \"array\" }, { \"type\" : \"string\" } ] } }, \"required\" : [ \"in\" ] }, { \"properties\" : { \"notIn\" : { \"oneOf\" : [ { \"type\" : \"array\" }, { \"type\" : \"string\" } ] } }, \"required\" : [ \"notIn\" ] }, { \"properties\" : { \"containsKey\" : { \"type\" : \"string\" } }, \"required\" : [ \"containsKey\" ] }, { \"properties\" : { \"notContainsKey\" : { \"type\" : \"string\" } }, \"required\" : [ \"notContainsKey\" ] }, { \"properties\" : { \"match\" : { \"type\" : \"string\" } }, \"required\" : [ \"match\" ] }, { \"properties\" : { \"notMatch\" : { \"type\" : \"string\" } }, \"required\" : [ \"notMatch\" ] }, { \"properties\" : { \"exists\" : { \"type\" : \"string\" } }, \"required\" : [ \"exists\" ] } ] } ] }, \"operatorNot\" : { \"properties\" : { \"not\" : { \"oneOf\" : [ { \"$ref\" : \"#/definitions/condition\" }, { \"$ref\" : \"#/definitions/operatorNot\" }, { \"$ref\" : \"#/definitions/operatorAnyOf\" }, { \"$ref\" : \"#/definitions/operatorAllOf\" } ] } }, \"required\" : [ \"not\" ], \"additionalProperties\" : false }, \"operatorAnyOf\" : { \"properties\" : { \"anyOf\" : { \"type\" : \"array\" , \"minItems\" : 1 , \"items\" : { \"oneOf\" : [ { \"$ref\" : \"#/definitions/condition\" }, { \"$ref\" : \"#/definitions/operatorNot\" }, { \"$ref\" : \"#/definitions/operatorAnyOf\" }, { \"$ref\" : \"#/definitions/operatorAllOf\" } ] } } }, \"required\" : [ \"anyOf\" ], \"additionalProperties\" : false }, \"operatorAllOf\" : { \"properties\" : { \"allOf\" : { \"type\" : \"array\" , \"minItems\" : 1 , \"items\" : { \"oneOf\" : [ { \"$ref\" : \"#/definitions/condition\" }, { \"$ref\" : \"#/definitions/operatorNot\" }, { \"$ref\" : \"#/definitions/operatorAnyOf\" }, { \"$ref\" : \"#/definitions/operatorAllOf\" } ] } } }, \"required\" : [ \"allOf\" ], \"additionalProperties\" : false } } }","title":"Schema"},{"location":"Infrastructure/Cloud/Azure/Azure-Storage/","text":"DNS Storage account access SAS token SAS tokens are generated from a storage account key; if the key is invalidated then so are all SAS tokens generated from it. The user delegation SAS token itself is meant to be appended to the end of the blob's URI. CloudSkills : 40:00 Tasks Add endpoints to Azure File Sync Group Register a server to the sync group by installing Azure File Sync agent on each server. When installing, you sign in with your subscription's credentials, then register the server by providing the Subscription, Resource Group, and Storage Sync Service names. Click Add Server Endpoint . This will display a dropdown of all servers with the agent installed and associated with the sync service. Upload blob Azure CLI Azure AzCopy az storage blob upload --container-name $containerName --account-name $accountName --account-key $accountKey --file $file --name $blobName AzCopy copy localFilePath https://storageAccount.blob.core.windows.net/destinationContainer/path/to/blob?SASToken Download a blob from a container Azure AzCopy AzCopy copy https://storageAccount.blob.core.windows.net/sourceContainer/path/to/blob?SASToken localFilePath Copy a blob from one container to another Azure AzCopy AzCopy /Source:https://sourceblob.blob.core.windows.net/sourcecontainer/ /Dest:https://deststorage.blob.core.windows.net/destcontainer/ /SourceKey:sourcekey /DestKey:destkey /Pattern:disk1.vhd $blobCopyState = Start-AzStorageBlobCopy -SrcBlob $blobName -SrcContainer $srcContainer -Context $srcContext -DestContainer $destContainer -DestBlob $vhdName -DestContext $destContext $srcStorageKey = Get-AzStorageAccountKey -ResourceGroupName $sourceg -Name $srcStorageAccount $destStorageKey = Get-AzStorageAccountKey -ResourceGroupName $destg -Name $destStorageAccount $srcContext = New-AzStorageContext -StorageAccountName $srcStorageAccount -StorageAccountKey $srcStorageKey . Value [ 0 ] $destContext = New-AzStorageContext -StorageAccountNAme $destStorageAccount -StorageAccountKey $destStorageKey . Value [ 0 ] # Create new container in destination account New-AzStorageContainer -Name $destContainer -Context $destContext # Make the copy $copiedBlob = Start-AzStorageBlobCopy -SrcBlob $blobName -SrcContainer $srcContainer -Context $srcContext -DestContainer $destContainer -DestBlob $blobName -DestContext $destContext az storage blob copy start --account-name $destStorageAccount --account-key $destStorageKey --destination-blob $blobName --source-account-name $srcStorageAccount --source-container $srcContainer --source-blob $blobName --source-account-key $srcStorageKey Monitor progress of the async blob copy $copiedBlob | Get-AzStorageBlobCopyState az storage blob show --account-name $destStorageAccount --account-key $destStorageKey --container-name $destContainer --name $blobName Create SAS token $storageKey = Get-AzStorageAccountKey -ResourceGroupName $g -Name $accountName $context = New-AzStorageContext -StorageAccountName $accountName -StorageAccountKey $storageKey [ 0 ]. Value $startTime = Get-Date $endTime = $startTime . AddHours ( 4 ) New-AzStorageBlobSASToken -Container $container -Blob $blob -Permission \"rwd\" -StartTime $startTime -ExpiryTime $startTime . AddHours ( 4 ) -Context $context az storage blob generate-sas --account-name \"storageAccount\" --account-key $storageAccountKey --container-name $container --name $blobName --permissions r --expiry \"2019-05-31\" Create container $storageKey = Get-AzStorageAccountKey -Name $storageAccount -ResourceGroupName $resourceGroup $context = New-AzStorageContext -StorageAccountName $storageAccount -StorageAccountKey $storageKey . Value [ 0 ] Set-AzCurrentStorageAccount -Context $context New-AzStorageContainer -Name $container -Permission Off Upload file as blob to new container Azure PowerShell Azure CLI Set-AzStorageBlobContent -File $localFile -Container $container -Blob $blobName az storage container create --account-name $storageaccount --name $containername --public-access off Ensure App Services, backup vault, and event hub have access to a storage account Get-AzVirtualNetwork -ResourceGroupName RG01 -Name VNET01 | Set-AzVirtualNetworkSubnetConfig -Name VSUBNET01 -AddressPrefix 10 . 0 . 0 . 0 / 24 -ServiceEndpoint Microsoft . Storage | Set-AzVirtualNetwork $subnet = Get-AzVirtualNetwork -ResourceGroupName RG01 -Name VNET01 | Get-AzVirtualNetworkSubnetConfig -Name VSUBNET01 Add-AzStorageAccountNetworkRule -ResourceGroupName VNET01 -Name Storage01 -VirtualNetworkResourceId $subnet . Id Update-AzStorageAccountNetworkRuleSet -ResourceGroupName RG01 -Name STORAGE01 -Bypass Azure . Services Troubleshoot Azure File Sync Several procedures to be used when Azure File Sync is having issues Collect logs to troubleshoot issues with Azure File Sync agent installation StorageSyncAgent.msi /l*v AFSInstaller.log Remove the server from registered sync group Error message \"This server is already registered during registration\" Import-Module \"C:\\Program Files\\Azure\\StorageSyncAgent\\StorageSync.Management.ServerCmdlets.dll\" Reset-StorageSyncServer Monitoring using Log Analytics Access Activity Log data (Portal) 1. Find Management + Governance in All Services 2. Open Activity Log 3. Click Logs icon at top of Activity Log view to select an existing Log Analytics (OMS) workspace or create a new one Storage account endpoints Virtual network service endpoint Sources - AZ-103 p. 112 - Configure Azure Storage firewalls and virtual networks Specify Microsoft.Storage in the service endpoint settings of the VNet subnet Configure which VNets can access a particular storage account Display virtual network rules Azure PowerShell Azure CLI Get-AzStorageAccountNetworkRuleSet -ResourceGroupName $rgName -AccountName $n | Select-Object VirtualNetworkRules az storage account network-rule list -g $rgName -n $n --query virtualNetworkRules Enable service endpoint for Azure Storage on an existing virtual network and subnet. Azure PowerShell Azure CLI Get-AzVirtualNetwork -ResourceGroupName $rgName -Name $n | Set-AzVirtualNetworkSubnetConfig -Name \"mysubnet\" -AddressPrefix \"10.0.0.0/24\" -ServiceEndpoint \"Microsoft.Storage\" | Set-AzVirtualNetwork az network vnet subnet update -g $rgName --vnet-name $n --name \"mysubnet\" --service-endpoints \"Microsoft.Storage\" Add network rule for VNet and subnet Azure PowerShell Azure CLI $subnet = Get-AzVirtualNetwork -ResourceGroupName $ng -Name $nn | Get-AzVirtualNetworkSubnetConfig -Name \"mysubnet\" Add-AzStorageAccountNetworkRule -ResourceGroupName $sg -Name $sn -VirtualNetworkResourceId $subnet . Id subnetid = $( az network vnet subnet show -g $ng --vnet-name $nn -n \"mysubnet\" --query id --output tsv ) az storage account network-rule add -g $sg -n $sn --subnet $subnetid Remove network rule ```powershell $subnet = Get-AzVirtualNetwork -ResourceGroupName $ng -Name $nn | Get-AzVirtualNetworkSubnetConfig -Name \"mysubnet\" Remove-AzStorageAccountNetworkRule -ResourceGroupName $sg -Name $sn -VirtualNetworkResourceId $subnet.Id ``` Bypass network rules to allow access for Azure services like Event Hub and Recovery Services Vault Azure PowerShell Azure CLI # Display exceptions for the storage account network rules Get-AzStorageAccountNetworkRuleSet -ResourceGroupName $g -Name $n | Select-Object Bypass # Configure exceptions to storage account network rules Update-AzStorageAccountNetworkRuleSet -ResourceGroupName $g -Name $n -Bypass AzureServices , Metrics , Logging # Display exceptions for the storage account network rules az storage account show -g $g -n $n --query networkRuleSet.bypass # Configure exceptions to storage account network rules az storage account update -g $g -n $n --bypass Logging Metrics AzureServices Configure Azure Storage firewalls and virtual networks AZ-103: p. 107, 114, 127","title":"Azure Storage"},{"location":"Infrastructure/Cloud/Azure/Azure-Storage/#dns","text":"","title":"DNS"},{"location":"Infrastructure/Cloud/Azure/Azure-Storage/#storage-account-access","text":"","title":"Storage account access"},{"location":"Infrastructure/Cloud/Azure/Azure-Storage/#sas-token","text":"SAS tokens are generated from a storage account key; if the key is invalidated then so are all SAS tokens generated from it. The user delegation SAS token itself is meant to be appended to the end of the blob's URI. CloudSkills : 40:00","title":"SAS token"},{"location":"Infrastructure/Cloud/Azure/Azure-Storage/#tasks","text":"","title":"Tasks"},{"location":"Infrastructure/Cloud/Azure/Azure-Storage/#add-endpoints-to-azure-file-sync-group","text":"Register a server to the sync group by installing Azure File Sync agent on each server. When installing, you sign in with your subscription's credentials, then register the server by providing the Subscription, Resource Group, and Storage Sync Service names. Click Add Server Endpoint . This will display a dropdown of all servers with the agent installed and associated with the sync service. Upload blob Azure CLI Azure AzCopy az storage blob upload --container-name $containerName --account-name $accountName --account-key $accountKey --file $file --name $blobName AzCopy copy localFilePath https://storageAccount.blob.core.windows.net/destinationContainer/path/to/blob?SASToken Download a blob from a container Azure AzCopy AzCopy copy https://storageAccount.blob.core.windows.net/sourceContainer/path/to/blob?SASToken localFilePath Copy a blob from one container to another Azure AzCopy AzCopy /Source:https://sourceblob.blob.core.windows.net/sourcecontainer/ /Dest:https://deststorage.blob.core.windows.net/destcontainer/ /SourceKey:sourcekey /DestKey:destkey /Pattern:disk1.vhd $blobCopyState = Start-AzStorageBlobCopy -SrcBlob $blobName -SrcContainer $srcContainer -Context $srcContext -DestContainer $destContainer -DestBlob $vhdName -DestContext $destContext $srcStorageKey = Get-AzStorageAccountKey -ResourceGroupName $sourceg -Name $srcStorageAccount $destStorageKey = Get-AzStorageAccountKey -ResourceGroupName $destg -Name $destStorageAccount $srcContext = New-AzStorageContext -StorageAccountName $srcStorageAccount -StorageAccountKey $srcStorageKey . Value [ 0 ] $destContext = New-AzStorageContext -StorageAccountNAme $destStorageAccount -StorageAccountKey $destStorageKey . Value [ 0 ] # Create new container in destination account New-AzStorageContainer -Name $destContainer -Context $destContext # Make the copy $copiedBlob = Start-AzStorageBlobCopy -SrcBlob $blobName -SrcContainer $srcContainer -Context $srcContext -DestContainer $destContainer -DestBlob $blobName -DestContext $destContext az storage blob copy start --account-name $destStorageAccount --account-key $destStorageKey --destination-blob $blobName --source-account-name $srcStorageAccount --source-container $srcContainer --source-blob $blobName --source-account-key $srcStorageKey","title":"Add endpoints to Azure File Sync Group"},{"location":"Infrastructure/Cloud/Azure/Azure-Storage/#monitor-progress-of-the-async-blob-copy","text":"$copiedBlob | Get-AzStorageBlobCopyState az storage blob show --account-name $destStorageAccount --account-key $destStorageKey --container-name $destContainer --name $blobName","title":"Monitor progress of the async blob copy"},{"location":"Infrastructure/Cloud/Azure/Azure-Storage/#create-sas-token","text":"$storageKey = Get-AzStorageAccountKey -ResourceGroupName $g -Name $accountName $context = New-AzStorageContext -StorageAccountName $accountName -StorageAccountKey $storageKey [ 0 ]. Value $startTime = Get-Date $endTime = $startTime . AddHours ( 4 ) New-AzStorageBlobSASToken -Container $container -Blob $blob -Permission \"rwd\" -StartTime $startTime -ExpiryTime $startTime . AddHours ( 4 ) -Context $context az storage blob generate-sas --account-name \"storageAccount\" --account-key $storageAccountKey --container-name $container --name $blobName --permissions r --expiry \"2019-05-31\"","title":"Create SAS token"},{"location":"Infrastructure/Cloud/Azure/Azure-Storage/#create-container","text":"$storageKey = Get-AzStorageAccountKey -Name $storageAccount -ResourceGroupName $resourceGroup $context = New-AzStorageContext -StorageAccountName $storageAccount -StorageAccountKey $storageKey . Value [ 0 ] Set-AzCurrentStorageAccount -Context $context New-AzStorageContainer -Name $container -Permission Off Upload file as blob to new container Azure PowerShell Azure CLI Set-AzStorageBlobContent -File $localFile -Container $container -Blob $blobName az storage container create --account-name $storageaccount --name $containername --public-access off","title":"Create container"},{"location":"Infrastructure/Cloud/Azure/Azure-Storage/#ensure-app-services-backup-vault-and-event-hub-have-access-to-a-storage-account","text":"Get-AzVirtualNetwork -ResourceGroupName RG01 -Name VNET01 | Set-AzVirtualNetworkSubnetConfig -Name VSUBNET01 -AddressPrefix 10 . 0 . 0 . 0 / 24 -ServiceEndpoint Microsoft . Storage | Set-AzVirtualNetwork $subnet = Get-AzVirtualNetwork -ResourceGroupName RG01 -Name VNET01 | Get-AzVirtualNetworkSubnetConfig -Name VSUBNET01 Add-AzStorageAccountNetworkRule -ResourceGroupName VNET01 -Name Storage01 -VirtualNetworkResourceId $subnet . Id Update-AzStorageAccountNetworkRuleSet -ResourceGroupName RG01 -Name STORAGE01 -Bypass Azure . Services","title":"Ensure App Services, backup vault, and event hub have access to a storage account"},{"location":"Infrastructure/Cloud/Azure/Azure-Storage/#troubleshoot-azure-file-sync","text":"Several procedures to be used when Azure File Sync is having issues Collect logs to troubleshoot issues with Azure File Sync agent installation StorageSyncAgent.msi /l*v AFSInstaller.log Remove the server from registered sync group Error message \"This server is already registered during registration\" Import-Module \"C:\\Program Files\\Azure\\StorageSyncAgent\\StorageSync.Management.ServerCmdlets.dll\" Reset-StorageSyncServer","title":"Troubleshoot Azure File Sync"},{"location":"Infrastructure/Cloud/Azure/Azure-Storage/#monitoring-using-log-analytics","text":"Access Activity Log data (Portal) 1. Find Management + Governance in All Services 2. Open Activity Log 3. Click Logs icon at top of Activity Log view to select an existing Log Analytics (OMS) workspace or create a new one","title":"Monitoring using Log Analytics"},{"location":"Infrastructure/Cloud/Azure/Azure-Storage/#storage-account-endpoints","text":"","title":"Storage account endpoints"},{"location":"Infrastructure/Cloud/Azure/Azure-Storage/#virtual-network-service-endpoint","text":"Sources - AZ-103 p. 112 - Configure Azure Storage firewalls and virtual networks Specify Microsoft.Storage in the service endpoint settings of the VNet subnet Configure which VNets can access a particular storage account Display virtual network rules Azure PowerShell Azure CLI Get-AzStorageAccountNetworkRuleSet -ResourceGroupName $rgName -AccountName $n | Select-Object VirtualNetworkRules az storage account network-rule list -g $rgName -n $n --query virtualNetworkRules Enable service endpoint for Azure Storage on an existing virtual network and subnet. Azure PowerShell Azure CLI Get-AzVirtualNetwork -ResourceGroupName $rgName -Name $n | Set-AzVirtualNetworkSubnetConfig -Name \"mysubnet\" -AddressPrefix \"10.0.0.0/24\" -ServiceEndpoint \"Microsoft.Storage\" | Set-AzVirtualNetwork az network vnet subnet update -g $rgName --vnet-name $n --name \"mysubnet\" --service-endpoints \"Microsoft.Storage\" Add network rule for VNet and subnet Azure PowerShell Azure CLI $subnet = Get-AzVirtualNetwork -ResourceGroupName $ng -Name $nn | Get-AzVirtualNetworkSubnetConfig -Name \"mysubnet\" Add-AzStorageAccountNetworkRule -ResourceGroupName $sg -Name $sn -VirtualNetworkResourceId $subnet . Id subnetid = $( az network vnet subnet show -g $ng --vnet-name $nn -n \"mysubnet\" --query id --output tsv ) az storage account network-rule add -g $sg -n $sn --subnet $subnetid Remove network rule ```powershell $subnet = Get-AzVirtualNetwork -ResourceGroupName $ng -Name $nn | Get-AzVirtualNetworkSubnetConfig -Name \"mysubnet\" Remove-AzStorageAccountNetworkRule -ResourceGroupName $sg -Name $sn -VirtualNetworkResourceId $subnet.Id ``` Bypass network rules to allow access for Azure services like Event Hub and Recovery Services Vault Azure PowerShell Azure CLI # Display exceptions for the storage account network rules Get-AzStorageAccountNetworkRuleSet -ResourceGroupName $g -Name $n | Select-Object Bypass # Configure exceptions to storage account network rules Update-AzStorageAccountNetworkRuleSet -ResourceGroupName $g -Name $n -Bypass AzureServices , Metrics , Logging # Display exceptions for the storage account network rules az storage account show -g $g -n $n --query networkRuleSet.bypass # Configure exceptions to storage account network rules az storage account update -g $g -n $n --bypass Logging Metrics AzureServices Configure Azure Storage firewalls and virtual networks AZ-103: p. 107, 114, 127","title":"Virtual network service endpoint"},{"location":"Infrastructure/Cloud/Azure/Azure-VPN/","text":"Authentication Azure P2S VPN connections support several authentication methods: Azure AD authentication (Windows 10 only) RADIUS server VPN Gateway native certificate authentication The VPN gateway acts as a pass-through forwarding authentication messages between the connecting device and the RADIUS server. The RADIUS server can be deployed on-premises or in the Azure VNet, and two such servers can be deployed for high availability. If deployed on-premises, a S2S VPN to the site is required, and ExpressRoute is not usable. AD domain authentication requires a RADIUS server that integrates with the AD server. Tasks Create local network gateway $localnw = New-AzLocalNetworkGateway -Name LocalNetGW -ResourceGroupName ExamRefRG -Location \"West Europe\" -GatewayIpAddress \"53.50.123.195\" -AddressPrefix \"10.5.0.0/16\" Create VPN connection $gateway = Get-AzVirtualNetworkGateway -Name VPNGW1 -ResourceGroupName ExamRefRG $conn = New-AzVirtualNetworkGatewayConnection -Name OnPremConnection -ResourceGroupName ExamRefRG -Location 'West Europe' -VirtualNetworkGateway1 $gateway -LocalNetworkGateway2 $localnw -ConnectionType IPsec -SharedKey \"abc123\" Create a VPN Gateway $rg = ExamRefRG Create gateway subnet in VNet1 Gateway subnets are normal subnets with the name \"GatewaySubnet\" $vnet1 = Get-AzVirtualNetwork -Name VNet1 -ResourceGroupName $rg $vnet1 . Subnets += New-AzVirtualNetworkSubnetConfig -Name GatewaySubnet -AddressPrefix 10 . 1 . 1 . 0 / 27 $vnet1 = Set-AzVirtualNetwork -VirtualNetwork $vnet1 Create VPN gateway in VNet1 $gwpip = New-AzPublicIpAddress -Name VNet1-GW-IP -ResourceGroupName $rg -Location 'North Europe' -AllocationMethod Dynamic $gwsubnet = Get-AzVirtualNetworkSubnetConfig -Name 'GatewaySubnet' -VirtualNetwork $vnet1 $gwipconf = New-AzVirtualNetworkGatewayIpConfig -Name GwIPConf -Subnet $gwsubnet -PublicIpAddress $gwpip $vnet1gw = New-AzVirtualNetworkGateway -Name VNet1-GW -ResourceGroupName $rg -Location 'North Europe' -IpConfigurations $gwipconf -GatewayType Vpn -VpnType RouteBased -GatewaySku VpnGw1 Create gateway subnets in VNet2 and VNet3 az network vnet subnet create --name GatewaySubnet --vnet-name VNet1 --resource-group ExamRefRG --address-prefixes 10 .1.1.0/27 az network public-ip create --name VNet1-GW-IP --resource-group ExamRefRG --location NorthEurope az network vnet-gateway create --name VNet1-GW --resource-group ExamRefRG --gateway-type vpn --sku VpnGw1 --vpn-type RouteBased --vnet VNet1 --public-ip-addresses VNet1-GW-IP --location NorthEurope Create a VPN gateway and VNet peering Create gateway subnets in VNet2 and VNet3 $vnet2 = Get-AzVirtualNetwork -Name VNet2 -ResourceGroupName ExamRefRG $vnet2 . Subnets += New-AzVirtualNetworkSubnetConfig -Name GatewaySubnet -AddressPrefix 10 . 2 . 1 . 0 / 27 $vnet2 = Set-AzVirtualNetwork -VirtualNetwork $vnet2 $vnet3 = Get-AzVirtualNetwork -Name VNet3 -ResourceGroupName ExamRefRG $vnet3 . Subnets += New-AzVirtualNetworkSubnetConfig -Name GatewaySubnet -AddressPrefix 10 . 3 . 1 . 0 / 27 $vnet3 = Set-AzVirtualNetwork -VirtualNetwork $vnet3 Create VPN gateway in VNet2 $gwpip2 = New-AzPublicIpAddress -Name VNet2-GW-IP -ResourceGroupName ExamRefRG -Location $vnet2 . Location -AllocationMethod Dynamic $gwsubnet2 = Get-AzVirtualNetworkSubnetConfig -Name 'GatewaySubnet' -VirtualNetwork $vnet2 $gwipconf2 = New-AzVirtualNetworkGatewayIpConfig -Name GwIPConf2 -Subnet $gwsubnet2 -PublicIpAddress $gwpip2 $vnet2gw = New-AzVirtualNetworkGateway -Name VNet2-GW -ResourceGroupNAme ExamRefR -Location $vnet2 . Location -IpConfigurations $gwipconf2 -GatewayType Vpn -VpnType RouteBased -GatewaySku VpnGw1 Create VPN gateway in VNet3 $gwpip3 = New-AzPublicIpAddress -Name VNet3-GW-IP -ResourceGroupName ExamRefR -Location $vnet3 . Location -AllocationMethod Dynamic $gwsubnet3 = Get-AzVirtualNetworkSubnetConfig -Name 'GatewaySubnet' -VirtualNetwork $vnet3 $gwipconf3 = New-AzVirtualNetworkGatewayIpConfig -Name GwIPConf3 -Subnet $gwsubnet3 -PublicIpAddress $gwpip3 $vnet3gw = New-AzVirtualNetworkGateway -Name VNet3-GW -ResourceGroupNAme ExamRefRG -Location $vnet3 . Location -IpConfigurations $gwipconf3 -GatewayType Vpn -VpnType RouteBased -GatewaySku VpnGw1 Create connections New-AzVirtualNetworkGatewayConnection -Name VNet2-to-VNet3 -ResourceGroupName ExamRefRG -Location $vnet2 . Location -VirtualNetworkGateway1 $vnet2gw -VirtualNetworkGateway2 $vnet3gw -ConnectionType VNet2VNet -SharedKey \"secretkey123\" New-AzVirtualNetworkGatewayConnection -Name VNet3-to-VNet2 -ResourceGroupName ExamRefRG -Location $vnet3 . Location -VirtualNetworkGateway1 $vnet3gw -VirtualNetworkGateway2 $vnet2gw -ConnectionType VNet2VNet -SharedKey \"secretkey123\" Create gateway subnets in VNet2 and VNet3 az network vnet subnet create --name GatewaySubnet --vnet-name VNet2 --resource-group ExamRefRG --address-prefixes 10 .2.1.0/27 az network vnet subnet create --name GatewaySubnet --vnet-name VNet3 --resource-group ExamRefRG --address-prefixes 10 .3.1.0/27 Create public IP addresses for use by VPN gateways az network public-ip create --name VNet2-GW-IP --resource-group ExamRefRG --location NorthEurope az network public-ip create --name VNet3-GW-IP --resource-group ExamRefRG --location WestEurope Create VPN gateways in VNet2 and VNet 3 az network vnet-gateway create --name VNet2-GW --resource-group ExamRefRG --gateway-type vpn --sku VpnGw1 --vpn-type RouteBased --vnet VNet2 --public-ip-addresses VNet2-GW-IP --location NorthEurope az network vnet-gateway create --name VNet3-GW --resource-group ExamRefRG --gateway-type vpn --sku VpnGw1 --vpn-type RouteBased --vnet VNet3 --public-ip-addresses VNet3-GW-IP --location WestEurope Create connections between VPN gateways az network vpn-connection create --name VNet2-to-VNet3 --resource-group ExamRefRG --vnet-gateway1 VNet2-GW --vnet-gateway2 VNet3-GW --shared-key secretkey123 --location NorthEurope az network vpn-connection create --name VNet3-to-VNet2 --resource-group ExamRefRG --vnet-gateway1 VNet3-GW --vnet-gateway2 VNet2-GW --shared-key secretkey123 --location WestEurope Use VPN Troubleshoot Get the Network Watcher resource $nw = Get-AzResource | Where ResourceType -eq Microsoft . Network / networkWatchers -and Location -eq WestEurope $networkWatcher = Get-AzNetworkWatcher -Name $nw . Name -ResourceGroupName $nw . ResourceGroupName Get the connection to troubleshoot $connection = Get-AzVirtualNetworkGatewayConnection -Name Vnet1-to-Vnet2 -ResourceGroupName ExamRefRG Start VPN Troubleshoot Start-AzNetworkWatcherResourceTroubleshooting -NetworkWatcher $networkWatcher -TargetResourceId $connection . Id -StorageId $sa . Id -StoragePath \" $( $sa . PrimaryEndpoints . Blob )$( $sc . name ) \" Create a storage account and container for logs az storage account create --name examrefstorage --location westeurope --resource-group ExamRefRG --sku Standard_LRS az storage account keys list --resource-group ExamRefRG --account-name examrefstorage az storage container create --account-name examrefstorage --account-key { storageAccountKey } --name logs Start VPN Troubleshoot az network watcher troubleshooting start --resource-group ExamRefRG --resource Vnet1-to-Vnet2 --resource-type vpnConnection --storage-account examrefstorage --storage-path https://examrefstorage.blob.core.windows.net/logs --output json Create S2S VPN AZ-103: 395 $lgwip = 53.50.123.195 $key = \"abc123\" $lgw = New-AzLocalNetworkGateway -ResourceGroupName $g -Name $n -Location $l -GatewayIpAddress $lgwip -AddressPrefix \"10.5.0.0/16\" $vgw = Get-AzVirtualNetworkGateway -ResourceGroupNAme -Name New-AzVirtualNetworkGatewayConnection -ResourceGroupName $g -Name $n -Location $l -VirtualNetworkGateway1 $vgw -LocalNetworkGateway2 $lgw -ConnectionType IPsec -SharedKey $key az network local-gateway create --gateway-ip-address $lgwip --name LocalNetGW --resource-group ExamRefRG --local-address-prefixes 10 .5.0.0/16 az network vpn-connection create --name OnPremConnection --resource-group ExamRefRG --vnet-gateway1 VPNGW1 --location WestEurope --shared-key $key --local-gateway2 LocalNetGW Sources VPN Gateway design Connect Azure VPN gateways to multiple on-premises policy-based VPN devices About VPN Gateway configuration settings Highly available cross-premises and VNet-to-VNet connectivity ExpressRoute connectivity models Connect a computer to a virtual network using P2S and RADIUS authentication: PowerShell","title":"Azure VPN"},{"location":"Infrastructure/Cloud/Azure/Azure-VPN/#authentication","text":"Azure P2S VPN connections support several authentication methods: Azure AD authentication (Windows 10 only) RADIUS server VPN Gateway native certificate authentication The VPN gateway acts as a pass-through forwarding authentication messages between the connecting device and the RADIUS server. The RADIUS server can be deployed on-premises or in the Azure VNet, and two such servers can be deployed for high availability. If deployed on-premises, a S2S VPN to the site is required, and ExpressRoute is not usable. AD domain authentication requires a RADIUS server that integrates with the AD server.","title":"Authentication"},{"location":"Infrastructure/Cloud/Azure/Azure-VPN/#tasks","text":"","title":"Tasks"},{"location":"Infrastructure/Cloud/Azure/Azure-VPN/#create-local-network-gateway","text":"$localnw = New-AzLocalNetworkGateway -Name LocalNetGW -ResourceGroupName ExamRefRG -Location \"West Europe\" -GatewayIpAddress \"53.50.123.195\" -AddressPrefix \"10.5.0.0/16\" Create VPN connection $gateway = Get-AzVirtualNetworkGateway -Name VPNGW1 -ResourceGroupName ExamRefRG $conn = New-AzVirtualNetworkGatewayConnection -Name OnPremConnection -ResourceGroupName ExamRefRG -Location 'West Europe' -VirtualNetworkGateway1 $gateway -LocalNetworkGateway2 $localnw -ConnectionType IPsec -SharedKey \"abc123\"","title":"Create local network gateway"},{"location":"Infrastructure/Cloud/Azure/Azure-VPN/#create-a-vpn-gateway","text":"$rg = ExamRefRG Create gateway subnet in VNet1 Gateway subnets are normal subnets with the name \"GatewaySubnet\" $vnet1 = Get-AzVirtualNetwork -Name VNet1 -ResourceGroupName $rg $vnet1 . Subnets += New-AzVirtualNetworkSubnetConfig -Name GatewaySubnet -AddressPrefix 10 . 1 . 1 . 0 / 27 $vnet1 = Set-AzVirtualNetwork -VirtualNetwork $vnet1 Create VPN gateway in VNet1 $gwpip = New-AzPublicIpAddress -Name VNet1-GW-IP -ResourceGroupName $rg -Location 'North Europe' -AllocationMethod Dynamic $gwsubnet = Get-AzVirtualNetworkSubnetConfig -Name 'GatewaySubnet' -VirtualNetwork $vnet1 $gwipconf = New-AzVirtualNetworkGatewayIpConfig -Name GwIPConf -Subnet $gwsubnet -PublicIpAddress $gwpip $vnet1gw = New-AzVirtualNetworkGateway -Name VNet1-GW -ResourceGroupName $rg -Location 'North Europe' -IpConfigurations $gwipconf -GatewayType Vpn -VpnType RouteBased -GatewaySku VpnGw1 Create gateway subnets in VNet2 and VNet3 az network vnet subnet create --name GatewaySubnet --vnet-name VNet1 --resource-group ExamRefRG --address-prefixes 10 .1.1.0/27 az network public-ip create --name VNet1-GW-IP --resource-group ExamRefRG --location NorthEurope az network vnet-gateway create --name VNet1-GW --resource-group ExamRefRG --gateway-type vpn --sku VpnGw1 --vpn-type RouteBased --vnet VNet1 --public-ip-addresses VNet1-GW-IP --location NorthEurope","title":"Create a VPN Gateway"},{"location":"Infrastructure/Cloud/Azure/Azure-VPN/#create-a-vpn-gateway-and-vnet-peering","text":"Create gateway subnets in VNet2 and VNet3 $vnet2 = Get-AzVirtualNetwork -Name VNet2 -ResourceGroupName ExamRefRG $vnet2 . Subnets += New-AzVirtualNetworkSubnetConfig -Name GatewaySubnet -AddressPrefix 10 . 2 . 1 . 0 / 27 $vnet2 = Set-AzVirtualNetwork -VirtualNetwork $vnet2 $vnet3 = Get-AzVirtualNetwork -Name VNet3 -ResourceGroupName ExamRefRG $vnet3 . Subnets += New-AzVirtualNetworkSubnetConfig -Name GatewaySubnet -AddressPrefix 10 . 3 . 1 . 0 / 27 $vnet3 = Set-AzVirtualNetwork -VirtualNetwork $vnet3 Create VPN gateway in VNet2 $gwpip2 = New-AzPublicIpAddress -Name VNet2-GW-IP -ResourceGroupName ExamRefRG -Location $vnet2 . Location -AllocationMethod Dynamic $gwsubnet2 = Get-AzVirtualNetworkSubnetConfig -Name 'GatewaySubnet' -VirtualNetwork $vnet2 $gwipconf2 = New-AzVirtualNetworkGatewayIpConfig -Name GwIPConf2 -Subnet $gwsubnet2 -PublicIpAddress $gwpip2 $vnet2gw = New-AzVirtualNetworkGateway -Name VNet2-GW -ResourceGroupNAme ExamRefR -Location $vnet2 . Location -IpConfigurations $gwipconf2 -GatewayType Vpn -VpnType RouteBased -GatewaySku VpnGw1 Create VPN gateway in VNet3 $gwpip3 = New-AzPublicIpAddress -Name VNet3-GW-IP -ResourceGroupName ExamRefR -Location $vnet3 . Location -AllocationMethod Dynamic $gwsubnet3 = Get-AzVirtualNetworkSubnetConfig -Name 'GatewaySubnet' -VirtualNetwork $vnet3 $gwipconf3 = New-AzVirtualNetworkGatewayIpConfig -Name GwIPConf3 -Subnet $gwsubnet3 -PublicIpAddress $gwpip3 $vnet3gw = New-AzVirtualNetworkGateway -Name VNet3-GW -ResourceGroupNAme ExamRefRG -Location $vnet3 . Location -IpConfigurations $gwipconf3 -GatewayType Vpn -VpnType RouteBased -GatewaySku VpnGw1 Create connections New-AzVirtualNetworkGatewayConnection -Name VNet2-to-VNet3 -ResourceGroupName ExamRefRG -Location $vnet2 . Location -VirtualNetworkGateway1 $vnet2gw -VirtualNetworkGateway2 $vnet3gw -ConnectionType VNet2VNet -SharedKey \"secretkey123\" New-AzVirtualNetworkGatewayConnection -Name VNet3-to-VNet2 -ResourceGroupName ExamRefRG -Location $vnet3 . Location -VirtualNetworkGateway1 $vnet3gw -VirtualNetworkGateway2 $vnet2gw -ConnectionType VNet2VNet -SharedKey \"secretkey123\" Create gateway subnets in VNet2 and VNet3 az network vnet subnet create --name GatewaySubnet --vnet-name VNet2 --resource-group ExamRefRG --address-prefixes 10 .2.1.0/27 az network vnet subnet create --name GatewaySubnet --vnet-name VNet3 --resource-group ExamRefRG --address-prefixes 10 .3.1.0/27 Create public IP addresses for use by VPN gateways az network public-ip create --name VNet2-GW-IP --resource-group ExamRefRG --location NorthEurope az network public-ip create --name VNet3-GW-IP --resource-group ExamRefRG --location WestEurope Create VPN gateways in VNet2 and VNet 3 az network vnet-gateway create --name VNet2-GW --resource-group ExamRefRG --gateway-type vpn --sku VpnGw1 --vpn-type RouteBased --vnet VNet2 --public-ip-addresses VNet2-GW-IP --location NorthEurope az network vnet-gateway create --name VNet3-GW --resource-group ExamRefRG --gateway-type vpn --sku VpnGw1 --vpn-type RouteBased --vnet VNet3 --public-ip-addresses VNet3-GW-IP --location WestEurope Create connections between VPN gateways az network vpn-connection create --name VNet2-to-VNet3 --resource-group ExamRefRG --vnet-gateway1 VNet2-GW --vnet-gateway2 VNet3-GW --shared-key secretkey123 --location NorthEurope az network vpn-connection create --name VNet3-to-VNet2 --resource-group ExamRefRG --vnet-gateway1 VNet3-GW --vnet-gateway2 VNet2-GW --shared-key secretkey123 --location WestEurope","title":"Create a VPN gateway and VNet peering"},{"location":"Infrastructure/Cloud/Azure/Azure-VPN/#use-vpn-troubleshoot","text":"Get the Network Watcher resource $nw = Get-AzResource | Where ResourceType -eq Microsoft . Network / networkWatchers -and Location -eq WestEurope $networkWatcher = Get-AzNetworkWatcher -Name $nw . Name -ResourceGroupName $nw . ResourceGroupName Get the connection to troubleshoot $connection = Get-AzVirtualNetworkGatewayConnection -Name Vnet1-to-Vnet2 -ResourceGroupName ExamRefRG Start VPN Troubleshoot Start-AzNetworkWatcherResourceTroubleshooting -NetworkWatcher $networkWatcher -TargetResourceId $connection . Id -StorageId $sa . Id -StoragePath \" $( $sa . PrimaryEndpoints . Blob )$( $sc . name ) \" Create a storage account and container for logs az storage account create --name examrefstorage --location westeurope --resource-group ExamRefRG --sku Standard_LRS az storage account keys list --resource-group ExamRefRG --account-name examrefstorage az storage container create --account-name examrefstorage --account-key { storageAccountKey } --name logs Start VPN Troubleshoot az network watcher troubleshooting start --resource-group ExamRefRG --resource Vnet1-to-Vnet2 --resource-type vpnConnection --storage-account examrefstorage --storage-path https://examrefstorage.blob.core.windows.net/logs --output json","title":"Use VPN Troubleshoot"},{"location":"Infrastructure/Cloud/Azure/Azure-VPN/#create-s2s-vpn","text":"AZ-103: 395 $lgwip = 53.50.123.195 $key = \"abc123\" $lgw = New-AzLocalNetworkGateway -ResourceGroupName $g -Name $n -Location $l -GatewayIpAddress $lgwip -AddressPrefix \"10.5.0.0/16\" $vgw = Get-AzVirtualNetworkGateway -ResourceGroupNAme -Name New-AzVirtualNetworkGatewayConnection -ResourceGroupName $g -Name $n -Location $l -VirtualNetworkGateway1 $vgw -LocalNetworkGateway2 $lgw -ConnectionType IPsec -SharedKey $key az network local-gateway create --gateway-ip-address $lgwip --name LocalNetGW --resource-group ExamRefRG --local-address-prefixes 10 .5.0.0/16 az network vpn-connection create --name OnPremConnection --resource-group ExamRefRG --vnet-gateway1 VPNGW1 --location WestEurope --shared-key $key --local-gateway2 LocalNetGW Sources VPN Gateway design Connect Azure VPN gateways to multiple on-premises policy-based VPN devices About VPN Gateway configuration settings Highly available cross-premises and VNet-to-VNet connectivity ExpressRoute connectivity models Connect a computer to a virtual network using P2S and RADIUS authentication: PowerShell","title":"Create S2S VPN"},{"location":"Infrastructure/Cloud/Azure/Kusto/","text":"In Kusto documentation T typically refers to the Table being queried: T | where Predicate <> is equivalent to != SecurityEvent | where Level <> 8 | where EventID==4672 Select columns to include, rename, or drop T | project X=C, // Rename column C to X A=2*B, // Calculate a new column A from the old B C=strcat(\"-\",tostring(C)), // Calculate a new column C from the old C B=2*B // Calculate a new column B from the old B StormEvents | extend label = case ( DamageProperty < 1000, \"Storm\", DamageProperty > 1000 and DamageProperty < 10000, \"Disaster\", \"Catastrophe\" ) | summarize count() by label The SecurityEvent table provided as part of the [Log Analytics][Log Analytics] workspace trainingg dataset contains event viewer logs typical of what a security analyst would analyze, with the following columns: TimeGenerated Account AccountType (Machine or User) Computer EventSourceName Channel CommandLine Find logons, producing number of logins per Computer for computers with names beginning with \"App\" SecurityEvent | where TimeGenerated between ( ago ( 14 d ).. ago ( 7 d )) | where EventID == 4624 | where Computer startswith \"App\" | summarize count () by Computer SecurityEvent | where EventID == 4688 | summarize count () by CommandLine , Computer id title director year length_minutes 1 Toy Story John Lasseter 1995 81 2 A Bug's Life John Lasseter 1998 95 3 Toy Story 2 John Lasseter 1999 93 4 Monsters, Inc. Pete Docter 2001 92 5 Finding Nemo Andrew Stanton 2003 107 6 The Incredibles Brad Bird 2004 116 7 Cars John Lasseter 2006 117 8 Ratatouille Brad Bird 2007 115 9 WALL-E Andrew Stanton 2008 104 10 Up Pete Docter 2009 101 11 Toy Story 3 Lee Unkrich 2010 103 12 Cars 2 John Lasseter 2011 120 13 Brave Brenda Chapman 2012 102 14 Monsters University Dan Scanlon 2013 110 Find the title of each film Movies | project title Number of reporting computers each hour AZ-103: 53 Heartbeat | summarize dcount(ComputerIP) by bin(TimeGenerated, 1h) | render timechart List top 10 VMs with most error events over the past day MeasureUp Event | where (EventLevelName == \"Error\") | where (TimeGenerated > ago(1days)) | summarize ErrorCount = count() by Computer | top 10 by ErrorCount desc Render a SQL query as KQL EXPLAIN SELECT name FROM greeks ; Count instances of a value movies | summarize movies_directed = count() by director Create a new column dynamically from others movies | extend age = 2020 - year | project name , age ; Hide secrets from the queries log print h\"Hello world!\"; .show Kusto clusters can be provisioned and Kusto databases created and manipulated using both PowerShell and Azure CLI. The Azure CLI kusto module will not be supported after 01/01/2021. az extension add -n kusto Create cluster Azure PowerShell Azure CLI New-AzKustoCluster -ResourceGroupName testrg -Name testnewkustocluster -Location 'East US' -SkuName Standard_D11_v2 -SkuTier Standard -EnableDoubleEncryption true az kusto cluster create --name --resource-group --sku az kusto database create Create table Connect to database #connect cluster('jasper.eastus').database('test'); Create table .create table starships (Name:string, Registry:string, Class:string, Crew:int32) Ingest data . ingest into table T h 'https://raw.githubusercontent.com/jasper-zanjani/dogfood/master/csv/greeks.csv' with ( ignoreFirstRecord = true ) Alternatively, define a new datatable inline let starships = datatable ( Name : string , Class : string , Registry : string , Crew : int ) [ \"USS Enterprise\" , \"Constitution\" , \"NCC-1701\" , 203 , \"USS Constitution\" , \"Constitution\" , \"NCC-1700\" , 204 , \"USS Defiant\" , \"Defiant\" , \"NX-74205\" , 50 , \"USS Voyager\" , \"Intrepid\" , \"NCC-74656\" , 141 , \"USS Enterprise\" , \"Galaxy\" , \"NCC-1701-D\" , 6000 , \"USS Reliant\" , \"Miranda\" , \"NCC-1864\" , 35 ]; Search for a word starships | search \"enterprise\" ; search in ( SecurityEvent ) \"Cryptographic\" | take 10 ; Datetime values support a menagerie of functions print datetime ( 2015 - 01 - 01 ) # 2015 - 01 - 01 00 : 00 : 00 .0000 print format_datetime ( datetime ( 2015 - 01 - 01 ), \"yyyy\" ) # 2015 Concatenate values from other columns. StormEvents | project EpisodeId , where_storm = strcat ( EventType , \" in \" , State ); Export data . export async to sql [ 'dbo.StormEventTypeTable' ] Sources Azure Data Explorer documentation How to start with Microsoft Azure Data Explorer KQL quick reference SQL to Kusto cheat sheet Kusto.Explorer Azure Sentinel webinar parts 1, 2 , 3 KQL syntax: count , take","title":"Kusto"},{"location":"Linux/","text":"Overview SystemD is the de facto Linux init system on modern distributions. A process runs in its own user address space , a protected space which can't be disturbed by other users all processes on a Linux system are child processes of a common parent: the init process which is executed by the kernel at boot time (PID 1) every Linux process inherits the environment (PATH variable, etc) and other attributes of its parent process Every process has a parent; a process can spawn children in a process that is actually made of two separate system calls. Shell-internal commands ( cd , echo , etc. and variable assignments) do not spawn child processes Shell scripts are executed by spawning a sub-shell, which becomes the script's parent External commands are spawned as children of the parent as described above Bootloaders like GRUB (GRand Unified Bootloader) turn on power supplies and scan buses and interfaces to locate the kernel image and the root filesystem. LILO (LInux LOader) is also another bootloader that can be found on older Linux systems. Microcontrollers may be listening when the system is nominally off; they typically have their own BIOS and kernels and are inaccessible from the main system: Baseboard Management Controller (BMC) responds to wake-on-LAN (WOL) Intel Management Engine (IME) x64 software suite for remote management of systems; firmware is based on Minix and runs on the Platform Controller Hub processor, not the main CPU System Management Mode (SMM) launches UEFI software Linux kernel is typically named vmlinux (or vmlinuz when compressed). Kernel ring buffer contains messages related to the Linux kernel. A ring buffer is a data structure that is always the same size; old messages are discarded as new ones come in, once the buffer is full. dmesg is used to see its contents, and the messages are also stored in /var/log/dmesg Kernel modules can be loaded, listed, or removed from the running kernel. Security Similar to DLL files on Windows systems, .so (\"shared object\") library files on Linux allow code to be shared by various processes. They are vulnerable to injection attacks . Library injection vulnerability One file in particular, linux-vdso.so.1 , finds and locates other shared libraries and is mapped by the kernel into the address space of every process. This library-loading mechanism can be exploited through the use of the environment variable LD_PRELOAD , which is considered the most convenient way to load a shared library in a process at startup. If defined, this variable is read by the system and the library is loaded immediately after linux-vdso.so.1 into every process that is run. This attack can be detected using the osquery tool. This tool represents the system as a relational database which can then be queried, in particular against the process_envs table. Filesystem access control lists (FACL) allow you to grant permissions to more than one group, i.e. in cases where more than one department of a corporation needs access to the same files. They are made up of access control entries (ACE). FACL permissions will be indicated in a ls -l command by the presence of a \"+\" after the symbolic notation for the traditional UGO permissions. Acl is a dependency of systemd . To enable it, add \",acl\" to options in fstab file, then mount/unmount disk. If enabling FACL on root partition, system has to be rebooted. Glossary ALSA Advanced Linux Sound Architecture (ALSA) replaced the earlier \"Open Sound System\". ( src ) ALSA kernel modules are designed to offer an interface that \"corresponds to that of the hardware\" to keep the modules simple, and similar cards will offer a similar interface. ALSA kernel modules offer two interfaces: operational and configuration Operational interface are exposed at /dev/, with 3 main types of devices: PCM devices, for recording or playing digitized sound samples, come in two varieties - output and input - and are numbered from 0, which is generally for analog multichannel sound. CTL or controls are for manipulating the internal mixer and routing of the card. Controls come in 3 types; Playback controls are associated with an output device or copy (input-to-output) routes Capture controls are associated with an input device or copy (output-to-input) routes Feature controls drive features of the card or mixer, usually just a switch to enable or disable the feature, though some also have levels. The Master Volume control is the most typical example, which allows control of the internal amplifier feature of the card. A more interesting example is that of a 3D spatializer that can be represented by a switch to enable or disable it as well as two levels. MIDI to control the MIDI port, if it exists Optionally, sequencer devices may also exist if the card has a builtin sound synthesizer with an associated timer device Configuration interfaces are exposed at /proc/asound/ tree (ref amixer ) Cards have input or output sockets , and the mixer is controlled by the CTL device and routes sound samples among devices and sockets. Typical channel assignments - 0 : front left - 1 : front right - 2 : rear left - 3 : rear right Berkeley Software Distribution (BSD) BSD began in the 70s and was based on AT&T original code. First source distributions required user to purchase a source license from AT&T, since much of the BSD source was derivative of UNIX. Berkeley finally released a \"wholly-BSD\" product as Network Release 1 in 1989, which satisfied vendor demand for the TCP/IP networking code for PC. Work immediately began to reconstruct the remaining functionality of UNIX, which was completed in Network Release 2, released in 1991, which was based entirely on Berkeley code. Eventually this resulted in the 386BSD distribution, which then spawned five interrelated BSD distros: BSDI (now Wind River), NetBSD, FreeBSD, OpenBSD, and Darwin/Mac OS X Unix System Laboratories (USL) sued BSDI after BSDI attempted to market its product as a real UNIX, and other BSD distributions were affected by disputed code. Ultimately 3 out of the 18,000 files that made up the Network Release 2 distribution were removed, which became known as BSD-lite, released in 1994. This legal dispute was partly to blame for Linux's rapid ascent in popularity. Distributions Alpine Linux is a security-oriented, lightweight Linux distribution used in containers and hardware. Clear Linux is a rolling release distro from Intel with a custom package management system based on bundles , collections of packages that contain everything an application requires, including dependencies. Clear's update process also has the ability to do delta downloads , preserving bandwidth. It does not provide access with unusual licenses, like ZFS, Chrome, or FFmpeg. SUSE OpenSUSE Leap is a rebuild of SUSE Linux Enterprise Server , similar to how CentOS was historically a rebuild of RHEL. SUSE Linux Enterprise Server (SLES) (\"slee\") is SUSE's fixed-release distribution of Linux intended for enterprises, and as such is comparable to Red Hat's RHEL. display manager Basically display managers are the login screens, while the GUI manipulated during normal use represents the desktop environment (i.e. GNOME, KDE, XFCE, etc). initrd (\"initial RAM disk\") A temporary file system that's loaded into memory when the system boots Pipewire Pipewire is a media server intended to facilitate audio and video handling in Linux as a replacement for PulseAudio and JACK. It exposes a graph-based processing engine that abstracts audio and video devices. PulseAudio PulseAudio is a sound server for POSIX OSes and a fixture on many Linux distributions. PulseAudio is built around sources and sinks (i.e. devices) connected to source outputs and sink inputs (streams) Source is an input device that produces samples, usually running a thread with its own event loop, generating sample chunks which are posted to all connected source outputs Source output is a recording stream which consumes samples from a source Sink is an output device that consumes samples, usually running a thread with its own event loop mixing sample chunks from connect sink inputs Sink input is a playback stream, connected to a sink and producing samples for it qmail MTA designed as a drop-in replacement for Sendmail, notable for being the first to be \"security-aware\". Its various modular subcomponents run independently and are mutually untrustful. It uses SMTP to exchange messages with other MTAs. It was written by Dan Bernstein, a professor of mathematics famous for litigating against the US government with regard to export controls on encryption algorithms. qmail was deprecated and removed from Arch repos in 2005. SMB Client/server protocol developed in the early 1980s by Intel, Microsoft, and IBM that has become the native protocol for file and printer sharing on Windows. It is implemented in the Samba application suite. WSL Windows Subsystem for Linux (WSL) is shipped with Windows and tied to the Windows release cycle. Windows ships from a single massive codebase, of which WSL is part. WSL was written mostly in C and and has 3 million monthly active users. WSL implements user services to connect to WSL distros and to run Windows-native applications like CMD.exe. WSL implements a 9P Protocol file server to provide seamless integration of the virtualized Linux filesystem and that of the Windows host. WSL 1 worked under a translation architecture where system calls were translated to NT kernel calls. This meant that applications that used system calls that were newer or more difficult to implement, like GUI applications or Docker, did not run on v1. WSL2 shifted to a lighweight virtualization model using the Linux kernel. Now Docker runs on WSL2 and GUI applications can run by using an X server. WSL v1 is available on Azure VMs if nested virtualization is enabled. WSL2 support is forthcoming. VHDs for WSL distributions are available at %LOCALAPPDATA%\\Packages\\<PackageFamilyName>\\LocalState where <PackageFamilyName> reflects the name of the Microsoft Store package of the distro, i.e.: - CanonicalGroupLimited.Ubuntu20.04onWindows_79rhkp1fndgsc - TheDebianProject.DebianGNULinux_76v4gfsz19hv4 Remove a WSL distribution wsl.exe --unregister Ubuntu-20.04 By default, WSL appears to copy the Windows native hosts file at %SystemRoot%\\System32\\drivers\\etc\\hosts to the distro's /etc/hosts file.","title":"Overview"},{"location":"Linux/#overview","text":"SystemD is the de facto Linux init system on modern distributions. A process runs in its own user address space , a protected space which can't be disturbed by other users all processes on a Linux system are child processes of a common parent: the init process which is executed by the kernel at boot time (PID 1) every Linux process inherits the environment (PATH variable, etc) and other attributes of its parent process Every process has a parent; a process can spawn children in a process that is actually made of two separate system calls. Shell-internal commands ( cd , echo , etc. and variable assignments) do not spawn child processes Shell scripts are executed by spawning a sub-shell, which becomes the script's parent External commands are spawned as children of the parent as described above Bootloaders like GRUB (GRand Unified Bootloader) turn on power supplies and scan buses and interfaces to locate the kernel image and the root filesystem. LILO (LInux LOader) is also another bootloader that can be found on older Linux systems. Microcontrollers may be listening when the system is nominally off; they typically have their own BIOS and kernels and are inaccessible from the main system: Baseboard Management Controller (BMC) responds to wake-on-LAN (WOL) Intel Management Engine (IME) x64 software suite for remote management of systems; firmware is based on Minix and runs on the Platform Controller Hub processor, not the main CPU System Management Mode (SMM) launches UEFI software Linux kernel is typically named vmlinux (or vmlinuz when compressed). Kernel ring buffer contains messages related to the Linux kernel. A ring buffer is a data structure that is always the same size; old messages are discarded as new ones come in, once the buffer is full. dmesg is used to see its contents, and the messages are also stored in /var/log/dmesg Kernel modules can be loaded, listed, or removed from the running kernel.","title":"Overview"},{"location":"Linux/#security","text":"Similar to DLL files on Windows systems, .so (\"shared object\") library files on Linux allow code to be shared by various processes. They are vulnerable to injection attacks . Library injection vulnerability One file in particular, linux-vdso.so.1 , finds and locates other shared libraries and is mapped by the kernel into the address space of every process. This library-loading mechanism can be exploited through the use of the environment variable LD_PRELOAD , which is considered the most convenient way to load a shared library in a process at startup. If defined, this variable is read by the system and the library is loaded immediately after linux-vdso.so.1 into every process that is run. This attack can be detected using the osquery tool. This tool represents the system as a relational database which can then be queried, in particular against the process_envs table. Filesystem access control lists (FACL) allow you to grant permissions to more than one group, i.e. in cases where more than one department of a corporation needs access to the same files. They are made up of access control entries (ACE). FACL permissions will be indicated in a ls -l command by the presence of a \"+\" after the symbolic notation for the traditional UGO permissions. Acl is a dependency of systemd . To enable it, add \",acl\" to options in fstab file, then mount/unmount disk. If enabling FACL on root partition, system has to be rebooted.","title":"Security"},{"location":"Linux/#glossary","text":"","title":"Glossary"},{"location":"Linux/#alsa","text":"Advanced Linux Sound Architecture (ALSA) replaced the earlier \"Open Sound System\". ( src ) ALSA kernel modules are designed to offer an interface that \"corresponds to that of the hardware\" to keep the modules simple, and similar cards will offer a similar interface. ALSA kernel modules offer two interfaces: operational and configuration Operational interface are exposed at /dev/, with 3 main types of devices: PCM devices, for recording or playing digitized sound samples, come in two varieties - output and input - and are numbered from 0, which is generally for analog multichannel sound. CTL or controls are for manipulating the internal mixer and routing of the card. Controls come in 3 types; Playback controls are associated with an output device or copy (input-to-output) routes Capture controls are associated with an input device or copy (output-to-input) routes Feature controls drive features of the card or mixer, usually just a switch to enable or disable the feature, though some also have levels. The Master Volume control is the most typical example, which allows control of the internal amplifier feature of the card. A more interesting example is that of a 3D spatializer that can be represented by a switch to enable or disable it as well as two levels. MIDI to control the MIDI port, if it exists Optionally, sequencer devices may also exist if the card has a builtin sound synthesizer with an associated timer device Configuration interfaces are exposed at /proc/asound/ tree (ref amixer ) Cards have input or output sockets , and the mixer is controlled by the CTL device and routes sound samples among devices and sockets. Typical channel assignments - 0 : front left - 1 : front right - 2 : rear left - 3 : rear right Berkeley Software Distribution (BSD) BSD began in the 70s and was based on AT&T original code. First source distributions required user to purchase a source license from AT&T, since much of the BSD source was derivative of UNIX. Berkeley finally released a \"wholly-BSD\" product as Network Release 1 in 1989, which satisfied vendor demand for the TCP/IP networking code for PC. Work immediately began to reconstruct the remaining functionality of UNIX, which was completed in Network Release 2, released in 1991, which was based entirely on Berkeley code. Eventually this resulted in the 386BSD distribution, which then spawned five interrelated BSD distros: BSDI (now Wind River), NetBSD, FreeBSD, OpenBSD, and Darwin/Mac OS X Unix System Laboratories (USL) sued BSDI after BSDI attempted to market its product as a real UNIX, and other BSD distributions were affected by disputed code. Ultimately 3 out of the 18,000 files that made up the Network Release 2 distribution were removed, which became known as BSD-lite, released in 1994. This legal dispute was partly to blame for Linux's rapid ascent in popularity.","title":"ALSA"},{"location":"Linux/#distributions","text":"Alpine Linux is a security-oriented, lightweight Linux distribution used in containers and hardware. Clear Linux is a rolling release distro from Intel with a custom package management system based on bundles , collections of packages that contain everything an application requires, including dependencies. Clear's update process also has the ability to do delta downloads , preserving bandwidth. It does not provide access with unusual licenses, like ZFS, Chrome, or FFmpeg. SUSE OpenSUSE Leap is a rebuild of SUSE Linux Enterprise Server , similar to how CentOS was historically a rebuild of RHEL. SUSE Linux Enterprise Server (SLES) (\"slee\") is SUSE's fixed-release distribution of Linux intended for enterprises, and as such is comparable to Red Hat's RHEL. display manager Basically display managers are the login screens, while the GUI manipulated during normal use represents the desktop environment (i.e. GNOME, KDE, XFCE, etc). initrd (\"initial RAM disk\") A temporary file system that's loaded into memory when the system boots","title":"Distributions"},{"location":"Linux/#pipewire","text":"Pipewire is a media server intended to facilitate audio and video handling in Linux as a replacement for PulseAudio and JACK. It exposes a graph-based processing engine that abstracts audio and video devices.","title":"Pipewire"},{"location":"Linux/#pulseaudio","text":"PulseAudio is a sound server for POSIX OSes and a fixture on many Linux distributions. PulseAudio is built around sources and sinks (i.e. devices) connected to source outputs and sink inputs (streams) Source is an input device that produces samples, usually running a thread with its own event loop, generating sample chunks which are posted to all connected source outputs Source output is a recording stream which consumes samples from a source Sink is an output device that consumes samples, usually running a thread with its own event loop mixing sample chunks from connect sink inputs Sink input is a playback stream, connected to a sink and producing samples for it qmail MTA designed as a drop-in replacement for Sendmail, notable for being the first to be \"security-aware\". Its various modular subcomponents run independently and are mutually untrustful. It uses SMTP to exchange messages with other MTAs. It was written by Dan Bernstein, a professor of mathematics famous for litigating against the US government with regard to export controls on encryption algorithms. qmail was deprecated and removed from Arch repos in 2005. SMB Client/server protocol developed in the early 1980s by Intel, Microsoft, and IBM that has become the native protocol for file and printer sharing on Windows. It is implemented in the Samba application suite.","title":"PulseAudio"},{"location":"Linux/#wsl","text":"Windows Subsystem for Linux (WSL) is shipped with Windows and tied to the Windows release cycle. Windows ships from a single massive codebase, of which WSL is part. WSL was written mostly in C and and has 3 million monthly active users. WSL implements user services to connect to WSL distros and to run Windows-native applications like CMD.exe. WSL implements a 9P Protocol file server to provide seamless integration of the virtualized Linux filesystem and that of the Windows host. WSL 1 worked under a translation architecture where system calls were translated to NT kernel calls. This meant that applications that used system calls that were newer or more difficult to implement, like GUI applications or Docker, did not run on v1. WSL2 shifted to a lighweight virtualization model using the Linux kernel. Now Docker runs on WSL2 and GUI applications can run by using an X server. WSL v1 is available on Azure VMs if nested virtualization is enabled. WSL2 support is forthcoming. VHDs for WSL distributions are available at %LOCALAPPDATA%\\Packages\\<PackageFamilyName>\\LocalState where <PackageFamilyName> reflects the name of the Microsoft Store package of the distro, i.e.: - CanonicalGroupLimited.Ubuntu20.04onWindows_79rhkp1fndgsc - TheDebianProject.DebianGNULinux_76v4gfsz19hv4 Remove a WSL distribution wsl.exe --unregister Ubuntu-20.04 By default, WSL appears to copy the Windows native hosts file at %SystemRoot%\\System32\\drivers\\etc\\hosts to the distro's /etc/hosts file.","title":"WSL"},{"location":"Linux/Concurrency/","text":"Concurrency Glossary Linearizability In concurrent programming, an operation is linearizable if it consists of an ordered list of callbacks that may be extended by adding response events such that: The extended list is serializable The serializable sequential history is a subset of the original unextended list Lock When a thread wants a lock already owned by another thread, the thread is blocked and must wait until the lock becomes free. Spinning locks are suitable for very short timeframes Sleeping locks, including mutexes The Linux kernel also provides CPU local locks Mutex In the Linux kernel, mutex refers to a particular locking primitive that enforces serialization on shared memory systems. Serializability In transaction processing, a transaction schedule is serializable if its outcome is equal to the outcome of its individual transactions executed serially. Because transactions are normally executed concurrently, this is the major correctness criterion for concurrent transactions. Spinlock A lock that causes a thread trying to acquire it to simply wait in a loop (\"spin\") while repeatedly checking whether the lock is available. Transaction In computer science, transactions are individual, indivisible operations that must succeed or fail as a complete unit. Transaction processing guards against hardware and software errors that might leave a transaction partially completed. These operations are executed on databases or modern filesystems to ensure the system is in a consistent state.","title":"Concurrency"},{"location":"Linux/Concurrency/#concurrency","text":"","title":"Concurrency"},{"location":"Linux/Concurrency/#glossary","text":"","title":"Glossary"},{"location":"Linux/Concurrency/#linearizability","text":"In concurrent programming, an operation is linearizable if it consists of an ordered list of callbacks that may be extended by adding response events such that: The extended list is serializable The serializable sequential history is a subset of the original unextended list","title":"Linearizability"},{"location":"Linux/Concurrency/#lock","text":"When a thread wants a lock already owned by another thread, the thread is blocked and must wait until the lock becomes free. Spinning locks are suitable for very short timeframes Sleeping locks, including mutexes The Linux kernel also provides CPU local locks","title":"Lock"},{"location":"Linux/Concurrency/#mutex","text":"In the Linux kernel, mutex refers to a particular locking primitive that enforces serialization on shared memory systems.","title":"Mutex"},{"location":"Linux/Concurrency/#serializability","text":"In transaction processing, a transaction schedule is serializable if its outcome is equal to the outcome of its individual transactions executed serially. Because transactions are normally executed concurrently, this is the major correctness criterion for concurrent transactions.","title":"Serializability"},{"location":"Linux/Concurrency/#spinlock","text":"A lock that causes a thread trying to acquire it to simply wait in a loop (\"spin\") while repeatedly checking whether the lock is available.","title":"Spinlock"},{"location":"Linux/Concurrency/#transaction","text":"In computer science, transactions are individual, indivisible operations that must succeed or fail as a complete unit. Transaction processing guards against hardware and software errors that might leave a transaction partially completed. These operations are executed on databases or modern filesystems to ensure the system is in a consistent state.","title":"Transaction"},{"location":"Linux/Containers/","text":"Containers Containers run applications in an isolated namespace , meaning it only has access to resources that are made available to it by the container runtime. Resource governance means that a container has access only to a specified number of processor cycles, system memory, and other resources. Containers allow applications to be packaged with their dependencies in container images , which will run the same regardless of underlying operating system or infrastructure and are downloaded from container registries like Docker Hub . Container registries are not to be confused with repositories , which are subcomponents of registries. Cgroups History \"Task Control Groups\" were first merged into kernel 2.6.24 with the ability for multiple hierarchies to be created. The logic behind the creation of multiple hierarchies was to enable maximum flexibility in policy creation. However, because a controller could only belong to a single hierarchy, after some years this began to be seen as a design flaw . This motivated a redesign into a single unified cgroup hierarchy , and v2 was merged in 3.16 and made stable in 4.5 . Control groups or cgroups are a Linux kernel feature that isolates, labels, and manages resources (CPU time, memory, and network bandwidth) for a collection of tasks (processes). Cgroup subsystems (also called controllers or resource controllers in documentation) represent a single resource (i.e. io , cpu , memory , devices ). freezer suspends or resumes tasks in a cgroup net_cls tags network packets with a class identifier that allows the Linux traffic controller to identify packets originating from a particular cgroup task net_prio allows network traffic to be prioritized per interface ns the namespace subsystem Since cgroups v2, all mounted controllers reside in a single unified hierarchy. A list of these is generated by the Linux kernel at /proc/cgroups . You can confirm that the cgroup2 filesystem is mounted at /sys/fs/cgroup mount -l | grep cgroup - Kubernetes History Kubernetes was first announced by Google in mid-2014. It had been developed by Google after deciding to open-source the Borg system, a cluster and container management system that formed the automation infrastructure that powered the entire Google enterprise. Kubernetes coalesced from a fusion between developers working on Borg and Compute Engine . Borg eventually evolved into Omega. By that time, Amazon had established a market advantage and the developers decided to change their approach by introducing a disruptive technology to drive the relevance of the Compute platform they had built. They created a ubiquitous abstraction that could run better than anyone else. At the time, Google had been trying to engage the Linux kernel team and trying to overcome their skepticism. Internally, the project was framed as offering \"Borg as a Service\", although there were concerns that Google was in danger of revealing trade secrets. Google ultimately donated Kubernetes to the Cloud Native Computing Foundation . Kubernetes's heptagonal logo is an allusion to when it was called \"Project 7\" as a reference to Star Trek's Borg character 7 of 9. Kubernetes (Greek for \"helmsman\", \"pilot\", or \"captain\" and \"k8s\" for short) has emerged as the leading container orchestrator in the industry since 2018. It provides a layer that abstracts infrastructure, including computers, networks, and other computers, for applications deployed on top. Kubernetes can be visualized as a system built from layers, with each higher layer abstracting the complexity of the lower levels. One server serves as the master , exposing an API for users and clients, assigning workloads, and orchestrating communication between other components. The processes on the master node are also called the control plane . Other machines in the cluster are called nodes or workers and accept and run workloads using available resources. A Kubernetes configuration files is called a kubeconfig . Kubernetes resources or objects , each associated with a URL, represent the configuration of a cluster. Resource and object are often used interchangeably, but more precisely the resource refers to the URL path that points to the object, and an object may be accessible through multiple resources. Every object type in the Kubernetes API has a controller (i.e. deployment controller, etc.) that reads desired state from the Spec section of the manifest and reports its actual state by writing to the Status section. An object's manifest , presented in JSON or YAML, represents its declarative configuration, and contains four sections: type metadata, specifying the type of resource object metadata, specifying name and other identifying information spec : desired state of resource state : produced strictly by the resource controller and represents the current status of resource An explanation of each field available in the API of any object type can be displayed on the command-line kubectl explain nodes kubectl explain no.spec Display the manifest of a node kubectl get node $NODE -o yaml kubectl describe node kind-worker-2 A pod is the most atomic unit of work which encompasses one or more tightly-coupled containers that will be deployed together on the same node. All containers in a pod share the same Linux namespace, hostname, IP address, network interfaces, and port space. This means containers in a pod can communicate with each other over localhost, although care must be taken to ensure individual containers do not attempt to use the same port. However their filesystems are isolated from one another unless they share a Volume . Every Pod occupies one address in a shared range, so communication between Pods is simple. Compute resources of containers can be limited at pod.spec.containers.resources . apiVersion : v1 kind : Pod metadata : name : nginx spec : containers : - image : nginx name : nginx resources : requests : memory : \"64Mi\" cpu : \"500m\" limits : memory : \"128Mi\" cpu : \"500m\" Kubernetes can monitor Pod health by using probes , which can be categorized by how they measure health: Readiness : i.e. Is the container ready to serve user requests? Liveness : i.e. Is the container running as intended? Tasks GKE Google Kubernetes Engine nodes are actually Google Compute Engine VMs. Create GKE cluster gcloud container clusters create hello-cluster --num-nodes = 1 # Standard cluster gcloud compute instances list # (1) gcloud container clusters create-auto hello-cluster # (2) gcloud container clusters describe hello-cluster If a default zone is set, an Autopilot cluster won't be able to be created without explicitly specifying --region . Save a Kubernetes cluster's credentials to a kubeconfig . gcloud container clusters get-credentials my-cluster Windows Server Windows Server 2016 supports Windows Server Containers and Hyper-V Containers , which create a separate copy of the operating system kernel for each container. The \"Containers\" feature must be installed on Windows Server 2016 hosts, and to create Hyper-V containers the Hyper-V role must also be installed (although the Hyper-V management tools are not necessary if VMs are not going to created). Windows container hosts need to have Windows installed to C:. Nano Server once could serve as Docker hosts, but no longer; Nano Servers are now intended to be deployed as containers themselves. The Powershell Docker module has been deprecated for years. Commands cgconfig.service cgconfig , which is a part of the libcgroup package, can be used to run at start time to reestablish predefined cgroups. kubectl Show available contexts kubectl config get-contexts Switch to a different context kubectl config use-context $NAMESPACE kubectl config use $NAMESPACE Display resources kubectl get nodes kubectl get pods kubectl get deployments kubectl describe nodes/gke-*4ff6f64a-6f4v Execute a command on a pod with only a single container, returning output kubectl exec $pod -- env Get a shell to a running container kubectl exec --stdin --tty $pod -- /bin/bash When a pod contains more than one container, the container must be specified with -c / --container . kubectl exec --stdin --tty $pod --container $container -- /bin/bash kubectl run nginx --image = nginx kubectl delete pod nginx kubectl create deployment nginx --image = nginx Number of replicas can be set on creation of a deployment by specifying an argument to --replicas kubectl create deployment nginx --image = nginx --replicas = 4 Replica count is set in an existing deployment by scaling kubectl scale deploy/nginx --replicas = 2 Expose a port kubectl expose deployment/nginx --port = 80 --type = LoadBalancer List Kubernetes objects kubectl api-resources Get a description of a resource kubectl explain nodes.status.addresses.address podman On RHEL, podman can be installed as a package or as part of a module dnf module install container-tools With few exceptions, podman exposes a command-line API that closely imitates that of Docker. Arch Linux On Arch, /etc/subuid and /etc/subgid have to be set . These are colon-delimited files that define the ranges for namespaced UIDs and GIDs to be used by each user. Conventionally, these ranges begin at 100,000 (for the first, primary user) and contain 65,536 possible values. terry:100000:65536 alice:165536:65536 Then podman has to be migrated podman system migrate Podman supports pulling from various repos using aliases that are defined in /etc/containers/registries.conf.d . RHEL and derivative distributions support additional aliases, some of which reference images that require a login . For example, Red Hat offers a Python 2.7 runtime from the RHSCL ( Red Hat Software Collections ) repository on registry.access.redhat.com, which does not require authentication. However, Python 3.8 is only available from registry.redhat.io, which does. Interestingly, other Python runtimes are available from the ubi7 and ubi8 repos from unauthenticated registries. Container images are stored in ~/.local/share/containers/storage . podman pull rhscl/httpd-24-rhel7 # (1) Alias to registry.access.redhat.com/rhscl/httpd-24-rhel7 The Z option is necessary on SELinux systems (like RHEL and derivatives) and tells Podman to label the content with a private unshared label. On systems running SELinux, rootless containers must be explicitly allowed to access bind mounts. Containerized processes are not allowed to access files without a SELinux label. podman run -d -v = /home/jasper/notes/site:/usr/share/nginx/html:Z -p = 8080 :80 --name = notes nginx podman run -d -v = /home/jasper/notes/site:/usr/local/apache2/htdocs:Z -p = 8080 :80 --name = notes httpd-24 Mapped ports can be displayed podman port -a Output a SystemD service file from a container to STDOUT (this must be redirected to a file) podman generate systemd notes \\ --restart-policy = always \\ --name \\ # (3) --files \\ # (2) --new \\ # (1) Yield unit files that do not expect containers and pods to exist but rather create them based on their configuration files. Generate a file with a name beginning with the prefix (which can be set with --container-prefix or --pod-prefix ) and followed by the ID or name (if --name is also specified) In conjunction with --files , name the service file after the container and not the ID number. systemd-cgls systemd-cgls recursively shows the contents of the selected cgroup hierarchy in a tree. Glossary apiVersion Kubernetes object field found in Type metadata. apiVersion is typically v1 , but for some object types the API group is specified, i.e. for Deployments: apiVersion : apps/v1 Dockerfile A Docker image consists of read-only layers , each of which represents an instruction that incrementally the changes the image being built up. Dockerfiles can be used to construct new images using docker build . The build process can be optimized by placing multiple commands in the same RUN instruction. Dockerfiles are named simply \"Dockerfile\" with no extension or variation. Node on Alpine Windows Server Nano Windows Server Core FROM alpine RUN apk update && apk add nodejs COPY . /app WORKDIR /app CMD [ \"node\" , \"index.js\" ] FROM microsoft/windowsservercore RUN powershell -command install-windowsfeature dhcp -includemanagementtools RUN powershell -configurationname microsoft.powershell -command add-dhcpserverv4scope -state active -activatepolicies $true -name scopetest -startrange 10 .0.0.100 -endrange 10 .0.0.200 -subnetmask 255 .255.255.0 RUN md boot COPY ./bootfile.wim c:/boot/ CMD powershell FROM microsoft/windowsservercore MAINTAINER @mike_pfeiffer RUN powershell.exe -Command Install-WindowsFeature Web-Server COPY ./websrc c:/inetpub/wwwroot CMD [ \"powershell\" ] Deployment A uniformly managed set of Pod instances, all based on the same container image. The Deployment controller enables release capabilities , the deployment of new Pod versions with no downtime. Exposing a Deployment creates a Service . Desired State Management The Desired State Management system is used by Kubernetes to describe a cluster's desired state declaratively. emptyDir Ephemeral Kubernetes volume type that shares the Pod's lifetime and where data is stored in RAM. emptyDir volumes can use tmpfs file systems. ENTRYPOINT Rarely used Docker declaration. When ENTRYPOINT is present, the CMD declaration becomes the default argument passed to the command in ENTRYPOINT. The Kubernetes --command flag ( pod.spec.containers.command resource) can override the contents of ENTRYPOINT. etcd Distributed key-value data store Event Kubernetes object type that contains information about what happened to the object. Events are deleted one hour after creation by default. kubectl get events kubectl get ev Unlike most other objects, Event manifests have no spec or status sections. Helm Helm is a package manager for Kubernetes. Helm packages are refered to as charts . Charts are a collection of files and directories that adhere to a specification. A chart is packed when tarred and gzipped. Chart.yaml contains metadata templates/ contains Kubernetes manifests potentially annotated with templating directives values.yaml provides default configuration It is managed using the helm CLI utility. Create a new chart helm create foo There is no longer a default Helm repository, although there are many available at the Artifact Hub kind Kubernetes object field found in the Type metadata which specifies the type of resource, i.e. Node , Deployment , Service , Pod , etc. kubeconfig YAML configuration file located at $HOME/.kube/config by default. A colon-delimited list of kubeconfigs can be specified by setting the KUBECONFIG environment variable. A kubeconfig can be explicitly specified with the --kubeconfig flag. Label Labels are key-value pairs that are attached to Kubernetes objects. Config for a Pod with two labels: apiVersion : v1 kind : Pod metadata : name : label-demo labels : environment : production app : nginx spec : containers : - name : nginx image : nginx:1.14.2 ports : - containerPort : 80 Master node A master node runs 3 processes, called master (control plane) components: kube-apiserver exposes a RESTful API and serves as a glue between other Kubernetes components kube-scheduler determines how to balance container workloads across nodes using an algorithm kube-controller-manager performs cluster operations like managing nodes and making changes to desired status millicore (m) One-thousandth of a vCPU or a CPU core and the preferred measurement unit of compute resources in Kubernetes (i.e. 128m = 12.8% of a CPU core and 2000m = 2 CPU cores). Namespaces Namespaces wrap global system resources (mount points, network devices, hostnames) in an abstraction that makes it appear to processes within that namespace that they have their own isolated instance of that resource. Process IDs in the same namespace can have access to one another, whereas those in different namespaces cannot. Spawning a process in a new namespace prevents it from seeing the host's context, so an interactive shell like zsh spawned in its own namespace will report its PID as 1 , even though the host will assign its own PID. Node A node or worker is any container host that accepts workloads from the master node. Each node is equipped with a container runtime like Docker, which it uses to create and destroy containers according to instructions from the master server. Each node runs 2 processes: kubelet communicates with Kubernetes cluster services kube-proxy handles container network routing using iptables rules PersistentVolume A PersistentVolume is a piece of storage in the cluster that has been provisioned using Storage Classes. PersistentVolumeClaim A PersistentVolumeClaim requests either Disk or File storage of a particular StorageClass, access mode, and size. It is bound to a PersistentVolume once an available storage resource has been assigned to the pod requesting it. Pod A pod is the most basic unit that K8s deals with, representing one or more tightly-coupled containers that should be controlled as a single application (typically one main container with subsidiary helper containers). Every container should have only a single process, so if several processes need to communicate they should be implemented as separate containers in a pod. A pod's containers should: operate closely together share a lifecycle always be scheduled on the same node Replica An instance of a Pod ReplicaSet ... Selector A label selector provides a way to identify a set of objects and is the core grouping primitive supported by Kubernetes. It can be made of multiple requirements that are comma-separated, all of which must be satisfied. There are two types of selector: Equality-based admits the operators = , != , and == . Set-based admits the operators in , notin , and exists . Equality-based selector Set-based selector environment = production tier != frontend environment in (production, qa) tier notin (frontend, backend) partition !partition Service A Service is an abstraction over a logical set of Pods and a policy by which to access them, i.e. a microservice. Because Pods are mortal, the Service controller keeps track of Pod addresses and publishes this information to the consumers of Services, a function called service discovery . tmpfs RAM-backed file system used in Docker containers Volume A volume is a special directory in the Docker host that can be mounted to the container that is used to achieve persistent storage. In Azure, a volume represents a way to store, retrieve, and persist data across pods and through the application lifecycle. In the context of Azure, Kubernetes can use two types of data volume: Azure Disks using Azure Premium (SSDs) or Azure Standard (HDDs). Azure Files using a SMB 3.0 share backed by an Azure Storage account. In Kubernetes, Volumes are an abstraction of file systems accessible from within a Pod's containers. Network storage device, such as gcePersistentVolume emptyDir , where the data is stored in RAM using Docker's tmpfs file system hostPath , where the volume is located within the node's file system. Because pods are expected to be created and destroyed on any node (which may themselves be destroyed and recreated), hostPath volumes are discommended. Volumes are declared in .spec.volumes and mounted into containers in .spec.containers[*].volumeMounts . emptyDir hostPath gcePersistentDisk apiVersion : v1 kind : Pod metadata : name : alpine spec : volumes : - name : data emptyDir : containers : - name : alpine image : alpine volumeMounts : - mountPath : \"/data\" name : \"data\" apiVersion : v1 kind : Pod metadata : name : alpine spec : volumes : - name : data hostPath : path : /var/data containers : - name : alpine image : alpine volumeMounts : - mountPath : \"/data\" name : \"data\" apiVersion : v1 kind : Pod metadata : name : alpine spec : volumes : - name : data gcePersistentDisk : pdName : my-disk fsType : ext4 containers : - name : alpine image : alpine volumeMounts : - mountPath : \"/data\" name : \"data\" Worker :material-kubernetes see Node","title":"Containers"},{"location":"Linux/Containers/#containers","text":"Containers run applications in an isolated namespace , meaning it only has access to resources that are made available to it by the container runtime. Resource governance means that a container has access only to a specified number of processor cycles, system memory, and other resources. Containers allow applications to be packaged with their dependencies in container images , which will run the same regardless of underlying operating system or infrastructure and are downloaded from container registries like Docker Hub . Container registries are not to be confused with repositories , which are subcomponents of registries.","title":"Containers"},{"location":"Linux/Containers/#cgroups","text":"History \"Task Control Groups\" were first merged into kernel 2.6.24 with the ability for multiple hierarchies to be created. The logic behind the creation of multiple hierarchies was to enable maximum flexibility in policy creation. However, because a controller could only belong to a single hierarchy, after some years this began to be seen as a design flaw . This motivated a redesign into a single unified cgroup hierarchy , and v2 was merged in 3.16 and made stable in 4.5 . Control groups or cgroups are a Linux kernel feature that isolates, labels, and manages resources (CPU time, memory, and network bandwidth) for a collection of tasks (processes). Cgroup subsystems (also called controllers or resource controllers in documentation) represent a single resource (i.e. io , cpu , memory , devices ). freezer suspends or resumes tasks in a cgroup net_cls tags network packets with a class identifier that allows the Linux traffic controller to identify packets originating from a particular cgroup task net_prio allows network traffic to be prioritized per interface ns the namespace subsystem Since cgroups v2, all mounted controllers reside in a single unified hierarchy. A list of these is generated by the Linux kernel at /proc/cgroups . You can confirm that the cgroup2 filesystem is mounted at /sys/fs/cgroup mount -l | grep cgroup -","title":"Cgroups"},{"location":"Linux/Containers/#kubernetes","text":"History Kubernetes was first announced by Google in mid-2014. It had been developed by Google after deciding to open-source the Borg system, a cluster and container management system that formed the automation infrastructure that powered the entire Google enterprise. Kubernetes coalesced from a fusion between developers working on Borg and Compute Engine . Borg eventually evolved into Omega. By that time, Amazon had established a market advantage and the developers decided to change their approach by introducing a disruptive technology to drive the relevance of the Compute platform they had built. They created a ubiquitous abstraction that could run better than anyone else. At the time, Google had been trying to engage the Linux kernel team and trying to overcome their skepticism. Internally, the project was framed as offering \"Borg as a Service\", although there were concerns that Google was in danger of revealing trade secrets. Google ultimately donated Kubernetes to the Cloud Native Computing Foundation . Kubernetes's heptagonal logo is an allusion to when it was called \"Project 7\" as a reference to Star Trek's Borg character 7 of 9. Kubernetes (Greek for \"helmsman\", \"pilot\", or \"captain\" and \"k8s\" for short) has emerged as the leading container orchestrator in the industry since 2018. It provides a layer that abstracts infrastructure, including computers, networks, and other computers, for applications deployed on top. Kubernetes can be visualized as a system built from layers, with each higher layer abstracting the complexity of the lower levels. One server serves as the master , exposing an API for users and clients, assigning workloads, and orchestrating communication between other components. The processes on the master node are also called the control plane . Other machines in the cluster are called nodes or workers and accept and run workloads using available resources. A Kubernetes configuration files is called a kubeconfig . Kubernetes resources or objects , each associated with a URL, represent the configuration of a cluster. Resource and object are often used interchangeably, but more precisely the resource refers to the URL path that points to the object, and an object may be accessible through multiple resources. Every object type in the Kubernetes API has a controller (i.e. deployment controller, etc.) that reads desired state from the Spec section of the manifest and reports its actual state by writing to the Status section. An object's manifest , presented in JSON or YAML, represents its declarative configuration, and contains four sections: type metadata, specifying the type of resource object metadata, specifying name and other identifying information spec : desired state of resource state : produced strictly by the resource controller and represents the current status of resource An explanation of each field available in the API of any object type can be displayed on the command-line kubectl explain nodes kubectl explain no.spec Display the manifest of a node kubectl get node $NODE -o yaml kubectl describe node kind-worker-2 A pod is the most atomic unit of work which encompasses one or more tightly-coupled containers that will be deployed together on the same node. All containers in a pod share the same Linux namespace, hostname, IP address, network interfaces, and port space. This means containers in a pod can communicate with each other over localhost, although care must be taken to ensure individual containers do not attempt to use the same port. However their filesystems are isolated from one another unless they share a Volume . Every Pod occupies one address in a shared range, so communication between Pods is simple. Compute resources of containers can be limited at pod.spec.containers.resources . apiVersion : v1 kind : Pod metadata : name : nginx spec : containers : - image : nginx name : nginx resources : requests : memory : \"64Mi\" cpu : \"500m\" limits : memory : \"128Mi\" cpu : \"500m\" Kubernetes can monitor Pod health by using probes , which can be categorized by how they measure health: Readiness : i.e. Is the container ready to serve user requests? Liveness : i.e. Is the container running as intended?","title":"Kubernetes"},{"location":"Linux/Containers/#tasks","text":"","title":"Tasks"},{"location":"Linux/Containers/#gke","text":"Google Kubernetes Engine nodes are actually Google Compute Engine VMs. Create GKE cluster gcloud container clusters create hello-cluster --num-nodes = 1 # Standard cluster gcloud compute instances list # (1) gcloud container clusters create-auto hello-cluster # (2) gcloud container clusters describe hello-cluster If a default zone is set, an Autopilot cluster won't be able to be created without explicitly specifying --region . Save a Kubernetes cluster's credentials to a kubeconfig . gcloud container clusters get-credentials my-cluster","title":"GKE"},{"location":"Linux/Containers/#windows-server","text":"Windows Server 2016 supports Windows Server Containers and Hyper-V Containers , which create a separate copy of the operating system kernel for each container. The \"Containers\" feature must be installed on Windows Server 2016 hosts, and to create Hyper-V containers the Hyper-V role must also be installed (although the Hyper-V management tools are not necessary if VMs are not going to created). Windows container hosts need to have Windows installed to C:. Nano Server once could serve as Docker hosts, but no longer; Nano Servers are now intended to be deployed as containers themselves. The Powershell Docker module has been deprecated for years.","title":"Windows Server"},{"location":"Linux/Containers/#commands","text":"","title":"Commands"},{"location":"Linux/Containers/#cgconfigservice","text":"cgconfig , which is a part of the libcgroup package, can be used to run at start time to reestablish predefined cgroups.","title":"cgconfig.service"},{"location":"Linux/Containers/#kubectl","text":"Show available contexts kubectl config get-contexts Switch to a different context kubectl config use-context $NAMESPACE kubectl config use $NAMESPACE Display resources kubectl get nodes kubectl get pods kubectl get deployments kubectl describe nodes/gke-*4ff6f64a-6f4v Execute a command on a pod with only a single container, returning output kubectl exec $pod -- env Get a shell to a running container kubectl exec --stdin --tty $pod -- /bin/bash When a pod contains more than one container, the container must be specified with -c / --container . kubectl exec --stdin --tty $pod --container $container -- /bin/bash kubectl run nginx --image = nginx kubectl delete pod nginx kubectl create deployment nginx --image = nginx Number of replicas can be set on creation of a deployment by specifying an argument to --replicas kubectl create deployment nginx --image = nginx --replicas = 4 Replica count is set in an existing deployment by scaling kubectl scale deploy/nginx --replicas = 2 Expose a port kubectl expose deployment/nginx --port = 80 --type = LoadBalancer List Kubernetes objects kubectl api-resources Get a description of a resource kubectl explain nodes.status.addresses.address","title":"kubectl"},{"location":"Linux/Containers/#podman","text":"On RHEL, podman can be installed as a package or as part of a module dnf module install container-tools With few exceptions, podman exposes a command-line API that closely imitates that of Docker. Arch Linux On Arch, /etc/subuid and /etc/subgid have to be set . These are colon-delimited files that define the ranges for namespaced UIDs and GIDs to be used by each user. Conventionally, these ranges begin at 100,000 (for the first, primary user) and contain 65,536 possible values. terry:100000:65536 alice:165536:65536 Then podman has to be migrated podman system migrate Podman supports pulling from various repos using aliases that are defined in /etc/containers/registries.conf.d . RHEL and derivative distributions support additional aliases, some of which reference images that require a login . For example, Red Hat offers a Python 2.7 runtime from the RHSCL ( Red Hat Software Collections ) repository on registry.access.redhat.com, which does not require authentication. However, Python 3.8 is only available from registry.redhat.io, which does. Interestingly, other Python runtimes are available from the ubi7 and ubi8 repos from unauthenticated registries. Container images are stored in ~/.local/share/containers/storage . podman pull rhscl/httpd-24-rhel7 # (1) Alias to registry.access.redhat.com/rhscl/httpd-24-rhel7 The Z option is necessary on SELinux systems (like RHEL and derivatives) and tells Podman to label the content with a private unshared label. On systems running SELinux, rootless containers must be explicitly allowed to access bind mounts. Containerized processes are not allowed to access files without a SELinux label. podman run -d -v = /home/jasper/notes/site:/usr/share/nginx/html:Z -p = 8080 :80 --name = notes nginx podman run -d -v = /home/jasper/notes/site:/usr/local/apache2/htdocs:Z -p = 8080 :80 --name = notes httpd-24 Mapped ports can be displayed podman port -a Output a SystemD service file from a container to STDOUT (this must be redirected to a file) podman generate systemd notes \\ --restart-policy = always \\ --name \\ # (3) --files \\ # (2) --new \\ # (1) Yield unit files that do not expect containers and pods to exist but rather create them based on their configuration files. Generate a file with a name beginning with the prefix (which can be set with --container-prefix or --pod-prefix ) and followed by the ID or name (if --name is also specified) In conjunction with --files , name the service file after the container and not the ID number.","title":"podman"},{"location":"Linux/Containers/#systemd-cgls","text":"systemd-cgls recursively shows the contents of the selected cgroup hierarchy in a tree.","title":"systemd-cgls"},{"location":"Linux/Containers/#glossary","text":"apiVersion Kubernetes object field found in Type metadata. apiVersion is typically v1 , but for some object types the API group is specified, i.e. for Deployments: apiVersion : apps/v1","title":"Glossary"},{"location":"Linux/Containers/#dockerfile","text":"A Docker image consists of read-only layers , each of which represents an instruction that incrementally the changes the image being built up. Dockerfiles can be used to construct new images using docker build . The build process can be optimized by placing multiple commands in the same RUN instruction. Dockerfiles are named simply \"Dockerfile\" with no extension or variation. Node on Alpine Windows Server Nano Windows Server Core FROM alpine RUN apk update && apk add nodejs COPY . /app WORKDIR /app CMD [ \"node\" , \"index.js\" ] FROM microsoft/windowsservercore RUN powershell -command install-windowsfeature dhcp -includemanagementtools RUN powershell -configurationname microsoft.powershell -command add-dhcpserverv4scope -state active -activatepolicies $true -name scopetest -startrange 10 .0.0.100 -endrange 10 .0.0.200 -subnetmask 255 .255.255.0 RUN md boot COPY ./bootfile.wim c:/boot/ CMD powershell FROM microsoft/windowsservercore MAINTAINER @mike_pfeiffer RUN powershell.exe -Command Install-WindowsFeature Web-Server COPY ./websrc c:/inetpub/wwwroot CMD [ \"powershell\" ] Deployment A uniformly managed set of Pod instances, all based on the same container image. The Deployment controller enables release capabilities , the deployment of new Pod versions with no downtime. Exposing a Deployment creates a Service . Desired State Management The Desired State Management system is used by Kubernetes to describe a cluster's desired state declaratively. emptyDir Ephemeral Kubernetes volume type that shares the Pod's lifetime and where data is stored in RAM. emptyDir volumes can use tmpfs file systems. ENTRYPOINT Rarely used Docker declaration. When ENTRYPOINT is present, the CMD declaration becomes the default argument passed to the command in ENTRYPOINT. The Kubernetes --command flag ( pod.spec.containers.command resource) can override the contents of ENTRYPOINT. etcd Distributed key-value data store Event Kubernetes object type that contains information about what happened to the object. Events are deleted one hour after creation by default. kubectl get events kubectl get ev Unlike most other objects, Event manifests have no spec or status sections.","title":"Dockerfile"},{"location":"Linux/Containers/#helm","text":"Helm is a package manager for Kubernetes. Helm packages are refered to as charts . Charts are a collection of files and directories that adhere to a specification. A chart is packed when tarred and gzipped. Chart.yaml contains metadata templates/ contains Kubernetes manifests potentially annotated with templating directives values.yaml provides default configuration It is managed using the helm CLI utility. Create a new chart helm create foo There is no longer a default Helm repository, although there are many available at the Artifact Hub kind Kubernetes object field found in the Type metadata which specifies the type of resource, i.e. Node , Deployment , Service , Pod , etc. kubeconfig YAML configuration file located at $HOME/.kube/config by default. A colon-delimited list of kubeconfigs can be specified by setting the KUBECONFIG environment variable. A kubeconfig can be explicitly specified with the --kubeconfig flag. Label Labels are key-value pairs that are attached to Kubernetes objects. Config for a Pod with two labels: apiVersion : v1 kind : Pod metadata : name : label-demo labels : environment : production app : nginx spec : containers : - name : nginx image : nginx:1.14.2 ports : - containerPort : 80 Master node A master node runs 3 processes, called master (control plane) components: kube-apiserver exposes a RESTful API and serves as a glue between other Kubernetes components kube-scheduler determines how to balance container workloads across nodes using an algorithm kube-controller-manager performs cluster operations like managing nodes and making changes to desired status millicore (m) One-thousandth of a vCPU or a CPU core and the preferred measurement unit of compute resources in Kubernetes (i.e. 128m = 12.8% of a CPU core and 2000m = 2 CPU cores).","title":"Helm"},{"location":"Linux/Containers/#namespaces","text":"Namespaces wrap global system resources (mount points, network devices, hostnames) in an abstraction that makes it appear to processes within that namespace that they have their own isolated instance of that resource. Process IDs in the same namespace can have access to one another, whereas those in different namespaces cannot. Spawning a process in a new namespace prevents it from seeing the host's context, so an interactive shell like zsh spawned in its own namespace will report its PID as 1 , even though the host will assign its own PID. Node A node or worker is any container host that accepts workloads from the master node. Each node is equipped with a container runtime like Docker, which it uses to create and destroy containers according to instructions from the master server. Each node runs 2 processes: kubelet communicates with Kubernetes cluster services kube-proxy handles container network routing using iptables rules PersistentVolume A PersistentVolume is a piece of storage in the cluster that has been provisioned using Storage Classes. PersistentVolumeClaim A PersistentVolumeClaim requests either Disk or File storage of a particular StorageClass, access mode, and size. It is bound to a PersistentVolume once an available storage resource has been assigned to the pod requesting it. Pod A pod is the most basic unit that K8s deals with, representing one or more tightly-coupled containers that should be controlled as a single application (typically one main container with subsidiary helper containers). Every container should have only a single process, so if several processes need to communicate they should be implemented as separate containers in a pod. A pod's containers should: operate closely together share a lifecycle always be scheduled on the same node Replica An instance of a Pod ReplicaSet ... Selector A label selector provides a way to identify a set of objects and is the core grouping primitive supported by Kubernetes. It can be made of multiple requirements that are comma-separated, all of which must be satisfied. There are two types of selector: Equality-based admits the operators = , != , and == . Set-based admits the operators in , notin , and exists . Equality-based selector Set-based selector environment = production tier != frontend environment in (production, qa) tier notin (frontend, backend) partition !partition Service A Service is an abstraction over a logical set of Pods and a policy by which to access them, i.e. a microservice. Because Pods are mortal, the Service controller keeps track of Pod addresses and publishes this information to the consumers of Services, a function called service discovery . tmpfs RAM-backed file system used in Docker containers Volume A volume is a special directory in the Docker host that can be mounted to the container that is used to achieve persistent storage. In Azure, a volume represents a way to store, retrieve, and persist data across pods and through the application lifecycle. In the context of Azure, Kubernetes can use two types of data volume: Azure Disks using Azure Premium (SSDs) or Azure Standard (HDDs). Azure Files using a SMB 3.0 share backed by an Azure Storage account. In Kubernetes, Volumes are an abstraction of file systems accessible from within a Pod's containers. Network storage device, such as gcePersistentVolume emptyDir , where the data is stored in RAM using Docker's tmpfs file system hostPath , where the volume is located within the node's file system. Because pods are expected to be created and destroyed on any node (which may themselves be destroyed and recreated), hostPath volumes are discommended. Volumes are declared in .spec.volumes and mounted into containers in .spec.containers[*].volumeMounts . emptyDir hostPath gcePersistentDisk apiVersion : v1 kind : Pod metadata : name : alpine spec : volumes : - name : data emptyDir : containers : - name : alpine image : alpine volumeMounts : - mountPath : \"/data\" name : \"data\" apiVersion : v1 kind : Pod metadata : name : alpine spec : volumes : - name : data hostPath : path : /var/data containers : - name : alpine image : alpine volumeMounts : - mountPath : \"/data\" name : \"data\" apiVersion : v1 kind : Pod metadata : name : alpine spec : volumes : - name : data gcePersistentDisk : pdName : my-disk fsType : ext4 containers : - name : alpine image : alpine volumeMounts : - mountPath : \"/data\" name : \"data\" Worker :material-kubernetes see Node","title":"Namespaces"},{"location":"Linux/Databases/","text":"Databases Azure Data Studio Cosmos DB emulator PostgreSQL PostgreSQL configs postgresql.conf and pg_hba.conf are stored where the PostgreSQL database cluster was initialized with initdb. This directory can be initialized anywhere, but the default in Red Hat systems is /var/lib/pgsql/data . Tasks Setup PostgresQL Arch Red Hat Ubuntu pacman -S postgresql su - postgres -c 'initdb --pgdata /var/lib/postgres/data' # (1) systemctl enable postgresql --now On Arch, this step appears to be necessary before the postgresql service can be enabled. initdb requires a directory to be explicitly specified using --pgdata or alternatively the PGDATA environment variable. dnf install libpq-devel mariadb-devel postgresql postgresql-server postgresql-setup --initdb # (1) systemctl enable postgresql --now This command facilitates initialization of the database cluster, which defaults to /var/lib/pgsql/data , similar to using initdb . apt install libpq-dev systemctl start postgresql sudo -u postgres psql Role setup CREATE ROLE username LOGIN INHERIT -- (1) CREATEDB -- (5) PASSWORD 'password' ; -- (3) GRANT postgres TO username ; -- (2) CREATE DATABASE username ; -- (4) exit Create a new user or \"role\". Like SSH, psql by default will use the currently logged-in username, which does not exist on a fresh installation. Grant group membership to the newly created user. Manually set a password for the newly created user. Single quotes are necessary here, as double quotes will cause an error. After role creation ALTER USER username WITH PASSWORD 'password' ; The default database which is logged into is also named after the currently logged-in user. Also the built-in command createdb can be used from the command-line. Database creation is a restricted operation granted by an attribute. After role creation ALTER USER username WITH CREATEDB ; SQL Starships CREATE TABLE starships ( name text , registry text , crew integer ); INSERT INTO starships ( name , registry , crew ) VALUES ( 'USS Enterprise' , 'NCC-1701' , 400 ); -- (1) For some reason, a double-quote \" produces an error, and only single-quotes are accepted. Cosmos DB ... Commands psql Enter an interactive shell to control a PostgreSQL server. Install dnf install postgresql psql -d database The interactive shell allows SQL queries to be run as well as meta-commands prefixed with a backslash \\ . \\ dt -- (1) Display tables sqlite3 sqlite3 is an interactive frontend to the SQLite library. Meta-commands , prefixed by . , can be used to examine database files or perform administrative operations. . databases -- (1) . tables -- (2) . show -- (3) . exit List names and files of attached databases. List names of tables matching a given pattern. Show the current values for various settings. Files can be provided on invocation from the command-line or they can be provided after the .open meta-command. . open database . db Without providing an argument on invocation, sqlite3 will open an in-memory database by default, which can also be explicitly specified with a meta-command. . open : memory :","title":"Databases"},{"location":"Linux/Databases/#databases","text":"Azure Data Studio Cosmos DB emulator","title":"Databases"},{"location":"Linux/Databases/#postgresql","text":"PostgreSQL configs postgresql.conf and pg_hba.conf are stored where the PostgreSQL database cluster was initialized with initdb. This directory can be initialized anywhere, but the default in Red Hat systems is /var/lib/pgsql/data .","title":"PostgreSQL"},{"location":"Linux/Databases/#tasks","text":"","title":"Tasks"},{"location":"Linux/Databases/#setup-postgresql","text":"Arch Red Hat Ubuntu pacman -S postgresql su - postgres -c 'initdb --pgdata /var/lib/postgres/data' # (1) systemctl enable postgresql --now On Arch, this step appears to be necessary before the postgresql service can be enabled. initdb requires a directory to be explicitly specified using --pgdata or alternatively the PGDATA environment variable. dnf install libpq-devel mariadb-devel postgresql postgresql-server postgresql-setup --initdb # (1) systemctl enable postgresql --now This command facilitates initialization of the database cluster, which defaults to /var/lib/pgsql/data , similar to using initdb . apt install libpq-dev systemctl start postgresql sudo -u postgres psql Role setup CREATE ROLE username LOGIN INHERIT -- (1) CREATEDB -- (5) PASSWORD 'password' ; -- (3) GRANT postgres TO username ; -- (2) CREATE DATABASE username ; -- (4) exit Create a new user or \"role\". Like SSH, psql by default will use the currently logged-in username, which does not exist on a fresh installation. Grant group membership to the newly created user. Manually set a password for the newly created user. Single quotes are necessary here, as double quotes will cause an error. After role creation ALTER USER username WITH PASSWORD 'password' ; The default database which is logged into is also named after the currently logged-in user. Also the built-in command createdb can be used from the command-line. Database creation is a restricted operation granted by an attribute. After role creation ALTER USER username WITH CREATEDB ;","title":"Setup PostgresQL"},{"location":"Linux/Databases/#sql","text":"","title":"SQL"},{"location":"Linux/Databases/#starships","text":"CREATE TABLE starships ( name text , registry text , crew integer ); INSERT INTO starships ( name , registry , crew ) VALUES ( 'USS Enterprise' , 'NCC-1701' , 400 ); -- (1) For some reason, a double-quote \" produces an error, and only single-quotes are accepted.","title":"Starships"},{"location":"Linux/Databases/#cosmos-db","text":"...","title":"Cosmos DB"},{"location":"Linux/Databases/#commands","text":"","title":"Commands"},{"location":"Linux/Databases/#psql","text":"Enter an interactive shell to control a PostgreSQL server. Install dnf install postgresql psql -d database The interactive shell allows SQL queries to be run as well as meta-commands prefixed with a backslash \\ . \\ dt -- (1) Display tables","title":"psql"},{"location":"Linux/Databases/#sqlite3","text":"sqlite3 is an interactive frontend to the SQLite library. Meta-commands , prefixed by . , can be used to examine database files or perform administrative operations. . databases -- (1) . tables -- (2) . show -- (3) . exit List names and files of attached databases. List names of tables matching a given pattern. Show the current values for various settings. Files can be provided on invocation from the command-line or they can be provided after the .open meta-command. . open database . db Without providing an argument on invocation, sqlite3 will open an in-memory database by default, which can also be explicitly specified with a meta-command. . open : memory :","title":"sqlite3"},{"location":"Linux/Display/","text":"Display Tasks Commands X11 X Test X11 with the config file automatically generated after Xorg -configure X -config $HOME /xorg.conf.new xhost Enable access control to X server xhost - Disable access control to X server, allowing clients from any host to connect (not unsafe if you use a firewall that allows only SSH) xhost + Add $HOST to list of authorized clients for X server xhost + $HOST Remove $HOST from list of authorized clients for X server xhost - $HOST Add $USER to ACL xhost si:localuser: $USER xmodmap Replacing Caps Lock with Escape ! Swap caps lock and escape remove Lock = Caps_Lock keysym Escape = Caps_Lock keysym Caps_Lock = Escape add Lock = Caps_Lock Xorg Enable automatic configuration of X11 server Xorg -configure xrandr Change resolution of DisplayPort1 to 1920x1080 xrandr --output DP1 --mode 1920x1080 Disable VGA1 output xrandr --output VGA1 --off Display current state of the system xrandr -q --query xset Dynamically add fonts [Haeder: 307][Haeder] xset fp+ /usr/local/fonts","title":"Display"},{"location":"Linux/Display/#display","text":"","title":"Display"},{"location":"Linux/Display/#tasks","text":"","title":"Tasks"},{"location":"Linux/Display/#commands","text":"","title":"Commands"},{"location":"Linux/Display/#x11","text":"X Test X11 with the config file automatically generated after Xorg -configure X -config $HOME /xorg.conf.new xhost Enable access control to X server xhost - Disable access control to X server, allowing clients from any host to connect (not unsafe if you use a firewall that allows only SSH) xhost + Add $HOST to list of authorized clients for X server xhost + $HOST Remove $HOST from list of authorized clients for X server xhost - $HOST Add $USER to ACL xhost si:localuser: $USER","title":"X11"},{"location":"Linux/Display/#xmodmap","text":"Replacing Caps Lock with Escape ! Swap caps lock and escape remove Lock = Caps_Lock keysym Escape = Caps_Lock keysym Caps_Lock = Escape add Lock = Caps_Lock","title":"xmodmap"},{"location":"Linux/Display/#xorg","text":"Enable automatic configuration of X11 server Xorg -configure","title":"Xorg"},{"location":"Linux/Display/#xrandr","text":"Change resolution of DisplayPort1 to 1920x1080 xrandr --output DP1 --mode 1920x1080 Disable VGA1 output xrandr --output VGA1 --off Display current state of the system xrandr -q --query","title":"xrandr"},{"location":"Linux/Display/#xset","text":"Dynamically add fonts [Haeder: 307][Haeder] xset fp+ /usr/local/fonts","title":"xset"},{"location":"Linux/Files/","text":"Files Glossary squashfs Squashfs is a compressed read-only filesystem for Linux using zlib compression for files, inodes, and directories. SGID When the set-group-ID bit for a directory is set, all files created therein are assigned to the directory's group and not to the file owner's default group. This is intended to facilitate file sharing. In this scenario, users are assigned to a group, and the group is assigned to shared directories with the SGID bit set. Sticky bit When the sticky bit is set on a directory, only root, the directory owner and the owner of a file can remove files in that directory. SUID The set-user-ID bit allows a file to be executed with the privileges of the file's owner. Commands chage Expire password in 30 days chage -E $( date -d +30days +%Y-%m-%d ) $USER chgrp Change ownership of $FILE to $USER and $GROUP chgrp $USER : $GROUP $FILE chmod Add permissions Remove permissions chmod +t $FILE # Sticky bit chmod g+s file # SGID chmod u+s file # SUID chmod -t $FILE # Sticky bit chmod g-s file # SGID chmod u-s file # SUID chown Change a file or directory's ownership. To change the user and group owner of a file to $USER and $GROUP : chown $USER : $GROUP $file Recursively grant $USER ownership to $PATH chown -R $USER $PATH Use a reference file to match the configuration of a particular file chown -vR --reference = . $PATH --preserve-root prevents changes to files in the root directory but only when used together with --recursive chown -cfR --preserve-root $USER du du does not double-count hard-linked files, so it can be used to analyze deduplication in app distribution solutions like Flatpak . Here the second command will display a smaller value for the 21.08 version of the freedesktop Platform runtime, indicating that hard-linked files have not been double-counted. du -sh /var/lib/flatpak/runtime/org.freedesktop.Platform/x86_64/21.08 du -sh /var/lib/flatpak/runtime/org.freedesktop.Platform/x86_64/21.08 /var/lib/flatpak/runtime/org.freedesktop.Platform/x86_64/20.08 find Search for files in a directory hierarchy Find all files owned by user find . -user $USER -exec allows a command to be executed for every foudn file, which has to be terminated with an escaped semicolon, i.e. \\; . Remove whitespace from filenames find . -type f -name \"* *\" -exec bash -c 'mv \"$0\" \"${0// /_}\"' {} \\; Find recently modified files/folders There are 3 timestamps associated with files in Linux atime \"access time\": last time file was accessed by a command or application mtime \"modify time\": last time file's contents were modified ctime \"change time\": last time file's attribute was modified Numerical arguments can be specified in 3 ways: +n greater than {n} days ago -n less than {n} days ago n exactly n days ago # Find only files that were modified more than 120 days ago find . -type f -mtime +120 -ls # Modified less than 15 days ago find . -type f -mtime -15 -ls # Modified exactly 10 days ago find . -type f -mtime 10 -ls # Find files modified over the past day find . -type f -newermt \"1 day ago\" -ls find . -type f -newermt \"-24 hours\" -ls find . -type f -newermt \"yesterday\" -ls # Find files created today find . -type f -ctime -1 -ls rename Use regular expressions to rename multiple files # Renaming file.old to file.new rename 's/old/new/' this.old # Use globbing to rename all matching files rename 's/old/new/' *.old rename 's/report/review/' * # Change all uppercase letters to lowercase rename 'y/A-Z/a-z/' * rsync a b e g l o p r t v z Copy $FILE locally rsync -zvr $FILE $PATH Copy $FILE to $PATH on remote $HOST rsync $FILE $HOST : $PATH Copy $FILE from $HOST to local $PATH rsync $HOST : $FILE $PATH Copy $DIR recursively rsync -zvr $DIR $PATH rsync -avz $DIR $PATH Copy to remote systems over SSH rsync -zvre ssh $DIR $HOST : $REMOTEPATH rsync -avze ssh $DIR $HOST : $REMOTEPATH Synchronize only specific file type rsync -zvre ssh --include '*.php' --exclude '*' $PATH setfacl The effect of ACLs can be illustrated with a web server. This command removes read access from a file which would otherwise be served by the Apache/httpd web server daemon. setfacl -m u:apache:- /var/www/html/index.html This can be resolved by granting read to the apache service account (or removing the entry altogether) setfacl -m u:apache:r /var/www/html/index.html setfacl -x u:apache /var/www/html/index.html setfacl -b /var/www/html/index.html","title":"Files"},{"location":"Linux/Files/#files","text":"","title":"Files"},{"location":"Linux/Files/#glossary","text":"","title":"Glossary"},{"location":"Linux/Files/#squashfs","text":"Squashfs is a compressed read-only filesystem for Linux using zlib compression for files, inodes, and directories.","title":"squashfs"},{"location":"Linux/Files/#sgid","text":"When the set-group-ID bit for a directory is set, all files created therein are assigned to the directory's group and not to the file owner's default group. This is intended to facilitate file sharing. In this scenario, users are assigned to a group, and the group is assigned to shared directories with the SGID bit set.","title":"SGID"},{"location":"Linux/Files/#sticky-bit","text":"When the sticky bit is set on a directory, only root, the directory owner and the owner of a file can remove files in that directory.","title":"Sticky bit"},{"location":"Linux/Files/#suid","text":"The set-user-ID bit allows a file to be executed with the privileges of the file's owner.","title":"SUID"},{"location":"Linux/Files/#commands","text":"","title":"Commands"},{"location":"Linux/Files/#chage","text":"Expire password in 30 days chage -E $( date -d +30days +%Y-%m-%d ) $USER","title":"chage"},{"location":"Linux/Files/#chgrp","text":"Change ownership of $FILE to $USER and $GROUP chgrp $USER : $GROUP $FILE","title":"chgrp"},{"location":"Linux/Files/#chmod","text":"Add permissions Remove permissions chmod +t $FILE # Sticky bit chmod g+s file # SGID chmod u+s file # SUID chmod -t $FILE # Sticky bit chmod g-s file # SGID chmod u-s file # SUID","title":"chmod"},{"location":"Linux/Files/#chown","text":"Change a file or directory's ownership. To change the user and group owner of a file to $USER and $GROUP : chown $USER : $GROUP $file Recursively grant $USER ownership to $PATH chown -R $USER $PATH Use a reference file to match the configuration of a particular file chown -vR --reference = . $PATH --preserve-root prevents changes to files in the root directory but only when used together with --recursive chown -cfR --preserve-root $USER","title":"chown"},{"location":"Linux/Files/#du","text":"du does not double-count hard-linked files, so it can be used to analyze deduplication in app distribution solutions like Flatpak . Here the second command will display a smaller value for the 21.08 version of the freedesktop Platform runtime, indicating that hard-linked files have not been double-counted. du -sh /var/lib/flatpak/runtime/org.freedesktop.Platform/x86_64/21.08 du -sh /var/lib/flatpak/runtime/org.freedesktop.Platform/x86_64/21.08 /var/lib/flatpak/runtime/org.freedesktop.Platform/x86_64/20.08","title":"du"},{"location":"Linux/Files/#find","text":"Search for files in a directory hierarchy Find all files owned by user find . -user $USER -exec allows a command to be executed for every foudn file, which has to be terminated with an escaped semicolon, i.e. \\; . Remove whitespace from filenames find . -type f -name \"* *\" -exec bash -c 'mv \"$0\" \"${0// /_}\"' {} \\; Find recently modified files/folders There are 3 timestamps associated with files in Linux atime \"access time\": last time file was accessed by a command or application mtime \"modify time\": last time file's contents were modified ctime \"change time\": last time file's attribute was modified Numerical arguments can be specified in 3 ways: +n greater than {n} days ago -n less than {n} days ago n exactly n days ago # Find only files that were modified more than 120 days ago find . -type f -mtime +120 -ls # Modified less than 15 days ago find . -type f -mtime -15 -ls # Modified exactly 10 days ago find . -type f -mtime 10 -ls # Find files modified over the past day find . -type f -newermt \"1 day ago\" -ls find . -type f -newermt \"-24 hours\" -ls find . -type f -newermt \"yesterday\" -ls # Find files created today find . -type f -ctime -1 -ls","title":"find"},{"location":"Linux/Files/#rename","text":"Use regular expressions to rename multiple files # Renaming file.old to file.new rename 's/old/new/' this.old # Use globbing to rename all matching files rename 's/old/new/' *.old rename 's/report/review/' * # Change all uppercase letters to lowercase rename 'y/A-Z/a-z/' *","title":"rename"},{"location":"Linux/Files/#rsync","text":"a b e g l o p r t v z Copy $FILE locally rsync -zvr $FILE $PATH Copy $FILE to $PATH on remote $HOST rsync $FILE $HOST : $PATH Copy $FILE from $HOST to local $PATH rsync $HOST : $FILE $PATH Copy $DIR recursively rsync -zvr $DIR $PATH rsync -avz $DIR $PATH Copy to remote systems over SSH rsync -zvre ssh $DIR $HOST : $REMOTEPATH rsync -avze ssh $DIR $HOST : $REMOTEPATH Synchronize only specific file type rsync -zvre ssh --include '*.php' --exclude '*' $PATH","title":"rsync"},{"location":"Linux/Files/#setfacl","text":"The effect of ACLs can be illustrated with a web server. This command removes read access from a file which would otherwise be served by the Apache/httpd web server daemon. setfacl -m u:apache:- /var/www/html/index.html This can be resolved by granting read to the apache service account (or removing the entry altogether) setfacl -m u:apache:r /var/www/html/index.html setfacl -x u:apache /var/www/html/index.html setfacl -b /var/www/html/index.html","title":"setfacl"},{"location":"Linux/Filters/","text":"Commands awk awk programs are equivalent to sed \"instructions\" can be defined inline or in a program file (also sometimes called source files). If no input files are specified awk can accept input from STDIN. Inline awk $OPTIONS $PROGRAM $INPUTFILES Program file awk $OPTIONS -f $PROGRAMFILE $INPUTFILES awk programs combine patterns and actions Patterns can be: regular expressions or fixed strings line numbers using builtin variable NR predefined patterns BEGIN or END , whose actions are executed before and after processing any lines of the data file, respectively Convert \":\" to newlines in $PATH environment variable echo $PATH | awk 'BEGIN {RS=\":\"} {print}' Print the first field of all files in the current directory, taking semicolon ; as the field separator, outputting filename, line number, and first field of matches, with colon : between the filename and line number awk 'BEGIN {FS=\";\"} /enable/ {print FILENAME \":\" FNR,$1}' * search for string MA in all files, outputting filename, line, and line number for matches awk ' /MA/ { OFS = \" \" print FILENAME OFS FNR OFS $0 } * change field separator ( FS ) to a colon ( : ) and run awkscr awk -F: -f awkscr /etc/passwd flag also works for awk awk -f script files ` ` -f print the first field of each line in the input file awk '{ print $1 }' list equivalent to grep MA * ( {print} is implied) awk '/MA/' * | awk '/MA/ {print}' * -F flag is followed by field separator awk -F, '/MA/ { print $1 }' list pipe output of free to awk to get free memory and total memory free -h | awk ' /^Mem | / { print $3 \"/\" $2 } pipe output of sensors to awk to get CPU temperature sensors | awk ' /^temp1/ { print $2 } replace initial \"fake.\" with \"real;\" in file fake_isbn awk 'sub(^fake.,\"real;\")' fake_isbn print all lines awk '1 { print }' file remove file header awk 'NR>1' file remove file header awk ' NR>1 { print } file print lines in a range awk 'NR>1 && NR < 4' file remove whitespace-only lines awk 'NF' file remove all blank lines awk '1' RS = '' file extract fields awk '{ print $1, $3}' FS = , OFS = , file perform column-wise calculations awk '{ SUM=SUM+$1 } END { print SUM }' FS = , OFS = , file count the number of nonempty lines awk '/./ { COUNT+=1 } END { print COUNT }' file count the number of nonempty lines awk 'NF { COUNT+=1 } END { print COUNT }' file count the number of nonempty lines awk '+$1 { COUNT+=1 } END { print COUNT }' file Arrays awk '+$1 { CREDITS[$3]+=$1 } END { for (NAME in CREDITS) print NAME, CREDITS[NAME] }' FS = , file Identify duplicate lines awk 'a[$0]++' file Remove duplicate lines awk '!a[$0]++' file Remove multiple spaces awk '$1=$1' file Join lines awk '{ print $3 }' FS = , ORS = ' ' file ; echo awk '+$1 { SUM+=$1; NUM+=1 } END { printf(\"AVG=%f\",SUM/NUM); }' FS = , file ` | format awk '+$1 { SUM+=$1; NUM+=1 } END { printf(\"AVG=%6.1f\",SUM/NUM); }' FS = , file Convert to uppercase awk '$3 { print toupper($0); }' file Change part of a string awk '{ $3 = toupper(substr($3,1,1)) substr($3,2) } $3' FS = , OFS = , file Split the second field (\"EXPDATE\") by spaces, storing the result into the array DATE; then print credits ($1) and username ($3) as well as the month (DATE[2]) and year (DATE[3]) awk '+$1 { split($2, DATE, \" \"); print $1,$3, DATE[2], DATE[3] }' FS = , OFS = , file awk '+$1 { split($4, GRP, \":\"); print $3, GRP[1], GRP[2] }' FS = , file awk '+$1 { split($4, GRP, /:+/); print $3, GRP[1], GRP[2] }' FS = , file Search and replace with comma awk '+$1 { gsub(/ +/, \"-\", $2); print }' FS = , file Adding date awk 'BEGIN { printf(\"UPDATED: \"); system(\"date\") } /^UPDATED:/ { next } 1' file Modify a field externally awk '+$1 { CMD | getline $5; close(CMD); print }' CMD = \"uuid -v4\" FS = , OFS = , file Invoke dynamically generated command awk '+$1 { cmd = sprintf(FMT, $2); cmd | getline $2; close(cmd); print }' FMT = 'date -I -d \"%s\"' FS = , file Join data awk '+$1 { CMD | getline $5; print }' CMD = 'od -vAn -w4 -t x /dev/urandom' FS = , file Add up all first records to {sum}, then print that number out at the end awk '{sum += $1} END {print sum}' file grep grep -R $TEXT $DIRECTORY head Print first 8 characters of $FILE head -c8 $FILE paste Merge lines of files Make a .csv file from two lists paste -d ',' file1 file2 Transpose rows paste -s file1 file2 sed sed (\"Stream-oriented editor\") is typically used for applying repetitive edits across all lines of multiple files. In particular it is, alongside awk one of the two primary commands which accept regular expressions in Unix systems. sed instructions can be defined inline or in a command file (i.e. script). Inline sed $OPTIONS $INSTRUCTION $FILE Command file sed $OPTIONS -f $SCRIPT $FILE sed instructions are made of two components: addresses (i.e. patterns) and procedures (i.e. actions). Run sed commands in $SCRIPT on $FILE sed -f $SCRIPT $FILE Suppress automatic printing of pattern space sed -n # --quiet , --silent Zero, one, or two addresses can precede a procedure. In the absence of an address, the procedure is executed over every line of input. With one address, the procedure will be executed over every line of input that matches. With two addresses, the procedure will be executed over groups of lines whereby: The first address selects the first line in the first group The second address selects the next subsequent line that it matches, which becomes the last line in the first group If no match for the second address is found, it point to the end of the file After the match, the selection process for the next group begins by searching for a match to the first address Addressing can be done in one of two ways: Line addressing , specifying line numbers separated by a comma (e.g. 3,7p ); $ represents the last line of input Context addressing , using a regular expression enclosed by forward slashes (e.g. /From:/p ) Edit the file in-place, but save a backup copy of the original with {suffix} appended to - the filename -i = suffix In some circles, sed is recommended as a replacement for other filters like head . Here, the first 10 lines of a file are displayed. sed 10q $FILE Display the top 10 processes by memory or cpu usage. ps axch -o cmd,%mem --sort = -%mem | sed 11q ps axch -o cmd:15,%cpu --sort = -%cpu | sed 11q Replace angle brackets with their HTML codes, piped in from a heredoc: sed -e 's/</\\&lt;/g' -e 's/>/\\&gt;/g' << EOF <!-- Display first two lines of file Without -n , each line will be printed twice sed -n '1,2p' emp.lst Prepending ! to the procedure reverses the sense of the command (YUG: 450) sed -n '3,$!p' emp.lst Display a range of lines sed -n '9,11p' emp.lst Use the -e flag to precede multiple instructions sed -n -e '1,2p' -e '7,9p' -e '$p' emp.lst Delete lines Delete second line alone sed '2d' myfile Delete a range of lines: from the 2nd through the 3rd sed '2,3d' myfile Delete a range of lines, from the first occurrence of 'second' to the line with the first occurrence of 'fourth' sed '/second/,/fourth/d' myfile Print all of a file except for specific lines Suppress any line with 'test' in it sed '/test/d' myfile Suppress from the 3rd line to EOF sed '3,$d' myfile Replace the first instance of the | character with : and display the first two lines [YUG:455] sed ' s/ | /:/ emp.lst | head -2 Replace all instances of the | character with : , displaying the first two lines [YUG:455] sed 's/|/:/g' emp.lst | head -2 Substitute HTML tags: sed 's/<I>/<EM>/g' These commands will replace \"director\" with \"executive director\" sed 's/director/executive director/' emp.lst sed 's/director/executive &/' emp.lst sed '/director/s//executive &/' emp.lst Searching for text Equivalent to grep MA * sed -n '/MA/p' * Stringing sed statements together with pipe Take lines beginning with \"fake\" and remove all instances of \"fake.\", piping them... remove all parentheses with content and count lines of output (results) sed -n '/^fake/s/fake\\.//p' * | sed -nr 's/\\(.*\\)//p' | wc -l Take lines of all files in CWD beginning with \"fake\" and remove all instances of string \"fake.\" Then remove all parentheses with any content within them and print only the top 10 lines sed -ne '/^fake/p' * | sed -n 's/fake\\.//p' | sed -nr 's/\\(.*\\)//p' | sed 11q Count the number of pipes replaced by piping output to cmp , which will use the -l option to output byte numbers of differing values, then counting the lines of output (YUG:456) sed 's/|/:/g' emp.lst | cmp -l - emp.lst | wc -l --> Output last lines beginning at 30th line from the start Short option POSIX tail -n = +30 tail --lines = +30 c d s Change the case of a string ] tr [ :upper: ] [ :lower: ] Remove a character or set of characters from a string or line of output tr -d \"text\" watch Execute $CMD at periods of $N seconds, watching its output CLKF watch $CMD -n $N Check memory usage in megabytes ( -m ) every 5 seconds Enki watch -n 5 free -m","title":"Commands"},{"location":"Linux/Filters/#commands","text":"","title":"Commands"},{"location":"Linux/Filters/#awk","text":"awk programs are equivalent to sed \"instructions\" can be defined inline or in a program file (also sometimes called source files). If no input files are specified awk can accept input from STDIN. Inline awk $OPTIONS $PROGRAM $INPUTFILES Program file awk $OPTIONS -f $PROGRAMFILE $INPUTFILES awk programs combine patterns and actions Patterns can be: regular expressions or fixed strings line numbers using builtin variable NR predefined patterns BEGIN or END , whose actions are executed before and after processing any lines of the data file, respectively Convert \":\" to newlines in $PATH environment variable echo $PATH | awk 'BEGIN {RS=\":\"} {print}' Print the first field of all files in the current directory, taking semicolon ; as the field separator, outputting filename, line number, and first field of matches, with colon : between the filename and line number awk 'BEGIN {FS=\";\"} /enable/ {print FILENAME \":\" FNR,$1}' * search for string MA in all files, outputting filename, line, and line number for matches awk ' /MA/ { OFS = \" \" print FILENAME OFS FNR OFS $0 } * change field separator ( FS ) to a colon ( : ) and run awkscr awk -F: -f awkscr /etc/passwd flag also works for awk awk -f script files ` ` -f print the first field of each line in the input file awk '{ print $1 }' list equivalent to grep MA * ( {print} is implied) awk '/MA/' * | awk '/MA/ {print}' * -F flag is followed by field separator awk -F, '/MA/ { print $1 }' list pipe output of free to awk to get free memory and total memory free -h | awk ' /^Mem | / { print $3 \"/\" $2 } pipe output of sensors to awk to get CPU temperature sensors | awk ' /^temp1/ { print $2 } replace initial \"fake.\" with \"real;\" in file fake_isbn awk 'sub(^fake.,\"real;\")' fake_isbn print all lines awk '1 { print }' file remove file header awk 'NR>1' file remove file header awk ' NR>1 { print } file print lines in a range awk 'NR>1 && NR < 4' file remove whitespace-only lines awk 'NF' file remove all blank lines awk '1' RS = '' file extract fields awk '{ print $1, $3}' FS = , OFS = , file perform column-wise calculations awk '{ SUM=SUM+$1 } END { print SUM }' FS = , OFS = , file count the number of nonempty lines awk '/./ { COUNT+=1 } END { print COUNT }' file count the number of nonempty lines awk 'NF { COUNT+=1 } END { print COUNT }' file count the number of nonempty lines awk '+$1 { COUNT+=1 } END { print COUNT }' file Arrays awk '+$1 { CREDITS[$3]+=$1 } END { for (NAME in CREDITS) print NAME, CREDITS[NAME] }' FS = , file Identify duplicate lines awk 'a[$0]++' file Remove duplicate lines awk '!a[$0]++' file Remove multiple spaces awk '$1=$1' file Join lines awk '{ print $3 }' FS = , ORS = ' ' file ; echo awk '+$1 { SUM+=$1; NUM+=1 } END { printf(\"AVG=%f\",SUM/NUM); }' FS = , file ` | format awk '+$1 { SUM+=$1; NUM+=1 } END { printf(\"AVG=%6.1f\",SUM/NUM); }' FS = , file Convert to uppercase awk '$3 { print toupper($0); }' file Change part of a string awk '{ $3 = toupper(substr($3,1,1)) substr($3,2) } $3' FS = , OFS = , file Split the second field (\"EXPDATE\") by spaces, storing the result into the array DATE; then print credits ($1) and username ($3) as well as the month (DATE[2]) and year (DATE[3]) awk '+$1 { split($2, DATE, \" \"); print $1,$3, DATE[2], DATE[3] }' FS = , OFS = , file awk '+$1 { split($4, GRP, \":\"); print $3, GRP[1], GRP[2] }' FS = , file awk '+$1 { split($4, GRP, /:+/); print $3, GRP[1], GRP[2] }' FS = , file Search and replace with comma awk '+$1 { gsub(/ +/, \"-\", $2); print }' FS = , file Adding date awk 'BEGIN { printf(\"UPDATED: \"); system(\"date\") } /^UPDATED:/ { next } 1' file Modify a field externally awk '+$1 { CMD | getline $5; close(CMD); print }' CMD = \"uuid -v4\" FS = , OFS = , file Invoke dynamically generated command awk '+$1 { cmd = sprintf(FMT, $2); cmd | getline $2; close(cmd); print }' FMT = 'date -I -d \"%s\"' FS = , file Join data awk '+$1 { CMD | getline $5; print }' CMD = 'od -vAn -w4 -t x /dev/urandom' FS = , file Add up all first records to {sum}, then print that number out at the end awk '{sum += $1} END {print sum}' file","title":"awk"},{"location":"Linux/Filters/#grep","text":"grep -R $TEXT $DIRECTORY","title":"grep"},{"location":"Linux/Filters/#head","text":"Print first 8 characters of $FILE head -c8 $FILE","title":"head"},{"location":"Linux/Filters/#paste","text":"Merge lines of files Make a .csv file from two lists paste -d ',' file1 file2 Transpose rows paste -s file1 file2","title":"paste"},{"location":"Linux/Filters/#sed","text":"sed (\"Stream-oriented editor\") is typically used for applying repetitive edits across all lines of multiple files. In particular it is, alongside awk one of the two primary commands which accept regular expressions in Unix systems. sed instructions can be defined inline or in a command file (i.e. script). Inline sed $OPTIONS $INSTRUCTION $FILE Command file sed $OPTIONS -f $SCRIPT $FILE sed instructions are made of two components: addresses (i.e. patterns) and procedures (i.e. actions). Run sed commands in $SCRIPT on $FILE sed -f $SCRIPT $FILE Suppress automatic printing of pattern space sed -n # --quiet , --silent Zero, one, or two addresses can precede a procedure. In the absence of an address, the procedure is executed over every line of input. With one address, the procedure will be executed over every line of input that matches. With two addresses, the procedure will be executed over groups of lines whereby: The first address selects the first line in the first group The second address selects the next subsequent line that it matches, which becomes the last line in the first group If no match for the second address is found, it point to the end of the file After the match, the selection process for the next group begins by searching for a match to the first address Addressing can be done in one of two ways: Line addressing , specifying line numbers separated by a comma (e.g. 3,7p ); $ represents the last line of input Context addressing , using a regular expression enclosed by forward slashes (e.g. /From:/p ) Edit the file in-place, but save a backup copy of the original with {suffix} appended to - the filename -i = suffix In some circles, sed is recommended as a replacement for other filters like head . Here, the first 10 lines of a file are displayed. sed 10q $FILE Display the top 10 processes by memory or cpu usage. ps axch -o cmd,%mem --sort = -%mem | sed 11q ps axch -o cmd:15,%cpu --sort = -%cpu | sed 11q Replace angle brackets with their HTML codes, piped in from a heredoc: sed -e 's/</\\&lt;/g' -e 's/>/\\&gt;/g' << EOF <!-- Display first two lines of file Without -n , each line will be printed twice sed -n '1,2p' emp.lst Prepending ! to the procedure reverses the sense of the command (YUG: 450) sed -n '3,$!p' emp.lst Display a range of lines sed -n '9,11p' emp.lst Use the -e flag to precede multiple instructions sed -n -e '1,2p' -e '7,9p' -e '$p' emp.lst Delete lines Delete second line alone sed '2d' myfile Delete a range of lines: from the 2nd through the 3rd sed '2,3d' myfile Delete a range of lines, from the first occurrence of 'second' to the line with the first occurrence of 'fourth' sed '/second/,/fourth/d' myfile Print all of a file except for specific lines Suppress any line with 'test' in it sed '/test/d' myfile Suppress from the 3rd line to EOF sed '3,$d' myfile Replace the first instance of the | character with : and display the first two lines [YUG:455] sed ' s/ | /:/ emp.lst | head -2 Replace all instances of the | character with : , displaying the first two lines [YUG:455] sed 's/|/:/g' emp.lst | head -2 Substitute HTML tags: sed 's/<I>/<EM>/g' These commands will replace \"director\" with \"executive director\" sed 's/director/executive director/' emp.lst sed 's/director/executive &/' emp.lst sed '/director/s//executive &/' emp.lst Searching for text Equivalent to grep MA * sed -n '/MA/p' * Stringing sed statements together with pipe Take lines beginning with \"fake\" and remove all instances of \"fake.\", piping them... remove all parentheses with content and count lines of output (results) sed -n '/^fake/s/fake\\.//p' * | sed -nr 's/\\(.*\\)//p' | wc -l Take lines of all files in CWD beginning with \"fake\" and remove all instances of string \"fake.\" Then remove all parentheses with any content within them and print only the top 10 lines sed -ne '/^fake/p' * | sed -n 's/fake\\.//p' | sed -nr 's/\\(.*\\)//p' | sed 11q Count the number of pipes replaced by piping output to cmp , which will use the -l option to output byte numbers of differing values, then counting the lines of output (YUG:456) sed 's/|/:/g' emp.lst | cmp -l - emp.lst | wc -l --> Output last lines beginning at 30th line from the start Short option POSIX tail -n = +30 tail --lines = +30 c d s Change the case of a string ] tr [ :upper: ] [ :lower: ] Remove a character or set of characters from a string or line of output tr -d \"text\"","title":"sed"},{"location":"Linux/Filters/#watch","text":"Execute $CMD at periods of $N seconds, watching its output CLKF watch $CMD -n $N Check memory usage in megabytes ( -m ) every 5 seconds Enki watch -n 5 free -m","title":"watch"},{"location":"Linux/GRUB/","text":"GRUB2 Kernel command line parameters passed in on boot can be queried during runtime: cat /proc/cmdline Tasks Resetting root password Append rd.break to the list of command-line parameters to GRUB. Once the shell is ready, run the following commands mount -o remount,rw /sysroot chroot /sysroot passwd root Text-mode installation RHEL can be installed from the console by providing inst.text as a kernel parameter on boot by pressing ++Tab++ on the GRUB splash screen. GRUB rescue prompt When GRUB2 is unable to find the GRUB folder or its contents are missing or corrupted, it displays the prompt grub rescue> This means it failed to load the normal module. howtoforge.com From GRUB rescue prompt: set prefix=(hd0,1)/boot/grub set root=(hd0,1) insmod normal normal After booting the system, GRUB should be updated and reinstalled: Update GRUB config file update-grub Reinstall GRUB grub-install /dev/sdx Commands grub-mkconfig Generate GRUB configuration grub-mkconfig -o /boot/grub/grub.cfg grub2-mkconfig grub2-mkconfig is used to create a GRUB2 config file from the settings defined in /etc/default/grub grub2-mkconfig -o = /boot/grub2/grub.cfg grub2-editenv Disable the Nouveau display driver while installing the proprietary Nvidia display driver on Fedora grub2-editenv - set \" $( grub2-editenv - list | grep kernelopts ) nouveau.modeset=0\" update-grub Update GRUB config file update-grub","title":"GRUB2"},{"location":"Linux/GRUB/#grub2","text":"Kernel command line parameters passed in on boot can be queried during runtime: cat /proc/cmdline","title":"GRUB2"},{"location":"Linux/GRUB/#tasks","text":"","title":"Tasks"},{"location":"Linux/GRUB/#resetting-root-password","text":"Append rd.break to the list of command-line parameters to GRUB. Once the shell is ready, run the following commands mount -o remount,rw /sysroot chroot /sysroot passwd root","title":"Resetting root password"},{"location":"Linux/GRUB/#text-mode-installation","text":"RHEL can be installed from the console by providing inst.text as a kernel parameter on boot by pressing ++Tab++ on the GRUB splash screen.","title":"Text-mode installation"},{"location":"Linux/GRUB/#grub-rescue-prompt","text":"When GRUB2 is unable to find the GRUB folder or its contents are missing or corrupted, it displays the prompt grub rescue> This means it failed to load the normal module. howtoforge.com From GRUB rescue prompt: set prefix=(hd0,1)/boot/grub set root=(hd0,1) insmod normal normal After booting the system, GRUB should be updated and reinstalled: Update GRUB config file update-grub Reinstall GRUB grub-install /dev/sdx","title":"GRUB rescue prompt"},{"location":"Linux/GRUB/#commands","text":"","title":"Commands"},{"location":"Linux/GRUB/#grub-mkconfig","text":"Generate GRUB configuration grub-mkconfig -o /boot/grub/grub.cfg","title":"grub-mkconfig"},{"location":"Linux/GRUB/#grub2-mkconfig","text":"grub2-mkconfig is used to create a GRUB2 config file from the settings defined in /etc/default/grub grub2-mkconfig -o = /boot/grub2/grub.cfg","title":"grub2-mkconfig"},{"location":"Linux/GRUB/#grub2-editenv","text":"Disable the Nouveau display driver while installing the proprietary Nvidia display driver on Fedora grub2-editenv - set \" $( grub2-editenv - list | grep kernelopts ) nouveau.modeset=0\"","title":"grub2-editenv"},{"location":"Linux/GRUB/#update-grub","text":"Update GRUB config file update-grub","title":"update-grub"},{"location":"Linux/IAM/","text":"Users Tasks User management Lock user usermod -L $USER # --lock passwd -l $USER # --lock Unlock user usermod -U $USER # --unlock passwd -u $USER # --unlock Groups Display groups of effective user id -Gn getent group | grep $( whoami ) - Commands chage Expire password in 30 days chage -E $( date -d +30days +%Y-%m-%d ) $USER getent Get entries from the passwd file getent passwd bob getent group dba_admins lastb Display failed logins for user lastb $USER sudo The /etc/sudoers file contains user specifications that define commands that users may execute. $USER $HOST = ($RUNAS) $CMD $USER : usernames, UIDs, group names when prefixed with % i.e. %wheel , or GIDs when prefixed with %# $HOST : hostnames, IP addresses, or a CIDR range (i.e. 192.0.2.0/24) $RUNAS : optional clause that controls the user or group sudo will run the command as. If a username is specified, sudo will not accept a -g argument when runing sudo. $CMD : full path to an executable, or a comma-delimited list of commands. Any of these elements can be replaced with the keyword ALL . Ansible service account ansible ALL=(ALL) NOPASSWD: ALL ``` title=\"Allow user to run only the mkdir command user ALL=/bin/mkdir ``` title=\"Allow user to run all commands without authenticating\" user ALL=(ALL) NOPASSWD: ALL Change timeout to 10 minutes Defaults timestamp_timeout=10 Change timeout to 10 minutes only for user linuxize Defaults:linuxize timestamp_timeout=10 gpasswd Administer /etc/group and /etc/gshadow Add user to group gpasswd -a $USER $GROUP Add user as admin of group gpasswd -A $USER $GROUP Remove user from group gpasswd -d $USER $GROUP groupadd groupdel groupmod useradd Add user useradd $USER \\ -m \\ # Create home directory -d $PATH \\ # Specify home directory -s /bin/bash \\ # Default shell -c $FULLNAME \\ # Note full name in comment -G $GROUP1 $GROUP2 \\ # Add groups -u $UID \\ # Specify user ID -e $DATE \\ # Specify expiration date (YYYY-MM-DD) -r \\ # System user Useradd's config is at /etc/default/useradd but it also inherits settings from /etc/login.defs . Example config # useradd defaults file for ArchLinux # original changes by TomK GROUP = users HOME = /home INACTIVE = -1 EXPIRE = SHELL = /bin/bash SKEL = /etc/skel CREATE_MAIL_SPOOL = no These settings can be displayed with: useradd -D userdel Delete an existing user account as well as the user's home directory userdel -r $USER usermod","title":"Users"},{"location":"Linux/IAM/#users","text":"","title":"Users"},{"location":"Linux/IAM/#tasks","text":"","title":"Tasks"},{"location":"Linux/IAM/#user-management","text":"Lock user usermod -L $USER # --lock passwd -l $USER # --lock Unlock user usermod -U $USER # --unlock passwd -u $USER # --unlock","title":"User management"},{"location":"Linux/IAM/#groups","text":"Display groups of effective user id -Gn getent group | grep $( whoami ) -","title":"Groups"},{"location":"Linux/IAM/#commands","text":"","title":"Commands"},{"location":"Linux/IAM/#chage","text":"Expire password in 30 days chage -E $( date -d +30days +%Y-%m-%d ) $USER","title":"chage"},{"location":"Linux/IAM/#getent","text":"Get entries from the passwd file getent passwd bob getent group dba_admins","title":"getent"},{"location":"Linux/IAM/#lastb","text":"Display failed logins for user lastb $USER","title":"lastb"},{"location":"Linux/IAM/#sudo","text":"The /etc/sudoers file contains user specifications that define commands that users may execute. $USER $HOST = ($RUNAS) $CMD $USER : usernames, UIDs, group names when prefixed with % i.e. %wheel , or GIDs when prefixed with %# $HOST : hostnames, IP addresses, or a CIDR range (i.e. 192.0.2.0/24) $RUNAS : optional clause that controls the user or group sudo will run the command as. If a username is specified, sudo will not accept a -g argument when runing sudo. $CMD : full path to an executable, or a comma-delimited list of commands. Any of these elements can be replaced with the keyword ALL . Ansible service account ansible ALL=(ALL) NOPASSWD: ALL ``` title=\"Allow user to run only the mkdir command user ALL=/bin/mkdir ``` title=\"Allow user to run all commands without authenticating\" user ALL=(ALL) NOPASSWD: ALL Change timeout to 10 minutes Defaults timestamp_timeout=10 Change timeout to 10 minutes only for user linuxize Defaults:linuxize timestamp_timeout=10","title":"sudo"},{"location":"Linux/IAM/#gpasswd","text":"Administer /etc/group and /etc/gshadow Add user to group gpasswd -a $USER $GROUP Add user as admin of group gpasswd -A $USER $GROUP Remove user from group gpasswd -d $USER $GROUP","title":"gpasswd"},{"location":"Linux/IAM/#groupadd","text":"","title":"groupadd"},{"location":"Linux/IAM/#groupdel","text":"","title":"groupdel"},{"location":"Linux/IAM/#groupmod","text":"","title":"groupmod"},{"location":"Linux/IAM/#useradd","text":"Add user useradd $USER \\ -m \\ # Create home directory -d $PATH \\ # Specify home directory -s /bin/bash \\ # Default shell -c $FULLNAME \\ # Note full name in comment -G $GROUP1 $GROUP2 \\ # Add groups -u $UID \\ # Specify user ID -e $DATE \\ # Specify expiration date (YYYY-MM-DD) -r \\ # System user Useradd's config is at /etc/default/useradd but it also inherits settings from /etc/login.defs . Example config # useradd defaults file for ArchLinux # original changes by TomK GROUP = users HOME = /home INACTIVE = -1 EXPIRE = SHELL = /bin/bash SKEL = /etc/skel CREATE_MAIL_SPOOL = no These settings can be displayed with: useradd -D","title":"useradd"},{"location":"Linux/IAM/#userdel","text":"Delete an existing user account as well as the user's home directory userdel -r $USER","title":"userdel"},{"location":"Linux/IAM/#usermod","text":"","title":"usermod"},{"location":"Linux/Kernel/","text":"Kernel Commands sysctl View and configure kernel parameters at runtime. Kernel parameters are tunable values that can be adjusted during runtime. Kernel parameters can be delimited with dots or slashes sysctl kernel.hostname sysctl kernel/hostname ```sh title=\"Suppress sysctl -n kernel.hostname Kernel parameters are set persistently by defining values in **/etc/sysctl.conf** or other .conf files placed in **/etc/sysctl.d/**. ```ini title=\"/etc/sysctl.conf\" net.ipv4.ip_forward=1 These values are then loaded into memory ad-hoc with: sysctl -p # --load The runtime can be manipulated directly from the command-line with a different flag\" sysctl -w net.ipv4.ip_forward = 1 Alternatively, values can be echoed to the virtual filesystem exposed at /proc/sys echo 1 > /proc/sys/net/ipv4/ip_forward Disable IPv6 net.ipv6.conf.all.disable_ipv6=1 net.ipv6.conf.default.disable_ipv6=1 Alternatively, kernel parameters can be viewed or even edited through the virtual filesystem mounted at /proc/sys uname uname # Display operating system \"Linux\" uname -m # Kernel architecture uname -r # Kernel release version uname -a Modules A family of commands exists to manipulate Linux modules, including: Floppy, which can be used safely as a stand-in for any module while learning the commands KVM Wireguard Display currently loaded modules. Output in three columns: Module name Module size (bytes) Processes, filesystems, or other modules using the module rmmod rmmod floppy # (1) Equivalent to modprobe -r floppy","title":"Kernel"},{"location":"Linux/Kernel/#kernel","text":"","title":"Kernel"},{"location":"Linux/Kernel/#commands","text":"","title":"Commands"},{"location":"Linux/Kernel/#sysctl","text":"View and configure kernel parameters at runtime. Kernel parameters are tunable values that can be adjusted during runtime. Kernel parameters can be delimited with dots or slashes sysctl kernel.hostname sysctl kernel/hostname ```sh title=\"Suppress sysctl -n kernel.hostname Kernel parameters are set persistently by defining values in **/etc/sysctl.conf** or other .conf files placed in **/etc/sysctl.d/**. ```ini title=\"/etc/sysctl.conf\" net.ipv4.ip_forward=1 These values are then loaded into memory ad-hoc with: sysctl -p # --load The runtime can be manipulated directly from the command-line with a different flag\" sysctl -w net.ipv4.ip_forward = 1 Alternatively, values can be echoed to the virtual filesystem exposed at /proc/sys echo 1 > /proc/sys/net/ipv4/ip_forward Disable IPv6 net.ipv6.conf.all.disable_ipv6=1 net.ipv6.conf.default.disable_ipv6=1 Alternatively, kernel parameters can be viewed or even edited through the virtual filesystem mounted at /proc/sys","title":"sysctl"},{"location":"Linux/Kernel/#uname","text":"uname # Display operating system \"Linux\" uname -m # Kernel architecture uname -r # Kernel release version uname -a","title":"uname"},{"location":"Linux/Kernel/#modules","text":"A family of commands exists to manipulate Linux modules, including: Floppy, which can be used safely as a stand-in for any module while learning the commands KVM Wireguard Display currently loaded modules. Output in three columns: Module name Module size (bytes) Processes, filesystems, or other modules using the module","title":"Modules"},{"location":"Linux/Kernel/#rmmod","text":"rmmod floppy # (1) Equivalent to modprobe -r floppy","title":"rmmod"},{"location":"Linux/Network/","text":"Networking The Linux kernel supports several packet-filtering mechanisms. Netfilter using the venerable iptables utility nftables subsystem, introduced with kernel 3.13 (2014), had been commonly assumed to eventually take the place of iptables. Firewall rules are implemented in an in-kernel VM. bpfilter Netfilter is a software firewall and packet filtering framework introduced with Linux 2.4.0 (2001) and controlled by the iptables command. Netfilter rules are stored in tables and in chains , and tables are associated with various chains. By convention, table names are specified in lowercase and chain names in uppercase. Every packet starts at the top of a chain and is matched rule by rule. When a match is found the specified action, called the target , is triggered: i.e. \"DROP\" or \"ACCEPT\". Tables INPUT OUTPUT FORWARD PREROUTING POSTROUTING filter \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f nat \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f mangle \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f raw \u2714\ufe0f \u2714\ufe0f There are five builtin netfilter chains , though user-defined chains are also possible: INPUT used for filtering incoming packets where the host itself is the destination packet. OUTPUT for outgoing packets, where the host is the source of the packet. FORWARD for filtering routed packets, where the host is router. PREROUTING used for DNAT or port forwarding POSTROUTING used for SNAT Netfilter tables : filter default nat for SNAT and DNAT mangle for packet alteration raw used only to mark packets that should not be handled by the connection tracking system using the NOTRACK target NetworkManager NetworkManager provides a high-level interface for the configuration of network interfaces. It was developed by Red Hat and released in late 2004. The config file format native to NetworkManager is the ini-format keyfile stored in /etc/NetworkManager/system-connections . These files define network interfaces, or connection profiles in NetworkManager's terminology. nmcli Control NetworkManager and report network status Display devices and statuses nmcli device status Display information on interfaces as well as status Including other network connections not managed by network manager (\"unmanaged\") or not connected (\"unavailable\") nmcli dev status Display what connections are enabled nmcli general status Display UUIDs associated with network connections nmcli connection show --active Display much more information on network devices nmcli device show Configure settings for network interface {ens01} via interactive shell nmcli connection edit ens01 List all connections NetworkManager has nmcli connection show Show settings for network interface {ens01} nmcli device show ens01 Show status for all devices nmcli device status Display currently configured hostname nmcli general hostname Set hostname to {hostname} nmcli general hostname hostname Show overall status of NetworkManager nmcli general status Migrate a connection profile nmcli connection migrate eth0 nmtui dnf install NetworkManager-tui nmtui is a curses-based TUI for control of NetworkManager. Netplan netplan is a utility for network configuration using YAML files that is the default network management tool used by recent versions of Ubuntu (since Ubuntu 17.10). Netplan is used as the default network management tool (previously ifconfig and its config at /etc/network/interfaces was used). Netplan supports two renderers or backends: NetworkManager and networkd . Netplan configs are YAML format and placed in /etc/netplan . Ubuntu installations usually come with a single config in this location named 01-network-manage-all-yaml , but many configs can be created in subdirectories. These are processed in lexicographical order regardless of subdirectory (unless there are multiple files with the same name). If a boolean or scalar parameter is defined in more than one config, the last value is assumed. Values that are sequences are concatenated. The default config may look something like this, with DHCP enabled. # Let NetworkManager manage all devices on this system network : version : 2 renderer : NetworkManager netplan The netplan utility can be used to load the on-disk configuration. Reload configuration temporarily netplan try Tasks Bridge A bridge is used to unite two or more network segments, typically used to establish communication channels between VMs, containers, and the host. Unlike the virtual bridge that Windows uses for WSL2 distributions, the bridge in Linux is strictly L2. That is, VMs connecting to the bridge are assigned IPs by the same DHCP server (i.e. the router) in the same subnet as that of the physical hosts. In Windows, the virtual bridge assigns an internal IP in a private range (usually 172.16.0.0/12), and connectivity to the host or the Internet has to be accomplished via NAT . ip link add virbr0 type bridge # (1) ip link set virbr0 up The link can be deleted thus: ip link delete virbr0 Adding an interface to the bridge is done by setting its master. ip link set enp2s0f0 master virbr0 # (1) This can be undone as follows: ip link set enp2s0f0 nomaster The iproute2 bridge utility can be used to verify the command has taken effect: bridge link This may interrupt network connectivity. In this case, the IP address must be removed from the linked interface and assigned to the bridge ip address delete 192 .168.1.3 dev enp2s0f0 ip address add 192 .168.1.3 dev virbr0 The default route in the routing table must also be amended. Note this is not the IP address of the interface but rather that of the gateway . Also note that this gateway must already have its own network segment defined. That is, in order for a default route to be defined at least one static route must also be defined, which is the gateway's own local subnet. ip route delete default ip route add default via 192 .168.1.1 Downloading files Wget defaults to file operations in a way that is more natural for downloading. wget $url Curl depends on piping and defaults to STDOUT in a manner similar to cat . curl -O $url Wireguard tunnel Red Hat Ubuntu dnf install wireguard-tools apt install wireguard Successful installation can be confirmed by running the following, which should produce no output (and no error) on success. sudo modprobe wireguard The first step in creating a Wireguard tunnel is to create a private key on each endpoint of the tunnel. The genkey subcommand creates a 44-character base64 encoded key ending in = which can be redirected to a file. If the file will be world-readable, the utility will ask you to change the umask. wg genkey # \u2593\u2593\u2591\u2591\u2591\u2593\u2591\u2592\u2593\u2592\u2593\u2592\u2591\u2593\u2591\u2592\u2591\u2593\u2591\u2592\u2592\u2591\u2593\u2591\u2593\u2591\u2592\u2591\u2592\u2593\u2592\u2592\u2592\u2591\u2591\u2592\u2592\u2591\u2591\u2591\u2592\u2592\u2593\u2593= The public key can be generated by piping the private key from STDIN or from the file. wg pubkey private > public wg genkey | tee private | wg pubkey > public Then a Wireguard interface is created, typically named wg0 , using network management utilities. ip link add wg0 type wireguard An IP address is assigned to that interface, to be used within the tunnel: ip addr add 10 .0.0.1/24 dev wg0 Now the private key is associated with the interface: wg set wg0 private-key ~/.config/wireguard/private Finally, the interface is brought up: ip link set wg0 up The public key of the peer is now associated with the Wireguard interface and the public IP and port of the other endpoint are specified. wg set wg0 peer $PUBKEY allowed-ips 10 .0.0.2/32 endpoint $IP The tunnel is dismantled by removing the interface. ip link delete wg0 Alternatively, many of these steps can be consolidated into creating a config for the Wireguard interface at /etc/wireguard/wg0.conf with the following contents: [Interface] PrivateKey = \u2593\u2593\u2591\u2591\u2591\u2593\u2591\u2592\u2593\u2592\u2593\u2592\u2591\u2593\u2591\u2592\u2591\u2593\u2591\u2592\u2592\u2591\u2593\u2591\u2593\u2591\u2592\u2591\u2592\u2593\u2592\u2592\u2592\u2591\u2591\u2592\u2592\u2591\u2591\u2591\u2592\u2592\u2593\u2593= Address = 10.0.0.1/24 PostUp = iptables -A FORWARD -i wg0 -j ACCEPT; iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE PostDown = iptables -D FORWARD -i wg0 -j ACCEPT; iptables -t nat -D POSTROUTING -o eth0 -j MASQUERADE ListenPort = 51820 [Peer] PublicKey = \u2593\u2592\u2593\u2592\u2593\u2593\u2591\u2593\u2592\u2592\u2591\u2591\u2592\u2592\u2592\u2593\u2592\u2591\u2592\u2593\u2592\u2591\u2592\u2592\u2593\u2592\u2591\u2592\u2591\u2591\u2591\u2591\u2592\u2591\u2592\u2592\u2592\u2591\u2591\u2592\u2592\u2593\u2591\u2592= AllowedIps = 10.0.0.2/32 Endpoint = 123.45.67.89:51820 Then to bring it up quickly: wg-quick up wg0 The same utility can be used to teardown the tunnel wg-quick down wg0 Static IP Static IP configuration varies by the network management toolset and backend presenton a system. Ubuntu systems use Netplan whereas other distributions most commonly use Network Manager . Netplan Network Manager network : version : 2 ethernets : eth0 : addresses : - 192.168.2.100/24 gateway4 : 192.168.2.1 nameservers : addresses : - 192.168.1.1 search : [] [connection] id = Ethernet uuid = abcdef01-2345-6789-0abc-def012345678 type = ethernet interface-name = eth0 [ethernet] [ipv4] address1 = 192.168.2.100/24,192.168.2.1 dns = 10.40.7.2 method = manual [ipv6] addr-gen-mode = stable-privacy method = auto Setting a static IP address on Red Hat distributions could involve multiple methods: nmcli commands NetworkManager keyfiles ifcfg files (prior to distributions downstream to Fedora 36) Commands curl Accept a self-signed certificate by skipping verification curl -k https://192.168.1.10 Use the dict network protocol to retrieve the definition of a word. ref curl dict://dict.org/d:<word> Sending a POST method to a FastAPI app ( src ) curl -X POST \"http://127.0.0.1:8000/purchase/item/\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\\\"name\\\":\\\"sample item\\\",\\\"info\\\":\\\"This is info for the item\\\",\\\"price\\\":40,\\\"qty\\\":2}\" firewall-cmd Frontend to Netfilter in Red Hat distributions. sh firewall-cmd --state # \"running\" s Firewalld has a runtime configuration and a saved, persistent configuration. Only the runtime configuration will be consulted for any command, unless the persistent configuration is specified with --permanent . The runtime configuration can be saved with this command, which obviates the need to execute every change twice. firewall-cmd --runtime-to-permanent Alternatively, the persistent configuration can be loaded into memory: firewall-cmd --reload Display firewall rules firewall-cmd --list-all --permanent Firewalld uses zones to define the level of trust for network connections. A connection can only be part of one zone, but a zone can be used for many network connections. Builtin zones have XML-format configs found in /usr/lib/firewalld/zones . firewall-cmd --get-active-zones # Display active zones along with interfaces firewall-cmd --info-zone = public # Inspect zone firewall-cmd --new-zone = testlab # Create new zone Firewalld rules are generally managed through builtin services . These bundle network settings together for well-known applications like SSH , etc. Builtin services are also XML-format configs found in /usr/lib/firewalld/services . Services firewall-cmd --list-services firewall-cmd --add-service = http firewall-cmd --remove-service = http Firewalld's config file is at /etc/firewalld/firewalld.conf /etc/firewalld/firewalld.conf AllowZoneDrifting = no Since RHEL 8, firewalld's backend has been changed to nftables. /etc/firewalld/firewalld.conf FirewallBackend = nftables ip ip address ip address add 192 .168.2.2 dev eth0 ip route # Add static route (this is sometimes done automatically by the system after adding an address) ip route add 192 .168.2.0/24 dev eth0 # Add default route ip route add default via 192 .168.2.1 dev eth0 ip link # Create new links ip link add virbr0 type bridge ip link add wg0 type wireguard # Listen for netlink messages ip monitor # Change the default gateway to 192.168.1.1 on eth0 ip route change default via 192 .168.1.1 dev eth0 # Bring interface up\" ip link set wlp2s0 up ip neighbor # Display ARP cache ip neighbor show # Delete ARP entry ip neighbor delete $IP_ADDR dev eth0 ip netns ip netns # (1) # We can create a network namespace then add two virtual Ethernet interfaces. # These are **peers**, meaning they are linked together as if connected to the same switch. ip netns add netns0 ip link add veth0 type veth peer name veth1 netns netns0 # We can then run a command in the **context** of a namespace. Without providing a context, # the default namespace is used and we can display veth0 but not veth1. If there are no other # links in the namespace (which there shouldn't be) then the **number** of the interface's # peer appears in the link's name. By running a command in the context of the new namespace # we can display veth1. The interface number of the link it's paired with in the # default namespace also appears in this link's name. ip link show # \"veth0@if2\" ip netns exec netns0 ip link show # \"veth1@if4\" # Now we assign an address to the namespaced link and bring it up ip netns exec netns0 ip addr add 10 .0.0.1/24 dev veth1 ip netns exec netns0 ip link set dev veth1 up # Pinging this IP from outside the namespace will not work because there is no route. # Adding an IP in the same subnet to veth0 creates the route. ip addr add 10 .0.0.2/24 dev veth0 # The interface must be brought up, which automatically adds a route to the routing table. ip link set dev veth0 up Network namespaces are mounted to /var/run/netns iptables A frontend for the kernel-level netfilter service, similar to firewalld . Rules are saved in a rulesfile which once may have been found at /etc/sysconfig/iptables , but this file does not exist on recent Fedora installations. Display rules as written on disk iptables --list-rules Reload configuration file iptables -F Accept SSH traffic from a particular IP iptables -A INPUT -p ssh -s 10 .0.222.222 -j ACCEPT Accept incoming TCP traffic to port 80 iptables -A INPUT -p tcp --dport 80 -j ACCEPT Change FORWARD chain policy iptables -P FORWARD ACCEPT # (1) By default, the INPUT chain accepts incoming packets. However, this policy can be changed by specifying a DROP rule specification. Allow incoming SSH connections only from a single IP address iptables -A INPUT -p tcp --dport 22 -j DROP iptables -A INPUT -p tcp --dport 22 -s 1 .2.3.4 -j ACCEPT Do not respond to pings iptables -t filter -A INPUT -p icmp -j DROP netcat The netcat utility allows testing of a host's ports, similar to ping , but more versatile because ping only uses the portless ICMP protocol. GNU and OpenBSD versions available Connect to host on port 80 nc example.com 80 Scan ports Single Multiple Range of ports nc -v -w 2 z 192 .168.56.1 22 nc -v -w 2 z 192 .168.56.1 22 80 nc -v -w 2 z 192 .168.56.1 22 -25 Transfer files between servers This example uses the pv utility to monitor progress. # Run `nc` in listening mode (`-l` option) on port 3000 tar -zcf - debian-10.0.0-amd64-xfce-CD-1.iso | pv | nc -l -p 3000 -q 5 # On the receiving client, to obtain the file: nc 192 .168.1.4 3000 | pv | tar -zxf - Create a command-line chat server # Create chat server listening on port 5000 nc -l -vv -p 5000 # Launch a chat session on the other system nc 192.168.56.1 5000 Find a service running on port Obtain port banners ( -n disables DNS lookup) nc -v -n 192 .168.56.110 80 Create stream sockets Create and listen on a UNIX-domain stream socket nc -lU /var/tmp/mysocket & ss -lpn | grep \"/var/tmp/\" Create a backdoor Netcat needs to listen on a chosen port (here 3001): -d disables reading from stdin; -e specifies the command to run on the target system nc -L -p 3001 -d -e cmd.exe Connect to {port} at {host} nc host port Netcat command that retrieves a webpage nc host port get nft nft list ruleset nft list tables nft list table ip filter # display just the filter table nft flush ruleset nmap Scan hosts from a text file nmap -iL hosts.txt Identify a host's operating system nmap -A localhost.example.com Determine whether a host has a firewall enabled nmap -sA localhost.example.com Scan a specified range of ports nmap -p 10 -300 localhost.example.com Perform a SYN TCP scan, stealthier than the TCP connect scan nmap -sT localhost.example.com Aggressive scan nmap -A 192 .168.1.0/24 Ping scan home network (not bothering with ports) nmap -sn 192 .168.1.0/24 Fast port scan using SYN packets nmap -sS -F 192 .168.1.0/24 Port scan using SYN (\"synchronize\") packet, first element of TCP handshake nmap -sS 192 .168.1.0/24 Port scan using normal TCP nmap -sT 192 .168.1.0/24 Port scan using UDP nmap -sU 192 .168.1.0/24 Xmas scan nmap -sX Scan a range of IPs [ref][Sec+ Lab] nmap 192 .168.27.0/24 > hosts.txt Identify operating system and scan ports using TCP SYN packets [ref][Sec+ Lab] nmap -O -sS 192 .168.27.0/24 > hosts.txt tcpdump Inspect actual IP packets Display all network data tcpdump -i eth0 Set snapshot length of capture (default: 65,535B) tcpdump -s ufw Program for managing a Netfilter firewall. Allow traffic associated with various services ufw allow ssh ufw allow http ufw allow https wg This is the main CLI frontend for Wireguard, the UDP -based tunneling protocol and application that was introduced with kernel 5.6. Fedora Ubuntu dnf install wireguard-tools apt install wireguard wget Accept a self-signed certificate by skipping verification wget --no-check-certificate $URL Glossary eBPF eBPF is an extended version of the Berkeley Packet Filter (BPF). It is a sandboxed environment that allows code to be inserted into the running kernel. Kernel functionality must normally be extended by building an entirely new kernel with custom modules or upstream patching of the Linux kernel. eBPF's architecture includes a JIT compiler that compiles the program's generic bytecode, which means eBPF programs run as efficiently as natively compiled kernel code. eBPF programs can be bound to kernel events, such as receipt of a packet from the NIC. bpftool is a core eBPF CLI tool. bpftool prog # Display running eBPF programs bpftool map show # Display maps ifcfg Historically, ifcfg (interface configuration) files were ini-format files found in /etc/sysconfig/network-scripts/ in Red Hat distributions. They were used to control network interfaces on the legacy \"network\" service, now part of the network-scripts package, which included the sysconfig.txt file which documents the ifcfg file format. After the introduction of NetworkManager , this format survived and was expanded with new directives specific to NetworkManager. By convention, the string value of the DEVICE directive was the suffix of the filename itself. ifcfg-eth0 DEVICE = eth0 BOOTPROTO = dhcp ONBOOT = yes TYPE = Ethernet The nmcli utility exposes a command that can change the configuration backend from ifcfg to a NetworkManager keyfile . Migrate a connection profile nmcli connection migrate eth0 Ifcfg file support was finally removed in RHEL 9 and Fedora 36 . If no ifcfg files are present, the configuration backend that supports them can be removed. dnf remove NetworkManager-initscripts-ifcfg-rh Netfilter Netfilter is a software firewall and packet filtering framework introduced with Linux 2.4.0 (2001) and controlled by the iptables command.","title":"Network"},{"location":"Linux/Network/#networking","text":"The Linux kernel supports several packet-filtering mechanisms. Netfilter using the venerable iptables utility nftables subsystem, introduced with kernel 3.13 (2014), had been commonly assumed to eventually take the place of iptables. Firewall rules are implemented in an in-kernel VM. bpfilter Netfilter is a software firewall and packet filtering framework introduced with Linux 2.4.0 (2001) and controlled by the iptables command. Netfilter rules are stored in tables and in chains , and tables are associated with various chains. By convention, table names are specified in lowercase and chain names in uppercase. Every packet starts at the top of a chain and is matched rule by rule. When a match is found the specified action, called the target , is triggered: i.e. \"DROP\" or \"ACCEPT\". Tables INPUT OUTPUT FORWARD PREROUTING POSTROUTING filter \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f nat \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f mangle \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f raw \u2714\ufe0f \u2714\ufe0f There are five builtin netfilter chains , though user-defined chains are also possible: INPUT used for filtering incoming packets where the host itself is the destination packet. OUTPUT for outgoing packets, where the host is the source of the packet. FORWARD for filtering routed packets, where the host is router. PREROUTING used for DNAT or port forwarding POSTROUTING used for SNAT Netfilter tables : filter default nat for SNAT and DNAT mangle for packet alteration raw used only to mark packets that should not be handled by the connection tracking system using the NOTRACK target","title":"Networking"},{"location":"Linux/Network/#networkmanager","text":"NetworkManager provides a high-level interface for the configuration of network interfaces. It was developed by Red Hat and released in late 2004. The config file format native to NetworkManager is the ini-format keyfile stored in /etc/NetworkManager/system-connections . These files define network interfaces, or connection profiles in NetworkManager's terminology.","title":"NetworkManager"},{"location":"Linux/Network/#nmcli","text":"Control NetworkManager and report network status Display devices and statuses nmcli device status Display information on interfaces as well as status Including other network connections not managed by network manager (\"unmanaged\") or not connected (\"unavailable\") nmcli dev status Display what connections are enabled nmcli general status Display UUIDs associated with network connections nmcli connection show --active Display much more information on network devices nmcli device show Configure settings for network interface {ens01} via interactive shell nmcli connection edit ens01 List all connections NetworkManager has nmcli connection show Show settings for network interface {ens01} nmcli device show ens01 Show status for all devices nmcli device status Display currently configured hostname nmcli general hostname Set hostname to {hostname} nmcli general hostname hostname Show overall status of NetworkManager nmcli general status Migrate a connection profile nmcli connection migrate eth0","title":"nmcli"},{"location":"Linux/Network/#nmtui","text":"dnf install NetworkManager-tui nmtui is a curses-based TUI for control of NetworkManager.","title":"nmtui"},{"location":"Linux/Network/#netplan","text":"netplan is a utility for network configuration using YAML files that is the default network management tool used by recent versions of Ubuntu (since Ubuntu 17.10). Netplan is used as the default network management tool (previously ifconfig and its config at /etc/network/interfaces was used). Netplan supports two renderers or backends: NetworkManager and networkd . Netplan configs are YAML format and placed in /etc/netplan . Ubuntu installations usually come with a single config in this location named 01-network-manage-all-yaml , but many configs can be created in subdirectories. These are processed in lexicographical order regardless of subdirectory (unless there are multiple files with the same name). If a boolean or scalar parameter is defined in more than one config, the last value is assumed. Values that are sequences are concatenated. The default config may look something like this, with DHCP enabled. # Let NetworkManager manage all devices on this system network : version : 2 renderer : NetworkManager","title":"Netplan"},{"location":"Linux/Network/#netplan_1","text":"The netplan utility can be used to load the on-disk configuration. Reload configuration temporarily netplan try","title":"netplan"},{"location":"Linux/Network/#tasks","text":"","title":"Tasks"},{"location":"Linux/Network/#bridge","text":"A bridge is used to unite two or more network segments, typically used to establish communication channels between VMs, containers, and the host. Unlike the virtual bridge that Windows uses for WSL2 distributions, the bridge in Linux is strictly L2. That is, VMs connecting to the bridge are assigned IPs by the same DHCP server (i.e. the router) in the same subnet as that of the physical hosts. In Windows, the virtual bridge assigns an internal IP in a private range (usually 172.16.0.0/12), and connectivity to the host or the Internet has to be accomplished via NAT . ip link add virbr0 type bridge # (1) ip link set virbr0 up The link can be deleted thus: ip link delete virbr0 Adding an interface to the bridge is done by setting its master. ip link set enp2s0f0 master virbr0 # (1) This can be undone as follows: ip link set enp2s0f0 nomaster The iproute2 bridge utility can be used to verify the command has taken effect: bridge link This may interrupt network connectivity. In this case, the IP address must be removed from the linked interface and assigned to the bridge ip address delete 192 .168.1.3 dev enp2s0f0 ip address add 192 .168.1.3 dev virbr0 The default route in the routing table must also be amended. Note this is not the IP address of the interface but rather that of the gateway . Also note that this gateway must already have its own network segment defined. That is, in order for a default route to be defined at least one static route must also be defined, which is the gateway's own local subnet. ip route delete default ip route add default via 192 .168.1.1","title":"Bridge"},{"location":"Linux/Network/#downloading-files","text":"Wget defaults to file operations in a way that is more natural for downloading. wget $url Curl depends on piping and defaults to STDOUT in a manner similar to cat . curl -O $url","title":"Downloading files"},{"location":"Linux/Network/#wireguard-tunnel","text":"Red Hat Ubuntu dnf install wireguard-tools apt install wireguard Successful installation can be confirmed by running the following, which should produce no output (and no error) on success. sudo modprobe wireguard The first step in creating a Wireguard tunnel is to create a private key on each endpoint of the tunnel. The genkey subcommand creates a 44-character base64 encoded key ending in = which can be redirected to a file. If the file will be world-readable, the utility will ask you to change the umask. wg genkey # \u2593\u2593\u2591\u2591\u2591\u2593\u2591\u2592\u2593\u2592\u2593\u2592\u2591\u2593\u2591\u2592\u2591\u2593\u2591\u2592\u2592\u2591\u2593\u2591\u2593\u2591\u2592\u2591\u2592\u2593\u2592\u2592\u2592\u2591\u2591\u2592\u2592\u2591\u2591\u2591\u2592\u2592\u2593\u2593= The public key can be generated by piping the private key from STDIN or from the file. wg pubkey private > public wg genkey | tee private | wg pubkey > public Then a Wireguard interface is created, typically named wg0 , using network management utilities. ip link add wg0 type wireguard An IP address is assigned to that interface, to be used within the tunnel: ip addr add 10 .0.0.1/24 dev wg0 Now the private key is associated with the interface: wg set wg0 private-key ~/.config/wireguard/private Finally, the interface is brought up: ip link set wg0 up The public key of the peer is now associated with the Wireguard interface and the public IP and port of the other endpoint are specified. wg set wg0 peer $PUBKEY allowed-ips 10 .0.0.2/32 endpoint $IP The tunnel is dismantled by removing the interface. ip link delete wg0 Alternatively, many of these steps can be consolidated into creating a config for the Wireguard interface at /etc/wireguard/wg0.conf with the following contents: [Interface] PrivateKey = \u2593\u2593\u2591\u2591\u2591\u2593\u2591\u2592\u2593\u2592\u2593\u2592\u2591\u2593\u2591\u2592\u2591\u2593\u2591\u2592\u2592\u2591\u2593\u2591\u2593\u2591\u2592\u2591\u2592\u2593\u2592\u2592\u2592\u2591\u2591\u2592\u2592\u2591\u2591\u2591\u2592\u2592\u2593\u2593= Address = 10.0.0.1/24 PostUp = iptables -A FORWARD -i wg0 -j ACCEPT; iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE PostDown = iptables -D FORWARD -i wg0 -j ACCEPT; iptables -t nat -D POSTROUTING -o eth0 -j MASQUERADE ListenPort = 51820 [Peer] PublicKey = \u2593\u2592\u2593\u2592\u2593\u2593\u2591\u2593\u2592\u2592\u2591\u2591\u2592\u2592\u2592\u2593\u2592\u2591\u2592\u2593\u2592\u2591\u2592\u2592\u2593\u2592\u2591\u2592\u2591\u2591\u2591\u2591\u2592\u2591\u2592\u2592\u2592\u2591\u2591\u2592\u2592\u2593\u2591\u2592= AllowedIps = 10.0.0.2/32 Endpoint = 123.45.67.89:51820 Then to bring it up quickly: wg-quick up wg0 The same utility can be used to teardown the tunnel wg-quick down wg0","title":"Wireguard tunnel"},{"location":"Linux/Network/#static-ip","text":"Static IP configuration varies by the network management toolset and backend presenton a system. Ubuntu systems use Netplan whereas other distributions most commonly use Network Manager . Netplan Network Manager network : version : 2 ethernets : eth0 : addresses : - 192.168.2.100/24 gateway4 : 192.168.2.1 nameservers : addresses : - 192.168.1.1 search : [] [connection] id = Ethernet uuid = abcdef01-2345-6789-0abc-def012345678 type = ethernet interface-name = eth0 [ethernet] [ipv4] address1 = 192.168.2.100/24,192.168.2.1 dns = 10.40.7.2 method = manual [ipv6] addr-gen-mode = stable-privacy method = auto Setting a static IP address on Red Hat distributions could involve multiple methods: nmcli commands NetworkManager keyfiles ifcfg files (prior to distributions downstream to Fedora 36)","title":"Static IP"},{"location":"Linux/Network/#commands","text":"","title":"Commands"},{"location":"Linux/Network/#curl","text":"Accept a self-signed certificate by skipping verification curl -k https://192.168.1.10 Use the dict network protocol to retrieve the definition of a word. ref curl dict://dict.org/d:<word> Sending a POST method to a FastAPI app ( src ) curl -X POST \"http://127.0.0.1:8000/purchase/item/\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\\\"name\\\":\\\"sample item\\\",\\\"info\\\":\\\"This is info for the item\\\",\\\"price\\\":40,\\\"qty\\\":2}\"","title":"curl"},{"location":"Linux/Network/#firewall-cmd","text":"Frontend to Netfilter in Red Hat distributions. sh firewall-cmd --state # \"running\" s Firewalld has a runtime configuration and a saved, persistent configuration. Only the runtime configuration will be consulted for any command, unless the persistent configuration is specified with --permanent . The runtime configuration can be saved with this command, which obviates the need to execute every change twice. firewall-cmd --runtime-to-permanent Alternatively, the persistent configuration can be loaded into memory: firewall-cmd --reload Display firewall rules firewall-cmd --list-all --permanent Firewalld uses zones to define the level of trust for network connections. A connection can only be part of one zone, but a zone can be used for many network connections. Builtin zones have XML-format configs found in /usr/lib/firewalld/zones . firewall-cmd --get-active-zones # Display active zones along with interfaces firewall-cmd --info-zone = public # Inspect zone firewall-cmd --new-zone = testlab # Create new zone Firewalld rules are generally managed through builtin services . These bundle network settings together for well-known applications like SSH , etc. Builtin services are also XML-format configs found in /usr/lib/firewalld/services . Services firewall-cmd --list-services firewall-cmd --add-service = http firewall-cmd --remove-service = http Firewalld's config file is at /etc/firewalld/firewalld.conf /etc/firewalld/firewalld.conf AllowZoneDrifting = no Since RHEL 8, firewalld's backend has been changed to nftables. /etc/firewalld/firewalld.conf FirewallBackend = nftables","title":"firewall-cmd"},{"location":"Linux/Network/#ip","text":"ip address ip address add 192 .168.2.2 dev eth0 ip route # Add static route (this is sometimes done automatically by the system after adding an address) ip route add 192 .168.2.0/24 dev eth0 # Add default route ip route add default via 192 .168.2.1 dev eth0 ip link # Create new links ip link add virbr0 type bridge ip link add wg0 type wireguard # Listen for netlink messages ip monitor # Change the default gateway to 192.168.1.1 on eth0 ip route change default via 192 .168.1.1 dev eth0 # Bring interface up\" ip link set wlp2s0 up ip neighbor # Display ARP cache ip neighbor show # Delete ARP entry ip neighbor delete $IP_ADDR dev eth0 ip netns ip netns # (1) # We can create a network namespace then add two virtual Ethernet interfaces. # These are **peers**, meaning they are linked together as if connected to the same switch. ip netns add netns0 ip link add veth0 type veth peer name veth1 netns netns0 # We can then run a command in the **context** of a namespace. Without providing a context, # the default namespace is used and we can display veth0 but not veth1. If there are no other # links in the namespace (which there shouldn't be) then the **number** of the interface's # peer appears in the link's name. By running a command in the context of the new namespace # we can display veth1. The interface number of the link it's paired with in the # default namespace also appears in this link's name. ip link show # \"veth0@if2\" ip netns exec netns0 ip link show # \"veth1@if4\" # Now we assign an address to the namespaced link and bring it up ip netns exec netns0 ip addr add 10 .0.0.1/24 dev veth1 ip netns exec netns0 ip link set dev veth1 up # Pinging this IP from outside the namespace will not work because there is no route. # Adding an IP in the same subnet to veth0 creates the route. ip addr add 10 .0.0.2/24 dev veth0 # The interface must be brought up, which automatically adds a route to the routing table. ip link set dev veth0 up Network namespaces are mounted to /var/run/netns","title":"ip"},{"location":"Linux/Network/#iptables","text":"A frontend for the kernel-level netfilter service, similar to firewalld . Rules are saved in a rulesfile which once may have been found at /etc/sysconfig/iptables , but this file does not exist on recent Fedora installations. Display rules as written on disk iptables --list-rules Reload configuration file iptables -F Accept SSH traffic from a particular IP iptables -A INPUT -p ssh -s 10 .0.222.222 -j ACCEPT Accept incoming TCP traffic to port 80 iptables -A INPUT -p tcp --dport 80 -j ACCEPT Change FORWARD chain policy iptables -P FORWARD ACCEPT # (1) By default, the INPUT chain accepts incoming packets. However, this policy can be changed by specifying a DROP rule specification. Allow incoming SSH connections only from a single IP address iptables -A INPUT -p tcp --dport 22 -j DROP iptables -A INPUT -p tcp --dport 22 -s 1 .2.3.4 -j ACCEPT Do not respond to pings iptables -t filter -A INPUT -p icmp -j DROP","title":"iptables"},{"location":"Linux/Network/#netcat","text":"The netcat utility allows testing of a host's ports, similar to ping , but more versatile because ping only uses the portless ICMP protocol. GNU and OpenBSD versions available Connect to host on port 80 nc example.com 80 Scan ports Single Multiple Range of ports nc -v -w 2 z 192 .168.56.1 22 nc -v -w 2 z 192 .168.56.1 22 80 nc -v -w 2 z 192 .168.56.1 22 -25 Transfer files between servers This example uses the pv utility to monitor progress. # Run `nc` in listening mode (`-l` option) on port 3000 tar -zcf - debian-10.0.0-amd64-xfce-CD-1.iso | pv | nc -l -p 3000 -q 5 # On the receiving client, to obtain the file: nc 192 .168.1.4 3000 | pv | tar -zxf - Create a command-line chat server # Create chat server listening on port 5000 nc -l -vv -p 5000 # Launch a chat session on the other system nc 192.168.56.1 5000 Find a service running on port Obtain port banners ( -n disables DNS lookup) nc -v -n 192 .168.56.110 80 Create stream sockets Create and listen on a UNIX-domain stream socket nc -lU /var/tmp/mysocket & ss -lpn | grep \"/var/tmp/\" Create a backdoor Netcat needs to listen on a chosen port (here 3001): -d disables reading from stdin; -e specifies the command to run on the target system nc -L -p 3001 -d -e cmd.exe Connect to {port} at {host} nc host port Netcat command that retrieves a webpage nc host port get","title":"netcat"},{"location":"Linux/Network/#nft","text":"nft list ruleset nft list tables nft list table ip filter # display just the filter table nft flush ruleset","title":"nft"},{"location":"Linux/Network/#nmap","text":"Scan hosts from a text file nmap -iL hosts.txt Identify a host's operating system nmap -A localhost.example.com Determine whether a host has a firewall enabled nmap -sA localhost.example.com Scan a specified range of ports nmap -p 10 -300 localhost.example.com Perform a SYN TCP scan, stealthier than the TCP connect scan nmap -sT localhost.example.com Aggressive scan nmap -A 192 .168.1.0/24 Ping scan home network (not bothering with ports) nmap -sn 192 .168.1.0/24 Fast port scan using SYN packets nmap -sS -F 192 .168.1.0/24 Port scan using SYN (\"synchronize\") packet, first element of TCP handshake nmap -sS 192 .168.1.0/24 Port scan using normal TCP nmap -sT 192 .168.1.0/24 Port scan using UDP nmap -sU 192 .168.1.0/24 Xmas scan nmap -sX Scan a range of IPs [ref][Sec+ Lab] nmap 192 .168.27.0/24 > hosts.txt Identify operating system and scan ports using TCP SYN packets [ref][Sec+ Lab] nmap -O -sS 192 .168.27.0/24 > hosts.txt","title":"nmap"},{"location":"Linux/Network/#tcpdump","text":"Inspect actual IP packets Display all network data tcpdump -i eth0 Set snapshot length of capture (default: 65,535B) tcpdump -s","title":"tcpdump"},{"location":"Linux/Network/#ufw","text":"Program for managing a Netfilter firewall. Allow traffic associated with various services ufw allow ssh ufw allow http ufw allow https","title":"ufw"},{"location":"Linux/Network/#wg","text":"This is the main CLI frontend for Wireguard, the UDP -based tunneling protocol and application that was introduced with kernel 5.6. Fedora Ubuntu dnf install wireguard-tools apt install wireguard","title":"wg"},{"location":"Linux/Network/#wget","text":"Accept a self-signed certificate by skipping verification wget --no-check-certificate $URL","title":"wget"},{"location":"Linux/Network/#glossary","text":"","title":"Glossary"},{"location":"Linux/Network/#ebpf","text":"eBPF is an extended version of the Berkeley Packet Filter (BPF). It is a sandboxed environment that allows code to be inserted into the running kernel. Kernel functionality must normally be extended by building an entirely new kernel with custom modules or upstream patching of the Linux kernel. eBPF's architecture includes a JIT compiler that compiles the program's generic bytecode, which means eBPF programs run as efficiently as natively compiled kernel code. eBPF programs can be bound to kernel events, such as receipt of a packet from the NIC. bpftool is a core eBPF CLI tool. bpftool prog # Display running eBPF programs bpftool map show # Display maps","title":"eBPF"},{"location":"Linux/Network/#ifcfg","text":"Historically, ifcfg (interface configuration) files were ini-format files found in /etc/sysconfig/network-scripts/ in Red Hat distributions. They were used to control network interfaces on the legacy \"network\" service, now part of the network-scripts package, which included the sysconfig.txt file which documents the ifcfg file format. After the introduction of NetworkManager , this format survived and was expanded with new directives specific to NetworkManager. By convention, the string value of the DEVICE directive was the suffix of the filename itself. ifcfg-eth0 DEVICE = eth0 BOOTPROTO = dhcp ONBOOT = yes TYPE = Ethernet The nmcli utility exposes a command that can change the configuration backend from ifcfg to a NetworkManager keyfile . Migrate a connection profile nmcli connection migrate eth0 Ifcfg file support was finally removed in RHEL 9 and Fedora 36 . If no ifcfg files are present, the configuration backend that supports them can be removed. dnf remove NetworkManager-initscripts-ifcfg-rh","title":"ifcfg"},{"location":"Linux/Network/#netfilter","text":"Netfilter is a software firewall and packet filtering framework introduced with Linux 2.4.0 (2001) and controlled by the iptables command.","title":"Netfilter"},{"location":"Linux/Package/","text":"Package management Commands add-apt-repository APT repositories ( /etc/apt/sources.list ) are made of three parts , delimited by whitespace: Source type : deb for binary packages or deb-src for source packages Base URL of the source: beginning with http:// , ftp:// , file:// , or even cdrom: Name of the chosen distribution followed by sections that differentiate packages by license. Kali contains main , non-free , and contrib . deb http://us-central1.gce.archive.ubuntu.com/ubuntu/ bionic main restricted deb http://us-central1.gce.archive.ubuntu.com/ubuntu/ bionic universe deb http://us-central1.gce.archive.ubuntu.com/ubuntu/ bionic-updates main restricted deb http://us-central1.gce.archive.ubuntu.com/ubuntu/ bionic-updates universe add-apt-repository \"deb http://security.ubuntu.com/ubuntu trusty-security main universe\" # Ubuntu add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $( lsb_release -cs ) stable\" # Docker add-apt-repository \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main\" # gcloud add-apt-repository \"deb http://security.ubuntu.com/ubuntu trusty-security main universe\" # mailx add-apt-repository \"deb [ arch=amd64 ] https://repo.mongodb.org/apt/ubuntu bionic/mongodb-org/4.2 multiverse\" # MongoDB add-apt-repository -y \"ppa:kgilmer/regolith-stable\" # Regolith Linux apt-key apt-key is typically used by piping a GPG key from curl. # Google Cloud SDK curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add - # Docker in WSL curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - Add key specified by apt in error message apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 68980A0EA10B4DE8 Install key from Mono apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 3FA7E0328081BFF6A14DA29AA6A19B38D3D831EF apt dnf View history of dnf commands dnf history dnf history userinstalled # View all packages installed by user Package groups can be specified using the group command or by prefixing the package group name with @ dnf info @virtualization # dnf group info virtualization dnf install @virtualization # dnf group install virtualization dnf install --with-optional @virtualization # Include optional packages Remove the configuration backend supporting the use of legacy ifcfg files in NetworkManager. dnf remove NetworkManager-initscripts-ifcfg-rh Modules are special package groups representing an application, runtime, or a set of tools. The Node.js module allows you to select several streams corresponding to major versions. dnf module install nodejs:12 Global dnf configuration is stored in either /etc/yum.conf or /etc/dnf.conf . [main] ; Exclude packages from updates permanently exclude = kernel* php* ; Suppress confirmation assumeyes = True The configuration can be dumped from the command-line (as root) dnf config-manager --dump Repositories are INI files placed in /etc/yum.repos.d/ , but they can also be displayed and manipulated from the command-line. Repositories # Display repos dnf repolist # -v # Add repo dnf config-manager --add-repo $REPO -URL # Disable repo dnf config-manager --set-disabled $REPO -NAME Example repos [docker-ce-stable] name = Docker CE Stable - $basearch baseurl = https://download.docker.com/linux/fedora/$releasever/$basearch/stable enabled = 1 gpgcheck = 1 gpgkey = https://download.docker.com/linux/fedora/gp [kubernetes] name = Kubernetes baseurl = https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64 enabled = 1 gpgcheck = 1 repo_gpgcheck = 1 gpgkey = https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg [google-cloud-sdk] name = Google Cloud SDK baseurl = https://packages.cloud.google.com/yum/repos/cloud-sdk-el7-x86_64 enabled = 1 gpgcheck = 1 repo_gpgcheck = 0 gpgkey = https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg Modules are collections of packages that are installed together. They often also have profiles available, which are variants of the module: i.e. client, server, common, devel, etc. dnf module list php dnf module install php:7.4/devel dnf module reset php flatpak Flatpak is one of several recent containerized application distribution solutions for Linux. Flatpak runtimes are compiled reproducibly using BuildStream and they are installed in /var/lib/flatpak/runtime . Like Steam, flatpak uses BubbleWrap to implement sandboxing. Flathub is the de facto Flatpak repo, but it must be added to flatpak installations manually. flatpak remote-add --if-not-exists flathub https://flathub.org/repo/flathub.flatpakrepo # Confirming success flatpak remotes Display installed flatpak applications, including runtime flatpak list --app --runtime Output columns can also be specified individually after --column (comma-delimited) flatpak list --app --columns = name,application,runtime Flatpak applications sometimes do not adopt the system theme . The workaround involves first granting some or all applications access to the themes folder. flatpak override --filesystem = $HOME /.themes Then apply the theme by setting the GTK_THEME environment variable. The value of this variable must be the folder name of a theme installed to the themes folder (typically ~/.themes ). flatpak override --env = GTK_THEME = my-theme The value of the current theme can be retrieved using gsettings gsettings get org.gnome.desktop.interface gtk-theme pacman pacman -Q # --query Display all orphaned dependencies (no longer needed) pacman -Qdt # --query --deps --unrequired Display only explicitly installed packages and versions pacman -Qe # --query --explicit Display explicitly installed packages, limiting output to program names pacman -Qeq # pacman --query --explicit --quiet Display all packages installed from the AUR pacman -Qm # --query --foreign Display all packages installed from main repos pacman -Qn # --query --native Find which package owns {file} pacman -Qo file # --query --owns List all install packages, filtering output to packages that are out-of-date on the local system pacman -Qu # --query --upgrades Remove $PACKAGE pacman -R $PACKAGE # --remove package Remove $PACKAGE , dependencies, and config files pacman -Rns $PACKAGE # --remove --recursive --nosave Remove $PACKAGE as well as its dependencies pacman -Rs # --remove --recursive Install $PACKAGE from the AUR pacman -S $PACKAGE # --sync Remove all packages from the cache as well as unused sync databases pacman -Scc # --sync --clean --clean Display information about {package} pacman -Si $PACKAGE # --sync --info package Search for $PACKAGE in AUR repos pacman -Ss $PACKAGE # --sync --search package Search for packages matching $PATTERN pacman -Ss $PATTERN # --sync --search pattern Update package database pacman -Sy # --sync --refresh Update all packages from AUR and official repos pacman -Syu # --sync --refresh --sysupgrade Force refresh of all package databases, even if they appear to be up-to-date pacman -Syy # --sync --refresh --refresh Download program updates but don't install them pacman -Syyuw # --sync --refresh --refresh --sysupgrade --downloadonly Get number of total installed packages pacman -Q | wc -l rpm Query repos for information on a package rpm -qi $PACKAGE # --query --info Upgrade or install a package, with progress bars rpm -Uvh $PACKAGE # --upgrade --verbose --hash Display version of Fedora rpm -E %fedora Import a keyring rpm --import \"https://build.opensuse.org/projects/home:manuelschneid3r/public_key\" snap Snap is one of several recent containerized application distribution solutions for Linux. Snap apps are slow to start because data is stored in squashfs images. Installation Red Hat dnf install -y snapd ln -s /var/lib/snapd/snap /snap","title":"Package management"},{"location":"Linux/Package/#package-management","text":"","title":"Package management"},{"location":"Linux/Package/#commands","text":"","title":"Commands"},{"location":"Linux/Package/#add-apt-repository","text":"APT repositories ( /etc/apt/sources.list ) are made of three parts , delimited by whitespace: Source type : deb for binary packages or deb-src for source packages Base URL of the source: beginning with http:// , ftp:// , file:// , or even cdrom: Name of the chosen distribution followed by sections that differentiate packages by license. Kali contains main , non-free , and contrib . deb http://us-central1.gce.archive.ubuntu.com/ubuntu/ bionic main restricted deb http://us-central1.gce.archive.ubuntu.com/ubuntu/ bionic universe deb http://us-central1.gce.archive.ubuntu.com/ubuntu/ bionic-updates main restricted deb http://us-central1.gce.archive.ubuntu.com/ubuntu/ bionic-updates universe add-apt-repository \"deb http://security.ubuntu.com/ubuntu trusty-security main universe\" # Ubuntu add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $( lsb_release -cs ) stable\" # Docker add-apt-repository \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main\" # gcloud add-apt-repository \"deb http://security.ubuntu.com/ubuntu trusty-security main universe\" # mailx add-apt-repository \"deb [ arch=amd64 ] https://repo.mongodb.org/apt/ubuntu bionic/mongodb-org/4.2 multiverse\" # MongoDB add-apt-repository -y \"ppa:kgilmer/regolith-stable\" # Regolith Linux","title":"add-apt-repository"},{"location":"Linux/Package/#apt-key","text":"apt-key is typically used by piping a GPG key from curl. # Google Cloud SDK curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add - # Docker in WSL curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - Add key specified by apt in error message apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 68980A0EA10B4DE8 Install key from Mono apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 3FA7E0328081BFF6A14DA29AA6A19B38D3D831EF","title":"apt-key"},{"location":"Linux/Package/#apt","text":"","title":"apt"},{"location":"Linux/Package/#dnf","text":"View history of dnf commands dnf history dnf history userinstalled # View all packages installed by user Package groups can be specified using the group command or by prefixing the package group name with @ dnf info @virtualization # dnf group info virtualization dnf install @virtualization # dnf group install virtualization dnf install --with-optional @virtualization # Include optional packages Remove the configuration backend supporting the use of legacy ifcfg files in NetworkManager. dnf remove NetworkManager-initscripts-ifcfg-rh Modules are special package groups representing an application, runtime, or a set of tools. The Node.js module allows you to select several streams corresponding to major versions. dnf module install nodejs:12 Global dnf configuration is stored in either /etc/yum.conf or /etc/dnf.conf . [main] ; Exclude packages from updates permanently exclude = kernel* php* ; Suppress confirmation assumeyes = True The configuration can be dumped from the command-line (as root) dnf config-manager --dump Repositories are INI files placed in /etc/yum.repos.d/ , but they can also be displayed and manipulated from the command-line. Repositories # Display repos dnf repolist # -v # Add repo dnf config-manager --add-repo $REPO -URL # Disable repo dnf config-manager --set-disabled $REPO -NAME Example repos [docker-ce-stable] name = Docker CE Stable - $basearch baseurl = https://download.docker.com/linux/fedora/$releasever/$basearch/stable enabled = 1 gpgcheck = 1 gpgkey = https://download.docker.com/linux/fedora/gp [kubernetes] name = Kubernetes baseurl = https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64 enabled = 1 gpgcheck = 1 repo_gpgcheck = 1 gpgkey = https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg [google-cloud-sdk] name = Google Cloud SDK baseurl = https://packages.cloud.google.com/yum/repos/cloud-sdk-el7-x86_64 enabled = 1 gpgcheck = 1 repo_gpgcheck = 0 gpgkey = https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg Modules are collections of packages that are installed together. They often also have profiles available, which are variants of the module: i.e. client, server, common, devel, etc. dnf module list php dnf module install php:7.4/devel dnf module reset php","title":"dnf"},{"location":"Linux/Package/#flatpak","text":"Flatpak is one of several recent containerized application distribution solutions for Linux. Flatpak runtimes are compiled reproducibly using BuildStream and they are installed in /var/lib/flatpak/runtime . Like Steam, flatpak uses BubbleWrap to implement sandboxing. Flathub is the de facto Flatpak repo, but it must be added to flatpak installations manually. flatpak remote-add --if-not-exists flathub https://flathub.org/repo/flathub.flatpakrepo # Confirming success flatpak remotes Display installed flatpak applications, including runtime flatpak list --app --runtime Output columns can also be specified individually after --column (comma-delimited) flatpak list --app --columns = name,application,runtime Flatpak applications sometimes do not adopt the system theme . The workaround involves first granting some or all applications access to the themes folder. flatpak override --filesystem = $HOME /.themes Then apply the theme by setting the GTK_THEME environment variable. The value of this variable must be the folder name of a theme installed to the themes folder (typically ~/.themes ). flatpak override --env = GTK_THEME = my-theme The value of the current theme can be retrieved using gsettings gsettings get org.gnome.desktop.interface gtk-theme","title":"flatpak"},{"location":"Linux/Package/#pacman","text":"pacman -Q # --query Display all orphaned dependencies (no longer needed) pacman -Qdt # --query --deps --unrequired Display only explicitly installed packages and versions pacman -Qe # --query --explicit Display explicitly installed packages, limiting output to program names pacman -Qeq # pacman --query --explicit --quiet Display all packages installed from the AUR pacman -Qm # --query --foreign Display all packages installed from main repos pacman -Qn # --query --native Find which package owns {file} pacman -Qo file # --query --owns List all install packages, filtering output to packages that are out-of-date on the local system pacman -Qu # --query --upgrades Remove $PACKAGE pacman -R $PACKAGE # --remove package Remove $PACKAGE , dependencies, and config files pacman -Rns $PACKAGE # --remove --recursive --nosave Remove $PACKAGE as well as its dependencies pacman -Rs # --remove --recursive Install $PACKAGE from the AUR pacman -S $PACKAGE # --sync Remove all packages from the cache as well as unused sync databases pacman -Scc # --sync --clean --clean Display information about {package} pacman -Si $PACKAGE # --sync --info package Search for $PACKAGE in AUR repos pacman -Ss $PACKAGE # --sync --search package Search for packages matching $PATTERN pacman -Ss $PATTERN # --sync --search pattern Update package database pacman -Sy # --sync --refresh Update all packages from AUR and official repos pacman -Syu # --sync --refresh --sysupgrade Force refresh of all package databases, even if they appear to be up-to-date pacman -Syy # --sync --refresh --refresh Download program updates but don't install them pacman -Syyuw # --sync --refresh --refresh --sysupgrade --downloadonly Get number of total installed packages pacman -Q | wc -l","title":"pacman"},{"location":"Linux/Package/#rpm","text":"Query repos for information on a package rpm -qi $PACKAGE # --query --info Upgrade or install a package, with progress bars rpm -Uvh $PACKAGE # --upgrade --verbose --hash Display version of Fedora rpm -E %fedora Import a keyring rpm --import \"https://build.opensuse.org/projects/home:manuelschneid3r/public_key\"","title":"rpm"},{"location":"Linux/Package/#snap","text":"Snap is one of several recent containerized application distribution solutions for Linux. Snap apps are slow to start because data is stored in squashfs images. Installation Red Hat dnf install -y snapd ln -s /var/lib/snapd/snap /snap","title":"snap"},{"location":"Linux/Process/","text":"Process management Commands chrt Lower the priority of tasks relative to others chrt -o nice Priorities range from 0-19 in csh (10 is default): lower values mean a higher priority. View priorities of jobs ps -l Run cmd at a higher priority nice -5 cmd & Run $CMD at a nice value of (positive) 10 nice -10 $CMD nice -n 10 nice $CMD ps Display processes in a tree-like display illustrating parent-child relationships ps -f # --forest Show system processes BSD syntax POSIX syntax ps ax ps -ef Display full listing of processes ps u # -f Display user processes ps xG # -a Display SELinux contexts for processes ps auxZ Display kernel threads ps -ef taskset Send a task to a specific core taskset -c","title":"Process management"},{"location":"Linux/Process/#process-management","text":"","title":"Process management"},{"location":"Linux/Process/#commands","text":"","title":"Commands"},{"location":"Linux/Process/#chrt","text":"Lower the priority of tasks relative to others chrt -o","title":"chrt"},{"location":"Linux/Process/#nice","text":"Priorities range from 0-19 in csh (10 is default): lower values mean a higher priority. View priorities of jobs ps -l Run cmd at a higher priority nice -5 cmd & Run $CMD at a nice value of (positive) 10 nice -10 $CMD nice -n 10 nice $CMD","title":"nice"},{"location":"Linux/Process/#ps","text":"Display processes in a tree-like display illustrating parent-child relationships ps -f # --forest Show system processes BSD syntax POSIX syntax ps ax ps -ef Display full listing of processes ps u # -f Display user processes ps xG # -a Display SELinux contexts for processes ps auxZ Display kernel threads ps -ef","title":"ps"},{"location":"Linux/Process/#taskset","text":"Send a task to a specific core taskset -c","title":"taskset"},{"location":"Linux/Random/","text":"Random There are two random-number devices in the kernel. Historically: /dev/random blocked until it had sufficient entropy to return a random value /dev/urandom never blocked but resorted to a pseudorandom number generator (PRNG) in the case of insufficient entropy However, in 2020 the behavior of /dev/random was changed to make it behave more like the getrandom syscall , in that it blocks only on initialization and provides cryptographic-strength random numbers thereafter without blocking. This has resulted in a blurring of the lines between the two random devices and an effort to remove /dev/urandom for good.","title":"Random"},{"location":"Linux/Random/#random","text":"There are two random-number devices in the kernel. Historically: /dev/random blocked until it had sufficient entropy to return a random value /dev/urandom never blocked but resorted to a pseudorandom number generator (PRNG) in the case of insufficient entropy However, in 2020 the behavior of /dev/random was changed to make it behave more like the getrandom syscall , in that it blocks only on initialization and provides cryptographic-strength random numbers thereafter without blocking. This has resulted in a blurring of the lines between the two random devices and an effort to remove /dev/urandom for good.","title":"Random"},{"location":"Linux/Red-Hat/","text":"Red Hat Red Hat distributions ... bla blabla Applications TuneD TuneD is a service that monitors the system and optimizes its performance under certain workloads. TuneD provides predefined profiles for power-saving and performance-boosting use cases. throughput-performance optimizes for throughput virtual-guest optimizes for performance balanced balances performance and power consumption powersave optimizes for power consumption These can be listed from the command-line: tuned-adm list profiles tuned-adm active tuned-adm recommend tuned-adm profile powersave # Select a profile tuned-adm profile virtual-guest powersave # Select a merged profile Dynamic tuning monitors system components during uptime and makes system changes dynamically. It is enabled by changing a setting in TuneD's config at /etc/tuned/tuned-main.conf : /etc/tuned/tuned-main.conf dynamic_tuning = 1 Storage autofs Auto File System offers an alternative way of mounting NFS shares that can save some system resources, especially when many shares are mounted. Autofs can mount NFS shares dynamically, only when accessed. dnf install -y autofs systemctl enable --now autofs.service Mounts are defined in configs called maps . There are three map types: master map is /etc/auto.master by default direct maps point to other files for mount details. They are notable for beginning with /- indirect maps also point to other files for mount details but provide an umbrella mount point which will contain all other mounts within it. Note that other mountpoints at this parent directory cannot coexist with autofs mounts. Here is an example indirect map that will mount to /data/sales. /etc/auto.master.d/data.autofs /data /etc/auto.data /etc/auto.data sales -rw,soft 192.168.33.101:/data/sales Map files also support wildcards. * 127.0.0.1:/home/& AutoFS's config is at /etc/autofs.conf . One important directive is master_map_name which defines the master map file. Stratis Stratis is an open-source managed pooled storage solution in the vein of ZFS or btrfs. Stratis block devices can be disks, partitions, LUKS-encrypted volumes, LVM logical volumes, or DM multipath devices. Stratis pools are mounted under /stratis and, like other pooled storage systems, support multiple filesystems. Stratis file systems are thinly provisioned and formatted with xfs , although vanilla xfs utilities cannot be used on Stratis file systems. dnf -y install stratisd stratis-cli systemctl enable --now stratisd Create a pool stratis pool create pool /dev/sda /dev/sdb /dev/sdc # (1) An error about the devices being \"owned\" can be resolved by wiping it. wipefs -a /dev/sda Display block devices managed by Stratis stratis blockdev # (1) This command is equivalent to pvs in LVM. Create filesystem stratis fs create pool files Confirm stratis fs /etc/fstab /stratis/pool/files /mnt/stratisfs xfs defaults,x-systemd.requires=stratisd.service 0 0 Expand pool stratis pool add-data pool /dev/sdb Save snapshot stratis fs snapshot pool files files-snapshot Restore from snapshot stratis fs rename files files-orig stratis fs rename files-snapshot files umount /mnt/files ; mount /mnt/files VDO Virtual disk optimizer (VDO) is a kernel module introduced in RHEL 7.5 that provides data deduplication and compression on block devices. The physical storage of a VDO volume is divided into a number of slabs , which are contiguous regions of the physical space. All slabs for a given volume have the same size, which can be any power of 2 multiple of 128 MB up to 32 GB (2 GB by default). The maximum number of slabs is 8,192. The maximum physical storage of the VDO is provided to the user on creation. Like LVM volumes, VDO volumes appear under /dev/mapper VDO appears not to be installed by default, but it is available in the BaseOS repo. dnf install vdo systemctl enable --now vdo Create a VDO volume vdo create --name = web_storage --device = /dev/xvdb --vdoLogicalSize = 10G vdostats --human-readable mkfs.xfs -K /dev/mapper/web_storage udevadm settle The fstab file requires a variety of options /dev/mapper/web_storage /mnt/web_storage xfs _netdev,x-systemd.device-timeout = 0 ,x-systemd.requires = vdo.service 0 0 Labs EX200 IAM We're going to lay the groundwork here and use these local accounts for all the subsequent tasks. You can write a script to do this, or do it by hand, from the data in the input file for the script. The file contents are: manny:1010:dba_admin,dba_managers,dba_staff moe:1011:dba_admin,dba_staff jack:1012:dba_intern,dba_staff marcia:1013:it_staff,it_managers jan:1014:dba_admin,dba_staff cindy:1015:dba_intern,dba_staff Set all user passwords to dbapass . Also, change the users' PRIMARY groups' GID to match their UID. Don't forget to check their home directories to make sure permisisons are correct! Enable the following command aliases: SOFTWARE SERVICES PROCESSES Add a new command alias named MESSAGES : /bin/tail -f /var/log/messages Enable superuser privilages for the following local groups: dba_managers : everything dba_admin : Command aliases: SOFTWARE, SERVICES, PROCESSES dba_intern : Command alias: MESSAGES Repos You'll need to configure three repositories and install some software: RHEL 8 BaseOS: Repository ID: [rhel-8-baseos-rhui-rpms] The mirrorlist is: https://rhui3.REGION.aws.ce.redhat.com/pulp/mirror/content/dist/rhel8/rhui/$releasever/$basearch/baseos/os The GPG key is located at: /etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release You will need to add SSL configuration: sslverify = 1 sslclientkey = /etc/pki/rhui/content-rhel8.key sslclientcert = /etc/pki/rhui/product/content-rhel8.crt sslcacert = /etc/pki/rhui/cdn.redhat.com-chain.crt RHEL 8 AppStream: Repository ID: [rhel-8-appstream-rhui-rpms] The mirrorlist is: https://rhui3.REGION.aws.ce.redhat.com/pulp/mirror/content/dist/rhel8/rhui/$releasever/$basearch/appstream/os The GPG key is located at: /etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release You will need to add SSL configuration: sslverify = 1 sslclientkey = /etc/pki/rhui/content-rhel8.key sslclientcert = /etc/pki/rhui/product/content-rhel8.crt sslcacert = /etc/pki/rhui/cdn.redhat.com-chain.crt EPEL: Repository ID: [epel] The baseurl is: https://download.fedoraproject.org/pub/epel/$releasever/Everything/$basearch Configure the repositories on the first server, then make an archive of the files, securely copy them to the second server, then unarchive the repository files on the second server. Install the default AppStream stream/profile for container-tools Install the youtube-dl package (from EPEL) Check for system updates, but don't install them Networking On the first server, configure the second interface's IPv4/IPv6 addresses using nmtui. IPv4: 10.0.1.20/24 IPv6: 2002:0a00:0114::/64 Manual, not Automatic (DHCP) for both interfaces Only IP addresses, no other fields Configure only, do not activate Logging By default, the systemd journal logs to memory in RHEL 8, in the location /run/log/journal. While this works fine, we'd like to make our journals persistent across reboots. Configure the systemd journal logs to be persistent on both servers, logging to /var/log/journal . Scheduling Create one at task and one cron job on the first server: The at job will create a file containing the string \"The at job ran\" in the file named /web/html/at.html , two minutes from the time you schedule it. The cron job will append to the /web/html/cron.html file every minute, echoing the date to the file. These files will be available via the web server on the first server after the \"Troubleshoot SELinux issues\" objective is completed. Chrony Time sync is not working on either of our servers. We need to fix that. Configure chrony to use the following server: server 169.254.169.123 iburst Make sure your work is persistent and check your work! GRUB On server1 , make the following changes: Increase the timeout using GRUB_TIMEOUT=10 Add the following line: GRUB_TIMEOUT_STYLE=hidden Add quiet to the end of the GRUB_CMDLINE_LINUX line Validate the changes in /boot/grub2/grub.cfg . Do not reboot the server. Storage On the second server: Create a VDO device with the first unused 5GB device. Name: web_storage Logical Size: 10GB Use the VDO device as an LVM physical volume. Create the following: Volume Group: web_vg Three 2G Logical Volumes with xfs file systems, mounted persistently at /mnt/web_storage_{dev,qa,prod}q : web_storage_dev web_storage_qa web_storage_prod We need to increase the swap on the second server. We're going to use half of our first unused 2G disk for this additional swap space. Configure the swap space non-destructively and persistently. On the second server, using the second 2G disk, create the following: Stratis pool: appteam Stratis file system, mounted persistently at /mnt/app_storage : appfs1 Shares Configure autofs on the first server to mount the user home directories on the second server at /export/home . On the second server, configure a NFS server with the following export: /home <first_server_private_IP>(rw,sync,no_root_squash) On the first server, configure autofs to mount the exported /home directory on the second server at /export/home . Change the home directories for our six users (manny|moe|jack|marcia|jan|cindy) to be /export/home/<user> and test. On the second server, create a directory at /home/dba_docs with: Group ownership: dba_staff Permissions: 770 , SGID and sticky bits set Create a link in each shared user's home directory to this directory, for easy access. Set the following ACLs: Read-only for jack and cindy Full permissions for marcia Container as service As the cloud_user user on the first server, create a persistent systemd container with the following: Image: registry.access.redhat.com/rhscl/httpd-24-rhel7 Port mappings: 8080 on the container to 8000 on the host Persistent storage at ~/web_data , mounted at /var/www/html in the container Container name: web_server SELinux The Apache web server on the first server won't start! Investigate this issue, and correct any other SELinux issues related to httpd that you may find. Firewall Make sure the firewall is installed, enabled and started on both servers. Configure the following services/ports: Server 1: ssh http Port 85 (tcp) Port 8000 (tcp) Server 2: ssh nfs nfs3 rpc-bind mountd Commands dnf View history of dnf commands dnf history dnf history userinstalled # View all packages installed by user Package groups can be specified using the group command or by prefixing the package group name with @ dnf info @virtualization # dnf group info virtualization dnf install @virtualization # dnf group install virtualization dnf install --with-optional @virtualization # Include optional packages Remove the configuration backend supporting the use of legacy ifcfg files in NetworkManager. dnf remove NetworkManager-initscripts-ifcfg-rh Modules are special package groups representing an application, runtime, or a set of tools. The Node.js module allows you to select several streams corresponding to major versions. dnf module install nodejs:12 Global dnf configuration is stored in either /etc/yum.conf or /etc/dnf.conf . [main] ; Exclude packages from updates permanently exclude = kernel* php* ; Suppress confirmation assumeyes = True The configuration can be dumped from the command-line (as root) dnf config-manager --dump Repositories are INI files placed in /etc/yum.repos.d/ , but they can also be displayed and manipulated from the command-line. Repositories # Display repos dnf repolist # -v # Add repo dnf config-manager --add-repo $REPO -URL # Disable repo dnf config-manager --set-disabled $REPO -NAME Example repos [docker-ce-stable] name = Docker CE Stable - $basearch baseurl = https://download.docker.com/linux/fedora/$releasever/$basearch/stable enabled = 1 gpgcheck = 1 gpgkey = https://download.docker.com/linux/fedora/gp [kubernetes] name = Kubernetes baseurl = https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64 enabled = 1 gpgcheck = 1 repo_gpgcheck = 1 gpgkey = https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg [google-cloud-sdk] name = Google Cloud SDK baseurl = https://packages.cloud.google.com/yum/repos/cloud-sdk-el7-x86_64 enabled = 1 gpgcheck = 1 repo_gpgcheck = 0 gpgkey = https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg Modules are collections of packages that are installed together. They often also have profiles available, which are variants of the module: i.e. client, server, common, devel, etc. dnf module list php dnf module install php:7.4/devel dnf module reset php firewall-cmd Frontend to Netfilter in Red Hat distributions. sh firewall-cmd --state # \"running\" s Firewalld has a runtime configuration and a saved, persistent configuration. Only the runtime configuration will be consulted for any command, unless the persistent configuration is specified with --permanent . The runtime configuration can be saved with this command, which obviates the need to execute every change twice. firewall-cmd --runtime-to-permanent Alternatively, the persistent configuration can be loaded into memory: firewall-cmd --reload Display firewall rules firewall-cmd --list-all --permanent Firewalld uses zones to define the level of trust for network connections. A connection can only be part of one zone, but a zone can be used for many network connections. Builtin zones have XML-format configs found in /usr/lib/firewalld/zones . firewall-cmd --get-active-zones # Display active zones along with interfaces firewall-cmd --info-zone = public # Inspect zone firewall-cmd --new-zone = testlab # Create new zone Firewalld rules are generally managed through builtin services . These bundle network settings together for well-known applications like SSH, etc. Builtin services are also XML-format configs found in /usr/lib/firewalld/services . Services firewall-cmd --list-services firewall-cmd --add-service = http firewall-cmd --remove-service = http Firewalld's config file is at /etc/firewalld/firewalld.conf /etc/firewalld/firewalld.conf AllowZoneDrifting = no Since RHEL 8, firewalld's backend has been changed to nftables. /etc/firewalld/firewalld.conf FirewallBackend = nftables httpd The Apache web server daemon is named httpd in RHEL and other RPM-based distributions. HTML content is served from /var/www/html by default, but this can be changed by modifying the DocumentRoot directive in the Apache config located at /etc/httpd/conf/httpd.conf . /etc/httpd/conf/httpd.conf DocumentRoot \"/web\" # ... <Directory \"/web\"> Containers must mount content to /usr/local/apache2/htdocs . Users can also serve files from their home directories, by default from a directory named public_html . podman On RHEL, podman can be installed as a package or as part of a module dnf module install container-tools With few exceptions, podman exposes a command-line API that closely imitates that of Docker. Arch Linux On Arch, /etc/subuid and /etc/subgid have to be set . These are colon-delimited files that define the ranges for namespaced UIDs and GIDs to be used by each user. Conventionally, these ranges begin at 100,000 (for the first, primary user) and contain 65,536 possible values. terry:100000:65536 alice:165536:65536 Then podman has to be migrated podman system migrate Podman supports pulling from various repos using aliases that are defined in /etc/containers/registries.conf.d . RHEL and derivative distributions support additional aliases, some of which reference images that require a login . For example, Red Hat offers a Python 2.7 runtime from the RHSCL ( Red Hat Software Collections ) repository on registry.access.redhat.com, which does not require authentication. However, Python 3.8 is only available from registry.redhat.io, which does. Interestingly, other Python runtimes are available from the ubi7 and ubi8 repos from unauthenticated registries. Container images are stored in ~/.local/share/containers/storage . podman pull rhscl/httpd-24-rhel7 # (1) Alias to registry.access.redhat.com/rhscl/httpd-24-rhel7 The Z option is necessary on SELinux systems (like RHEL and derivatives) and tells Podman to label the content with a private unshared label. On systems running SELinux, rootless containers must be explicitly allowed to access bind mounts. Containerized processes are not allowed to access files without a SELinux label. podman run -d -v = /home/jasper/notes/site:/usr/share/nginx/html:Z -p = 8080 :80 --name = notes nginx podman run -d -v = /home/jasper/notes/site:/usr/local/apache2/htdocs:Z -p = 8080 :80 --name = notes httpd-24 Mapped ports can be displayed podman port -a Output a SystemD service file from a container to STDOUT (this must be redirected to a file) podman generate systemd notes \\ --restart-policy = always \\ --name \\ # (3) --files \\ # (2) --new \\ # (1) Yield unit files that do not expect containers and pods to exist but rather create them based on their configuration files. Generate a file with a name beginning with the prefix (which can be set with --container-prefix or --pod-prefix ) and followed by the ID or name (if --name is also specified) In conjunction with --files , name the service file after the container and not the ID number. rpm Query repos for information on a package rpm -qi $PACKAGE # --query --info Upgrade or install a package, with progress bars rpm -Uvh $PACKAGE # --upgrade --verbose --hash Display version of Fedora rpm -E %fedora Import a keyring rpm --import \"https://build.opensuse.org/projects/home:manuelschneid3r/public_key\" rpmkeys Manage RPM keyrings Import a keyring rpmkeys --import $PUBKEY vdo Manage kernel VDO devices and related configuration information. vdo create --name = storage --device = /dev/xvdb --vdoLogicalSize = 10G vdostats Display configuration and statistics of VDO volumes vdostats --human-readable Glossary CentOS A community distribution of Linux that was created by Gregory Kurtzer in 2004 and acquired by Red Hat in 2014. It has traditionally been considered downstream to RHEL , incorporating changes to RHEL after a delay of several months. In fact, it is a rebuild of the publicly available source RPMs (SRPMs) of RHEL packages, which historically allowed CentOS maintainers to simply package and ship them rebranded . For years, CentOS was the distribution of choice for experienced Linux administrators who did not feel the need to pay for Red Hat's support. In December 2020, Red Hat announced that CentOS 8 support will end at the end of 2021 (rather than 2029), while CentOS 7 will continue to be supported until 2024. This represented a shift in focus from CentOS Linux to CentOS Stream and a change from a fixed-release (or \"stable point release\") to a rolling-release distribution model. CentOS Stream was announced in September 2019 as a distribution of CentOS maintained on a model previously misidentified as rolling-release but now described as \"continuously delivered\", organized into Streams. CentOS Stream originated in an effort to get more community participation in development of RHEL, rather than merely passive consumption. Fedora Fedora is a community distribution supported by Red Hat launched as \"Fedora Core\" in 2003. It has traditionally been considered upstream to RHEL, serving as a testing ground for new features and improvements. Fedora CoreOS is a Fedora edition built specifically for running containerized workloads securely and at scale. Because containers can be deployed across many nodes for redundancy, the system can be updated and rebooted automatically without affecting uptime. CoreOS systems are meant to be immutable infrastructure , meaning they are only configured through the provisioning process and not modified in-place. All systems start with a generic OS image, but on first boot it uses a system called Ignition to read an Ignition config (which is converted from a Fedora CoreOS Config file) from the cloud or a remote URL, by which it provisions itself, creating disk partitions, file systems, users, etc.","title":"Red Hat"},{"location":"Linux/Red-Hat/#red-hat","text":"Red Hat distributions ... bla blabla","title":"Red Hat"},{"location":"Linux/Red-Hat/#applications","text":"","title":"Applications"},{"location":"Linux/Red-Hat/#tuned","text":"TuneD is a service that monitors the system and optimizes its performance under certain workloads. TuneD provides predefined profiles for power-saving and performance-boosting use cases. throughput-performance optimizes for throughput virtual-guest optimizes for performance balanced balances performance and power consumption powersave optimizes for power consumption These can be listed from the command-line: tuned-adm list profiles tuned-adm active tuned-adm recommend tuned-adm profile powersave # Select a profile tuned-adm profile virtual-guest powersave # Select a merged profile Dynamic tuning monitors system components during uptime and makes system changes dynamically. It is enabled by changing a setting in TuneD's config at /etc/tuned/tuned-main.conf : /etc/tuned/tuned-main.conf dynamic_tuning = 1","title":"TuneD"},{"location":"Linux/Red-Hat/#storage","text":"","title":"Storage"},{"location":"Linux/Red-Hat/#autofs","text":"Auto File System offers an alternative way of mounting NFS shares that can save some system resources, especially when many shares are mounted. Autofs can mount NFS shares dynamically, only when accessed. dnf install -y autofs systemctl enable --now autofs.service Mounts are defined in configs called maps . There are three map types: master map is /etc/auto.master by default direct maps point to other files for mount details. They are notable for beginning with /- indirect maps also point to other files for mount details but provide an umbrella mount point which will contain all other mounts within it. Note that other mountpoints at this parent directory cannot coexist with autofs mounts. Here is an example indirect map that will mount to /data/sales. /etc/auto.master.d/data.autofs /data /etc/auto.data /etc/auto.data sales -rw,soft 192.168.33.101:/data/sales Map files also support wildcards. * 127.0.0.1:/home/& AutoFS's config is at /etc/autofs.conf . One important directive is master_map_name which defines the master map file.","title":"autofs"},{"location":"Linux/Red-Hat/#stratis","text":"Stratis is an open-source managed pooled storage solution in the vein of ZFS or btrfs. Stratis block devices can be disks, partitions, LUKS-encrypted volumes, LVM logical volumes, or DM multipath devices. Stratis pools are mounted under /stratis and, like other pooled storage systems, support multiple filesystems. Stratis file systems are thinly provisioned and formatted with xfs , although vanilla xfs utilities cannot be used on Stratis file systems. dnf -y install stratisd stratis-cli systemctl enable --now stratisd Create a pool stratis pool create pool /dev/sda /dev/sdb /dev/sdc # (1) An error about the devices being \"owned\" can be resolved by wiping it. wipefs -a /dev/sda Display block devices managed by Stratis stratis blockdev # (1) This command is equivalent to pvs in LVM. Create filesystem stratis fs create pool files Confirm stratis fs /etc/fstab /stratis/pool/files /mnt/stratisfs xfs defaults,x-systemd.requires=stratisd.service 0 0 Expand pool stratis pool add-data pool /dev/sdb Save snapshot stratis fs snapshot pool files files-snapshot Restore from snapshot stratis fs rename files files-orig stratis fs rename files-snapshot files umount /mnt/files ; mount /mnt/files","title":"Stratis"},{"location":"Linux/Red-Hat/#vdo","text":"Virtual disk optimizer (VDO) is a kernel module introduced in RHEL 7.5 that provides data deduplication and compression on block devices. The physical storage of a VDO volume is divided into a number of slabs , which are contiguous regions of the physical space. All slabs for a given volume have the same size, which can be any power of 2 multiple of 128 MB up to 32 GB (2 GB by default). The maximum number of slabs is 8,192. The maximum physical storage of the VDO is provided to the user on creation. Like LVM volumes, VDO volumes appear under /dev/mapper VDO appears not to be installed by default, but it is available in the BaseOS repo. dnf install vdo systemctl enable --now vdo Create a VDO volume vdo create --name = web_storage --device = /dev/xvdb --vdoLogicalSize = 10G vdostats --human-readable mkfs.xfs -K /dev/mapper/web_storage udevadm settle The fstab file requires a variety of options /dev/mapper/web_storage /mnt/web_storage xfs _netdev,x-systemd.device-timeout = 0 ,x-systemd.requires = vdo.service 0 0","title":"VDO"},{"location":"Linux/Red-Hat/#labs","text":"","title":"Labs"},{"location":"Linux/Red-Hat/#ex200","text":"","title":"EX200"},{"location":"Linux/Red-Hat/#iam","text":"We're going to lay the groundwork here and use these local accounts for all the subsequent tasks. You can write a script to do this, or do it by hand, from the data in the input file for the script. The file contents are: manny:1010:dba_admin,dba_managers,dba_staff moe:1011:dba_admin,dba_staff jack:1012:dba_intern,dba_staff marcia:1013:it_staff,it_managers jan:1014:dba_admin,dba_staff cindy:1015:dba_intern,dba_staff Set all user passwords to dbapass . Also, change the users' PRIMARY groups' GID to match their UID. Don't forget to check their home directories to make sure permisisons are correct! Enable the following command aliases: SOFTWARE SERVICES PROCESSES Add a new command alias named MESSAGES : /bin/tail -f /var/log/messages Enable superuser privilages for the following local groups: dba_managers : everything dba_admin : Command aliases: SOFTWARE, SERVICES, PROCESSES dba_intern : Command alias: MESSAGES","title":"IAM"},{"location":"Linux/Red-Hat/#repos","text":"You'll need to configure three repositories and install some software: RHEL 8 BaseOS: Repository ID: [rhel-8-baseos-rhui-rpms] The mirrorlist is: https://rhui3.REGION.aws.ce.redhat.com/pulp/mirror/content/dist/rhel8/rhui/$releasever/$basearch/baseos/os The GPG key is located at: /etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release You will need to add SSL configuration: sslverify = 1 sslclientkey = /etc/pki/rhui/content-rhel8.key sslclientcert = /etc/pki/rhui/product/content-rhel8.crt sslcacert = /etc/pki/rhui/cdn.redhat.com-chain.crt RHEL 8 AppStream: Repository ID: [rhel-8-appstream-rhui-rpms] The mirrorlist is: https://rhui3.REGION.aws.ce.redhat.com/pulp/mirror/content/dist/rhel8/rhui/$releasever/$basearch/appstream/os The GPG key is located at: /etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release You will need to add SSL configuration: sslverify = 1 sslclientkey = /etc/pki/rhui/content-rhel8.key sslclientcert = /etc/pki/rhui/product/content-rhel8.crt sslcacert = /etc/pki/rhui/cdn.redhat.com-chain.crt EPEL: Repository ID: [epel] The baseurl is: https://download.fedoraproject.org/pub/epel/$releasever/Everything/$basearch Configure the repositories on the first server, then make an archive of the files, securely copy them to the second server, then unarchive the repository files on the second server. Install the default AppStream stream/profile for container-tools Install the youtube-dl package (from EPEL) Check for system updates, but don't install them","title":"Repos"},{"location":"Linux/Red-Hat/#networking","text":"On the first server, configure the second interface's IPv4/IPv6 addresses using nmtui. IPv4: 10.0.1.20/24 IPv6: 2002:0a00:0114::/64 Manual, not Automatic (DHCP) for both interfaces Only IP addresses, no other fields Configure only, do not activate","title":"Networking"},{"location":"Linux/Red-Hat/#logging","text":"By default, the systemd journal logs to memory in RHEL 8, in the location /run/log/journal. While this works fine, we'd like to make our journals persistent across reboots. Configure the systemd journal logs to be persistent on both servers, logging to /var/log/journal .","title":"Logging"},{"location":"Linux/Red-Hat/#scheduling","text":"Create one at task and one cron job on the first server: The at job will create a file containing the string \"The at job ran\" in the file named /web/html/at.html , two minutes from the time you schedule it. The cron job will append to the /web/html/cron.html file every minute, echoing the date to the file. These files will be available via the web server on the first server after the \"Troubleshoot SELinux issues\" objective is completed.","title":"Scheduling"},{"location":"Linux/Red-Hat/#chrony","text":"Time sync is not working on either of our servers. We need to fix that. Configure chrony to use the following server: server 169.254.169.123 iburst Make sure your work is persistent and check your work!","title":"Chrony"},{"location":"Linux/Red-Hat/#grub","text":"On server1 , make the following changes: Increase the timeout using GRUB_TIMEOUT=10 Add the following line: GRUB_TIMEOUT_STYLE=hidden Add quiet to the end of the GRUB_CMDLINE_LINUX line Validate the changes in /boot/grub2/grub.cfg . Do not reboot the server.","title":"GRUB"},{"location":"Linux/Red-Hat/#storage_1","text":"On the second server: Create a VDO device with the first unused 5GB device. Name: web_storage Logical Size: 10GB Use the VDO device as an LVM physical volume. Create the following: Volume Group: web_vg Three 2G Logical Volumes with xfs file systems, mounted persistently at /mnt/web_storage_{dev,qa,prod}q : web_storage_dev web_storage_qa web_storage_prod We need to increase the swap on the second server. We're going to use half of our first unused 2G disk for this additional swap space. Configure the swap space non-destructively and persistently. On the second server, using the second 2G disk, create the following: Stratis pool: appteam Stratis file system, mounted persistently at /mnt/app_storage : appfs1","title":"Storage"},{"location":"Linux/Red-Hat/#shares","text":"Configure autofs on the first server to mount the user home directories on the second server at /export/home . On the second server, configure a NFS server with the following export: /home <first_server_private_IP>(rw,sync,no_root_squash) On the first server, configure autofs to mount the exported /home directory on the second server at /export/home . Change the home directories for our six users (manny|moe|jack|marcia|jan|cindy) to be /export/home/<user> and test. On the second server, create a directory at /home/dba_docs with: Group ownership: dba_staff Permissions: 770 , SGID and sticky bits set Create a link in each shared user's home directory to this directory, for easy access. Set the following ACLs: Read-only for jack and cindy Full permissions for marcia","title":"Shares"},{"location":"Linux/Red-Hat/#container-as-service","text":"As the cloud_user user on the first server, create a persistent systemd container with the following: Image: registry.access.redhat.com/rhscl/httpd-24-rhel7 Port mappings: 8080 on the container to 8000 on the host Persistent storage at ~/web_data , mounted at /var/www/html in the container Container name: web_server","title":"Container as service"},{"location":"Linux/Red-Hat/#selinux","text":"The Apache web server on the first server won't start! Investigate this issue, and correct any other SELinux issues related to httpd that you may find.","title":"SELinux"},{"location":"Linux/Red-Hat/#firewall","text":"Make sure the firewall is installed, enabled and started on both servers. Configure the following services/ports: Server 1: ssh http Port 85 (tcp) Port 8000 (tcp) Server 2: ssh nfs nfs3 rpc-bind mountd","title":"Firewall"},{"location":"Linux/Red-Hat/#commands","text":"","title":"Commands"},{"location":"Linux/Red-Hat/#dnf","text":"View history of dnf commands dnf history dnf history userinstalled # View all packages installed by user Package groups can be specified using the group command or by prefixing the package group name with @ dnf info @virtualization # dnf group info virtualization dnf install @virtualization # dnf group install virtualization dnf install --with-optional @virtualization # Include optional packages Remove the configuration backend supporting the use of legacy ifcfg files in NetworkManager. dnf remove NetworkManager-initscripts-ifcfg-rh Modules are special package groups representing an application, runtime, or a set of tools. The Node.js module allows you to select several streams corresponding to major versions. dnf module install nodejs:12 Global dnf configuration is stored in either /etc/yum.conf or /etc/dnf.conf . [main] ; Exclude packages from updates permanently exclude = kernel* php* ; Suppress confirmation assumeyes = True The configuration can be dumped from the command-line (as root) dnf config-manager --dump Repositories are INI files placed in /etc/yum.repos.d/ , but they can also be displayed and manipulated from the command-line. Repositories # Display repos dnf repolist # -v # Add repo dnf config-manager --add-repo $REPO -URL # Disable repo dnf config-manager --set-disabled $REPO -NAME Example repos [docker-ce-stable] name = Docker CE Stable - $basearch baseurl = https://download.docker.com/linux/fedora/$releasever/$basearch/stable enabled = 1 gpgcheck = 1 gpgkey = https://download.docker.com/linux/fedora/gp [kubernetes] name = Kubernetes baseurl = https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64 enabled = 1 gpgcheck = 1 repo_gpgcheck = 1 gpgkey = https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg [google-cloud-sdk] name = Google Cloud SDK baseurl = https://packages.cloud.google.com/yum/repos/cloud-sdk-el7-x86_64 enabled = 1 gpgcheck = 1 repo_gpgcheck = 0 gpgkey = https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg Modules are collections of packages that are installed together. They often also have profiles available, which are variants of the module: i.e. client, server, common, devel, etc. dnf module list php dnf module install php:7.4/devel dnf module reset php","title":"dnf"},{"location":"Linux/Red-Hat/#firewall-cmd","text":"Frontend to Netfilter in Red Hat distributions. sh firewall-cmd --state # \"running\" s Firewalld has a runtime configuration and a saved, persistent configuration. Only the runtime configuration will be consulted for any command, unless the persistent configuration is specified with --permanent . The runtime configuration can be saved with this command, which obviates the need to execute every change twice. firewall-cmd --runtime-to-permanent Alternatively, the persistent configuration can be loaded into memory: firewall-cmd --reload Display firewall rules firewall-cmd --list-all --permanent Firewalld uses zones to define the level of trust for network connections. A connection can only be part of one zone, but a zone can be used for many network connections. Builtin zones have XML-format configs found in /usr/lib/firewalld/zones . firewall-cmd --get-active-zones # Display active zones along with interfaces firewall-cmd --info-zone = public # Inspect zone firewall-cmd --new-zone = testlab # Create new zone Firewalld rules are generally managed through builtin services . These bundle network settings together for well-known applications like SSH, etc. Builtin services are also XML-format configs found in /usr/lib/firewalld/services . Services firewall-cmd --list-services firewall-cmd --add-service = http firewall-cmd --remove-service = http Firewalld's config file is at /etc/firewalld/firewalld.conf /etc/firewalld/firewalld.conf AllowZoneDrifting = no Since RHEL 8, firewalld's backend has been changed to nftables. /etc/firewalld/firewalld.conf FirewallBackend = nftables","title":"firewall-cmd"},{"location":"Linux/Red-Hat/#httpd","text":"The Apache web server daemon is named httpd in RHEL and other RPM-based distributions. HTML content is served from /var/www/html by default, but this can be changed by modifying the DocumentRoot directive in the Apache config located at /etc/httpd/conf/httpd.conf . /etc/httpd/conf/httpd.conf DocumentRoot \"/web\" # ... <Directory \"/web\"> Containers must mount content to /usr/local/apache2/htdocs . Users can also serve files from their home directories, by default from a directory named public_html .","title":"httpd"},{"location":"Linux/Red-Hat/#podman","text":"On RHEL, podman can be installed as a package or as part of a module dnf module install container-tools With few exceptions, podman exposes a command-line API that closely imitates that of Docker. Arch Linux On Arch, /etc/subuid and /etc/subgid have to be set . These are colon-delimited files that define the ranges for namespaced UIDs and GIDs to be used by each user. Conventionally, these ranges begin at 100,000 (for the first, primary user) and contain 65,536 possible values. terry:100000:65536 alice:165536:65536 Then podman has to be migrated podman system migrate Podman supports pulling from various repos using aliases that are defined in /etc/containers/registries.conf.d . RHEL and derivative distributions support additional aliases, some of which reference images that require a login . For example, Red Hat offers a Python 2.7 runtime from the RHSCL ( Red Hat Software Collections ) repository on registry.access.redhat.com, which does not require authentication. However, Python 3.8 is only available from registry.redhat.io, which does. Interestingly, other Python runtimes are available from the ubi7 and ubi8 repos from unauthenticated registries. Container images are stored in ~/.local/share/containers/storage . podman pull rhscl/httpd-24-rhel7 # (1) Alias to registry.access.redhat.com/rhscl/httpd-24-rhel7 The Z option is necessary on SELinux systems (like RHEL and derivatives) and tells Podman to label the content with a private unshared label. On systems running SELinux, rootless containers must be explicitly allowed to access bind mounts. Containerized processes are not allowed to access files without a SELinux label. podman run -d -v = /home/jasper/notes/site:/usr/share/nginx/html:Z -p = 8080 :80 --name = notes nginx podman run -d -v = /home/jasper/notes/site:/usr/local/apache2/htdocs:Z -p = 8080 :80 --name = notes httpd-24 Mapped ports can be displayed podman port -a Output a SystemD service file from a container to STDOUT (this must be redirected to a file) podman generate systemd notes \\ --restart-policy = always \\ --name \\ # (3) --files \\ # (2) --new \\ # (1) Yield unit files that do not expect containers and pods to exist but rather create them based on their configuration files. Generate a file with a name beginning with the prefix (which can be set with --container-prefix or --pod-prefix ) and followed by the ID or name (if --name is also specified) In conjunction with --files , name the service file after the container and not the ID number.","title":"podman"},{"location":"Linux/Red-Hat/#rpm","text":"Query repos for information on a package rpm -qi $PACKAGE # --query --info Upgrade or install a package, with progress bars rpm -Uvh $PACKAGE # --upgrade --verbose --hash Display version of Fedora rpm -E %fedora Import a keyring rpm --import \"https://build.opensuse.org/projects/home:manuelschneid3r/public_key\"","title":"rpm"},{"location":"Linux/Red-Hat/#rpmkeys","text":"Manage RPM keyrings Import a keyring rpmkeys --import $PUBKEY","title":"rpmkeys"},{"location":"Linux/Red-Hat/#vdo_1","text":"Manage kernel VDO devices and related configuration information. vdo create --name = storage --device = /dev/xvdb --vdoLogicalSize = 10G","title":"vdo"},{"location":"Linux/Red-Hat/#vdostats","text":"Display configuration and statistics of VDO volumes vdostats --human-readable","title":"vdostats"},{"location":"Linux/Red-Hat/#glossary","text":"","title":"Glossary"},{"location":"Linux/Red-Hat/#centos","text":"A community distribution of Linux that was created by Gregory Kurtzer in 2004 and acquired by Red Hat in 2014. It has traditionally been considered downstream to RHEL , incorporating changes to RHEL after a delay of several months. In fact, it is a rebuild of the publicly available source RPMs (SRPMs) of RHEL packages, which historically allowed CentOS maintainers to simply package and ship them rebranded . For years, CentOS was the distribution of choice for experienced Linux administrators who did not feel the need to pay for Red Hat's support. In December 2020, Red Hat announced that CentOS 8 support will end at the end of 2021 (rather than 2029), while CentOS 7 will continue to be supported until 2024. This represented a shift in focus from CentOS Linux to CentOS Stream and a change from a fixed-release (or \"stable point release\") to a rolling-release distribution model. CentOS Stream was announced in September 2019 as a distribution of CentOS maintained on a model previously misidentified as rolling-release but now described as \"continuously delivered\", organized into Streams. CentOS Stream originated in an effort to get more community participation in development of RHEL, rather than merely passive consumption.","title":"CentOS"},{"location":"Linux/Red-Hat/#fedora","text":"Fedora is a community distribution supported by Red Hat launched as \"Fedora Core\" in 2003. It has traditionally been considered upstream to RHEL, serving as a testing ground for new features and improvements. Fedora CoreOS is a Fedora edition built specifically for running containerized workloads securely and at scale. Because containers can be deployed across many nodes for redundancy, the system can be updated and rebooted automatically without affecting uptime. CoreOS systems are meant to be immutable infrastructure , meaning they are only configured through the provisioning process and not modified in-place. All systems start with a generic OS image, but on first boot it uses a system called Ignition to read an Ignition config (which is converted from a Fedora CoreOS Config file) from the cloud or a remote URL, by which it provisions itself, creating disk partitions, file systems, users, etc.","title":"Fedora"},{"location":"Linux/Ricing/","text":"Tiling window managers i3 is perhaps the most popular tiling window manager, typically used with polybar. awesome is written and configured in Lua. It originated as a fork of dwm , it offers creature comforts that make it the easiest for neophytes to tiling window managers. bspwm (\"Binary Space Partitioning Window Manager\") uses tree partitioning as the logic for organizing tiles, with the default being the \"dwindle\" pattern. Notably, it uses two config files: .bspwmrc which determines what programs to autoload but doesn't contain any key bindings .sxhkdrc which uses a syntax similar to i3 or herbstluft. dwm : One of the oldest and lightest tiling window managers. Because Suckless wants the source code not to exceed 2,000 lines of code, a lot of functionality is incorporated by means of patches , which modify the source code using diff files. herbstluft has a single pool of workspaces that is shared across all monitors. xmonad is written and configured in Haskell, making it challenging to configure. All window managers place an INI-format .desktop file in /usr/share/xsessions/ . xmonad.desktop [Desktop Entry] Name = xmonad Comment = Tiling window manager Exec = xmonad-start Terminal = false [Window Manager] SessionManaged = true Xmonad Install dnf install xmonad \\ xterm dmenu # (1) At the recommendation of DistroTube. Default keybindings which must be known by the neophyte: Default keybinding Description ++Alt+Shift+Enter++ Terminal ++Alt+Shift+C++ Close pane The config is typically placed in $XDG_CONFIG_HOME/xmonad/ (resolving to ~/.config/xmonad/), although other locations are possible. A good default config is available here . Change Mod key to Super myModMask = mod4Mask","title":"Tiling window managers"},{"location":"Linux/Ricing/#tiling-window-managers","text":"i3 is perhaps the most popular tiling window manager, typically used with polybar. awesome is written and configured in Lua. It originated as a fork of dwm , it offers creature comforts that make it the easiest for neophytes to tiling window managers. bspwm (\"Binary Space Partitioning Window Manager\") uses tree partitioning as the logic for organizing tiles, with the default being the \"dwindle\" pattern. Notably, it uses two config files: .bspwmrc which determines what programs to autoload but doesn't contain any key bindings .sxhkdrc which uses a syntax similar to i3 or herbstluft. dwm : One of the oldest and lightest tiling window managers. Because Suckless wants the source code not to exceed 2,000 lines of code, a lot of functionality is incorporated by means of patches , which modify the source code using diff files. herbstluft has a single pool of workspaces that is shared across all monitors. xmonad is written and configured in Haskell, making it challenging to configure. All window managers place an INI-format .desktop file in /usr/share/xsessions/ . xmonad.desktop [Desktop Entry] Name = xmonad Comment = Tiling window manager Exec = xmonad-start Terminal = false [Window Manager] SessionManaged = true","title":"Tiling window managers"},{"location":"Linux/Ricing/#xmonad","text":"Install dnf install xmonad \\ xterm dmenu # (1) At the recommendation of DistroTube. Default keybindings which must be known by the neophyte: Default keybinding Description ++Alt+Shift+Enter++ Terminal ++Alt+Shift+C++ Close pane The config is typically placed in $XDG_CONFIG_HOME/xmonad/ (resolving to ~/.config/xmonad/), although other locations are possible. A good default config is available here . Change Mod key to Super myModMask = mod4Mask","title":"Xmonad"},{"location":"Linux/Security/","text":"Security GPG GPG keys are used to sign packages and repos. For example, yum repos and apt incorporate APIs and handle GPG keys. The KWallet Manager and GNOME Keyring (Seahorse) applications can also be used to manage GPG keys. PAM Pluggable authentication modules form an authentication framework that can be used by \"PAM-aware applications\". These applications have config files that are found in /etc/pam.d The various pam modules have man pages prefixed with pam_ , i.e. \"pam_wheel\" etc. Commands gpg Generate a public and private (\"secret\") key pair (\"keyring\") after displaying interactive prompts to the user, who must enter real name and email address and specify variables like key length, encryption algorithm etc. gpg --full-generate-key gpg --generate-key # (1) Generate a new keyring using current default parameters. The rngd daemon found in the rng-tools package can be enabled for additional entropy if needed by the system. pacman -S community/rng-tools The generates a public and private key in ~/.gnupg . The public key, which can be distributed publicly so that people can encrypt messages to the user, is named pubring.kbx More than one master keypair can be generated in this manner, even for the same email address. Decrypt file gpg file.txt Export GPG public key gpg --export --output ~/jdoe.pub Import another person's public key gpg --import jdoe.pub List available GPG keys gpg --list-key Encrypt a file gpg --encrypt -r jdoe@dplaptop.lab.itpro.tv $FILE Sign $FILE without encrypting it (produces file.asc) gpg --clearsign $FILE Import another person's public key gpg --import ~/jdoe.pub Send keys to $SERVER gpg --send-keys keyIDs --keyserver $SERVER pass The standard unix password manager , backed by GPG, is a command-line password manager and MFA program. The first step in using pass is generating a new key pair. Generate a public and private (\"secret\") key pair (\"keyring\") after displaying interactive prompts to the user, who must enter real name and email address and specify variables like key length, encryption algorithm etc. gpg --full-generate-key gpg --generate-key # (1) Generate a new keyring using current default parameters. The rngd daemon found in the rng-tools package can be enabled for additional entropy if needed by the system. pacman -S community/rng-tools The generates a public and private key in ~/.gnupg . The public key, which can be distributed publicly so that people can encrypt messages to the user, is named pubring.kbx More than one master keypair can be generated in this manner, even for the same email address. Display public keys gpg -k # --list-keys Unwanted keys can be deleted by specifying the public key: gpg --delete-secret-and-public-keys \u2592\u2592\u2593\u2591\u2591\u2592\u2593\u2591\u2593\u2591\u2593\u2591\u2593\u2592\u2591\u2591\u2592\u2592\u2591\u2591\u2591\u2592\u2593\u2591\u2592\u2591\u2592\u2591\u2593\u2591\u2592\u2592\u2592\u2593\u2592\u2592\u2593\u2591\u2593\u2592\u2591 Now a password store can be initialized by providing that same email address. This email is stored at ~/.password-store/.gpg-id pass init email@example.com Add password pass add email This produces a binary, encrypted file at ~/.password-store/email.gpg . The password can be retrieved, after authenticating with the master password, with the following: pass email # (1) In fact, because this is simply a GPG encrypted file, GPG could be used equivalently. In fact, this appears to be the command executed by the pass shell script. gpg -dq ~/.password-store/email.gpg Display names of passwords pass ls # (1) This command is equivalent to using tree on the password store directory. tree ~/.password-store Pass can also handle OTP generation for MFA, as long as you can retrieve the OTP URI (beginning with otpauth://). QR code images can be deciphered with zbarimg to retrieve these URIs. pass otp add mimecast # (1) Note that otpauth URLs usually contain an embedded email address, which must match that of the intialized password store. If this identity does not match, an error that read \"There is no assurance this key belongs to the named user\" is produced . Resources Luke Smith video","title":"Security"},{"location":"Linux/Security/#security","text":"","title":"Security"},{"location":"Linux/Security/#gpg","text":"GPG keys are used to sign packages and repos. For example, yum repos and apt incorporate APIs and handle GPG keys. The KWallet Manager and GNOME Keyring (Seahorse) applications can also be used to manage GPG keys.","title":"GPG"},{"location":"Linux/Security/#pam","text":"Pluggable authentication modules form an authentication framework that can be used by \"PAM-aware applications\". These applications have config files that are found in /etc/pam.d The various pam modules have man pages prefixed with pam_ , i.e. \"pam_wheel\" etc.","title":"PAM"},{"location":"Linux/Security/#commands","text":"","title":"Commands"},{"location":"Linux/Security/#gpg_1","text":"Generate a public and private (\"secret\") key pair (\"keyring\") after displaying interactive prompts to the user, who must enter real name and email address and specify variables like key length, encryption algorithm etc. gpg --full-generate-key gpg --generate-key # (1) Generate a new keyring using current default parameters. The rngd daemon found in the rng-tools package can be enabled for additional entropy if needed by the system. pacman -S community/rng-tools The generates a public and private key in ~/.gnupg . The public key, which can be distributed publicly so that people can encrypt messages to the user, is named pubring.kbx More than one master keypair can be generated in this manner, even for the same email address. Decrypt file gpg file.txt Export GPG public key gpg --export --output ~/jdoe.pub Import another person's public key gpg --import jdoe.pub List available GPG keys gpg --list-key Encrypt a file gpg --encrypt -r jdoe@dplaptop.lab.itpro.tv $FILE Sign $FILE without encrypting it (produces file.asc) gpg --clearsign $FILE Import another person's public key gpg --import ~/jdoe.pub Send keys to $SERVER gpg --send-keys keyIDs --keyserver $SERVER","title":"gpg"},{"location":"Linux/Security/#pass","text":"The standard unix password manager , backed by GPG, is a command-line password manager and MFA program. The first step in using pass is generating a new key pair. Generate a public and private (\"secret\") key pair (\"keyring\") after displaying interactive prompts to the user, who must enter real name and email address and specify variables like key length, encryption algorithm etc. gpg --full-generate-key gpg --generate-key # (1) Generate a new keyring using current default parameters. The rngd daemon found in the rng-tools package can be enabled for additional entropy if needed by the system. pacman -S community/rng-tools The generates a public and private key in ~/.gnupg . The public key, which can be distributed publicly so that people can encrypt messages to the user, is named pubring.kbx More than one master keypair can be generated in this manner, even for the same email address. Display public keys gpg -k # --list-keys Unwanted keys can be deleted by specifying the public key: gpg --delete-secret-and-public-keys \u2592\u2592\u2593\u2591\u2591\u2592\u2593\u2591\u2593\u2591\u2593\u2591\u2593\u2592\u2591\u2591\u2592\u2592\u2591\u2591\u2591\u2592\u2593\u2591\u2592\u2591\u2592\u2591\u2593\u2591\u2592\u2592\u2592\u2593\u2592\u2592\u2593\u2591\u2593\u2592\u2591 Now a password store can be initialized by providing that same email address. This email is stored at ~/.password-store/.gpg-id pass init email@example.com Add password pass add email This produces a binary, encrypted file at ~/.password-store/email.gpg . The password can be retrieved, after authenticating with the master password, with the following: pass email # (1) In fact, because this is simply a GPG encrypted file, GPG could be used equivalently. In fact, this appears to be the command executed by the pass shell script. gpg -dq ~/.password-store/email.gpg Display names of passwords pass ls # (1) This command is equivalent to using tree on the password store directory. tree ~/.password-store Pass can also handle OTP generation for MFA, as long as you can retrieve the OTP URI (beginning with otpauth://). QR code images can be deciphered with zbarimg to retrieve these URIs. pass otp add mimecast # (1) Note that otpauth URLs usually contain an embedded email address, which must match that of the intialized password store. If this identity does not match, an error that read \"There is no assurance this key belongs to the named user\" is produced . Resources Luke Smith video","title":"pass"},{"location":"Linux/Shell/","text":"Shells fish Fish switch statements look completely different from bash case statements, with an incompatible syntax. Conditionally setting $PATH: switch \" $PATH \" # (1) case \"* $HOME /.cargo/bin*\" # (2) echo '$PATH already contains $HOME/.cargo/bin' # (3) case '*' set --global PATH $HOME /.cargo/bin $PATH # (4) end # (5) Because the $PATH is rendered as a list delimited by whitespace, without quotes this statement will be expanded to many arguments and will produce an error. Double quotes must be used, because with single quotes fish will not expand the $HOME variable. I have not found an empty placeholder similar to pass in Python which could simply occupy space here. Without a statement, fish appears to execute the following block by default. Environment variables use the set keyword. The --universal option, which would otherwise make sense here, does not work because $PATH is a global variable. Note that there is no equal sign, only a space separating the variable identifier and value. Bash equivalent case \": ${ PATH } :\" in *: \" $HOME /.cargo/bin\" :* ) ;; * ) export PATH = \" $HOME /.cargo/bin: $PATH \" ;; esac Setting environment variables set -x EDITOR /usr/bin/vim # (1) Without -x this variable will not be visible to applications. Bash equivalent export EDITOR = /usr/bin/vim Fish for-in loops are concluded with end . Set metadata in a loop for i in $( exa Godfrey* ) echo Processing $i set title $( string replace -r \"\\(.*mp3 $ \" \"\" $i ) # (1) ffmpeg -i $i -metadata title = \" $title \" -metadata album = \"Godfrey\" -metadata artist = \"Vlad TV\" -codec copy output/ $i end string replace is used here to remove the ending of a filename, including extension. bash The systemwide config for bash is at /etc/profile .","title":"Shells"},{"location":"Linux/Shell/#shells","text":"","title":"Shells"},{"location":"Linux/Shell/#fish","text":"Fish switch statements look completely different from bash case statements, with an incompatible syntax. Conditionally setting $PATH: switch \" $PATH \" # (1) case \"* $HOME /.cargo/bin*\" # (2) echo '$PATH already contains $HOME/.cargo/bin' # (3) case '*' set --global PATH $HOME /.cargo/bin $PATH # (4) end # (5) Because the $PATH is rendered as a list delimited by whitespace, without quotes this statement will be expanded to many arguments and will produce an error. Double quotes must be used, because with single quotes fish will not expand the $HOME variable. I have not found an empty placeholder similar to pass in Python which could simply occupy space here. Without a statement, fish appears to execute the following block by default. Environment variables use the set keyword. The --universal option, which would otherwise make sense here, does not work because $PATH is a global variable. Note that there is no equal sign, only a space separating the variable identifier and value. Bash equivalent case \": ${ PATH } :\" in *: \" $HOME /.cargo/bin\" :* ) ;; * ) export PATH = \" $HOME /.cargo/bin: $PATH \" ;; esac Setting environment variables set -x EDITOR /usr/bin/vim # (1) Without -x this variable will not be visible to applications. Bash equivalent export EDITOR = /usr/bin/vim Fish for-in loops are concluded with end . Set metadata in a loop for i in $( exa Godfrey* ) echo Processing $i set title $( string replace -r \"\\(.*mp3 $ \" \"\" $i ) # (1) ffmpeg -i $i -metadata title = \" $title \" -metadata album = \"Godfrey\" -metadata artist = \"Vlad TV\" -codec copy output/ $i end string replace is used here to remove the ending of a filename, including extension.","title":"fish"},{"location":"Linux/Shell/#bash","text":"The systemwide config for bash is at /etc/profile .","title":"bash"},{"location":"Linux/Storage/","text":"Storage Resources Unlike ZFS which has a lot of material in written and video form for potential users to learn from, BtrFS appears not to have much available. BtrFS does have an official wiki , but written articles on FOSS blogs focus on operation from the command-line but don't do a good job of describing the taxonomy of concepts, aside from the glossary . Users of ZFS , in contrast, have taken the trouble to create introductory material, including Ars Technica's ZFS 101 article, and many talks by enthusiasts like Philip Paeps . This might be because btrfs's concepts seem less well thought-out, or at least more poorly described. For example, the term subvolume is used in btrfs but the container for subvolumes is not \"volume\" but rather \"top-level subvolume\". Jim Salter from Ars Technica (who wrote the ZFS 101 article above) appears to have devoted some effort to fleshing out the topic: Examining btrfs, Linux's perpetually half-finished filesystem Install and configure Samba server Install Samba4 on RHEL 8 for File Sharing on Windows FreeNAS 11.3 - How to Set Up Windows SMB Shares BtrFS Creating and Destroying ZFS Storage Pools Managing devices in ZFS storage pools Getting started with btrfs for Linux Understanding Linux filesystems: ext4 and beyond Tasks Create virtual disks fallocate -l 100M /tmp/disk0 # Create sparse file losetup -f /tmp/disk0 # Create loopback device Formatting filesystems mkfs.ext4 /dev/sda1 mkfs.xfs /dev/sda2 Check filesystems fsck.ext4 /dev/sda1 xfs_repair /dev/sda2 HDD serial numbers Produce a CSV of hard disk identifiers and their serial numbers using hdparm, grep, cut, and output redirection. for l in { a..w } do echo -n \"/dev/sd $l ,\" >> drives hdparm -I /dev/sd $l | grep 'Serial Number' - | cut -d : -f 2 | tr -d '[:space:]' >> drives echo '' >> drives ; done Samba Configure Samba mkdir /samba # Create a directory for the share chmod -R 0777 /samba chown -R nobody:nobody /samba # Remove ownership, not necessary Open firewall rule (not strictly necessary) firewall-cmd --permanent --add-service = samba firewall-cmd --reload firewall-cmd --list-services # verify Configure the main Samba config file at /etc/samba/smb.conf . The name in brackets becomes the name of the share. [samba] comment = Samba on Ubuntu path = /samba read only = no browsable = yes Verify configuration testparm Set SELinux context of share directory semanage fcontext -a -t samba_share_t '/samba(/.*)?' restorecon -vvFR /samba <!-- Allow SELinux to work with Samba setsebool -P samba_export_all_ro on Set up a Samba account for $USER smbpasswd -a $USER Restart Samba service systemctl restart smbd --> Browse all available shares smbclient -L $HOST Access samba share at $SHARE at server $HOST using user credential $USER smbclient // $HOST / $USER -U $USER # (1) This will display the Samba CLI smb: \\> On TrueNAS, the option to \"Allow Guest Access\" should be turned on, unless password-based authentication for specific users is desired. Also, the directory must have write permissions enabled to allow uploading. chmod o+w Bizarrely, the ability to navigate into subdirectories appears to depend on the owner execute bit. This may have something to do with anonymous guest access. chmod u+x Permanently mounting a Samba share in /etc/fstab //nas/Videos /home/jasper/Videos cifs guest,uid=1000,iocharset=utf8 0 0 Then mount the fstab file mount -a NFS NFS is a distributed filesystem based on the RPC protocol that provides transparent access to remote disks. Modern NFS deployments in the wild are usually versions 3 or 4: V4 has superior performance, requires only the additional rpc.mountd service, and TCP port 2049 to be open V3 requires additional services ( rpcbind , lockd , rpc.statd ) and many firewall ports NFS shares are enabled using the /etc/exports file. /etc/exports /export/web_data1 *(ro,sync) /export/web_data2 127.0.0.1(rw,sync,no_root_squash) Once exports are defined, the NFS server can be started systemctl enable --now nfs-server.service Exports on localhost can be displayed using showmount showmount -e Shares can be mounted in /etc/fstab using the following syntax: 127.0.0.1:/export/web_data1 /mnt/nfs_web_data1 nfs defaults,_netdev 0 0 127.0.0.1:/export/web_data2 /mnt/nfs_web_data2 nfs defaults,_netdev 0 0 Better still is using autofs . Resources How to Share Files Using NFS : Linux Server Training 101 autofs Auto File System offers an alternative way of mounting NFS shares that can save some system resources, especially when many shares are mounted. Autofs can mount NFS shares dynamically, only when accessed. dnf install -y autofs systemctl enable --now autofs.service Mounts are defined in configs called maps . There are three map types: master map is /etc/auto.master by default direct maps point to other files for mount details. They are notable for beginning with /- indirect maps also point to other files for mount details but provide an umbrella mount point which will contain all other mounts within it. Note that other mountpoints at this parent directory cannot coexist with autofs mounts. Here is an example indirect map that will mount to /data/sales. /etc/auto.master.d/data.autofs /data /etc/auto.data /etc/auto.data sales -rw,soft 192.168.33.101:/data/sales Map files also support wildcards. * 127.0.0.1:/home/& AutoFS's config is at /etc/autofs.conf . One important directive is master_map_name which defines the master map file. LVM volume pvcreate /dev/vd { b,c,d } vgcreate group /dev/vd { b,c,d } lvcreate -l 100 %FREE -n volume group VDO Virtual disk optimizer (VDO) is a kernel module introduced in RHEL 7.5 that provides data deduplication and compression on block devices. The physical storage of a VDO volume is divided into a number of slabs , which are contiguous regions of the physical space. All slabs for a given volume have the same size, which can be any power of 2 multiple of 128 MB up to 32 GB (2 GB by default). The maximum number of slabs is 8,192. The maximum physical storage of the VDO is provided to the user on creation. Like LVM volumes, VDO volumes appear under /dev/mapper VDO appears not to be installed by default, but it is available in the BaseOS repo. dnf install vdo systemctl enable --now vdo Create a VDO volume vdo create --name = web_storage --device = /dev/xvdb --vdoLogicalSize = 10G vdostats --human-readable mkfs.xfs -K /dev/mapper/web_storage udevadm settle The fstab file requires a variety of options /dev/mapper/web_storage /mnt/web_storage xfs _netdev,x-systemd.device-timeout = 0 ,x-systemd.requires = vdo.service 0 0 Stratis Stratis is an open-source managed pooled storage solution in the vein of ZFS or btrfs. Stratis block devices can be disks, partitions, LUKS-encrypted volumes, LVM logical volumes, or DM multipath devices. Stratis pools are mounted under /stratis and, like other pooled storage systems, support multiple filesystems. Stratis file systems are thinly provisioned and formatted with xfs , although vanilla xfs utilities cannot be used on Stratis file systems. dnf -y install stratisd stratis-cli systemctl enable --now stratisd Create a pool stratis pool create pool /dev/sda /dev/sdb /dev/sdc # (1) An error about the devices being \"owned\" can be resolved by wiping it. wipefs -a /dev/sda Display block devices managed by Stratis stratis blockdev # (1) This command is equivalent to pvs in LVM. Create filesystem stratis fs create pool files Confirm stratis fs /etc/fstab /stratis/pool/files /mnt/stratisfs xfs defaults,x-systemd.requires=stratisd.service 0 0 Expand pool stratis pool add-data pool /dev/sdb Save snapshot stratis fs snapshot pool files files-snapshot Restore from snapshot stratis fs rename files files-orig stratis fs rename files-snapshot files umount /mnt/files ; mount /mnt/files Glossary ARC ARC serves as ZFS 's read cache mechanism and avoids the thrashing possible with standard OS page caches by using a more efficient algorithm. btrfs B-Tree Filesystem \"butter fs\" is an open-source CoW filesystem that offers many of the same features as ZFS . It was founded by Chris Mason in 2007. By 2009, btrfs 1.0 was accepted into the mainline Linux kernel. Btrfs was adopted by SUSE Enterprise Linux , but support was dropped by Red Hat in 2017. A B-tree is a self-balancing tree data structure used by btrfs to organize and store metadata. The superblock holds pointers to the tree roots of the tree of tree roots and the chunk tree. block group In btrfs the fundamental unit of storage allocation consisting of one or more chunks , depending on RAID level, each stored on a different device . boot environment Allow changes to OS installations to be reverted copy-on-write In a CoW filesystem like ZFS and btrfs , when data on the filesystem is modified, that data is copied first before being modified and then written back to a different free location. The main advantage of this method is that the original data extent is not modified, so the risk of data corruption or partial update due to power failure is eliminated. This ensures that writes are atomic and the filesystem will always be in a consistent state. dataset In ZFS , datasets represent mountable filesystems. Improves on the use of traditional use of partitions in, say, Linux installations where mount points are typically separate partitions. Datasets allow quotas and other rules to be enforced. extent In btrfs , an extent is the fundamental storage unit corresponding to a contiguous sequence of bytes on disk that holds file data. Files can be fragmented into multiple extents, and this fragmentation can be measured using the filefrag CLI utility. Extended File System Ext was first implemented in 1992 by Remy Card to address limitations in the MINIX filesystem, which was used to develop the first Linux kernel. It could address up to 2GB of storage and handle 255-character filenames and had only one timestap per file. Ext2 was developed by Remy Card only a year after ext's release as a commercial-grade filesystem, influenced by BSD's Berkeley Fast File System. It was prone to corruption if the system crashed or lost power while data was being written and performance losses due to fragmentation. Nevertheless, it was quickly and widely adopted, and still used as a format for USB drives. Ext3 was adopted by mainline Linux in 2001 and uses journaling , whereby disk writes are stored as transactions in a special allocation, which allows a rebooted system to roll back incomplete transactions. Ext4 was added to mainline Linux in 2008, developed by Theodore Ts'o, and improves upon ext3 but is still reliant on old technology. inode An inode (index node) is a data structure that stores all the metadata about a file but not its name or data. subvolume A tree of files and directories inside a btrfs that can be mounted as if it were an independent filesystem. Each btrfs filesystem has at least one subvolume that contains everything else in the filesystem, called the top-level subvolume . thrashing All OSes implement the page cache using the LRU algorithm, which maintains a queue of the most recently read blocks. As more recent blocks are read, older blocks are evicted from the bottom of the queue even if they are more frequently accessed. This process is referred to as thrashing . RAID hole Condition in which a stripe is only partially written before the system crashes, making the array inconsistent and corrupt after a restart RAIDz RAIDz1 , RAIDz2 , and RAIDz3 are special varieties of parity RAID in ZFS : the number indicates how many parity blocks are allocated to each data stripe . Resilvering Process of rebuilding redundant groups after disk replacement SMB Server Message Block ( SMB ) is a client/server protocol developed in the early 1980s by Intel, Microsoft, and IBM that has become the native protocol for file and printer sharing on Windows. It is implemented in the Samba application suite. CIFS (Common Internet File System, pronounced \"sifs\") is a dialect and implementation of SMB whose acronym has survived despite the fact the protocol itself has fallen into disuse. vdev In ZFS a vdev (\"virtual device\") is an abstraction of one or more storage devices. This is equivalent to a volume group in LVM. A collection of vdevs constitutes a zpool . Vdevs support one of five topologies : Single-device vdevs cannot survive any failure Mirror vdevs duplicate every block on each of their devices RAIDz1 RAIDz2 RAIDz3 Special support classes of vdev: CACHE LOG (also SLOG ), because it usually has faster write performance, provides the pool with a separate vdev to store the ZIL in. SPECIAL volume A ZFS volume is a dataset that represents a block device. They are created with the -V option and can be found under /dev/zvol . zfs create -V 5gb tank/vol A volume can also be shared as an iSCSI target by setting the shareiscsi property on the volume. ZED Daemon that will listen to kernel events related to ZFS , conducting action defined in zedlets . zfs-fuse ZFS filesystem daemon ZFS ZFS is a technology that combines the functions of a 128-bit CoW filesystem, a volume manager, and software RAID. Like RAID, ZFS attempts to achieve data reliability by abstracting volumes over physical devices . But ZFS improves on RAID with error handling: it can use checksum information to correct corrupted files. This is unlike hardware RAID mirrors, where failures occur silently and are typically only detected upon reading a corrupt file. ZFS writes use CoW meaning they are atomic and aren't affected by issues like RAID holes . ZFS can also transparently compress data written to datasets. ZFS on Linux (ZOL) is considered the ugly stepchild of the ZFS community despite the fact that the Linux implementation has the most features and the most community support. ZFS is too tightly bound to the operation of the kernel to operate in true userspace, and that is why each implementation is different for operating systems. ZIL The ZIL is a special storage area used for synchronous writes. Most writes are asynchronous , where the filesystem is allowed to aggregate and commit them in batches to reduce fragmentation and increase throughput. Synchronous writes in ZFS are committed to the ZIL while also kept in memory. Writes saved to the ZIL are committed to main storage in normal TXGs moments later. Normally, the ZIL is written to and never read from again. Writes saved to ZIL are committed to main storage from RAM in normal TXGs after a few moments and unlinked from the ZIL . The ZIL is only read during pool imports that occur after a crash and restart. The ZIL can be placed on the LOG vdev to take advantage of higher write performance during sync writes. The ZIL is typically mirrored because that is where data can be lost. zpool A zpool is the largest structure in the ZFS taxonomy, representing an independent collection of one or more vdevs . Essentially, a zpool is a JBOD with special characteristics. Writes are distributed across available vdevs in accordance with their available free space, such that they fill more or less evenly. A utilization awareness mechanism built into ZFS also accounts for if one vdev is significantly more busy than another, i.e. reading. In such a case, writes to that busy vdev will be deferred in favor of less busy ones. Zpools are automatically mounted at root upon creation (without the need to edit fstab). Commands btrfs Show storage consumed, including how much is shared by all snapshots btrfs fi du /home -s btrfs fi df /home fallocate Create a file of a given size with the --length / -l option 1 gigabyte 1 gibibyte fallocate -l 1GB $FILENAME # gigabyte fallocate -l 1G $FILENAME # gibibyte hdparm hdparm -I /dev/sda losetup Create a loopback device (i.e. a virtual block device) losetup -f /tmp/file1 lsblk Display filesystems lsblk -f # --fs sfdisk Script-based partition table editor, similar to fdisk and gdisk , which can be run interactively. It does not interface with GPT format, neither is it designed for large partitions. [ref][11] List partitions on all devices Display size of {partition} or {device} This command produces the size of {partition} (i.e. /dev/sda1 ) or even {device} ( /dev/sda ) in blocks sfdisk -s partition sfdisk -s device Apply consistency checks to {partition} or {device} sfdisk -V partition sfdisk --verify device Create a partition sfdisk device Save sectors changed This command will allow recovery using the following command sfdisk /dev/hdd -O hdd-partition-sectors.save Recovery Man page indicates this flag is no longer supported, and recommends use of dd instead. sfdisk /dev/hdd -I hdd-partition-sectors.save shred Write random data to an unmounted disk for {n} passes shred --iterations = n LVM lvm lvm version lvresize Resize existent logical volume Marketing in volume group vg1 to have an additional 10 gigabytes of space lvresize -L +10G /dev/vg1/Marketing pvcreate pvcreate /dev/sd { a,b,c } lvresize Resize existent logical volume Marketing in volume group vg1 to have an additional 10 gigabytes of space lvresize -L +10G /dev/vg1/Marketing It is possible to use LVM to format the storage media when installing CentOS or RHEL on a virtual machine, even if there is only a single disk. This will result in a swap partition being created as a small logical volume. This can be removed: swapoff cs/swap lvremove cs/swap Then the remaining logical volume mounted to root can be expanded: lvresize -l 100 %VG cs/root pvcreate pvcreate /dev/loop0","title":"Storage"},{"location":"Linux/Storage/#storage","text":"Resources Unlike ZFS which has a lot of material in written and video form for potential users to learn from, BtrFS appears not to have much available. BtrFS does have an official wiki , but written articles on FOSS blogs focus on operation from the command-line but don't do a good job of describing the taxonomy of concepts, aside from the glossary . Users of ZFS , in contrast, have taken the trouble to create introductory material, including Ars Technica's ZFS 101 article, and many talks by enthusiasts like Philip Paeps . This might be because btrfs's concepts seem less well thought-out, or at least more poorly described. For example, the term subvolume is used in btrfs but the container for subvolumes is not \"volume\" but rather \"top-level subvolume\". Jim Salter from Ars Technica (who wrote the ZFS 101 article above) appears to have devoted some effort to fleshing out the topic: Examining btrfs, Linux's perpetually half-finished filesystem Install and configure Samba server Install Samba4 on RHEL 8 for File Sharing on Windows FreeNAS 11.3 - How to Set Up Windows SMB Shares BtrFS Creating and Destroying ZFS Storage Pools Managing devices in ZFS storage pools Getting started with btrfs for Linux Understanding Linux filesystems: ext4 and beyond","title":"Storage"},{"location":"Linux/Storage/#tasks","text":"","title":"Tasks"},{"location":"Linux/Storage/#create-virtual-disks","text":"fallocate -l 100M /tmp/disk0 # Create sparse file losetup -f /tmp/disk0 # Create loopback device","title":"Create virtual disks"},{"location":"Linux/Storage/#formatting-filesystems","text":"mkfs.ext4 /dev/sda1 mkfs.xfs /dev/sda2 Check filesystems fsck.ext4 /dev/sda1 xfs_repair /dev/sda2","title":"Formatting filesystems"},{"location":"Linux/Storage/#hdd-serial-numbers","text":"Produce a CSV of hard disk identifiers and their serial numbers using hdparm, grep, cut, and output redirection. for l in { a..w } do echo -n \"/dev/sd $l ,\" >> drives hdparm -I /dev/sd $l | grep 'Serial Number' - | cut -d : -f 2 | tr -d '[:space:]' >> drives echo '' >> drives ; done","title":"HDD serial numbers"},{"location":"Linux/Storage/#samba_1","text":"Configure Samba mkdir /samba # Create a directory for the share chmod -R 0777 /samba chown -R nobody:nobody /samba # Remove ownership, not necessary Open firewall rule (not strictly necessary) firewall-cmd --permanent --add-service = samba firewall-cmd --reload firewall-cmd --list-services # verify Configure the main Samba config file at /etc/samba/smb.conf . The name in brackets becomes the name of the share. [samba] comment = Samba on Ubuntu path = /samba read only = no browsable = yes Verify configuration testparm Set SELinux context of share directory semanage fcontext -a -t samba_share_t '/samba(/.*)?' restorecon -vvFR /samba <!-- Allow SELinux to work with Samba setsebool -P samba_export_all_ro on Set up a Samba account for $USER smbpasswd -a $USER Restart Samba service systemctl restart smbd --> Browse all available shares smbclient -L $HOST Access samba share at $SHARE at server $HOST using user credential $USER smbclient // $HOST / $USER -U $USER # (1) This will display the Samba CLI smb: \\> On TrueNAS, the option to \"Allow Guest Access\" should be turned on, unless password-based authentication for specific users is desired. Also, the directory must have write permissions enabled to allow uploading. chmod o+w Bizarrely, the ability to navigate into subdirectories appears to depend on the owner execute bit. This may have something to do with anonymous guest access. chmod u+x Permanently mounting a Samba share in /etc/fstab //nas/Videos /home/jasper/Videos cifs guest,uid=1000,iocharset=utf8 0 0 Then mount the fstab file mount -a","title":"Samba"},{"location":"Linux/Storage/#nfs","text":"NFS is a distributed filesystem based on the RPC protocol that provides transparent access to remote disks. Modern NFS deployments in the wild are usually versions 3 or 4: V4 has superior performance, requires only the additional rpc.mountd service, and TCP port 2049 to be open V3 requires additional services ( rpcbind , lockd , rpc.statd ) and many firewall ports NFS shares are enabled using the /etc/exports file. /etc/exports /export/web_data1 *(ro,sync) /export/web_data2 127.0.0.1(rw,sync,no_root_squash) Once exports are defined, the NFS server can be started systemctl enable --now nfs-server.service Exports on localhost can be displayed using showmount showmount -e Shares can be mounted in /etc/fstab using the following syntax: 127.0.0.1:/export/web_data1 /mnt/nfs_web_data1 nfs defaults,_netdev 0 0 127.0.0.1:/export/web_data2 /mnt/nfs_web_data2 nfs defaults,_netdev 0 0 Better still is using autofs . Resources How to Share Files Using NFS : Linux Server Training 101","title":"NFS"},{"location":"Linux/Storage/#autofs","text":"Auto File System offers an alternative way of mounting NFS shares that can save some system resources, especially when many shares are mounted. Autofs can mount NFS shares dynamically, only when accessed. dnf install -y autofs systemctl enable --now autofs.service Mounts are defined in configs called maps . There are three map types: master map is /etc/auto.master by default direct maps point to other files for mount details. They are notable for beginning with /- indirect maps also point to other files for mount details but provide an umbrella mount point which will contain all other mounts within it. Note that other mountpoints at this parent directory cannot coexist with autofs mounts. Here is an example indirect map that will mount to /data/sales. /etc/auto.master.d/data.autofs /data /etc/auto.data /etc/auto.data sales -rw,soft 192.168.33.101:/data/sales Map files also support wildcards. * 127.0.0.1:/home/& AutoFS's config is at /etc/autofs.conf . One important directive is master_map_name which defines the master map file.","title":"autofs"},{"location":"Linux/Storage/#lvm-volume","text":"pvcreate /dev/vd { b,c,d } vgcreate group /dev/vd { b,c,d } lvcreate -l 100 %FREE -n volume group","title":"LVM volume"},{"location":"Linux/Storage/#vdo","text":"Virtual disk optimizer (VDO) is a kernel module introduced in RHEL 7.5 that provides data deduplication and compression on block devices. The physical storage of a VDO volume is divided into a number of slabs , which are contiguous regions of the physical space. All slabs for a given volume have the same size, which can be any power of 2 multiple of 128 MB up to 32 GB (2 GB by default). The maximum number of slabs is 8,192. The maximum physical storage of the VDO is provided to the user on creation. Like LVM volumes, VDO volumes appear under /dev/mapper VDO appears not to be installed by default, but it is available in the BaseOS repo. dnf install vdo systemctl enable --now vdo Create a VDO volume vdo create --name = web_storage --device = /dev/xvdb --vdoLogicalSize = 10G vdostats --human-readable mkfs.xfs -K /dev/mapper/web_storage udevadm settle The fstab file requires a variety of options /dev/mapper/web_storage /mnt/web_storage xfs _netdev,x-systemd.device-timeout = 0 ,x-systemd.requires = vdo.service 0 0","title":"VDO"},{"location":"Linux/Storage/#stratis","text":"Stratis is an open-source managed pooled storage solution in the vein of ZFS or btrfs. Stratis block devices can be disks, partitions, LUKS-encrypted volumes, LVM logical volumes, or DM multipath devices. Stratis pools are mounted under /stratis and, like other pooled storage systems, support multiple filesystems. Stratis file systems are thinly provisioned and formatted with xfs , although vanilla xfs utilities cannot be used on Stratis file systems. dnf -y install stratisd stratis-cli systemctl enable --now stratisd Create a pool stratis pool create pool /dev/sda /dev/sdb /dev/sdc # (1) An error about the devices being \"owned\" can be resolved by wiping it. wipefs -a /dev/sda Display block devices managed by Stratis stratis blockdev # (1) This command is equivalent to pvs in LVM. Create filesystem stratis fs create pool files Confirm stratis fs /etc/fstab /stratis/pool/files /mnt/stratisfs xfs defaults,x-systemd.requires=stratisd.service 0 0 Expand pool stratis pool add-data pool /dev/sdb Save snapshot stratis fs snapshot pool files files-snapshot Restore from snapshot stratis fs rename files files-orig stratis fs rename files-snapshot files umount /mnt/files ; mount /mnt/files","title":"Stratis"},{"location":"Linux/Storage/#glossary","text":"ARC ARC serves as ZFS 's read cache mechanism and avoids the thrashing possible with standard OS page caches by using a more efficient algorithm.","title":"Glossary"},{"location":"Linux/Storage/#btrfs","text":"B-Tree Filesystem \"butter fs\" is an open-source CoW filesystem that offers many of the same features as ZFS . It was founded by Chris Mason in 2007. By 2009, btrfs 1.0 was accepted into the mainline Linux kernel. Btrfs was adopted by SUSE Enterprise Linux , but support was dropped by Red Hat in 2017. A B-tree is a self-balancing tree data structure used by btrfs to organize and store metadata. The superblock holds pointers to the tree roots of the tree of tree roots and the chunk tree. block group In btrfs the fundamental unit of storage allocation consisting of one or more chunks , depending on RAID level, each stored on a different device . boot environment Allow changes to OS installations to be reverted copy-on-write In a CoW filesystem like ZFS and btrfs , when data on the filesystem is modified, that data is copied first before being modified and then written back to a different free location. The main advantage of this method is that the original data extent is not modified, so the risk of data corruption or partial update due to power failure is eliminated. This ensures that writes are atomic and the filesystem will always be in a consistent state.","title":"btrfs"},{"location":"Linux/Storage/#dataset","text":"In ZFS , datasets represent mountable filesystems. Improves on the use of traditional use of partitions in, say, Linux installations where mount points are typically separate partitions. Datasets allow quotas and other rules to be enforced. extent In btrfs , an extent is the fundamental storage unit corresponding to a contiguous sequence of bytes on disk that holds file data. Files can be fragmented into multiple extents, and this fragmentation can be measured using the filefrag CLI utility. Extended File System Ext was first implemented in 1992 by Remy Card to address limitations in the MINIX filesystem, which was used to develop the first Linux kernel. It could address up to 2GB of storage and handle 255-character filenames and had only one timestap per file. Ext2 was developed by Remy Card only a year after ext's release as a commercial-grade filesystem, influenced by BSD's Berkeley Fast File System. It was prone to corruption if the system crashed or lost power while data was being written and performance losses due to fragmentation. Nevertheless, it was quickly and widely adopted, and still used as a format for USB drives. Ext3 was adopted by mainline Linux in 2001 and uses journaling , whereby disk writes are stored as transactions in a special allocation, which allows a rebooted system to roll back incomplete transactions. Ext4 was added to mainline Linux in 2008, developed by Theodore Ts'o, and improves upon ext3 but is still reliant on old technology.","title":"dataset"},{"location":"Linux/Storage/#inode","text":"An inode (index node) is a data structure that stores all the metadata about a file but not its name or data. subvolume A tree of files and directories inside a btrfs that can be mounted as if it were an independent filesystem. Each btrfs filesystem has at least one subvolume that contains everything else in the filesystem, called the top-level subvolume . thrashing All OSes implement the page cache using the LRU algorithm, which maintains a queue of the most recently read blocks. As more recent blocks are read, older blocks are evicted from the bottom of the queue even if they are more frequently accessed. This process is referred to as thrashing . RAID hole Condition in which a stripe is only partially written before the system crashes, making the array inconsistent and corrupt after a restart RAIDz RAIDz1 , RAIDz2 , and RAIDz3 are special varieties of parity RAID in ZFS : the number indicates how many parity blocks are allocated to each data stripe . Resilvering Process of rebuilding redundant groups after disk replacement SMB Server Message Block ( SMB ) is a client/server protocol developed in the early 1980s by Intel, Microsoft, and IBM that has become the native protocol for file and printer sharing on Windows. It is implemented in the Samba application suite. CIFS (Common Internet File System, pronounced \"sifs\") is a dialect and implementation of SMB whose acronym has survived despite the fact the protocol itself has fallen into disuse.","title":"inode"},{"location":"Linux/Storage/#vdev","text":"In ZFS a vdev (\"virtual device\") is an abstraction of one or more storage devices. This is equivalent to a volume group in LVM. A collection of vdevs constitutes a zpool . Vdevs support one of five topologies : Single-device vdevs cannot survive any failure Mirror vdevs duplicate every block on each of their devices RAIDz1 RAIDz2 RAIDz3 Special support classes of vdev: CACHE LOG (also SLOG ), because it usually has faster write performance, provides the pool with a separate vdev to store the ZIL in. SPECIAL","title":"vdev"},{"location":"Linux/Storage/#volume","text":"A ZFS volume is a dataset that represents a block device. They are created with the -V option and can be found under /dev/zvol . zfs create -V 5gb tank/vol A volume can also be shared as an iSCSI target by setting the shareiscsi property on the volume. ZED Daemon that will listen to kernel events related to ZFS , conducting action defined in zedlets . zfs-fuse ZFS filesystem daemon","title":"volume"},{"location":"Linux/Storage/#zfs","text":"ZFS is a technology that combines the functions of a 128-bit CoW filesystem, a volume manager, and software RAID. Like RAID, ZFS attempts to achieve data reliability by abstracting volumes over physical devices . But ZFS improves on RAID with error handling: it can use checksum information to correct corrupted files. This is unlike hardware RAID mirrors, where failures occur silently and are typically only detected upon reading a corrupt file. ZFS writes use CoW meaning they are atomic and aren't affected by issues like RAID holes . ZFS can also transparently compress data written to datasets. ZFS on Linux (ZOL) is considered the ugly stepchild of the ZFS community despite the fact that the Linux implementation has the most features and the most community support. ZFS is too tightly bound to the operation of the kernel to operate in true userspace, and that is why each implementation is different for operating systems. ZIL The ZIL is a special storage area used for synchronous writes. Most writes are asynchronous , where the filesystem is allowed to aggregate and commit them in batches to reduce fragmentation and increase throughput. Synchronous writes in ZFS are committed to the ZIL while also kept in memory. Writes saved to the ZIL are committed to main storage in normal TXGs moments later. Normally, the ZIL is written to and never read from again. Writes saved to ZIL are committed to main storage from RAM in normal TXGs after a few moments and unlinked from the ZIL . The ZIL is only read during pool imports that occur after a crash and restart. The ZIL can be placed on the LOG vdev to take advantage of higher write performance during sync writes. The ZIL is typically mirrored because that is where data can be lost.","title":"ZFS"},{"location":"Linux/Storage/#zpool","text":"A zpool is the largest structure in the ZFS taxonomy, representing an independent collection of one or more vdevs . Essentially, a zpool is a JBOD with special characteristics. Writes are distributed across available vdevs in accordance with their available free space, such that they fill more or less evenly. A utilization awareness mechanism built into ZFS also accounts for if one vdev is significantly more busy than another, i.e. reading. In such a case, writes to that busy vdev will be deferred in favor of less busy ones. Zpools are automatically mounted at root upon creation (without the need to edit fstab).","title":"zpool"},{"location":"Linux/Storage/#commands","text":"","title":"Commands"},{"location":"Linux/Storage/#btrfs_1","text":"Show storage consumed, including how much is shared by all snapshots btrfs fi du /home -s btrfs fi df /home","title":"btrfs"},{"location":"Linux/Storage/#fallocate","text":"Create a file of a given size with the --length / -l option 1 gigabyte 1 gibibyte fallocate -l 1GB $FILENAME # gigabyte fallocate -l 1G $FILENAME # gibibyte","title":"fallocate"},{"location":"Linux/Storage/#hdparm","text":"hdparm -I /dev/sda","title":"hdparm"},{"location":"Linux/Storage/#losetup","text":"Create a loopback device (i.e. a virtual block device) losetup -f /tmp/file1","title":"losetup"},{"location":"Linux/Storage/#lsblk","text":"Display filesystems lsblk -f # --fs","title":"lsblk"},{"location":"Linux/Storage/#sfdisk","text":"Script-based partition table editor, similar to fdisk and gdisk , which can be run interactively. It does not interface with GPT format, neither is it designed for large partitions. [ref][11] List partitions on all devices Display size of {partition} or {device} This command produces the size of {partition} (i.e. /dev/sda1 ) or even {device} ( /dev/sda ) in blocks sfdisk -s partition sfdisk -s device Apply consistency checks to {partition} or {device} sfdisk -V partition sfdisk --verify device Create a partition sfdisk device Save sectors changed This command will allow recovery using the following command sfdisk /dev/hdd -O hdd-partition-sectors.save Recovery Man page indicates this flag is no longer supported, and recommends use of dd instead. sfdisk /dev/hdd -I hdd-partition-sectors.save","title":"sfdisk"},{"location":"Linux/Storage/#shred","text":"Write random data to an unmounted disk for {n} passes shred --iterations = n","title":"shred"},{"location":"Linux/Storage/#lvm","text":"","title":"LVM"},{"location":"Linux/Storage/#lvm_1","text":"lvm version","title":"lvm"},{"location":"Linux/Storage/#lvresize","text":"Resize existent logical volume Marketing in volume group vg1 to have an additional 10 gigabytes of space lvresize -L +10G /dev/vg1/Marketing","title":"lvresize"},{"location":"Linux/Storage/#pvcreate","text":"pvcreate /dev/sd { a,b,c }","title":"pvcreate"},{"location":"Linux/Storage/#lvresize_1","text":"Resize existent logical volume Marketing in volume group vg1 to have an additional 10 gigabytes of space lvresize -L +10G /dev/vg1/Marketing It is possible to use LVM to format the storage media when installing CentOS or RHEL on a virtual machine, even if there is only a single disk. This will result in a swap partition being created as a small logical volume. This can be removed: swapoff cs/swap lvremove cs/swap Then the remaining logical volume mounted to root can be expanded: lvresize -l 100 %VG cs/root","title":"lvresize"},{"location":"Linux/Storage/#pvcreate_1","text":"pvcreate /dev/loop0","title":"pvcreate"},{"location":"Linux/SystemD/","text":"SystemD SystemD is the de facto Linux init system since replacing Sysvinit and Upstart in all major distributions. SystemD organizes resources into units , which can be managed by daemons and manipulated by SystemD utilities. It was designed by a pair of Red Hat developers in 2010 to be a general purpose system manager. It offers parallel execution, explicit dependencies between services, an escape from slow shell scripts, and per-daemon resource control and watchdogs. Tasks Scheduling services Services can be scheduled to start with timers . sshd.timer [Unit] Description = Starts sshd service at beginning of workday, and shuts it down at the end. [Timer] Unit = sshd.service OnCalendar = Mon..Fri *-*-* 09:00:00 [Install] WantedBy = timers.target Now, when stopping sshd manually the following output is printed. Warning: Stopping sshd.service, but it can still be activated by: sshd.timer The service can be scheduled to shutdown within the service file itself using the RuntimeMaxSec directive. sshd.service RuntimeMaxSec = 36000 # i.e. 10 hours This unfortunately will result in the service being reported as failed . This failure can be cleared with this command: systemctl reset-failed Masking On TrueNAS, the libvirtd socket is masked by default. This means that virsh is not able to connect to the hypervisor until it is unmasked and the service restarted. systemctl unmask libvirtd.socket systemctl restart libvirtd.service virsh connect qemu:///system Glossary Service files Service files are a type of unit file which have replaced earlier init scripts and describe how to manage a service or application on the server. Active services are placed in /etc/systemd/system , whereas inactive service files distributed with installed packages are placed in /usr/lib/systemd/system . Docker container as a service: [Unit] Description = Notes Container (Docker) [Service] ExecStart = /usr/bin/docker start notes [Install] WantedBy = multi-user.target Slice A slice unit is a unit configuration file ending in \".slice\" which manages resources of a group of processes. SystemD slices implement and build on Linux cgroups . Slices exist in a hierarchy below the root slice ( -.slice ) and are used to group scopes and services Scopes contain unrelated processes but not necessarily hierarchically Services are from unit files or Transient Runtime Services and contain processes Root slices themselves only contain scopes and other slices. user.slice contains all user-related slices and scopes, named after the pattern user-UID.slice session.slice is created for every login session system.slice contain slices, scopes, and services machine.slice contains all container-related slices, scopes, and services. Services can be assigned to specific slices explicitly by editing the value of the Slice key in the service file. Keys like CPUWeight can assign cgroup resource controls. Other such controls can be viewed in the systemd.resource-control(5) man page. [Unit] Slice = user.slice CPUWeight = 50 SysVinit SysVInit is the oldest init system used in Linux. In SysVinit, which used bash scripts to run and manage servicesj, processes were started serially and synchronously, wasting time and system resources. For years, a common mitigation was to run services in the background, simulating concurrency. Target files Target files are equivalent to SysVInit runlevels. SystemD target SysVInit runlevel poweroff.target 0 rescue.target 1 multi-user.target 3 graphical.target 5 reboot.target 6 emergency.target emergency Timers Timer files are systemd unit files with names ending in .timer that control service files . For each timer file, a matching unit file must exist describing the unit to activate when the timer elapses. By default, systemd will search for a service file with a filename matching that of the timer, but failing that a specific unit can be specified with the Unit key within the timer file itself. Display timers systemctl list-timers systemctl status *timer Like other unit files, timer files may include Unit and Install sections, but must include the Timer section. Specifying time is done using timestamps which can be monotonic or realtime . Monotonic timers are defined relative to various system hooks using the following directives: OnActiveSec, OnBootSec, OnStartupSec, OnUnitActiveSec, and OnUnitInactiveSec. Realtime timers define timers according to calendar event expressions , denoting real-world dates and times as humans understand them. Validate timestamps: systemd-analyze calendar '*-*-* 00:00:00' --iterations systemd-run can be used for one-off events as a substitute for anacron. systemd-run --on-active = -30sec /bin/touch /home/user/file This command creates a transient unit file, whose name is provided in the output. systemctl cat run-u97.service Unit files Unit files are case-sensitive .ini files organized into sections. Unit files can be found in several directories: /lib/systemd/system where the system's copy of unit files are placed by default /etc/systemd/system where unit files override the system default /run/systemd/system where run-time unit definitions are found and given a higher priority than the system default in /lib but lower than that in /etc. These unit files are created dynamically and lost on reboot. Unit files come in many different types which can be identified by their filename extension (i.e. .service , target , etc.). Upstart Upstart was an init system developed by Canonical for Ubuntu meant to replace SysVinit , but it was abandoned in 2014. Commands hostnamectl Permanently change hostname hostnamectl set-hostname $HOSTNAME journalctl Clean up old logs journalctl --disk-usage # (3) journalctl --rotate # (1) journalctl --vacuum-time = 1d # (2) Ask journal daemon to rotate journal files, immediately archiving and renaming currently active journal files. --vacuum-size , --vacuum-time , and --vacuum-files can be used singly or in combination to enforce limits on archived journal files. Show current disk usage of all journal files Display logs journalctl -r # --reverse (1) journalctl -f # --follow (2) Display output in reverse (newest entries first) Continuously update the display as new log entries are created By default, SystemD logs to memory. This can be changed by adjusting /etc/systemd/journald.conf . This requires the directory /var/log/journal to exist. Persistent logging [Journal] Storage = persistent localectl Change locale to French localectl set-locale LANG = fr_FR.utf8 loginctl Enable user lingering , which allows users that are not logged in to run long-running services. loginctl enable-linger loginctl show-user | grep Linger - # Confirm systemctl Services systemctl list-unit-files --type = service # Display all services systemctl enable --now $SERVICE # Configure service to start on boot and start it immediately systemctl status $SERVICE systemctl is-active $SERVICE systemctl disable $SERVICE systemctl mask $SERVICE # Prevent service from being started inadvertently by another process systemctl restart $SERVICE Boot targets systemctl get-default systemctl set-default graphical.target systemctl isolate emergency.target # Change target systemctl suspend # Suspend system --user specifies the service manager of the calling user. systemctl --user enable --now container-notes.service # (1) systemctl --user status container-notes.service Here, container-notes.service has been created at ~/.config/systemd/user systemd-analyze Check security of a service systemd-analyze security sshd.service systemd-cgls systemd-cgls recursively shows the contents of the selected cgroup hierarchy in a tree. systemd-delta Show files that are overridden with systemd. Display differences among files when they are overridden systemd-delta --diff","title":"SystemD"},{"location":"Linux/SystemD/#systemd","text":"SystemD is the de facto Linux init system since replacing Sysvinit and Upstart in all major distributions. SystemD organizes resources into units , which can be managed by daemons and manipulated by SystemD utilities. It was designed by a pair of Red Hat developers in 2010 to be a general purpose system manager. It offers parallel execution, explicit dependencies between services, an escape from slow shell scripts, and per-daemon resource control and watchdogs.","title":"SystemD"},{"location":"Linux/SystemD/#tasks","text":"","title":"Tasks"},{"location":"Linux/SystemD/#scheduling-services","text":"Services can be scheduled to start with timers . sshd.timer [Unit] Description = Starts sshd service at beginning of workday, and shuts it down at the end. [Timer] Unit = sshd.service OnCalendar = Mon..Fri *-*-* 09:00:00 [Install] WantedBy = timers.target Now, when stopping sshd manually the following output is printed. Warning: Stopping sshd.service, but it can still be activated by: sshd.timer The service can be scheduled to shutdown within the service file itself using the RuntimeMaxSec directive. sshd.service RuntimeMaxSec = 36000 # i.e. 10 hours This unfortunately will result in the service being reported as failed . This failure can be cleared with this command: systemctl reset-failed","title":"Scheduling services"},{"location":"Linux/SystemD/#masking","text":"On TrueNAS, the libvirtd socket is masked by default. This means that virsh is not able to connect to the hypervisor until it is unmasked and the service restarted. systemctl unmask libvirtd.socket systemctl restart libvirtd.service virsh connect qemu:///system","title":"Masking"},{"location":"Linux/SystemD/#glossary","text":"","title":"Glossary"},{"location":"Linux/SystemD/#service-files","text":"Service files are a type of unit file which have replaced earlier init scripts and describe how to manage a service or application on the server. Active services are placed in /etc/systemd/system , whereas inactive service files distributed with installed packages are placed in /usr/lib/systemd/system . Docker container as a service: [Unit] Description = Notes Container (Docker) [Service] ExecStart = /usr/bin/docker start notes [Install] WantedBy = multi-user.target","title":"Service files"},{"location":"Linux/SystemD/#slice","text":"A slice unit is a unit configuration file ending in \".slice\" which manages resources of a group of processes. SystemD slices implement and build on Linux cgroups . Slices exist in a hierarchy below the root slice ( -.slice ) and are used to group scopes and services Scopes contain unrelated processes but not necessarily hierarchically Services are from unit files or Transient Runtime Services and contain processes Root slices themselves only contain scopes and other slices. user.slice contains all user-related slices and scopes, named after the pattern user-UID.slice session.slice is created for every login session system.slice contain slices, scopes, and services machine.slice contains all container-related slices, scopes, and services. Services can be assigned to specific slices explicitly by editing the value of the Slice key in the service file. Keys like CPUWeight can assign cgroup resource controls. Other such controls can be viewed in the systemd.resource-control(5) man page. [Unit] Slice = user.slice CPUWeight = 50","title":"Slice"},{"location":"Linux/SystemD/#sysvinit","text":"SysVInit is the oldest init system used in Linux. In SysVinit, which used bash scripts to run and manage servicesj, processes were started serially and synchronously, wasting time and system resources. For years, a common mitigation was to run services in the background, simulating concurrency.","title":"SysVinit"},{"location":"Linux/SystemD/#target-files","text":"Target files are equivalent to SysVInit runlevels. SystemD target SysVInit runlevel poweroff.target 0 rescue.target 1 multi-user.target 3 graphical.target 5 reboot.target 6 emergency.target emergency","title":"Target files"},{"location":"Linux/SystemD/#timers","text":"Timer files are systemd unit files with names ending in .timer that control service files . For each timer file, a matching unit file must exist describing the unit to activate when the timer elapses. By default, systemd will search for a service file with a filename matching that of the timer, but failing that a specific unit can be specified with the Unit key within the timer file itself. Display timers systemctl list-timers systemctl status *timer Like other unit files, timer files may include Unit and Install sections, but must include the Timer section. Specifying time is done using timestamps which can be monotonic or realtime . Monotonic timers are defined relative to various system hooks using the following directives: OnActiveSec, OnBootSec, OnStartupSec, OnUnitActiveSec, and OnUnitInactiveSec. Realtime timers define timers according to calendar event expressions , denoting real-world dates and times as humans understand them. Validate timestamps: systemd-analyze calendar '*-*-* 00:00:00' --iterations systemd-run can be used for one-off events as a substitute for anacron. systemd-run --on-active = -30sec /bin/touch /home/user/file This command creates a transient unit file, whose name is provided in the output. systemctl cat run-u97.service","title":"Timers"},{"location":"Linux/SystemD/#unit-files","text":"Unit files are case-sensitive .ini files organized into sections. Unit files can be found in several directories: /lib/systemd/system where the system's copy of unit files are placed by default /etc/systemd/system where unit files override the system default /run/systemd/system where run-time unit definitions are found and given a higher priority than the system default in /lib but lower than that in /etc. These unit files are created dynamically and lost on reboot. Unit files come in many different types which can be identified by their filename extension (i.e. .service , target , etc.).","title":"Unit files"},{"location":"Linux/SystemD/#upstart","text":"Upstart was an init system developed by Canonical for Ubuntu meant to replace SysVinit , but it was abandoned in 2014.","title":"Upstart"},{"location":"Linux/SystemD/#commands","text":"","title":"Commands"},{"location":"Linux/SystemD/#hostnamectl","text":"Permanently change hostname hostnamectl set-hostname $HOSTNAME","title":"hostnamectl"},{"location":"Linux/SystemD/#journalctl","text":"Clean up old logs journalctl --disk-usage # (3) journalctl --rotate # (1) journalctl --vacuum-time = 1d # (2) Ask journal daemon to rotate journal files, immediately archiving and renaming currently active journal files. --vacuum-size , --vacuum-time , and --vacuum-files can be used singly or in combination to enforce limits on archived journal files. Show current disk usage of all journal files Display logs journalctl -r # --reverse (1) journalctl -f # --follow (2) Display output in reverse (newest entries first) Continuously update the display as new log entries are created By default, SystemD logs to memory. This can be changed by adjusting /etc/systemd/journald.conf . This requires the directory /var/log/journal to exist. Persistent logging [Journal] Storage = persistent","title":"journalctl"},{"location":"Linux/SystemD/#localectl","text":"Change locale to French localectl set-locale LANG = fr_FR.utf8","title":"localectl"},{"location":"Linux/SystemD/#loginctl","text":"Enable user lingering , which allows users that are not logged in to run long-running services. loginctl enable-linger loginctl show-user | grep Linger - # Confirm","title":"loginctl"},{"location":"Linux/SystemD/#systemctl","text":"Services systemctl list-unit-files --type = service # Display all services systemctl enable --now $SERVICE # Configure service to start on boot and start it immediately systemctl status $SERVICE systemctl is-active $SERVICE systemctl disable $SERVICE systemctl mask $SERVICE # Prevent service from being started inadvertently by another process systemctl restart $SERVICE Boot targets systemctl get-default systemctl set-default graphical.target systemctl isolate emergency.target # Change target systemctl suspend # Suspend system --user specifies the service manager of the calling user. systemctl --user enable --now container-notes.service # (1) systemctl --user status container-notes.service Here, container-notes.service has been created at ~/.config/systemd/user","title":"systemctl"},{"location":"Linux/SystemD/#systemd-analyze","text":"Check security of a service systemd-analyze security sshd.service","title":"systemd-analyze"},{"location":"Linux/SystemD/#systemd-cgls","text":"systemd-cgls recursively shows the contents of the selected cgroup hierarchy in a tree.","title":"systemd-cgls"},{"location":"Linux/SystemD/#systemd-delta","text":"Show files that are overridden with systemd. Display differences among files when they are overridden systemd-delta --diff","title":"systemd-delta"},{"location":"Linux/Virtualization/","text":"Virtualization The typical virtualization stack on Linux is referred to as QEMU / KVM ; both of these are separate technologies. Tasks Check CPU for virtualization support grep -E 'svm|vmx' /proc/cpuinfo # (1) AMD CPUs will have svm in the flags section, whereas Intel CPUs will have vmx . Virtual machine The easiest way to create a VM is with the Boxes GNOME Desktop Environment application or virt-manager. virt-install \\ --cdrom = /tmp/debian-9.0.0-amd64-netinst.iso \\ --vcpus = 1 --memory = 1024 --disk size = 5 \\ --os-variant = debian8 --name = linuxconfig-vm \\ Virtual networking Using Boxes , a new interface will be created (apparently named tap0 , etc) and slaved to a bridge, or a bridge will be created if none exist. Console access In order to enable console access to a domain, the serial-gettty@.service service must be started. systemctl enable --now serial-getty@ttyS0.service Now console access is available from the host virsh console $DOMAIN Note that the console will remain at 24 lines by 80 columns no matter if the terminal window is resized. Virtual disk CoW filesystems If the disk image is created on a filesystem that does not support O_DIRECT (ref. man 5 open ), i.e. COW filesystem like btrfs and ZFS, the cache must be disabled. This appears to be impossible on btrfs, so disk images must be created on partitions with non-CoW alternatives, like ext4 and xfs. A virtual disk can be created in various ways and in various formats (the sparse QCOW2 disk image format, associated with QEMU, is preferred). One way is to create the image using a utility like qemu-img . Create qcow2 disk image qemu-img create -f qcow2 disk0.qcow2 5G # (1) By omitting -f , qemu-img will create a RAW format file. Alternatively, and more circuitously, a volume can be created within a storage pool . virsh pool-define-as --name Disks --type dir --target /disk/Disks # (1) virsh pool-start Disks # (2) virsh vol-create-as Disks disk0.qcow2 10G --format qcow2 A pool is deleted by first making sure its contents are deleted (but not the containing folder, in the case of a directory-based storage pool) virsh pool-destroy Disks # Destroy the contents virsh pool-delete Disks # Delete directory virsh pool-undefine Disks # Delete resource Alternatively, activate pool on boot virsh pool-autostart Disks Regardless of the method, the disk image is then attached to the domain, whether or not it is running. virsh attach-disk rhel \\ --source /tmp/Disks/disk0.qcow2 \\ --target vdb --cache none \\ --driver qemu --subdriver qcow2 \\ --persistent # (1) --persistent is necessary for the disk to remain attached after a shutdown. If the domain is not running, --config is necessary. virsh attach-disk rhel \\ --source /tmp/Disks/disk0.qcow2 \\ --target vdb --cache none \\ --driver qemu --subdriver qcow2 \\ --config This can be reversed with the following command. virsh detach-disk rhel /tmp/Disks/disk0.qcow2 virhs detach-disk rhel vdb # Specifying target instead Snapshots Snapshots of VMs can be taken from within Boxes or via the command-line: virsh snapshot-create-as rhel --name \"Disks added\" These produce XML records that can then be viewed: note that the field that Boxes uses to identify each snapshot is actually \"description\". virsh snapshot-dumpxml rhel \"Disks added\" Snapshots can be renamed by changing the value of the name element in a text editor. virsh snapshot-edit rhel --snapshotname \"Disks added\" --rename Revert to a snapshot virsh snapshot-revert rhel --snapshotname Custom resolution Specify a custom resolution in a VM using KVM cvt 2560 1440 xrandr --newmode \"2560x1440_60.00\" 312 .25 2560 2752 3024 3488 1440 1443 1448 1493 -hsync +vsync xrandr --addmode Virtual-1 2560x1440_60.00 xrandr --output Virtual-1 --mode 2560x1440_60.0 On a Hyper-V VM, this method will not work, but a simple change to a line in /etc/default/grub will do the trick after running update-grub and restarting GRUB_CMDLINE_LINUX_DEFAULT = \"quiet splash video=hyperv_fb:1920x1080\" Commands virt-install virsh virsh is the main interface for managing libvirt guest domains . In virsh terminology, the word domain refers to a VM. virsh commands can be entered from the virsh shell or from the command-line. virsh list --all virsh start rhel virsh shutdown rhel virsh destroy rhel # Forcefully virsh undefine rhel virsh autostart rhel # (1) Start domain at boot Inspect a running domain virsh dominfo $DOMAIN virsh domiflist $DOMAIN virsh domblklist $DOMAIN virsh domrename $DOMAIN $NEWNAME ` Virsh uses XML documents to define domain resources. virsh dumpxml $DOMAIN virsh edit $DOMAIN Several virsh commands come in two varieties, one to handle XML documents that define resources and another to define them inline from the command-line. XML CLI Description pool-define pool-define-as Define a persistent storage pool vol-create vol-create-as Create a volume The commands ending in -as also support use of the --print-xml option, which will output the equivalent XML document to stdout. Glossary Boxes Boxes is a GNOME Desktop Environment application intended as a more user-friendly alternative to virt-manager to make virtualization easier for end-users. domain In libvirt, a domain is a guest VM. KVM KVM is a FreeBSD and Linux kernel module that allows the kernel to function as a hypervisor. KVM was first merged into kernel 2.6.20 . libvirt libvirt is an open-source API, daemon, and management tool for managing virtualization. It is a C toolkit to interact with the virtualization capabilities of the Linux KVM module , but it can also be used with KVM along with other virtualization technologies like QEMU . QEMU QEMU is an open-source emulator that interoperates with KVM to run VMs at near-native speed when the guest architecture is the same as that of the host. storage pool In libvirt, a storage pool is a file, directory, or storage device managed by libvirt to provide storage for domains .","title":"Virtualization"},{"location":"Linux/Virtualization/#virtualization","text":"The typical virtualization stack on Linux is referred to as QEMU / KVM ; both of these are separate technologies.","title":"Virtualization"},{"location":"Linux/Virtualization/#tasks","text":"","title":"Tasks"},{"location":"Linux/Virtualization/#check-cpu-for-virtualization-support","text":"grep -E 'svm|vmx' /proc/cpuinfo # (1) AMD CPUs will have svm in the flags section, whereas Intel CPUs will have vmx .","title":"Check CPU for virtualization support"},{"location":"Linux/Virtualization/#virtual-machine","text":"The easiest way to create a VM is with the Boxes GNOME Desktop Environment application or virt-manager. virt-install \\ --cdrom = /tmp/debian-9.0.0-amd64-netinst.iso \\ --vcpus = 1 --memory = 1024 --disk size = 5 \\ --os-variant = debian8 --name = linuxconfig-vm \\","title":"Virtual machine"},{"location":"Linux/Virtualization/#virtual-networking","text":"Using Boxes , a new interface will be created (apparently named tap0 , etc) and slaved to a bridge, or a bridge will be created if none exist.","title":"Virtual networking"},{"location":"Linux/Virtualization/#console-access","text":"In order to enable console access to a domain, the serial-gettty@.service service must be started. systemctl enable --now serial-getty@ttyS0.service Now console access is available from the host virsh console $DOMAIN Note that the console will remain at 24 lines by 80 columns no matter if the terminal window is resized.","title":"Console access"},{"location":"Linux/Virtualization/#virtual-disk","text":"CoW filesystems If the disk image is created on a filesystem that does not support O_DIRECT (ref. man 5 open ), i.e. COW filesystem like btrfs and ZFS, the cache must be disabled. This appears to be impossible on btrfs, so disk images must be created on partitions with non-CoW alternatives, like ext4 and xfs. A virtual disk can be created in various ways and in various formats (the sparse QCOW2 disk image format, associated with QEMU, is preferred). One way is to create the image using a utility like qemu-img . Create qcow2 disk image qemu-img create -f qcow2 disk0.qcow2 5G # (1) By omitting -f , qemu-img will create a RAW format file. Alternatively, and more circuitously, a volume can be created within a storage pool . virsh pool-define-as --name Disks --type dir --target /disk/Disks # (1) virsh pool-start Disks # (2) virsh vol-create-as Disks disk0.qcow2 10G --format qcow2 A pool is deleted by first making sure its contents are deleted (but not the containing folder, in the case of a directory-based storage pool) virsh pool-destroy Disks # Destroy the contents virsh pool-delete Disks # Delete directory virsh pool-undefine Disks # Delete resource Alternatively, activate pool on boot virsh pool-autostart Disks Regardless of the method, the disk image is then attached to the domain, whether or not it is running. virsh attach-disk rhel \\ --source /tmp/Disks/disk0.qcow2 \\ --target vdb --cache none \\ --driver qemu --subdriver qcow2 \\ --persistent # (1) --persistent is necessary for the disk to remain attached after a shutdown. If the domain is not running, --config is necessary. virsh attach-disk rhel \\ --source /tmp/Disks/disk0.qcow2 \\ --target vdb --cache none \\ --driver qemu --subdriver qcow2 \\ --config This can be reversed with the following command. virsh detach-disk rhel /tmp/Disks/disk0.qcow2 virhs detach-disk rhel vdb # Specifying target instead","title":"Virtual disk"},{"location":"Linux/Virtualization/#snapshots","text":"Snapshots of VMs can be taken from within Boxes or via the command-line: virsh snapshot-create-as rhel --name \"Disks added\" These produce XML records that can then be viewed: note that the field that Boxes uses to identify each snapshot is actually \"description\". virsh snapshot-dumpxml rhel \"Disks added\" Snapshots can be renamed by changing the value of the name element in a text editor. virsh snapshot-edit rhel --snapshotname \"Disks added\" --rename Revert to a snapshot virsh snapshot-revert rhel --snapshotname","title":"Snapshots"},{"location":"Linux/Virtualization/#custom-resolution","text":"Specify a custom resolution in a VM using KVM cvt 2560 1440 xrandr --newmode \"2560x1440_60.00\" 312 .25 2560 2752 3024 3488 1440 1443 1448 1493 -hsync +vsync xrandr --addmode Virtual-1 2560x1440_60.00 xrandr --output Virtual-1 --mode 2560x1440_60.0 On a Hyper-V VM, this method will not work, but a simple change to a line in /etc/default/grub will do the trick after running update-grub and restarting GRUB_CMDLINE_LINUX_DEFAULT = \"quiet splash video=hyperv_fb:1920x1080\"","title":"Custom resolution"},{"location":"Linux/Virtualization/#commands","text":"","title":"Commands"},{"location":"Linux/Virtualization/#virt-install","text":"","title":"virt-install"},{"location":"Linux/Virtualization/#virsh","text":"virsh is the main interface for managing libvirt guest domains . In virsh terminology, the word domain refers to a VM. virsh commands can be entered from the virsh shell or from the command-line. virsh list --all virsh start rhel virsh shutdown rhel virsh destroy rhel # Forcefully virsh undefine rhel virsh autostart rhel # (1) Start domain at boot Inspect a running domain virsh dominfo $DOMAIN virsh domiflist $DOMAIN virsh domblklist $DOMAIN virsh domrename $DOMAIN $NEWNAME ` Virsh uses XML documents to define domain resources. virsh dumpxml $DOMAIN virsh edit $DOMAIN Several virsh commands come in two varieties, one to handle XML documents that define resources and another to define them inline from the command-line. XML CLI Description pool-define pool-define-as Define a persistent storage pool vol-create vol-create-as Create a volume The commands ending in -as also support use of the --print-xml option, which will output the equivalent XML document to stdout.","title":"virsh"},{"location":"Linux/Virtualization/#glossary","text":"","title":"Glossary"},{"location":"Linux/Virtualization/#boxes","text":"Boxes is a GNOME Desktop Environment application intended as a more user-friendly alternative to virt-manager to make virtualization easier for end-users.","title":"Boxes"},{"location":"Linux/Virtualization/#domain","text":"In libvirt, a domain is a guest VM.","title":"domain"},{"location":"Linux/Virtualization/#kvm","text":"KVM is a FreeBSD and Linux kernel module that allows the kernel to function as a hypervisor. KVM was first merged into kernel 2.6.20 .","title":"KVM"},{"location":"Linux/Virtualization/#libvirt","text":"libvirt is an open-source API, daemon, and management tool for managing virtualization. It is a C toolkit to interact with the virtualization capabilities of the Linux KVM module , but it can also be used with KVM along with other virtualization technologies like QEMU .","title":"libvirt"},{"location":"Linux/Virtualization/#qemu","text":"QEMU is an open-source emulator that interoperates with KVM to run VMs at near-native speed when the guest architecture is the same as that of the host.","title":"QEMU"},{"location":"Linux/Virtualization/#storage-pool","text":"In libvirt, a storage pool is a file, directory, or storage device managed by libvirt to provide storage for domains .","title":"storage pool"},{"location":"Linux/Applications/bind/","text":"BIND apt install bind9 bind9-utils bind9-dnsutils -y Set BIND to IPv4 mode in the service parameters file: /etc/default/bind9 OPTIONS = \"-4 -u bind\" BIND configs have a unique syntax that make heavy use of the semicolon. The main config is at /etc/named.conf on Arch and RHEL systems and at /etc/bind/named.conf on Ubuntu. A DNS zone is a database with resource records for a specific sub-tree in the domain space. A DNS zone requires a start of authority (SOA) record . For readability, admins typically break the record apart into lines with comments describing each field following a semicolon. Representative SOA record @ IN SOA ns1.example.com. hostmaster.example.com. ( 2022070601 ; serial number 1d ; refresh period 3h ; retry period 3d ; expire time 3h ) ; minimum TTL Allow recursive queries from trusted clients /etc/bind/named.conf.options acl \"trusted\" { 10.128.10.11; # ns1 - can be set to localhost 10.128.20.12; # ns2 10.128.100.101; # host1 10.128.200.102; # host2 }; Allow recursion /etc/bind/named.conf.options options { directory \"/var/cache/bind\"; recursion yes; allow-recursion {trusted; }; listen-on { 0.0.0.0; }; allow-transfer { none; }; forwarders { 192.168.1.1; }; }; Now zone files can be specified in named.conf.local . An additional zone and zone file must be specified for every private subnet. /etc/bind/named.conf.local zone \"mydns\" { type master; file \"/etc/bind/zones/db.mydns\"; } The actual zone files can be copied from /etc/bind/db.local and edited manually. /etc/bind/zones/db.mydns","title":"BIND"},{"location":"Linux/Applications/bind/#bind","text":"apt install bind9 bind9-utils bind9-dnsutils -y Set BIND to IPv4 mode in the service parameters file: /etc/default/bind9 OPTIONS = \"-4 -u bind\" BIND configs have a unique syntax that make heavy use of the semicolon. The main config is at /etc/named.conf on Arch and RHEL systems and at /etc/bind/named.conf on Ubuntu. A DNS zone is a database with resource records for a specific sub-tree in the domain space. A DNS zone requires a start of authority (SOA) record . For readability, admins typically break the record apart into lines with comments describing each field following a semicolon. Representative SOA record @ IN SOA ns1.example.com. hostmaster.example.com. ( 2022070601 ; serial number 1d ; refresh period 3h ; retry period 3d ; expire time 3h ) ; minimum TTL Allow recursive queries from trusted clients /etc/bind/named.conf.options acl \"trusted\" { 10.128.10.11; # ns1 - can be set to localhost 10.128.20.12; # ns2 10.128.100.101; # host1 10.128.200.102; # host2 }; Allow recursion /etc/bind/named.conf.options options { directory \"/var/cache/bind\"; recursion yes; allow-recursion {trusted; }; listen-on { 0.0.0.0; }; allow-transfer { none; }; forwarders { 192.168.1.1; }; }; Now zone files can be specified in named.conf.local . An additional zone and zone file must be specified for every private subnet. /etc/bind/named.conf.local zone \"mydns\" { type master; file \"/etc/bind/zones/db.mydns\"; } The actual zone files can be copied from /etc/bind/db.local and edited manually. /etc/bind/zones/db.mydns","title":"BIND"},{"location":"Linux/Applications/ffmpeg/","text":"ffmpeg Convert format ffmpeg is most often used to convert file formats for media from the command-line Convert mp3 to m4a ffmpeg -i media. { mp3,m4a } Specify metadata Metadata is defined as key/value pairs, although not all formats support all metadata. This example adds metadata but does not reencode the input file. ffmpeg -i $INPUTFILE -metadata title = $TITLE -metadata year = $YEAR -codec copy $OUTPUTFILE Concatenating multiple files It is possible to combine many files into one. The canonical way of doing this is by first assembling a list of filenames. These must appear in a specific format : file 'file1.mp3' file 'file2.mp3' # etc... This can be done quickly by piping the output of ls to a file, then editing it manually. echo $( ls -1 *.mp3 ) > files Then this file can be used by ffmpeg, specifying the concat demuxer as the argument to -f ffmpeg -f concat -i files -c copy compilation.mp3 Chapters will be accepted with the right container (apparently not mp3). Note that mp3 files cannot be placed into a m4a container without re-encoding. Also note that the -map_metadata option must be specified after the second infile, because its argument refers to the second infile as if it were zero-indexed. ffmpeg -f concat -i files -i chapters -map_metadata 1 -c copy compilation.m4a","title":"ffmpeg"},{"location":"Linux/Applications/ffmpeg/#ffmpeg","text":"","title":"ffmpeg"},{"location":"Linux/Applications/ffmpeg/#convert-format","text":"ffmpeg is most often used to convert file formats for media from the command-line Convert mp3 to m4a ffmpeg -i media. { mp3,m4a }","title":"Convert format"},{"location":"Linux/Applications/ffmpeg/#specify-metadata","text":"Metadata is defined as key/value pairs, although not all formats support all metadata. This example adds metadata but does not reencode the input file. ffmpeg -i $INPUTFILE -metadata title = $TITLE -metadata year = $YEAR -codec copy $OUTPUTFILE","title":"Specify metadata"},{"location":"Linux/Applications/ffmpeg/#concatenating-multiple-files","text":"It is possible to combine many files into one. The canonical way of doing this is by first assembling a list of filenames. These must appear in a specific format : file 'file1.mp3' file 'file2.mp3' # etc... This can be done quickly by piping the output of ls to a file, then editing it manually. echo $( ls -1 *.mp3 ) > files Then this file can be used by ffmpeg, specifying the concat demuxer as the argument to -f ffmpeg -f concat -i files -c copy compilation.mp3 Chapters will be accepted with the right container (apparently not mp3). Note that mp3 files cannot be placed into a m4a container without re-encoding. Also note that the -map_metadata option must be specified after the second infile, because its argument refers to the second infile as if it were zero-indexed. ffmpeg -f concat -i files -i chapters -map_metadata 1 -c copy compilation.m4a","title":"Concatenating multiple files"},{"location":"Linux/Applications/gnome/","text":"GNOME GTK3 attempted to get away from strong dependency on theming engines by introducing CSS stylesheets. This was supposed to make application theming simple and portable. In GTK4 you can choose either a theming engine or CSS stylesheets. Keyring GNOME Keyring is a collection of components that store and manage application access to secrets, passwords, keys, and certificates. GNOME Keyring can be managed: Seahorse via GUI secret-tool which uses (and is included in) libsecret gnome-keyring-query which uses the archived libgnome-keyring Extensions GNOME Extensions provide a variety of popular hacks and changes to the Shell. They are managed by gnome-extensions-app but they are typically added from the GNOME Extensions website using a browser plugin. These extensions are added to ~/.local/share/gnome-shell/extensions , but many of them can also be made available to all users by installing them using a package manager, in which case they are placed in /usr/share/gnome-shell/extensions . Extensions appear to be mostly JavaScript applications, so they can probably simply be git cloned into the respective directories as well. Configuration dconf is a key-based blob database for storing GNOME configurations and application settings. These settings are stored as keys grouped under paths in a way analogous to the Windows Registry. dconf is also a CLI utility for reading and writing individual values or entire directories to and from a dconf database. Direct manipulation of dconf is discouraged, rather users and developers are encouraged to use dconf-editor or gsettings . GSettings is a high-level API for application settings that serves as the frontend for dconf as well as a CLI utility for changing user settings. A dconf profile is a list of binary dconf databases, typically stored at /etc/dconf/profile/user Here is a representative dconf profile. user is the name of the user database, typically found at ~/.config/dconf/ or /etc/dconf/profile/user and local and site refer to binary databases named as such in /etc/dconf/db/ . service-db:keyfile/user # (1) user-db:user system-db:local system-db:site This line sets the dconf keyfile backend , required when home directories are mounted over NFS Keyfiles are INI-format configs placed in directories like local.d/ that allow dconf settings to be specified declaratively. [org/gnome/desktop/input-sources] xkb-options = ['terminate:ctrl_alt_bksp', 'compose:ralt'] # (1) Equivalent to: gsettings set org.gnome.desktop.input-sources xkb-options =[ 'terminate:ctrl_alt_bksp' , 'compose:ralt' ] Tasks Desktop background Create a keyfile for the local database in /etc/dconf/db/local.d/01-background [org/gnome/desktop/background] picture-uri = 'file:///usr/local/share/backgrounds/wallpaper.jpg' picture-options = 'scaled' primary-color = '000000' secondary-color = 'FFFFFF' Custom application shortcut Custom shortcuts are stored in dconf using a \"relocatable schema\" which has three keys: name , command , and binding . gsettings set org.gnome.setting-daemon.plugins.media-keys.custom-keybinding:/org/gnome/settings-daemon/plugins/media-keys/custom-keybindings/custom0 name 'Terminal' gsettings set org.gnome.setting-daemon.plugins.media-keys.custom-keybinding:/org/gnome/settings-daemon/plugins/media-keys/custom-keybindings/custom0 binding '<Super>Enter' gsettings set org.gnome.setting-daemon.plugins.media-keys.custom-keybinding:/org/gnome/settings-daemon/plugins/media-keys/custom-keybindings/custom0 command '/usr/bin/gnome-terminal' Note that this doesn't seem to work... File associations File associations are stored in .desktop files stored in /usr/share/applications/ . These INI-format files store all kinds of metadata on installed applications, including names and keywords in all supported languages. Filetypes are stored under the MimeType key as semicolon-delimited MIME Types . [Desktop Entry] Type = Application MimeType = application/x-newtype Name = My Application 1 Exec = myapplication1 MIME Type descriptors as stored as XML files stored in /usr/share/mime/packages/ : <?xml version=\"1.0\" encoding=\"UTF-8\"?> <mime-info xmlns= \"http://www.freedesktop.org/standards/shared-mime-info\" > <mime-type type= \"application/x-newtype\" > <comment> new mime type </comment> <glob pattern= \"*.xyz\" /> </mime-type> </mime-info> Applications gio GIO (Gnome Input/Output) is a library that facilitates interaction with virtual file systems. Applications built with the GIO library can access GVFS mounts, which can have many backends. GIO commands appear to substitute for common POSIX commands and GNU utilities (e.g. gio cat , gio mkdir , gio rename , etc). Set custom GIO metadata # Read (empty) attribute of new file gio info -a 'metadata::*' /tmp/myfile # Create attribute gio set -t string /tmp/myfile 'metadata::mynote' 'Please remember to delete this file!' # Rename file gio move /tmp/myfile /tmp/newfile # Confirm that attribute still exists gio info -a 'metadata::*' /tmp/newfile # (1) gio info appears to have replaced the earlier gvfs-info and gvfs-mime utilities used to inspect registered MIME types . gsettings gsettings is the CLI frontend intended to support changes to GNOME application settings, stored in dconf databases. Examples # Change function of Caps Lock gsettings set org.gnome.desktop.input-sources xkb-options \"['caps:ctrl_modifier']\" # Change mouse cursor size to various sizes. This can also be done in GNOME as Settings > Accessibility gsettings set org.gnome.desktop.interface cursor-size 24 # (1) # Enable GTK Inspector gsettings set org.gtk.Settings.Debug enable-inspector-keybinding true # (2) Valid sizes include 24, 32, 48, 64, and 96 Can be run with Ctrl + Shift + D notify-send Used for displaying desktop notifications on GNOME Desktop Environment notify-send -i face-smile Hello \"Hello, World!\"","title":"GNOME"},{"location":"Linux/Applications/gnome/#gnome","text":"GTK3 attempted to get away from strong dependency on theming engines by introducing CSS stylesheets. This was supposed to make application theming simple and portable. In GTK4 you can choose either a theming engine or CSS stylesheets.","title":"GNOME"},{"location":"Linux/Applications/gnome/#keyring","text":"GNOME Keyring is a collection of components that store and manage application access to secrets, passwords, keys, and certificates. GNOME Keyring can be managed: Seahorse via GUI secret-tool which uses (and is included in) libsecret gnome-keyring-query which uses the archived libgnome-keyring","title":"Keyring"},{"location":"Linux/Applications/gnome/#extensions","text":"GNOME Extensions provide a variety of popular hacks and changes to the Shell. They are managed by gnome-extensions-app but they are typically added from the GNOME Extensions website using a browser plugin. These extensions are added to ~/.local/share/gnome-shell/extensions , but many of them can also be made available to all users by installing them using a package manager, in which case they are placed in /usr/share/gnome-shell/extensions . Extensions appear to be mostly JavaScript applications, so they can probably simply be git cloned into the respective directories as well.","title":"Extensions"},{"location":"Linux/Applications/gnome/#configuration","text":"dconf is a key-based blob database for storing GNOME configurations and application settings. These settings are stored as keys grouped under paths in a way analogous to the Windows Registry. dconf is also a CLI utility for reading and writing individual values or entire directories to and from a dconf database. Direct manipulation of dconf is discouraged, rather users and developers are encouraged to use dconf-editor or gsettings . GSettings is a high-level API for application settings that serves as the frontend for dconf as well as a CLI utility for changing user settings. A dconf profile is a list of binary dconf databases, typically stored at /etc/dconf/profile/user Here is a representative dconf profile. user is the name of the user database, typically found at ~/.config/dconf/ or /etc/dconf/profile/user and local and site refer to binary databases named as such in /etc/dconf/db/ . service-db:keyfile/user # (1) user-db:user system-db:local system-db:site This line sets the dconf keyfile backend , required when home directories are mounted over NFS Keyfiles are INI-format configs placed in directories like local.d/ that allow dconf settings to be specified declaratively. [org/gnome/desktop/input-sources] xkb-options = ['terminate:ctrl_alt_bksp', 'compose:ralt'] # (1) Equivalent to: gsettings set org.gnome.desktop.input-sources xkb-options =[ 'terminate:ctrl_alt_bksp' , 'compose:ralt' ]","title":"Configuration"},{"location":"Linux/Applications/gnome/#tasks","text":"","title":"Tasks"},{"location":"Linux/Applications/gnome/#desktop-background","text":"Create a keyfile for the local database in /etc/dconf/db/local.d/01-background [org/gnome/desktop/background] picture-uri = 'file:///usr/local/share/backgrounds/wallpaper.jpg' picture-options = 'scaled' primary-color = '000000' secondary-color = 'FFFFFF'","title":"Desktop background"},{"location":"Linux/Applications/gnome/#custom-application-shortcut","text":"Custom shortcuts are stored in dconf using a \"relocatable schema\" which has three keys: name , command , and binding . gsettings set org.gnome.setting-daemon.plugins.media-keys.custom-keybinding:/org/gnome/settings-daemon/plugins/media-keys/custom-keybindings/custom0 name 'Terminal' gsettings set org.gnome.setting-daemon.plugins.media-keys.custom-keybinding:/org/gnome/settings-daemon/plugins/media-keys/custom-keybindings/custom0 binding '<Super>Enter' gsettings set org.gnome.setting-daemon.plugins.media-keys.custom-keybinding:/org/gnome/settings-daemon/plugins/media-keys/custom-keybindings/custom0 command '/usr/bin/gnome-terminal' Note that this doesn't seem to work...","title":"Custom application shortcut"},{"location":"Linux/Applications/gnome/#file-associations","text":"File associations are stored in .desktop files stored in /usr/share/applications/ . These INI-format files store all kinds of metadata on installed applications, including names and keywords in all supported languages. Filetypes are stored under the MimeType key as semicolon-delimited MIME Types . [Desktop Entry] Type = Application MimeType = application/x-newtype Name = My Application 1 Exec = myapplication1 MIME Type descriptors as stored as XML files stored in /usr/share/mime/packages/ : <?xml version=\"1.0\" encoding=\"UTF-8\"?> <mime-info xmlns= \"http://www.freedesktop.org/standards/shared-mime-info\" > <mime-type type= \"application/x-newtype\" > <comment> new mime type </comment> <glob pattern= \"*.xyz\" /> </mime-type> </mime-info>","title":"File associations"},{"location":"Linux/Applications/gnome/#applications","text":"","title":"Applications"},{"location":"Linux/Applications/gnome/#gio","text":"GIO (Gnome Input/Output) is a library that facilitates interaction with virtual file systems. Applications built with the GIO library can access GVFS mounts, which can have many backends. GIO commands appear to substitute for common POSIX commands and GNU utilities (e.g. gio cat , gio mkdir , gio rename , etc). Set custom GIO metadata # Read (empty) attribute of new file gio info -a 'metadata::*' /tmp/myfile # Create attribute gio set -t string /tmp/myfile 'metadata::mynote' 'Please remember to delete this file!' # Rename file gio move /tmp/myfile /tmp/newfile # Confirm that attribute still exists gio info -a 'metadata::*' /tmp/newfile # (1) gio info appears to have replaced the earlier gvfs-info and gvfs-mime utilities used to inspect registered MIME types .","title":"gio"},{"location":"Linux/Applications/gnome/#gsettings","text":"gsettings is the CLI frontend intended to support changes to GNOME application settings, stored in dconf databases. Examples # Change function of Caps Lock gsettings set org.gnome.desktop.input-sources xkb-options \"['caps:ctrl_modifier']\" # Change mouse cursor size to various sizes. This can also be done in GNOME as Settings > Accessibility gsettings set org.gnome.desktop.interface cursor-size 24 # (1) # Enable GTK Inspector gsettings set org.gtk.Settings.Debug enable-inspector-keybinding true # (2) Valid sizes include 24, 32, 48, 64, and 96 Can be run with Ctrl + Shift + D","title":"gsettings"},{"location":"Linux/Applications/gnome/#notify-send","text":"Used for displaying desktop notifications on GNOME Desktop Environment notify-send -i face-smile Hello \"Hello, World!\"","title":"notify-send"},{"location":"Linux/Applications/nginx/","text":"Nginx Nginx (\"engine-x\") is described as an event-based reverse proxy server . This refers to the fact that it has an asynchronous architecture, unlike its competitors Apache and IIS which create a new blocking thread per connection. Nginx is much newer than Apache which started in 1995, although it has seen widespread adoption since 2008, growing mostly at Apache's expense. A typical and favored deployment is to place Nginx in the front-end and Apache in the back-end to combine the advantages of both platforms. Nginx follows the convention of even version numbers being stable and odd numbers being mainline or development. Red Hat Ubuntu # /etc/yum.repos.d/nginx.repo [ nginx ] name = nginx repo baseurl = http://nginx.org/packages/centos/7/ $basearch / gpgcheck = 0 enabled = 1 dnf install nginx # /etc/apt/sources.list deb http://nginx.org/packages/ubuntu/ trusty nginx deb-src http://nginx.org/packages/ubuntu/ trusty nginx curl -fsSL http://nginx.org/keys/nginx_signing.key apt-key add nginx_signing.key apt install nginx Depending on installation method and distribution, configurations can exist in various directories. A config can be explicitly specified at runtime with --conf-path / -c . This option also appears in the output of ps for the Nginx master process, which is one way of interrogating which config is being used for the current Nginx instance. Nginx can also be interrogated for its default config with -t Nginx config files contain directives : Simple directives like listen *:80; contain a name, multiple optional parameters, and a closing semicolon. Parameters themselves can pass a value after an equal sign, i.e. backlog=511 . Context directives (or simply \"contexts\", also \"block directives\") like events , http , and server wrap a group of other directives in a pair of braces and can be nested. Most simple directives can only be declared in specific contexts. There is also an implied main context which wraps all the contents of the file, and putting a simple directive into the main context means making it a top-level statemtn. Comments can be written using # Examples A very simple representative config that creates an HTTP server listening on port 80 of every network interface, with no HTTP Host specified, from the specified root path: Default events { } http { server { } } Expanded with explicit values user nobody nogroup ; worker_processes 1 ; events { worker_connections 512 ; } http { server { listen *:80 ; server_name \"\" ; root /usr/share/nginx/html ; } } http { server { listen 8080 ; root /www ; location /images { root / ; } } } events { } nginx -s stop nginx -s start nginx restart Reverse proxy Each Nginx virtual server should be described by a file in the /etc/nginx/sites-available directory. These are linked to by symlinks placed in /etc/nginx/sites-enabled . Configuring a reverse proxy involves associating routes to proxied servers in these virtual server configs. server { listen 80 ; location / { proxy_pass \"http://127.0.0.1:8000\" ; } } The configuration to serve static files placed in the local directory /path/to/staticfiles from the URL /static is: location /static/ { root /path/to/staticfiles/ } Load balancer A load balancer is similar to a reverse proxy, with the following differences. Load balancers perform reverse proxy across many backends, rather than a single one Load balancers operate at either Layer 7 or Layer 4, whereas a reverse proxy operates only at Level 7 Load balancers are expected to handle much higher scale. Load balancers themselves tend to be load balanced by DNS servers, which can serve multiple A records to clients which are supposed to choose one of the IP addresses at random. Some DNS providers like AWS Route 53 randomize the order of these records per query. http { upstream backend { server 192 .0.2.10 ; server 192 .0.2.11 ; } server { listen 80 ; location / { proxy_pass http://backend ; } } } \ud83d\udcd8 Glossary default server block First server block defined in the config events Context that governs connection processing configuration. Events can only be declared in the main context and there can only be a single events defined within the configuration. http Context that allows configuration of HTTP servers and typically serves as a container for the server context. Directives that are intended to apply to multiple server contexts are typically placed within the http context. listen Simple directive in the server context that configures the network interfaces and ports to listen on for requests. server { listen *:80 ; listen *:81 ; } location Context directive used when there is not a 1:1 mapping between path of the HTTP request and the filesystem. Prefix location blocks , the most basic implementation, allow various root directories to be specified depending on the request path. Various modifiers can be used to modify how the request path is matched. When resolving paths, the most specific match is used. Location blocks in order of precedence: Exact match modifier ( = ) Order matters with regex blocks because the first match will be used. Non-regex prefix ( ^~ ) overrides any regex match Case-sensitive regex ( ~ ) Case-insensitive regex ( ~* ) Prefix (no modifier) location /images/ { root /var/www/images ; } location = /images/business_cat.gif { # ... } location ~ \\.(gif|jpg)$ { # ... } location ~ * \\.(GIF|JPG) $ { # ... } location ^~ /foobar/images { root /var/www/foobar ; } server (context directive) http-context context directive. server (simple directive) upstream -context directive that specifies a backend node. Additional parameters can specify load-balancing behavior. - **weight** controls weighting of backend nodes (default value is 1) - **max_fails** controls the number of times that the server can be marked as unhealthy before it is removed from the pool - **fail_timeout** controls the time a server is removed from the pool when it is marked unhealthy, and for how long `max_fails` is good for server_name Server context simple directive that enables virtual hosting. Values of this directive are matched to the HTTP GET request header's Host . If no matches are found, nginx uses the default server block . upstream http-context simple directive that can, using the server simple directive, specify a pool of backend server. upstream backend { server 192.0.2.10 : 443 weight=3 ; server app1.example.com ; server unix:/u/apps/my_app/current/tmp/unicorn.sock ; } Each server directive can additionally user Main context simple directive that sets the Unix user and group that nginx worker processes will run as; by default nobody and the group is nogroup . worker process The non-root nginx process that serves incoming HTTP requests. Each worker process is single-threaded and runs a non-blocking event loop to process requests efficiently. worker_connections Main context simple directive that sets the maximum number of simultaneous connections that can be opened by each worker process . worker_processes Main context simple directive that sets the number of worker processes to serve HTTP requests (1 by default).","title":"Nginx"},{"location":"Linux/Applications/nginx/#nginx","text":"Nginx (\"engine-x\") is described as an event-based reverse proxy server . This refers to the fact that it has an asynchronous architecture, unlike its competitors Apache and IIS which create a new blocking thread per connection. Nginx is much newer than Apache which started in 1995, although it has seen widespread adoption since 2008, growing mostly at Apache's expense. A typical and favored deployment is to place Nginx in the front-end and Apache in the back-end to combine the advantages of both platforms. Nginx follows the convention of even version numbers being stable and odd numbers being mainline or development. Red Hat Ubuntu # /etc/yum.repos.d/nginx.repo [ nginx ] name = nginx repo baseurl = http://nginx.org/packages/centos/7/ $basearch / gpgcheck = 0 enabled = 1 dnf install nginx # /etc/apt/sources.list deb http://nginx.org/packages/ubuntu/ trusty nginx deb-src http://nginx.org/packages/ubuntu/ trusty nginx curl -fsSL http://nginx.org/keys/nginx_signing.key apt-key add nginx_signing.key apt install nginx Depending on installation method and distribution, configurations can exist in various directories. A config can be explicitly specified at runtime with --conf-path / -c . This option also appears in the output of ps for the Nginx master process, which is one way of interrogating which config is being used for the current Nginx instance. Nginx can also be interrogated for its default config with -t Nginx config files contain directives : Simple directives like listen *:80; contain a name, multiple optional parameters, and a closing semicolon. Parameters themselves can pass a value after an equal sign, i.e. backlog=511 . Context directives (or simply \"contexts\", also \"block directives\") like events , http , and server wrap a group of other directives in a pair of braces and can be nested. Most simple directives can only be declared in specific contexts. There is also an implied main context which wraps all the contents of the file, and putting a simple directive into the main context means making it a top-level statemtn. Comments can be written using #","title":"Nginx"},{"location":"Linux/Applications/nginx/#examples","text":"A very simple representative config that creates an HTTP server listening on port 80 of every network interface, with no HTTP Host specified, from the specified root path: Default events { } http { server { } } Expanded with explicit values user nobody nogroup ; worker_processes 1 ; events { worker_connections 512 ; } http { server { listen *:80 ; server_name \"\" ; root /usr/share/nginx/html ; } } http { server { listen 8080 ; root /www ; location /images { root / ; } } } events { } nginx -s stop nginx -s start nginx restart","title":"Examples"},{"location":"Linux/Applications/nginx/#reverse-proxy","text":"Each Nginx virtual server should be described by a file in the /etc/nginx/sites-available directory. These are linked to by symlinks placed in /etc/nginx/sites-enabled . Configuring a reverse proxy involves associating routes to proxied servers in these virtual server configs. server { listen 80 ; location / { proxy_pass \"http://127.0.0.1:8000\" ; } } The configuration to serve static files placed in the local directory /path/to/staticfiles from the URL /static is: location /static/ { root /path/to/staticfiles/ }","title":"Reverse proxy"},{"location":"Linux/Applications/nginx/#load-balancer","text":"A load balancer is similar to a reverse proxy, with the following differences. Load balancers perform reverse proxy across many backends, rather than a single one Load balancers operate at either Layer 7 or Layer 4, whereas a reverse proxy operates only at Level 7 Load balancers are expected to handle much higher scale. Load balancers themselves tend to be load balanced by DNS servers, which can serve multiple A records to clients which are supposed to choose one of the IP addresses at random. Some DNS providers like AWS Route 53 randomize the order of these records per query. http { upstream backend { server 192 .0.2.10 ; server 192 .0.2.11 ; } server { listen 80 ; location / { proxy_pass http://backend ; } } }","title":"Load balancer"},{"location":"Linux/Applications/nginx/#glossary","text":"default server block First server block defined in the config events Context that governs connection processing configuration. Events can only be declared in the main context and there can only be a single events defined within the configuration. http Context that allows configuration of HTTP servers and typically serves as a container for the server context. Directives that are intended to apply to multiple server contexts are typically placed within the http context. listen Simple directive in the server context that configures the network interfaces and ports to listen on for requests. server { listen *:80 ; listen *:81 ; } location Context directive used when there is not a 1:1 mapping between path of the HTTP request and the filesystem. Prefix location blocks , the most basic implementation, allow various root directories to be specified depending on the request path. Various modifiers can be used to modify how the request path is matched. When resolving paths, the most specific match is used. Location blocks in order of precedence: Exact match modifier ( = ) Order matters with regex blocks because the first match will be used. Non-regex prefix ( ^~ ) overrides any regex match Case-sensitive regex ( ~ ) Case-insensitive regex ( ~* ) Prefix (no modifier) location /images/ { root /var/www/images ; } location = /images/business_cat.gif { # ... } location ~ \\.(gif|jpg)$ { # ... } location ~ * \\.(GIF|JPG) $ { # ... } location ^~ /foobar/images { root /var/www/foobar ; } server (context directive) http-context context directive. server (simple directive) upstream -context directive that specifies a backend node. Additional parameters can specify load-balancing behavior. - **weight** controls weighting of backend nodes (default value is 1) - **max_fails** controls the number of times that the server can be marked as unhealthy before it is removed from the pool - **fail_timeout** controls the time a server is removed from the pool when it is marked unhealthy, and for how long `max_fails` is good for server_name Server context simple directive that enables virtual hosting. Values of this directive are matched to the HTTP GET request header's Host . If no matches are found, nginx uses the default server block . upstream http-context simple directive that can, using the server simple directive, specify a pool of backend server. upstream backend { server 192.0.2.10 : 443 weight=3 ; server app1.example.com ; server unix:/u/apps/my_app/current/tmp/unicorn.sock ; } Each server directive can additionally user Main context simple directive that sets the Unix user and group that nginx worker processes will run as; by default nobody and the group is nogroup . worker process The non-root nginx process that serves incoming HTTP requests. Each worker process is single-threaded and runs a non-blocking event loop to process requests efficiently. worker_connections Main context simple directive that sets the maximum number of simultaneous connections that can be opened by each worker process . worker_processes Main context simple directive that sets the number of worker processes to serve HTTP requests (1 by default).","title":"\ud83d\udcd8 Glossary"},{"location":"Linux/Applications/rhythmbox/","text":"Rhythmbox Rhythmbox's database is an XML file located at $HOME/.local/share/rhythmbox/rhythmdb.xml <?xml version=\"1.0\" standalone=\"yes\"?> <rhythmdb version= \"2.0\" > <entry type= \"song\" > <title/> <genre/> <artist/> <album/> <location> file:///Music/... </location> <!-- snip --> </entry> </rhythmdb>","title":"Rhythmbox"},{"location":"Linux/Applications/rhythmbox/#rhythmbox","text":"Rhythmbox's database is an XML file located at $HOME/.local/share/rhythmbox/rhythmdb.xml <?xml version=\"1.0\" standalone=\"yes\"?> <rhythmdb version= \"2.0\" > <entry type= \"song\" > <title/> <genre/> <artist/> <album/> <location> file:///Music/... </location> <!-- snip --> </entry> </rhythmdb>","title":"Rhythmbox"},{"location":"Linux/Applications/screen/","text":"screen","title":"screen"},{"location":"Linux/Applications/screen/#screen","text":"","title":"screen"},{"location":"Linux/Applications/selinux/","text":"SELinux Info SELinux (Arch Linux Wiki) SELinux implements Mandatory Access Control (MAC) in Linux, which is distinguished from traditional Linux access controls (file permission octets, the use of sudo, etc) which constitute Discretionary Access Control (DAC) . SELinux's config is at /etc/selinux/config Example config # This file controls the state of SELinux on the system. # SELINUX= can take one of these three values: # enforcing - SELinux security policy is enforced. # permissive - SELinux prints warnings instead of enforcing. # disabled - No SELinux policy is loaded. SELINUX = enforcing # SELINUXTYPE= can take one of these three values: # targeted - Targeted processes are protected, # minimum - Modification of targeted policy. Only selected processes are protected. # mls - Multi Level Security protection. SELINUXTYPE = targeted Contexts SELinux security contexts define access controls and are also referred to as \"labels\". All system objects have such contexts associated with them, stored in the extended attributes of the filesystem. Contexts are compared to subject , verb , and object in English sentences and have the following structure: user_u:role_r:type_t:level The user or user identity can be associated with one or more roles. User identities are suffixed with _u , and there a eight such identities builtin. By default, all non-root users are mapped to unconfined_u as is root itself, which means they operate with unlimited privileges. Users labeled with user_u cannot run su or sudo or programs in their home directories. The role is an attribute of the RBAC security model that classifies who is allowed to access what (domains, types). It can be associated to one or more types and is suffixed with _r . The type (for file contexts) or domain (for process contexts) defines what processes or domains the user can access. Types are suffixed with _t . Certain files have their own types, like passwd_file_t which is associated with /etc/passwd. Builtin and user-created file contexts are stored in the file_contexts and file_contexts.local files under /etc/selinux/targeted/contexts/files/ . A level is an attribute of Multi-Level Security and Multi-Category Security. SELinux extends existing utilities to handle contexts with the -Z flag: ps auxZ ls -Z id -Z Booleans In SELinux, booleans refers to optional settings that can be turned on and off. Tasks Samba share Enabling a Samba file share requires setting a specific SELinux context using semanage semanage fcontext -a -t samba_share_t '/samba(/.*)?' The context must then be restored restorecon -vvFR /samba Troubleshooting Apache In this scenario, an Apache httpd daemon fails to start due to SELinux. SELinux provides recommended commands to resolve the issue in the audit log at /var/log/messages : grep httpd /var/log/messages | less Generate and install a new policy module from the logs ausearch -c httpd --raw | audit2allow -M my-httpd semodule -i my-httpd Apache home directories A feature of Apache is that users can host personal websites from the directory named public_html in their home directories. When Apache policies are in effect, this directory is automatically given the http_user_content_t tag, which will allow the httpd daemon to host the website at the path /~user where \"user\" is the name of the user. curl localhost/~user/index.html However, without a specific boolean enabled, the files will not be accessible: setsebool -P httpd_enable_homedirs 1 Apache port Default Apache settings appear in the main config file located at /etc/httpd/conf/httpd.conf . For example, the default directory served by Apache can be changed by setting a new value for the DocumentRoot directive. /etc/httpd/conf/httpd.conf DocumentRoot \"/web\" # ... <Directory \"/web\"> Create example content echo \"Hello, World!\" > /web/index.html The context for the content must be set. # Set context manually chcon -R /web -t http_content_t # Alternatively, set policy and restore the contexts semanage -a -t httpd_sys_content_t '/web(/.*)?' restorecon -R /web The default port can also be changed with the Listen directive. /etc/httpd/conf/httpd.conf Listen 1000 Now Apache will attempt to serve /web from port 1000, however while SELinux is enforcing policy this port will not be accessible to the daemon, and in fact Apache will exit with an error code. These errors can be inspected in a variety of ways. journalctl -xe ausearch -m AVC -ts recent # Find SELinux message IDs, which provide remediation tips: grep sealert /var/log/messages sealert -l $MESSAGE_ID Add port context semanage port -a -t http_port_t -p tcp 1000 Now starting httpd succeeds, and we can confirm that port 1000 is open: ss -nlt Commands SELinux commands are separated into get/set varieties similar to PowerShell cmdlets: get set object getenforce setenforce Operating mode getsebool setsebool Booleans Similarly, restore change object restorecon chcon Contexts audit2allow Generate policy module from logs of denied operations ausearch -c httpd --raw | audit2allow -M my-httpd ausearch Display events in a date range ausearch --start $STARTDATE --end $ENDDATE Search events for today for logins of UID 500 ausearch --start today --loginuid 500 Search for events associated with an executable. ausearch -c httpd --raw Display recent (10 minutes) events ausearch -m AVC -ts recent chcon Change context of a file to be hosted via httpd chcon system_u:object_r:httpd_sys_content_t:s0 index.html chcon -t httpd_sys_content_t index.html # (1) Change only the type portion of the context. getenforce getenforce displays the operating mode of SELinux, which can be one of three values: enforcing permissive disabled getsebool Display all booleans getsebool -a # (1) A more descriptive listing of the booleans can be displayed with semanage semanage bool -l restorecon Restore security context policy restorecon -R /web seinfo List users seinfo -u semanage semanage is used to configure certain elements of SELinux policy without requiring modification to or recompilation from policy sources. File contexts semanage fcontext -l semanage fcontext -a -t httpd_sys_content_t /web Port contexts semanage port -l semanage port -a -t http_port_t -p tcp 1000 Booleans semanage bool -l Examine the mapping between Linux login names and SELinux users. semanage login -l semodule Install a policy module semodule -i my-httpd.pp sestatus sestatus setenforce setenforce 0 # Permissive setenforce 1 # Enforcing setsebool # Allow SELinux to work with Samba (-P makes the change persistent) setsebool -P samba_export_all_ro 1 # Allow httpd to serve HTML from home directories setsebool -P httpd_enable_homedirs 1 # Prevent runtime changes to SELinux mode setsebool -P secure_mode_policyload 1","title":"SELinux"},{"location":"Linux/Applications/selinux/#selinux","text":"Info SELinux (Arch Linux Wiki) SELinux implements Mandatory Access Control (MAC) in Linux, which is distinguished from traditional Linux access controls (file permission octets, the use of sudo, etc) which constitute Discretionary Access Control (DAC) . SELinux's config is at /etc/selinux/config Example config # This file controls the state of SELinux on the system. # SELINUX= can take one of these three values: # enforcing - SELinux security policy is enforced. # permissive - SELinux prints warnings instead of enforcing. # disabled - No SELinux policy is loaded. SELINUX = enforcing # SELINUXTYPE= can take one of these three values: # targeted - Targeted processes are protected, # minimum - Modification of targeted policy. Only selected processes are protected. # mls - Multi Level Security protection. SELINUXTYPE = targeted","title":"SELinux"},{"location":"Linux/Applications/selinux/#contexts","text":"SELinux security contexts define access controls and are also referred to as \"labels\". All system objects have such contexts associated with them, stored in the extended attributes of the filesystem. Contexts are compared to subject , verb , and object in English sentences and have the following structure: user_u:role_r:type_t:level The user or user identity can be associated with one or more roles. User identities are suffixed with _u , and there a eight such identities builtin. By default, all non-root users are mapped to unconfined_u as is root itself, which means they operate with unlimited privileges. Users labeled with user_u cannot run su or sudo or programs in their home directories. The role is an attribute of the RBAC security model that classifies who is allowed to access what (domains, types). It can be associated to one or more types and is suffixed with _r . The type (for file contexts) or domain (for process contexts) defines what processes or domains the user can access. Types are suffixed with _t . Certain files have their own types, like passwd_file_t which is associated with /etc/passwd. Builtin and user-created file contexts are stored in the file_contexts and file_contexts.local files under /etc/selinux/targeted/contexts/files/ . A level is an attribute of Multi-Level Security and Multi-Category Security. SELinux extends existing utilities to handle contexts with the -Z flag: ps auxZ ls -Z id -Z","title":"Contexts"},{"location":"Linux/Applications/selinux/#booleans_1","text":"In SELinux, booleans refers to optional settings that can be turned on and off.","title":"Booleans"},{"location":"Linux/Applications/selinux/#tasks","text":"","title":"Tasks"},{"location":"Linux/Applications/selinux/#samba-share","text":"Enabling a Samba file share requires setting a specific SELinux context using semanage semanage fcontext -a -t samba_share_t '/samba(/.*)?' The context must then be restored restorecon -vvFR /samba","title":"Samba share"},{"location":"Linux/Applications/selinux/#troubleshooting-apache","text":"In this scenario, an Apache httpd daemon fails to start due to SELinux. SELinux provides recommended commands to resolve the issue in the audit log at /var/log/messages : grep httpd /var/log/messages | less Generate and install a new policy module from the logs ausearch -c httpd --raw | audit2allow -M my-httpd semodule -i my-httpd","title":"Troubleshooting Apache"},{"location":"Linux/Applications/selinux/#apache-home-directories","text":"A feature of Apache is that users can host personal websites from the directory named public_html in their home directories. When Apache policies are in effect, this directory is automatically given the http_user_content_t tag, which will allow the httpd daemon to host the website at the path /~user where \"user\" is the name of the user. curl localhost/~user/index.html However, without a specific boolean enabled, the files will not be accessible: setsebool -P httpd_enable_homedirs 1","title":"Apache home directories"},{"location":"Linux/Applications/selinux/#apache-port","text":"Default Apache settings appear in the main config file located at /etc/httpd/conf/httpd.conf . For example, the default directory served by Apache can be changed by setting a new value for the DocumentRoot directive. /etc/httpd/conf/httpd.conf DocumentRoot \"/web\" # ... <Directory \"/web\"> Create example content echo \"Hello, World!\" > /web/index.html The context for the content must be set. # Set context manually chcon -R /web -t http_content_t # Alternatively, set policy and restore the contexts semanage -a -t httpd_sys_content_t '/web(/.*)?' restorecon -R /web The default port can also be changed with the Listen directive. /etc/httpd/conf/httpd.conf Listen 1000 Now Apache will attempt to serve /web from port 1000, however while SELinux is enforcing policy this port will not be accessible to the daemon, and in fact Apache will exit with an error code. These errors can be inspected in a variety of ways. journalctl -xe ausearch -m AVC -ts recent # Find SELinux message IDs, which provide remediation tips: grep sealert /var/log/messages sealert -l $MESSAGE_ID Add port context semanage port -a -t http_port_t -p tcp 1000 Now starting httpd succeeds, and we can confirm that port 1000 is open: ss -nlt","title":"Apache port"},{"location":"Linux/Applications/selinux/#commands","text":"SELinux commands are separated into get/set varieties similar to PowerShell cmdlets: get set object getenforce setenforce Operating mode getsebool setsebool Booleans Similarly, restore change object restorecon chcon Contexts","title":"Commands"},{"location":"Linux/Applications/selinux/#audit2allow","text":"Generate policy module from logs of denied operations ausearch -c httpd --raw | audit2allow -M my-httpd","title":"audit2allow"},{"location":"Linux/Applications/selinux/#ausearch","text":"Display events in a date range ausearch --start $STARTDATE --end $ENDDATE Search events for today for logins of UID 500 ausearch --start today --loginuid 500 Search for events associated with an executable. ausearch -c httpd --raw Display recent (10 minutes) events ausearch -m AVC -ts recent","title":"ausearch"},{"location":"Linux/Applications/selinux/#chcon","text":"Change context of a file to be hosted via httpd chcon system_u:object_r:httpd_sys_content_t:s0 index.html chcon -t httpd_sys_content_t index.html # (1) Change only the type portion of the context.","title":"chcon"},{"location":"Linux/Applications/selinux/#getenforce","text":"getenforce displays the operating mode of SELinux, which can be one of three values: enforcing permissive disabled","title":"getenforce"},{"location":"Linux/Applications/selinux/#getsebool","text":"Display all booleans getsebool -a # (1) A more descriptive listing of the booleans can be displayed with semanage semanage bool -l","title":"getsebool"},{"location":"Linux/Applications/selinux/#restorecon","text":"Restore security context policy restorecon -R /web","title":"restorecon"},{"location":"Linux/Applications/selinux/#seinfo","text":"List users seinfo -u","title":"seinfo"},{"location":"Linux/Applications/selinux/#semanage","text":"semanage is used to configure certain elements of SELinux policy without requiring modification to or recompilation from policy sources. File contexts semanage fcontext -l semanage fcontext -a -t httpd_sys_content_t /web Port contexts semanage port -l semanage port -a -t http_port_t -p tcp 1000 Booleans semanage bool -l Examine the mapping between Linux login names and SELinux users. semanage login -l","title":"semanage"},{"location":"Linux/Applications/selinux/#semodule","text":"Install a policy module semodule -i my-httpd.pp","title":"semodule"},{"location":"Linux/Applications/selinux/#sestatus","text":"sestatus","title":"sestatus"},{"location":"Linux/Applications/selinux/#setenforce","text":"setenforce 0 # Permissive setenforce 1 # Enforcing","title":"setenforce"},{"location":"Linux/Applications/selinux/#setsebool","text":"# Allow SELinux to work with Samba (-P makes the change persistent) setsebool -P samba_export_all_ro 1 # Allow httpd to serve HTML from home directories setsebool -P httpd_enable_homedirs 1 # Prevent runtime changes to SELinux mode setsebool -P secure_mode_policyload 1","title":"setsebool"},{"location":"Linux/Applications/ssh/","text":"SSH SSH is a secure protocol and the most common way of remotely administering Linux servers and other equipment. SSH uses symmetrical encryption , which means a single key encrypts outbound messages to and inbound messages from the other participant. The SSH session is established in two stages: Negotiate session key Authenticate the user The symmetrical key used for the session, called the session key , is negotiated through the asymmetrical Diffie-Hellman key exchange protocol. This algorithm combines private data with public data from the other participant to produce the identical, session key, and the encryption used for the rest of the connection is called binary packet protocol . The simplest and least secure method of authentication is password-based. Although the password is sent through the encryption, it is still considered vulnerable to brute-force attacks. SSH key pairs , which are asymmetric, are recommended. These are what is generated by ssh-keygen , and stored in $HOME/.ssh with names that reflect the encryption algorithm, i.e. \"id_rsa\" and \"id_rsa.pub\", etc. The public key, used to encrypt data for the private key, can be freely shared. In fact, this is the purpose of ssh-copy-id , to share the public key. However the private key, which is used for decryption, must be kept secret. The client sends an ID for the key pair it wants to authenticate with to the server. The server then checks the authorized_keys file of the requested account for the ID. A random number is generated by the server, encrypted with the public key, and sent to the client. The client then decrypts the random number and combines it with the shared session key and calculates the MD5 hash. This hash is then sent back to the server, which checks the calculation. Note that the SSH server is named openssh-server in Ubuntu repos and the service is named ssh , as opposed to sshd on Red Hat systems. Client config Host home HostName 192.168.1.1 User root Port 50022 SetEnv BAT_THEME=OneHalfLight # (1) LocalForward 8080 localhost:8080 # (2) SetEnv allows environment variables to be set in a remote session. However, these environment variables must be explicitly specified in the server's sshd_config file. This entry will set a specific syntax highlighting theme for use on the bat CLI utility. AllowEnv BAT_THEME This is equivalent to the following command: ssh -L 8080 :localhost:8080 $SERVER /etc/sshd_config /etc/ssh/sshd_config is the configuration for the SSH server daemon. Disable cleartext passwords PasswordAuthentication no Disable root login PermitRootLogin no Tasks Port forwarding Port forwarding is accomplished in one of two ways, local or remote with respect to the server not the client. Local ( -L ): connections to the client are forwarded through the SSH tunnel to the SSH server. This technique is used to provide functionality similar to a VPN, where remote access is made possible to content on a private network, such as file shares or web applications that are not exposed publicly. Remote ( -R )... Dynamic Here, a private web application served locally on ssh-server will be served on the client at the same port. The first \"localhost\" can actually be omitted, since the connection will be exposed on localhost host by default and is almost universally. The confusing part is the second \"localhost\", because that is actually in reference to the ssh server itself. ssh -L localhost:80:localhost:80 ssh-server It is actually possible to forward a request to another host on ssh-server's network, creating a jump box. Here a connection on the client's localhost:81 is forwarded to ssh-server, which then sends it to 192.168.1.1:80. ssh -L 81 :192.168.1.1:80 ssh-server X forwarding ssh -Y user@host Have remote system use local computer {me.luna.edu}'s X display export DISPLAY = me.luna.edu:0 SSH to a transient server To prevent recording an unimportant server to the client's known hosts file, change UserKnownHostsFile to \"/dev/null\". To suppress the warning about the unfamiliar address, change StrictHostKeyChecking to \"no\". ssh user@host -o \"UserKnownHostsFile=/dev/null\" -o \"StrictHostKeyChecking=no\" Alternatively, place these settings in a config, with an appropriate IP address range. fail2ban Fail2ban is an intrusion prevention framework written in Python and that runs as a service. It can be installed from most distributions' repos. The jail is a key concept in f2b that couples filters and actions definitions. Fail2ban is configured through .ini-format configs found in /etc/fail2ban . It is recommended not to edit the default configs ending in .config but rather to create a custom config called jail.local which will be automatically loaded by the service. Example jail [sshd] enabled = true port = ssh filter = sshd logpath = /var/log/auth.log maxretry = 0 findtime = 300 bantime = 3600 Failed logins can be checked by running lastb , and connections are also logged to SystemD. journalctl -ru sshd Display banned IPs fail2ban-client banned Ban IP address manually fail2ban-client set sshd banip $IPADDRESS Commands endlessh Log verbosity # Silent endlessh # Normal endlessh -v # Debug endlessh -vv Log verbosity LogLevel 0 # silent LogLevel 1 # normal LogLevel 2 # debug ssh-agent eval $( ssh-agent ) ssh-copy-id This command copies the SSH public key to a specified account's ~/.ssh/authorized_keys file. In Windows, this command is not available, so a workaround is to simply pipe the public key over SSH itself. type $env :USERPROFILE \\. ssh \\i d_rsa.pub | ssh { IP-ADDRESS-OR-FQDN } \"cat >> .ssh/authorized_keys\" ssh-keygen Generate host keys sudo ssh-keygen -A","title":"SSH"},{"location":"Linux/Applications/ssh/#ssh","text":"SSH is a secure protocol and the most common way of remotely administering Linux servers and other equipment. SSH uses symmetrical encryption , which means a single key encrypts outbound messages to and inbound messages from the other participant. The SSH session is established in two stages: Negotiate session key Authenticate the user The symmetrical key used for the session, called the session key , is negotiated through the asymmetrical Diffie-Hellman key exchange protocol. This algorithm combines private data with public data from the other participant to produce the identical, session key, and the encryption used for the rest of the connection is called binary packet protocol . The simplest and least secure method of authentication is password-based. Although the password is sent through the encryption, it is still considered vulnerable to brute-force attacks. SSH key pairs , which are asymmetric, are recommended. These are what is generated by ssh-keygen , and stored in $HOME/.ssh with names that reflect the encryption algorithm, i.e. \"id_rsa\" and \"id_rsa.pub\", etc. The public key, used to encrypt data for the private key, can be freely shared. In fact, this is the purpose of ssh-copy-id , to share the public key. However the private key, which is used for decryption, must be kept secret. The client sends an ID for the key pair it wants to authenticate with to the server. The server then checks the authorized_keys file of the requested account for the ID. A random number is generated by the server, encrypted with the public key, and sent to the client. The client then decrypts the random number and combines it with the shared session key and calculates the MD5 hash. This hash is then sent back to the server, which checks the calculation. Note that the SSH server is named openssh-server in Ubuntu repos and the service is named ssh , as opposed to sshd on Red Hat systems.","title":"SSH"},{"location":"Linux/Applications/ssh/#client-config","text":"Host home HostName 192.168.1.1 User root Port 50022 SetEnv BAT_THEME=OneHalfLight # (1) LocalForward 8080 localhost:8080 # (2) SetEnv allows environment variables to be set in a remote session. However, these environment variables must be explicitly specified in the server's sshd_config file. This entry will set a specific syntax highlighting theme for use on the bat CLI utility. AllowEnv BAT_THEME This is equivalent to the following command: ssh -L 8080 :localhost:8080 $SERVER","title":"Client config"},{"location":"Linux/Applications/ssh/#etcsshd_config","text":"/etc/ssh/sshd_config is the configuration for the SSH server daemon. Disable cleartext passwords PasswordAuthentication no Disable root login PermitRootLogin no","title":"/etc/sshd_config"},{"location":"Linux/Applications/ssh/#tasks","text":"","title":"Tasks"},{"location":"Linux/Applications/ssh/#port-forwarding","text":"Port forwarding is accomplished in one of two ways, local or remote with respect to the server not the client. Local ( -L ): connections to the client are forwarded through the SSH tunnel to the SSH server. This technique is used to provide functionality similar to a VPN, where remote access is made possible to content on a private network, such as file shares or web applications that are not exposed publicly. Remote ( -R )... Dynamic Here, a private web application served locally on ssh-server will be served on the client at the same port. The first \"localhost\" can actually be omitted, since the connection will be exposed on localhost host by default and is almost universally. The confusing part is the second \"localhost\", because that is actually in reference to the ssh server itself. ssh -L localhost:80:localhost:80 ssh-server It is actually possible to forward a request to another host on ssh-server's network, creating a jump box. Here a connection on the client's localhost:81 is forwarded to ssh-server, which then sends it to 192.168.1.1:80. ssh -L 81 :192.168.1.1:80 ssh-server","title":"Port forwarding"},{"location":"Linux/Applications/ssh/#x-forwarding","text":"ssh -Y user@host Have remote system use local computer {me.luna.edu}'s X display export DISPLAY = me.luna.edu:0","title":"X forwarding"},{"location":"Linux/Applications/ssh/#ssh-to-a-transient-server","text":"To prevent recording an unimportant server to the client's known hosts file, change UserKnownHostsFile to \"/dev/null\". To suppress the warning about the unfamiliar address, change StrictHostKeyChecking to \"no\". ssh user@host -o \"UserKnownHostsFile=/dev/null\" -o \"StrictHostKeyChecking=no\" Alternatively, place these settings in a config, with an appropriate IP address range.","title":"SSH to a transient server"},{"location":"Linux/Applications/ssh/#fail2ban","text":"Fail2ban is an intrusion prevention framework written in Python and that runs as a service. It can be installed from most distributions' repos. The jail is a key concept in f2b that couples filters and actions definitions. Fail2ban is configured through .ini-format configs found in /etc/fail2ban . It is recommended not to edit the default configs ending in .config but rather to create a custom config called jail.local which will be automatically loaded by the service. Example jail [sshd] enabled = true port = ssh filter = sshd logpath = /var/log/auth.log maxretry = 0 findtime = 300 bantime = 3600 Failed logins can be checked by running lastb , and connections are also logged to SystemD. journalctl -ru sshd Display banned IPs fail2ban-client banned Ban IP address manually fail2ban-client set sshd banip $IPADDRESS","title":"fail2ban"},{"location":"Linux/Applications/ssh/#commands","text":"","title":"Commands"},{"location":"Linux/Applications/ssh/#endlessh","text":"Log verbosity # Silent endlessh # Normal endlessh -v # Debug endlessh -vv Log verbosity LogLevel 0 # silent LogLevel 1 # normal LogLevel 2 # debug","title":"endlessh"},{"location":"Linux/Applications/ssh/#ssh-agent","text":"eval $( ssh-agent )","title":"ssh-agent"},{"location":"Linux/Applications/ssh/#ssh-copy-id","text":"This command copies the SSH public key to a specified account's ~/.ssh/authorized_keys file. In Windows, this command is not available, so a workaround is to simply pipe the public key over SSH itself. type $env :USERPROFILE \\. ssh \\i d_rsa.pub | ssh { IP-ADDRESS-OR-FQDN } \"cat >> .ssh/authorized_keys\"","title":"ssh-copy-id"},{"location":"Linux/Applications/ssh/#ssh-keygen","text":"Generate host keys sudo ssh-keygen -A","title":"ssh-keygen"},{"location":"Linux/Applications/syncthing/","text":"SyncThing","title":"Syncthing"},{"location":"Linux/Applications/syncthing/#syncthing","text":"","title":"SyncThing"},{"location":"Linux/Applications/vim/","text":"vim Unlike WYSIWYG editors which optimize input for writing text, vim optimizes for editing it. Vim offers a composable language for expressing these editing changes whose syntax can be composed into two elements, operations and text objects , which are analogous to verbs and nouns in language. YouTube The framework of understanding vim's syntax as a language appears to date back to an influential 2011 Stack Overflow post . On Unix-derived operating systems the main config file for Vim is placed at $HOME/.vimrc . On Windows it is placed at $HOME/_vimrc . Syntax normal Use :normal to define a series of normal-mode commands Select all lines of a buffer : normal ggVG Keybindings There are two kinds of keybindings in Vim Recursive using command words map , nmap , vmap , etc. In these keybindings, the mapping itself is interpreted. Nonrecursive There are two types of keycodes: Vim keycodes which are identifiable as being in angle brackets: <Space> , <Return> , etc Terminal keycodes that appear similar to ^[[1;2A . These may or may not be identifiable with the keycodes which the Linux kernel maps to raw keyboard scancodes . [ref][archwiki:Keyboard_input] The leader key is used to create more complicated keybindings using any arbitrary keypress, for example using , or <Space> . let mapleader = ' ' Autocommands Autocommands expose an API that allows handling editor events like BufNewFile , BufReadPost , BufWritePost , BufWinLeave , and especially to implement functionality specific to filetypes. Highlight added lines in green and removed lines in red in .diff files filetype on augroup PatchDiffHighlight autocmd! autocmd FileType diff syntax enable augroup END Turn syntax highlighting on only for certain filetypes augroup PatchDiffHighlight autocmd! autocmd BufEnter *.patch,*.rej,*.diff syntax enable augroup END Color Elements : Directory Identifier LineNr NonText Normal String Title VertSplit Comment Constant Cursor Folded Function Keyword Number PreProc SpecialKey Special Statement StatusLineNC StatusLine Todo Type Visual Change the color of ELEMENT highlight ELEMENT ctermfg = COLOR ctermbg = COLOR guifg = #abc123 guibg = #abc123 Select alternative colorschemes : colo [rscheme] < tab > Display all available colorschemes : colo < C - d > Clear custom color commands : highlight clear : hi clear Set file format to Unix/DOS : set fileformat = unix : set fileformat = dos Completion Context-aware completion Ctrl + X Ctrl + L Omni completion Ctrl + X Ctrl + O Tasks Line numbers :set rnu Mapping keys Map Alt + J and Alt + K to move lines of text up or down nnoremap <A-j> :m .+1<CR>== nnoremap <A-k> :m .-2<CR>== inoremap <A-j> <Esc>:m .+1<CR>==gi inoremap <A-k> <Esc>:m .-2<CR>==gi vnoremap <A-j> :m '>+1<CR>gv=gv vnoremap <A-k> :m '<-2<CR>gv=gv Yanking STDOUT To run a shell command from the normal mode command line, you simply run the ! ( \"bang\" ) command in normal mode. :! env However to store the output of that command into a register , you must run a command like the following, which stores the output of the shell command into the a register. : let @ a = system ( 'env' ) The register signified by @\" will be placed into the buffer by the put command ( p ). : let @\" = system ( 'env' ) Alternatively : put = system ( 'env' ) Filetype-associated settings Set indentation behavior specific to YAML autocmd FileType yaml setlocal ai ts=2 sw=2 et Plugins Vim 8 supports native loading of plugins (put in $HOME/.vim/pack/start/ vim-plug is a popular plugin manager. Install a plugin to provide Rust language support Plug 'rust-lang/rust.vim' Mouse support From here set mouse = a Language definition Syntax highlighting for various languages are stored in syntax files , stored in /usr/share/vim/vim82/syntax . Defining highlighting for pymdownx snippets syn match markdownPymdownxSnippet '^-\\{2,}8<-\\{2,} .*' \" (1) hi def link markdownPymdownxSnippet Error Note that the quantifier specifying at least two instances of the preceding hyphen requires the initial brace to be escaped. However, the open angle bracket does not.","title":"Vim"},{"location":"Linux/Applications/vim/#vim","text":"Unlike WYSIWYG editors which optimize input for writing text, vim optimizes for editing it. Vim offers a composable language for expressing these editing changes whose syntax can be composed into two elements, operations and text objects , which are analogous to verbs and nouns in language. YouTube The framework of understanding vim's syntax as a language appears to date back to an influential 2011 Stack Overflow post . On Unix-derived operating systems the main config file for Vim is placed at $HOME/.vimrc . On Windows it is placed at $HOME/_vimrc .","title":"vim"},{"location":"Linux/Applications/vim/#syntax","text":"","title":"Syntax"},{"location":"Linux/Applications/vim/#normal","text":"Use :normal to define a series of normal-mode commands Select all lines of a buffer : normal ggVG","title":"normal"},{"location":"Linux/Applications/vim/#keybindings","text":"There are two kinds of keybindings in Vim Recursive using command words map , nmap , vmap , etc. In these keybindings, the mapping itself is interpreted. Nonrecursive There are two types of keycodes: Vim keycodes which are identifiable as being in angle brackets: <Space> , <Return> , etc Terminal keycodes that appear similar to ^[[1;2A . These may or may not be identifiable with the keycodes which the Linux kernel maps to raw keyboard scancodes . [ref][archwiki:Keyboard_input] The leader key is used to create more complicated keybindings using any arbitrary keypress, for example using , or <Space> . let mapleader = ' '","title":"Keybindings"},{"location":"Linux/Applications/vim/#autocommands","text":"Autocommands expose an API that allows handling editor events like BufNewFile , BufReadPost , BufWritePost , BufWinLeave , and especially to implement functionality specific to filetypes. Highlight added lines in green and removed lines in red in .diff files filetype on augroup PatchDiffHighlight autocmd! autocmd FileType diff syntax enable augroup END Turn syntax highlighting on only for certain filetypes augroup PatchDiffHighlight autocmd! autocmd BufEnter *.patch,*.rej,*.diff syntax enable augroup END","title":"Autocommands"},{"location":"Linux/Applications/vim/#color","text":"Elements : Directory Identifier LineNr NonText Normal String Title VertSplit Comment Constant Cursor Folded Function Keyword Number PreProc SpecialKey Special Statement StatusLineNC StatusLine Todo Type Visual Change the color of ELEMENT highlight ELEMENT ctermfg = COLOR ctermbg = COLOR guifg = #abc123 guibg = #abc123 Select alternative colorschemes : colo [rscheme] < tab > Display all available colorschemes : colo < C - d > Clear custom color commands : highlight clear : hi clear Set file format to Unix/DOS : set fileformat = unix : set fileformat = dos","title":"Color"},{"location":"Linux/Applications/vim/#completion","text":"Context-aware completion Ctrl + X Ctrl + L Omni completion Ctrl + X Ctrl + O","title":"Completion"},{"location":"Linux/Applications/vim/#tasks","text":"","title":"Tasks"},{"location":"Linux/Applications/vim/#line-numbers","text":":set rnu","title":"Line numbers"},{"location":"Linux/Applications/vim/#mapping-keys","text":"Map Alt + J and Alt + K to move lines of text up or down nnoremap <A-j> :m .+1<CR>== nnoremap <A-k> :m .-2<CR>== inoremap <A-j> <Esc>:m .+1<CR>==gi inoremap <A-k> <Esc>:m .-2<CR>==gi vnoremap <A-j> :m '>+1<CR>gv=gv vnoremap <A-k> :m '<-2<CR>gv=gv","title":"Mapping keys"},{"location":"Linux/Applications/vim/#yanking-stdout","text":"To run a shell command from the normal mode command line, you simply run the ! ( \"bang\" ) command in normal mode. :! env However to store the output of that command into a register , you must run a command like the following, which stores the output of the shell command into the a register. : let @ a = system ( 'env' ) The register signified by @\" will be placed into the buffer by the put command ( p ). : let @\" = system ( 'env' ) Alternatively : put = system ( 'env' )","title":"Yanking STDOUT"},{"location":"Linux/Applications/vim/#filetype-associated-settings","text":"Set indentation behavior specific to YAML autocmd FileType yaml setlocal ai ts=2 sw=2 et","title":"Filetype-associated settings"},{"location":"Linux/Applications/vim/#plugins","text":"Vim 8 supports native loading of plugins (put in $HOME/.vim/pack/start/ vim-plug is a popular plugin manager. Install a plugin to provide Rust language support Plug 'rust-lang/rust.vim'","title":"Plugins"},{"location":"Linux/Applications/vim/#mouse-support","text":"From here set mouse = a","title":"Mouse support"},{"location":"Linux/Applications/vim/#language-definition","text":"Syntax highlighting for various languages are stored in syntax files , stored in /usr/share/vim/vim82/syntax . Defining highlighting for pymdownx snippets syn match markdownPymdownxSnippet '^-\\{2,}8<-\\{2,} .*' \" (1) hi def link markdownPymdownxSnippet Error Note that the quantifier specifying at least two instances of the preceding hyphen requires the initial brace to be escaped. However, the open angle bracket does not.","title":"Language definition"},{"location":"Linux/Applications/youtube-dl/","text":"youtube-dl Download the best video quality with the mp4 format and the best audio quality of any format. youtube-dl -f 'bestvideo[ext=mp4]+bestaudio'","title":"Youtube dl"},{"location":"Linux/Applications/youtube-dl/#youtube-dl","text":"Download the best video quality with the mp4 format and the best audio quality of any format. youtube-dl -f 'bestvideo[ext=mp4]+bestaudio'","title":"youtube-dl"},{"location":"Misc/","text":"Processing cookbooks Cookbooks are collections of tasks with representative implementations (e.g. Azure commands and procedures for the AZ-103.) Number tasks for easy reference, indexing, and linking in markdown Catalog tasks and desciptions in a spreadsheet Copy catalog with task and description to markdown. This will serve as both an index of tasks as well as the skeleton for the content. Use multiple cursors to introduce #### heading syntax before the task identifier, followed by a carriage return before the one-line description of the task. This will ensure that the task is easily found by identifier. These should be collected in a single-cell table, producing a \"cloud\" of tasks.\" Fill markdown with syntax, producing a true reference of the source's syntax Map each form-based feature (e.g. commands) to tasks in a spreadsheet (Command | Task). Once organized by command, the resulting associations can form another table of content which associates form features to tasks. These should be placed in another single-cell cloud where each token is followed by links to the tasks in which it appears. The tokens should be organized, either by command group or roughly by domain. Index form-based features at the top of the markdown as a concordance. Bootloaders bootloader : software located in the first sector (Master Boot Record) of a HDD, which is read by the BIOS - implementing interruptions requires knowledge of Assembler - expertise in low-level programming in C - Java and C# produce intermediate code, which must be executed by a special virtual machine - mixed-code technique requires at least two compilers (one for Assembler and C, another as a linker to join the *.obj files to create a single executable file) Bots Discord Create the bot user on Discord and register it with a guild. Write code that uses Discord\u2019s APIs and implements your bot\u2019s behaviors. Create a Discord connection ^ A Client is an object that represents a connection to Discord, handling events, tracking state, and interacting with Discord APIs. # bot.py import os , discord from dotenv import load_dotenv # Install via `pip install -U python-dotenv` load_dotenv () token = os . getenv ( 'DISCORD_TOKEN' ) client = discord . Client () @client . event async def on_ready (): print ( f ' { client . user } has connected to Discord!' ) client . run ( token ) Store token in .env file .env should be placed in the same directory as bot.py # .env DISCORD_TOKEN={your-bot-token} Twitch Nightbot Mee6 Ruby bot programming Ruby library \"socket\" allows integration with Twitch's IRC API , which provides an oauth token which can be stored as password. Command write_to_system appears to be what is needed to concatenate IRC commands PASS #{@password , NICK #{@nickname} , USER #{@nickname} 0 * #{@nickname} , and JOIN #@{channel} \\ From the REPL, you instantiate an instance of the class after running the script, which will allow passing messages to the chat room by using an instance method ^ bot = TwitchBot . new bot . write_to_chat \"Hello world\" VMware Hypervisor, similar to Hyper-V, but provided at a cost, with a robust command-line interface via PowerShell. ^","title":"Index"},{"location":"Misc/#processing-cookbooks","text":"Cookbooks are collections of tasks with representative implementations (e.g. Azure commands and procedures for the AZ-103.) Number tasks for easy reference, indexing, and linking in markdown Catalog tasks and desciptions in a spreadsheet Copy catalog with task and description to markdown. This will serve as both an index of tasks as well as the skeleton for the content. Use multiple cursors to introduce #### heading syntax before the task identifier, followed by a carriage return before the one-line description of the task. This will ensure that the task is easily found by identifier. These should be collected in a single-cell table, producing a \"cloud\" of tasks.\" Fill markdown with syntax, producing a true reference of the source's syntax Map each form-based feature (e.g. commands) to tasks in a spreadsheet (Command | Task). Once organized by command, the resulting associations can form another table of content which associates form features to tasks. These should be placed in another single-cell cloud where each token is followed by links to the tasks in which it appears. The tokens should be organized, either by command group or roughly by domain. Index form-based features at the top of the markdown as a concordance.","title":"Processing cookbooks"},{"location":"Misc/#bootloaders","text":"bootloader : software located in the first sector (Master Boot Record) of a HDD, which is read by the BIOS - implementing interruptions requires knowledge of Assembler - expertise in low-level programming in C - Java and C# produce intermediate code, which must be executed by a special virtual machine - mixed-code technique requires at least two compilers (one for Assembler and C, another as a linker to join the *.obj files to create a single executable file)","title":"Bootloaders"},{"location":"Misc/#bots","text":"","title":"Bots"},{"location":"Misc/#discord","text":"Create the bot user on Discord and register it with a guild. Write code that uses Discord\u2019s APIs and implements your bot\u2019s behaviors. Create a Discord connection ^ A Client is an object that represents a connection to Discord, handling events, tracking state, and interacting with Discord APIs. # bot.py import os , discord from dotenv import load_dotenv # Install via `pip install -U python-dotenv` load_dotenv () token = os . getenv ( 'DISCORD_TOKEN' ) client = discord . Client () @client . event async def on_ready (): print ( f ' { client . user } has connected to Discord!' ) client . run ( token ) Store token in .env file .env should be placed in the same directory as bot.py # .env DISCORD_TOKEN={your-bot-token}","title":"Discord"},{"location":"Misc/#twitch","text":"Nightbot Mee6","title":"Twitch"},{"location":"Misc/#ruby-bot-programming","text":"Ruby library \"socket\" allows integration with Twitch's IRC API , which provides an oauth token which can be stored as password. Command write_to_system appears to be what is needed to concatenate IRC commands PASS #{@password , NICK #{@nickname} , USER #{@nickname} 0 * #{@nickname} , and JOIN #@{channel} \\ From the REPL, you instantiate an instance of the class after running the script, which will allow passing messages to the chat room by using an instance method ^ bot = TwitchBot . new bot . write_to_chat \"Hello world\"","title":"Ruby bot programming"},{"location":"Misc/#vmware","text":"Hypervisor, similar to Hyper-V, but provided at a cost, with a robust command-line interface via PowerShell. ^","title":"VMware"},{"location":"Misc/Akro-Mils/","text":"Akro-Mils *: already own Giant bins for cube storage Bin Length divider H L W $ ea. 30260 * 40260 10.000 18.000 11.000 25.67 30280 6.000 20.000 12.375 17.53 Cabinets <21\" of horizontal room Bin Length divider H L W $ ea. 30230 * 40230 5.000 10.875 5.000 5.27 30235 * 40230 5.000 10.875 11.000 9.18 30255 40230 5.000 10.875 16.500 11.00 Pantry closet <25\" of horizontal room Bin Length divider H L W $ ea. 30120 n/a 4.000 11.625 4.125 2.77 30150 n/a 4.000 11.625 8.375 3.47 30040 n/a 6.000 11.625 4.125 4.18 30080 n/a 6.000 11.625 8.375 5.71 S-13396 n/a 4.000 12.000 4.000 1.45 S-13398 n/a 4.000 12.000 8.500 2.65 S-16257 * n/a 6.000 12.000 4.000 2.60 S-16277 * n/a 6.000 12.000 8.500 4.50","title":"Akro-Mils"},{"location":"Misc/Akro-Mils/#akro-mils","text":"*: already own Giant bins for cube storage Bin Length divider H L W $ ea. 30260 * 40260 10.000 18.000 11.000 25.67 30280 6.000 20.000 12.375 17.53 Cabinets <21\" of horizontal room Bin Length divider H L W $ ea. 30230 * 40230 5.000 10.875 5.000 5.27 30235 * 40230 5.000 10.875 11.000 9.18 30255 40230 5.000 10.875 16.500 11.00 Pantry closet <25\" of horizontal room Bin Length divider H L W $ ea. 30120 n/a 4.000 11.625 4.125 2.77 30150 n/a 4.000 11.625 8.375 3.47 30040 n/a 6.000 11.625 4.125 4.18 30080 n/a 6.000 11.625 8.375 5.71 S-13396 n/a 4.000 12.000 4.000 1.45 S-13398 n/a 4.000 12.000 8.500 2.65 S-16257 * n/a 6.000 12.000 4.000 2.60 S-16277 * n/a 6.000 12.000 8.500 4.50","title":"Akro-Mils"},{"location":"Misc/CI-CD/","text":"CI/CD Github Actions All that's needed is a yaml file in .github/workflows with the following keys: YouTube - name - on - jobs In order to use the Python projects need to have a .spec file GitHub actions can be either Docker or JavaScript name : Build and publish presentation with reveal-md on : push jobs : release : name : Build & Publish runs-on : ubuntu-latest steps : - uses : actions/checkout@v1 - name : Install dependencies and build presentation run : | sudo npm install -g reveal-md --unsafe-perm sudo reveal-md Presentation.md --static _site --highlight-... on src on : push : branches : - main jobs src jobs : release : if : github.actor == 'JackMcKew' && startsWith(github.event.head_commit.message, 'Update README') name : Build runs-on : ubuntu-latest steps : - uses : actions/checkout@v1 - name : Set up Python 3.7 uses : actions/setup-python@v1 with : python-version : 3.7 - name : Install dependencies & Convert README.ipynb run : | python -m pip install --upgrade pip pip install -r requirements.txt jupyter nbconvert --template \"pythoncodeblocks.tpl\" --ClearMetadataPreprocessor.enabled=True --ClearOutput.enabled=True --to markdown README.ipynb - name : Commit files run : | git config --local user.email \"action@github.com\" git config --local user.name \"GitHub Action\" git add README.md git commit -m \"Convert README.ipynb to README.md\" -a - name : Push changes if : success() uses : ad-m/github-push-action@master with : branch : main github_token : ${{ secrets.ACCESS_TOKEN }}","title":"CI/CD"},{"location":"Misc/CI-CD/#cicd","text":"","title":"CI/CD"},{"location":"Misc/CI-CD/#github-actions","text":"All that's needed is a yaml file in .github/workflows with the following keys: YouTube - name - on - jobs In order to use the Python projects need to have a .spec file GitHub actions can be either Docker or JavaScript name : Build and publish presentation with reveal-md on : push jobs : release : name : Build & Publish runs-on : ubuntu-latest steps : - uses : actions/checkout@v1 - name : Install dependencies and build presentation run : | sudo npm install -g reveal-md --unsafe-perm sudo reveal-md Presentation.md --static _site --highlight-...","title":"Github Actions"},{"location":"Misc/CI-CD/#on","text":"src on : push : branches : - main","title":"on"},{"location":"Misc/CI-CD/#jobs","text":"src jobs : release : if : github.actor == 'JackMcKew' && startsWith(github.event.head_commit.message, 'Update README') name : Build runs-on : ubuntu-latest steps : - uses : actions/checkout@v1 - name : Set up Python 3.7 uses : actions/setup-python@v1 with : python-version : 3.7 - name : Install dependencies & Convert README.ipynb run : | python -m pip install --upgrade pip pip install -r requirements.txt jupyter nbconvert --template \"pythoncodeblocks.tpl\" --ClearMetadataPreprocessor.enabled=True --ClearOutput.enabled=True --to markdown README.ipynb - name : Commit files run : | git config --local user.email \"action@github.com\" git config --local user.name \"GitHub Action\" git add README.md git commit -m \"Convert README.ipynb to README.md\" -a - name : Push changes if : success() uses : ad-m/github-push-action@master with : branch : main github_token : ${{ secrets.ACCESS_TOKEN }}","title":"jobs"},{"location":"Misc/Computers/","text":"Computers Item Cost (USD) Rosewill chassis (15 bays) 225 Chassis railkit 72 Asus ROG Strix X570-F 200 Radeon RX 6600 XT 360 Ryzen 7 5700 G 300 Power Supply 120 32 GB DDR4 RAM 130 SATA Expansion card 61 GPU VRAM (GB) PassMark Cost (USD) Asus Radeon RX 6600 XT 8 15862 480 XFX Radeon RX 6600 XT 8 15862 360 ASRock Radeon RX 6600 XT 8 360 CPU Cores (threads) Cost Ryzen 7 5700G ( APU ) 8 (16) 299 Ryzen 7 5700X 8 (16) 287 Ryzen 5 5600 6 (12) 200 Ryzen 7 5800X 8 (16) 315 Ryzen 7 4700G 8 (16) 240 Stan NAS Cases Picture Form factor 3.5\" bays Cost (USD) Mini-ITX 0 2 4 70 88 106 Mini-ITX 1 170 Mini-ITX 4 85 Mini-ITX 5 145 Motherboards Model Form Factor SATA ports M.2 slots Price ASRock A520M Mini ITX 4 1 105 ASRock B550 Phantom Mini-ITX 4 2 170 ASUS ROG STRIX B450-I Mini-ITX 4 2 178 ASUS ROG Strix X570-I Mini-ITX 4 2 265","title":"Computers"},{"location":"Misc/Computers/#computers","text":"Item Cost (USD) Rosewill chassis (15 bays) 225 Chassis railkit 72 Asus ROG Strix X570-F 200 Radeon RX 6600 XT 360 Ryzen 7 5700 G 300 Power Supply 120 32 GB DDR4 RAM 130 SATA Expansion card 61 GPU VRAM (GB) PassMark Cost (USD) Asus Radeon RX 6600 XT 8 15862 480 XFX Radeon RX 6600 XT 8 15862 360 ASRock Radeon RX 6600 XT 8 360 CPU Cores (threads) Cost Ryzen 7 5700G ( APU ) 8 (16) 299 Ryzen 7 5700X 8 (16) 287 Ryzen 5 5600 6 (12) 200 Ryzen 7 5800X 8 (16) 315 Ryzen 7 4700G 8 (16) 240","title":"Computers"},{"location":"Misc/Computers/#stan-nas","text":"","title":"Stan NAS"},{"location":"Misc/Computers/#cases","text":"Picture Form factor 3.5\" bays Cost (USD) Mini-ITX 0 2 4 70 88 106 Mini-ITX 1 170 Mini-ITX 4 85 Mini-ITX 5 145","title":"Cases"},{"location":"Misc/Computers/#motherboards","text":"Model Form Factor SATA ports M.2 slots Price ASRock A520M Mini ITX 4 1 105 ASRock B550 Phantom Mini-ITX 4 2 170 ASUS ROG STRIX B450-I Mini-ITX 4 2 178 ASUS ROG Strix X570-I Mini-ITX 4 2 265","title":"Motherboards"},{"location":"Misc/Diaz/","text":"Joey Diaz Chinese restaurants in Boulder, CO Puerto Rican Nelson Ralphie May falling on a dollhouse","title":"Joey Diaz"},{"location":"Misc/Diaz/#joey-diaz","text":"Chinese restaurants in Boulder, CO Puerto Rican Nelson Ralphie May falling on a dollhouse","title":"Joey Diaz"},{"location":"Misc/Intelligence/","text":"\ud83d\udd0e Intelligence Inside the military's secret undercover army China Chinese writers often wrote on intelligence matters in contrast with Western writers[^1] Historical writings on intelligence continue to be used by military institutions of PRC Fragments of earlier works \u201cprotobooks\u201d survive in later works. Sunzi wrote that warfare must be undertaken only after a comprehensive analysis of relative strengths and weaknesses of combatants, the world\u2019s first net assessment procedure. He also advocated deception on the battlefield. Later writers responded to Sunzi, updating and interpreting his precepts: Guan Zhong Li Quan, Li Jing. Sunzi condemned generals who failed to gather necessary intelligence and disparaged divination and consulting ancestors in favor of hard intelligence. Later writers like Shi Zimei also upbraided rulers who ignored these rules. Works advocating covert action - Toubi Fuban - Warring States period works on statecraft, e.g. Han Feizi Sunzi\u2019s Five Types of Spy Local Spy : Knowledgeable spies living outside their native habitat, including emigrants, travelers and exiles (e.g. defectors) Internal spy : Officials Turned spy : Double agent Living spy : Talented people dispatched abroad to observe and then report back Dead spy : Spies deliberately sent on suicide missions without their knowledge. Enemy agents could also be deceived and provided with false information, causing them to be executed when they report false information to their masters. Subversion By end of Spring and Autumn Period, subversion was an accepted goal of espionage. Assassination used most often in periods of fragmentation. Six Secret Teachings compiled two chapters on systematic programs of subversion. These two chapters, with Sunzi\u2019s \u201cEmploying Spies\u201d in Art of War formed the basis of espionage as late as the Ming Dynasty. Later military writers also devoted at least a few paragraphs to subversion: Le Quan\u2019s Techniques for Secret Plots, Huqian Jing, Bingfa Baiyan. These took historical and semihistorical episodes as examples (Wu Yue Chunqua). Covert action and subversion used by Qin in the western hinterland. \u201cDebauching\u201d kings Sunzi: \u201cYou must first know the names of the defensive commander, his assistants, staff, door guards, and attendants for any armies that you want to strike, cities you want to attack, and men you want to assassinate.\u201d Egypt Egyptian intelligence had its origins in the urban policing organizations established during the British Khedive and occupation. Mamur Zapt s were the de facto chiefs of secret police in Cairo and Alexandria, operating networks of mukbireen informants, by the time of British occupation in 1882. The Central Special Office (CSO) was established in 1911 after the Coptic Prime Minister was assassinated by a young nationalist. When discontent exploded in 1919, and CSO couldn\u2019t respond effectively, the Interior Ministry formed the Special Section (SS) to centralize intelligence collection. CSO was closed and files transferred to SS in 1925. Egyptian Military Intelligence was being indigenized in the late 1930s. MI officers were complicit in the army coup of 23 July 1952. Once the Free Officers were in power, they set about creating a new intelligence community. The Military Intelligence Directorate (MID) as it was then called, became active in covert action and subversion across the Middle East and Africa. Fear of communist subversion fueled the creation of the General Intelligence Directorate (GID) , subordinate to the Interior Minister, which was led at first by Zakaria Muhi al-Din and stood up within months of the coup. GID inherited and took the place of the old political police organs. GID was renamed the State Security Investigations Service (SSIS) in 1971. As part of Gamal Abd al-Nasser\u2019s emphasis on covert action, the Egyptian General Intelligence Service (EGIS) was established by March 1954, also headed by Zakaria Muhi al-Din. EGIS was the capstone of the new Egyptian government\u2019s emphasis on projecting influence through covert action, and it was modeled after the CIA. During the heyday of Egyptian subversion abroad, Salah Nasr was EGIS Director. Egyptian covert action grew out of the effort to end the British occupation, remove British influence from government organs, and eject the British from the Canal Zone. In 1952, MID created a Special Resistance Office to form a guerrilla army against British military units based along the Suez Canal. The campaign became the template for Egyptian subversion abroad for the next decade and half. MID also sent officers to Sudan to recruit sources and build influence for possible unity with Sudan. While Egypt occupied the Gaza Strip, Palestinian infiltrators were a source of irritation for MID. However, MID sponsored infiltrations using Bedouins, Palestinians, and even its own scouts for intelligence-gathering. These infiltrations eventually were made from southern Lebanon as well. The death of an Israeli settler during one such raid sparked a cycle of reprisals that led to war. The four years after the British withdrawal from the Suez in 1956 marked Egypt at the height of its regional power. Nasser was the hero of the Arab world, and Cairo was the wellspring of a vigorous new Pan-Arab nationalism. However, Egyptian subversion against conservative Arab states and British protectorates was checked in many places by the United States. After Egypt and Syria joined the United Arab Republic (UAR), Nasser began covert action campaigns in Jordan and Lebanon, combining subversion with radio propaganda. In Lebanon, UAR meddling triggered a larger crisis that drew the attention of the US and ended Egyptian arming of Lebanese insurgents. A 1958 coup in Iraq initially drew feelersfrom the Egyptians, who hoped to draw Iraq into their circle of influence. However, old animosities arose again, and by 1959 Egypt was broadcasting radio propaganda, and senior UAR spymasters backed a short-lived army revolt in Mosul. Alienated by Nasser, Saudi Arabia fended off feeble Egypt\u2019s attempts to use its military presence and attache corps there for subversion. Nasser created African Affairs Branch, led by Mohamed Fa\u2019iq, to pursue Egyptian interests clandestinely across Africa. Key targets were Ethiopia and Congo. Egypt promoted Eritrean Liberation Front (ELF) after Suez Crisis and assisted Eritrean declaration of independence by 1960. Close relations with Somalia, which also had territorial disputes with Ethiopia. Egypt provided money, arms to friendly regime in Congo, flew Ghanaian and Egyptian troops to prop up regime. Also provided arms to resistance movements in Angola, Cameroon, Nigeria, and Portuguese Guiana, and training to Congo Brazzaville and Tanzania. MID broke down under pressure of war. Israelis had strong SIGINT and intercepted an insecure phone call between Nasser and King Hussein. Israel also captured a valuable cache of Egyptian intelligence documents in Gaza on MID\u2019s Palestinian infiltrators. Voice of the Arabs (VOA) served as a propaganda arm buttressing Egypt\u2019s foreign subversion policies. Daily radio programming was handled by Egyptian State Broadcasting (ESB) and used transmitters supplied by CIA. ESB turned to former Nazi propagandists, who must have felt at home with anti-Jewish programming. ESB grew out of EGIS promotion of VOA. EGIS had a representative on ESB\u2019s board of directors. EGIS was also responsible for running clandestine radio stations, reliant usually reliant on mobile transmitters. Voice of the Arab Nation targeted Iraq during the 1956 war when the British and French knocked out overt ESB transmitters. Egypt also operated a clandestine radio station called Voice of Free Iran in the early 1960s and gave support to anti-shah clerics. Egyptian intelligence had been preparing Yemen for subversion since 1950s. VOA broadcasted propaganda, and the signal to begin the coup in September 1962 was delivered by VOA. Egyptian army was overwhelmed by Yemeni rural insurgency, because Cairo didn\u2019t plan for a long war. No good cultural intelligence on Yemeni tribes. Intervention antagonized Great Britain and Saudi Arabia. MID provided training to National Liberation Front (NLF) which was set up by Egypt as an anti-British umbrella organization. NLF systematically kidnapped, tortured, and executed Aden SB officers and informants. Egypt still relied on broken Enigma-based encryption. GCHQ and NSA had access to military COMINT. Despite successful Egyptian-backed coup and energetic covert action campaign against British Protectorate in South Yemen, Egypt could not maintain political leverage over Yemen. South Yemen turned to GDR for intelligence training. The British were Egypt\u2019s first instructors in counterintelligence, and also some of its first victims. MI and Cairo SB busted a German spy ring in cooperation with British intelligence (The Key to Rebecca). Operation Susannah was an Israeli terror plot meant to undermine international faith in Cairo\u2019s ability to secure the Suez Canal. In 1954, an Israeli case officer activated sabotage cells among the remaining Egyptian Jewry. Egyptians doubled the Israeli case officer and rolled up the ring. Huge success for nascent Egyptian CI and was an impetus in formation of EGIS. GID rolled up a spy network led by the British expat James Swinburn in 1956. Unfortunately, this had the knock-on effect of forcing the British to rely on their formidable SIGINT collection. GCHQ had broken Egypt\u2019s diplomatic cipher and routinely intercepted Egyptian military communications. Egyptian CI penetrated several Israeli spy rings after the Suez War. The Goudswaard Ring of European and Egyptian nationals was recruited by Mossad. Eventually the GID turned them and were using them to pass disinformation. The Thomas Ring gathered military documents and passed them to Israel in microfilm hidden in furniture which was then exported. In 1961, GID rolled up this network. In the run-up to the Yom Kippur War, Egyptian denial and deception extended to their erstwhile allies the Soviet Union. Soviet photoreconnaissance satellites monitored the war. After the Yom Kippur War, CIA resumed its relationship with Egypt, and along with providing training and equipment, CIA began to aggressively recruit from throughout Egyptian society. Egypt joined the Safari Club (a coalition of intelligence agencies from France, Saudi Arabia, Iran, and Morocco), which provided aid to Zaire, Somalia, Djibouti and anti-Qadhafi groups in Libya and Egypt as well as the Afghan resistance. After Camp David, Egyptian intelligence had to contend with Libyan subversion and radio propaganda, mimicking Egyptian tactics of yore. Covert action escalated to a short border war in 1977. Libyan subversion continued, and in 1978 SSIS uncovered a clandestine organization for the \u201cliberation\u201d of Egypt through sabotage and assassination. Egypt also stopped an Iranian who planned to undermine the peace process by conducting and bomb attacks in Egypt. In 1979, Egyptian CI began a covert war with Soviet and East European services. SSIS claimed to have uncovered a Bulgarian intelligence network that year. In 1981, Soviet and Hungarian diplomats were expelled after Egyptian security discovered a spy ring allegedly inciting sectarian unrest. Israel attempted to subvert Egypt early on by using the Jewish population in Cairo and Alexandria as a base. IDF created sabotage cells which were activated in 1954 when Egypt and UK were negotiating a British withdrawal from the Canal Zone. Operation Susannah was rolled up, and the operatives brutally tortured in a MID prison. The case had political ripple effects within Israel that reached into the 1960s. Several Israeli spy rings were rolled up in the late 1950s and 1960s. The Goudswaard Ring, run by Mossad in a false-flag operation that posed it as a NATO service, was doubled and began passing disinformation. The Thomas Ring, which gathered military documents and passed them to Israel in microfilm hidden in exported furniture, was rolled up by GID in 1961. EGIS claimed to have run a \u201csuper-spy\u201d named Jack Bitton before and during the Six-Day War (1967). Bitton opened a travel agency in Tel Aviv in in 1954 or 1955. Some claim he was doubled by the Israelis, but Egypt claims he provided warning of Israel\u2019s attack through his contacts with senior Israelis. For Egyptians, Bitton is Egypt\u2019s master spy and the subject of a highly popular television serial in the 1980s. Israeli spy Wolfgang Lotz mapped out Egypt\u2019s SAM facilities and other important military infrastructure. Israel also operated an agent \u201cSulayman\u201d who reported the movements of individual Egyptian units during the war. Sulayman may have provided encryption material, because Israeli MI (Aman) broke Egypt\u2019s military cipher before the war. Israeli deception measures went undetected by the Egyptians. IDF created an entire fake military unit in Eilat, complete with encrypted and unencrypted radio traffic, which succeeded in tricking the Egyptians. In the run-up to the Yom Kippur War, Egypt suffered from COMSEC shortfalls which were aggravated by excellent Israeli SIGINT. After the 1967 war, Israeli SIGINT stations on the banks of the Suez could reach deeper into Egypt. Israel even planted listening devices on communications lines used by an Egyptian headquarters complex. Egyptian CI also discovered bugs in hollowed-out telephone poles which Israel was using to tap military communications between Red Sea bases and Cairo. Israel\u2019s \u201cTop Source\u201d, Ashraf Marwan, was turned by the Egyptians before the onset of war and gave false warnings of Egyptian attack that cost Israel money and lulled it into a state of complacency. Anwar Sadat and MID formulated a plan of strategic deception to accomplish diplomatic victory in a limited war (British deception before Battle of el-Alamein). EGIS planted deceptive media stories on military unpreparedness. Military coordinated movements to avoid US imagery satellites. Brinksmanship produced calm-alarm-calm cycle that bred complacency in Israelis. Egyptians improved intelligence collection by recruiting Bedouins and obtaining valuable intelligence on Israeli plans and codenames. Also leveraged OSINT. Ashraf Marwan, Sadat\u2019s new presidential gatekeeper, was recruited by Israel but doubled by Egyptians, gave false warnings of imminent attacks. After early success in the war, Israelis exploited weaknesses and were ready to pounce when ceasefire was declared. Egypt and Israel made peace and began sharing intelligence on Palestinians. Sources: Sirrs, Owen L. The Egyptian Intelligence Service: A History of the Mukhabarat, 1910-2009 (Studies in Intelligence) . Finland Finnish economy\u2019s reliance on technology is growing and so is economic espionage. National innovation system is considered protected. Finland\u2019s intelligence community - Finnish Security Intelligence Service (Suojehpolisi, Supo) has been performing counterespionage, counterterrorism, and security since 1949. - Finnish Military Intelligence Command (FINMIC) is under Defense Command\u2019s Intelligence Division. - Finnish Intelligence Research Establishment (Viestikoelaitos) is Finnish Defense Force\u2019s (FDF) SIGINT unit, under Finnish Air Force (FAF). Predecessors to Supo - Detective Central Police (Etsiva keskuspoliisi) 1919-1937 - State Police (Vattriollinen poliisi, Valpo) 1937-1949 - Red Valpo, when dominated by Communists, 1945-1949 - Finnish Security Police est. 1949, name changed to Supo 2010. Organizational structure Supo is subordinate to the Ministry of Interior, while other Nordic services are subordinate to the Ministry of Justice. Supo handles no foreign HUMINT sources. Four organizational reforms over past 20 years - In 1992, Supo had three directorates: Counterespionage, Security, and Development and Support - In 1998, operational and development matters were divided more clearly - In 2004, Supo was reorganized to develop research and analysis functions. Line organization was introduced. - In 2009, Supo was divided into Operational and Strategic Branches. - Operational Branch : Counterespionage Unit, Counterintelligence Unit, Security and Regional Unit, Field Surveillance Unit - Strategic Branch : Situational Awareness Unit, International Relations Unit, Internal Surveillance Unit - Communications Office directly subordinate to C/Supo Personnel Supo employs 220 people and has a 17 million euro budget. - 55% are police personnel (30% command, 40% senior, 30% officers) - 1/3 of employees have a degree - Average age of 44 years Iran In 1978-1979 (1357 H.S.) some former SAVAK officers revealed what they knew in a televised news conference. SAVAK had 9 or 10 chief directorates: First Chief Directorate : administration Second Chief Directorate : foreign intelligence Third Chief Directorate : internal security Fourth Chief Directorate : counterintelligence Fifth Chief Directorate : technical espionage, including telephone intercept and clandestine audio recordings, censorship, photography, and flaps and seals Sixth Chief Directorate : budgeting Seventh Chief Directorate : analysis, including compilation of daily \"bulletins\" Eighth Chief Directorate : counterespionage Ninth Chief Directorate : individual biographies and passport affairs Iraj Faridi, former chief of operations of the first section of the Third Chief Directorate, mentioned the Trident intelligence agreement but seemed to think the relationship with Turkey, not Israel, was sensitive. He claimed to have 17 years of service in SAVAK during the press conference. Mohsen Toluei Trident A three-way intelligence sharing agreement between Iran, Israel, and Turkey. This was prominently covered by the press when former Mossad officer Yossi Alpher published his book Periphery: Israel's Search for Allies in the Middle East . Russia Kouzminov, Alexander. Biological espionage: Special Operations . Directorate S, Department 12 Responsible for obtaining intelligence to aid the Soviet BW program, as well as new strains of pathogens. Requirements included Western governments, state commissions, civil defense and laboratories. Vladimir Kuzichkin worked as Illegals support officer in S/12 for 10 years Laboratory X founded in 1920s, transferred to NKVD in 1937. Soviet biological espionage - Morris and Leontina Cohen, controlled by Gordon Lonsdale, obtained information on British BW program. - Marcus Klinberg was director of Israeli BW program, spied for USSR. Communications Intelligence and Tsarist Russia by Thomas R. Hammant Ministry of Foreign Affairs Since Peter the Great, COMINT involving foreign governments and their representatives was the responsibility of the Ministry of Foreign Affairs (MID). Methods used included opening diplomatic letters (perlustration) and decrypting the contents, if encrypted, by cryptanalysis or purchasing codebooks. MID was aided by the \u201cBlack Cabinets\u201d of the Imperial Russian Postal Service. Post offices in major cities of the Russian Empire had Black Cabinets, which photographed contents of suspect correspondence and disseminating the information to the appropriate ministry. When the contents of a letter were encrypted, it was worked on not by a Black Cabinet but by a \u201csimilar establishment attached to the Ministry of Foreign Affairs.\u201d Little is known about MID\u2019s cryptographic organization, but it may have been brought under the control of the Minister of Foreign Affairs himself in the 1900s. Codebooks could be easily acquired on the open market. Russian COMINT in World War I Army and Navy operated independent COMINT elements. At each Army HQ, radio intelligence operations were controlled by Chief of Army Communications through his assistant for technical matters. Each army\u2019s radio battalion had a radio intelligence squad or section which operated two stations: one monitored enemy communications, and the other station then recorded them once detected. Intercepts of encrypted German Army radiograms were sent to a \u201cspecial bureau\u201d of Chief Directorate of the General Staff in St. Petersburg for cryptanalysis. Generally, COMINT was poorly organized under the Russian Army. Black Sea and Baltic Sea Fleets established independent COMINT services in autumn 1914 after German naval codebooks were recovered from a sunken German cruiser. Copies of the codebook were shared with the British and French, and there was continued COMINT collaboration between the Allies throughout the war. The Baltic Sea Fleet\u2019s first radio intercept station was established close to Tallin. Intelligence was sent by underground cable to the Communications Service of the Southern Region. Each region (North, East, and South) had a Central Radio Station (CRS) that produced all-source intelligence and supported fleet communications. By 1916, Northern Region had 5 DF and 5 intercept stations, and Southern Region had 5 DF and 4 intercept stations. Southern Region also established a Radio Intelligence Center, probably to administer COMINT from these stations, which was subordinate to the CRS, and other Regions may have had similar units. The Baltic Sea Fleet\u2019s greatest debt to COMINT was accrued on 31 July 1915, when the Russians learned the German Navy planned to seize the city of Riga. Cryptanalysis of the messages, aerial reconnaissance, and shore-based observation posts allowed Russian ships to be ready for the attack when it came, and the Russians prevailed. The Black Sea Fleet\u2019s first radio intercept station was at Sevastopol, and although its Communications Service had two regions to administer, there are fewer details available about Black Sea Fleet COMINT. Black Sea Fleet was greatly helped by Turkey\u2019s use of German codes during the war. In December 1916, Black Sea Fleet decrypted information that indicated a German submarine was to return to Constantinople and included the location of the mine-swept channel by which it was to pass. Russian minelayers went to work, and within 48 hours the Russians learned the submarine had been sunk. From the Okhrana to the KGB, by Christopher Andrew Okhrana est. 1881 Soviet intelligence MO and methods rooted in Tsarist secret service Okhrana active measures campaign to persuade French investors to invest in Russia. By 1914, a quarter of France\u2019s FDI was in Russia, three times as much as in its own empire, 80% of it in government loans. Peter Rachkowsky, head of Okhrana\u2019s Paris-based Foreign Agency from 1884 to 1982 may have been responsible for producing the Protocols of the Elders of Zion. HUMINT Colonel Alfred Redl, senior Austrian MI officer. In winter 1901-2, Colonel Batyushin, head of Russian MI in Warsaw, discovered Redl was a homosexual. Redl sold Austria\u2019s mobilization plans against Russia and Serbia until his suicide in 1913. Roman Malinovsky, worker who became one of the most trusted Bolsheviks. One of 6 Bolshevik deputies to Duma (1912), then chair of Bolshevik faction when Mensheviks broke off. When Lenin smelled a rat and set up a committee to investigate the possibility of penetration, Malinovsky was a member of the committee. Okhrana sent him out of the country with a 6000 ruble payoff, and when Okhrana files were opened in 1917 Lenin couldn\u2019t believe Malinovsky had been a traitor. SIGINT Okhrana began stealing cipher material to assist SIGINT at the beginning of the 20th century, decades before anyone but the French. Okhrana bribed embassy to services to make impressions of keys and smuggle them out, as well as papers to be photographed. SIGINT was used in diplomatic negotiations with Germany over the Bosphorus Canal. Bolshevik Revolution damaged SIGINT, dispersed codebreakers and cryptologists to other countries, where they sometimes joined those SIGINT services. For a decade after the Revolution, Soviet diplomatic traffic was easily decrypted. Soviets adopted the one-time pad in 1927. GB adopted the tactic of stealing cryptographic material. KGB and GRU operated a joint unit headed by Gleb Boki. SIGINT was responsible for a panic regarding a possible Japanese surprise attack 1931-1932. Intercepted attache cables were published in the Moscow press. Soviets provided edited excerpts of diplomatic intercepts to Germany to influence Germans into signing Molotov-Ribbentrop Pact. Maskirovka on Eastern Front achieved comparable results to British and Western denial and deception (XX system) Capture of hundreds of German signals personnel among tens of thousands of German POWs resulted in a windfall for Soviet SIGINT, 1943. Multiple penetrations of NSA, 1959-1963. Sword and the Shield, the by Christopher Andrew Chapter 2: From Lenin\u2019s Cheka to Stalin\u2019s OGPU The paranoid instincts and shadowy methods of the Cheka and its successors were motivated by persecution of Bolshevik revolutionaries during the Tsarist period and provoked by agents provocateurs planted by the Tsarist Okhrana and foreign powers. - Cheka founded on 19171220 only weeks after Bolshevik Revolution: Feliks Dzerzhinsky - Foreign Intelligence Department (INO) established 19201220: Made use chiefly of Illegals because Soviet state had no legal residencies abroad - Foiled plots encouraged paranoia of young Cheka - Envoys\u2019 plot by naive young diplomats, caught in the net laid by the Cheka - Agents provocateurs Eduard Berzin and Yan Buikis: Berzin received Order of the Red Star, became Cheka officer, but then fell victim to Stalin\u2019s Terror and was shot in 1937; Buikis survived by changing his identity - Okhrana agents provocateurs - Bolshevik experience as an underground movement: Use of pseudonyms (\u2019Lenin\u2019, \u2018Stalin\u2019) Chapter 3: The Great Illegals The Great Illegals of the interwar period leveraged their personal flair and charisma to achieve remarkable successes against target countries with very weak security posture. Some of their earliest successes were in obtaining diplomatic cipher material, which was passed to a large SIGINT agency where diplomatic traffic was deciphered. Stalin didn\u2019t trust anyone to analyze the intelligence for him and acted as his own intelligence analyst. This reinforced his warped worldview as the secret services produced reports that catered to his paranoid suspicions. - Great Illegals were unique and remarkable spies: multilingual Central Europeans with great faith in Communist future; freer from bureaucracy, in comparison to post-war period - Target countries had very lax security - First successes were in obtaining diplomatic ciphers - Dmitri Aleksandrovich Bystroletov (HANS, ANDREI): very handsome, extroverted: portrait hangs in secret memory room of SVR Center in Yasenovo - Seduced female staff in foreign embassies: Prague, 1927: seduced 29 y.o. secretary in French embassy (LAROCHE) who provided British and Italian diplomatic ciphers and classified communiques for 2 years - Agents introduced ANDREI to other sources of information - Oldham , who provided British ciphers, provided introduction to Raymond Oake (SHELLEY) - De Ry provided Italian ciphers, provided introduction to Rodolphe Lemoine - Rodolphe Lemoine (JOSEPH) - Passion for espionage: began work for French Deuxi\u00e8me Bureau (DB) in 1918 - Recruited German cipher clerk in 1931 who was DB\u2019s most important source for a decade - Some intel fed into Enigma code breaking machines - Eventually passed to Ignace Reiss (RAYMOND), who defected in 1937 - Henri Christian Piecke (COOPER) - Flamboyant Dutch artist - John H. King (MAG) - Irish, hated English - Classified Foreign Office communications coroborated by DUNCAN (below) - Moisei Markovich Akselrod (OST, OSTO) - Jewish family, born 1898 - hired by INO in 1922 - Multilingual: Arabic, French, German, English, Italian - Francesco Constantini (DUNCAN) - Classified documents, ciphers from British Embassy in Rome - Also sold documents to Italian intelligence - Continued to provide intelligence after dismissal through brother Secondo Constantini (DUDLEY), also employed at embassy - Executed during Great Terror - Joint OGPU/Fourth Department SIGINT agency decrypted diplomatic traffic - largest SIGINT agency in the world at the time - No analysis of intelligence - Stalin considered analysis to be \u201cguesswork\u201d - Conspiracy theories of Stalin\u2019s continue to survive to this day Chapter 4: The Magnificent Five Arnold Deutsch established the recruiting strategy for the Magnificent Five, young talents in Oxford and Cambridge Universities with Communist sympathies who became the most successful Soviet penetrations of Western governments during WW2. Arnold Deutsch (STEFAN, OTTO): True believer in Communism, chemistry PhD from Vienna University, five years after entering as undergraduate; began work for INO in 1932. Recruited 20 agents, including C5, over 4 years as controller Kim Philby (SOHNCHEN, SYNOK): heterosexual athlete; was not productive before 1937, when he was sent to Spain as war correspondent, wounded, and ultimately awarded medal by Franco, whom he was supposed to assassinate Donald MacLean (WAISE, SIROTA): bisexual, approached by Philby in 1934; Foreign Office Guy Burgess (MADCHEN): flamboyant homosexual and social butterfly Joined SIS in 1938, in newly founded Section D (covert action and influence) Anthony Blunt: homosexual, introduction by Burgess Talent spotter John Cairncross (MOLI\u00c8RE, LISZT): polygamist, spotted by Blunt, approached by Burgess, recruited by Klugmann; Foreign Office Norman Klugmann (MER): prominent Communist activist who acted as talent spotter for NKVD, recr. 1936. Given away by Ignace Poretsky in 1937 Teodor Maly (MANN) Hungarian POW during WW1, joined Bolsheviks during Revolution Head of London residency in 1936, where he completed recruitment of C5 with Deutsch Recalled to Moscow during Great Terror, shot in 1937 (Ch. 5) Internal turmoil in Soviet Union affected espionage Hunt for Trotskyites became priority by end of 1937 Great Terror: all 3 of Deutsch\u2019s residents during residency in London were executed Chapter 5: Terror The fantasy of a Trotskyite conspiracy increasingly obsessed Stalin during the 1930s, who directed the NKVD and OGPU to penetrate Trotsky\u2019s organization. Trotskyites became targets of a cell of assassins called the Administration for Special Tasks, based out of the Paris residency. The Great Terror resulted in the liquidation of so many NKVD officers that tradecraft suffered. The Cambridge Five themselves, despite the quality of intelligence they provided, were suspected of being plants. \u2022 Mark Zborowski (MAKS, MAK, TULIP, KANT): Russian-born Polish Communist who deeply penetrated Trotsky\u2019s entourage \u25e6 Confidant of Lev Sedov, elder son of Trotsky \u25aa Entrusted with key to Sedov\u2019s letterbox and Trotsky\u2019s most confidential files and archives \u25aa Convinced Sedov to go to a Russian clinic for appendicitis while assassination was being planned \u25e6 After Sedov\u2019s death, encouraged internecine warfare between Trotskyites \u25e6 Orlov knew his first name and attempted to warn Trotsky after defection in 1938 \u2022 NKVD Administration for Special Tasks specialized in assassination and abduction, especially in France, headed by Yasha Serebryansky, resident in Paris \u25e6 Largest section of Soviet foreign intelligence by 1938, claiming to have 212 illegals in 16 countries \u25e6 Trained members of International Brigades in sabotage \u25e6 Main task was surveillance and destabilization of French Trotskyites \u25aa theft of Trotsky\u2019s papers from a Paris flat coordinated by Zborowski, who escaped suspicion \u25e6 Abduction of General Yevgeni Karlovich Miller \u25aa Entourage penetrated: Miller\u2019s deputy was NKVD agent \u25aa Another illegal was used to surveil Miller \u25aa Miller disappeared in broad daylight on a Paris street, drugged, packed in a heavy trunk, and sent to Moscow by Soviet freighter where he was interrogated and shot \u25e6 Assassination of Lev Sedov \u25aa Op was aborted after furor re. NKVD involvement in Miller\u2019s disappearance \u25aa Sedov developed appendicitis, died mysteriously a few days after a successful operation in Russian clinic (at Zborowski\u2019s insistence) \u25aa NKVD had a sophisticated medical section called the Kamera, experimented with lethal drugs \u25e6 Assassination of Rudolf Klement: secretary of Trotsky\u2019s Fourth International \u25e6 Assassination of Ignace Poretsky (Reiss, RAYMOND) using machine gun and chocolates laced with strychnine \u25e6 Assassination of Leon Trotsky: operation UTKA \u201cduck\u201d became chief Soviet foreign policy objective, to be effected by three groups \u25aa Penetration by illegal Ram\u00f3n Mercader (RAYMOND, alias Frank Jacson sic ), who seduced a Trotskyite secretary \u25aa Succeeded in killing Trotsky with icepick, caught and sentenced to 20 years imprisonment \u25aa Hero\u2019s welcome in Moscow 1960 \u25aa Assault on villa led by David Alfaro Siqueiros (KONE), Communist painter \u25aa Iosif Romualdovich Grigulevich (MAKS, FELIPE), member of Serebryanksy\u2019s cell, real leader of assault \u25aa Escaped to Argentina where he planted hundreds of mines in cargo ships bound for Germany \u2022 Spanish Civil War was training ground for saboteurs and battlefield against Franco\u2019s fascists as well as Trotskyites \u25e6 Orlov coordinated two-front war in Spain, ultimate goal was to build a secret police force under Soviet control \u25aa NKVD assassins murdered Andreu Nin, head of a Trotskyist workers\u2019 organization, as well as dozens of other Trotskyites in Spain \u25aa Orlov eventually defected to the US \u25e6 Stanislav Alekseyevich Vaupshasov: top assassin - Led raids on Polish and Lithuanian border villages dressed in Polish and Lithuanian army uniforms in the 1920s - Murdered a colleague in 1929 - Constructed and guarded secret crematorium which disposed of NKVD victims (SVR still considers this topic sensitive, gave hush money to female relative of the NKVD agent in charge of guarding this crematorium) - Great Terror sprung from Stalin\u2019s obsession with counterrevolutionaries - Leadership of NKVD liquidated and re-liquidated - Nikolai Ivanovich Yezhov, head of NKVD 1936-1938: author of Great Terror, replaced Yagoda who soon made absurd confessions - Replaced by Lavrenti Beria in December 1938 before accused of conspiracy - Abram Slutsky, chief of INO, poisoned by cyanide in 1938 - Slutsky\u2019s successors also shot before the end of the same year - NKVD officers were liquidated - Had to be careful even of body language or sighing - Officers most quick to denounce peers of imaginary crimes were most likely to survive Most of the Great Illegals were liquidated by 1938, except for: - Deutsch who was betrayed in 1937 by Ignace Poretsky (Reiss, RAYMOND) - Bystroletov brutally tortured before confession, imprisonment; wife sent to gulag where she cut her own throat with a kitchen knife; mother poisoned herself - Serebryansky himself recalled to Moscow and condemned in 1938 - Show trials depicted a vast, absurd conspiracy authored by Stalin, who proofread transcripts before publication - Great Terror and British operations - C5 transferred to legal residency, where controllers were much less experienced - MacLean seduced his new Soviet controller (NORMA, ADA) - Condemnation of C5\u2019s controllers and recruiters as enemies of the people placed their intelligence and reliability under question - Beria eventually disbanded the residency in 1940, Centre ordered all contact with Philby and Burgess to be broken off - Ideological commitment of C5 remained strong even after the Molotov-Ribbentropp pact - British agents were motivated out of revilement for fascism - Some agents ended their espionage - Histories of Stalin era still whitewash the emphasis on assassination of political opposition in Western Europe South Africa Intelligence's role in supporting counter-revolutionary operations of the apartheid state, as well as in the AND and SACP's efforts to overthrow the state. Five key uses of intelligence in counter-revolutionary struggle:[^4] Targeting enemies of the state, internal (including South West Africa) and external (including in Europe) ZA relationships with other states (Angola, Botswana, Mozambique, Tanzania, Zambia, and Zimbabwe) as anti colonial movement spread, isolating ZA Anti-Communist paradigm of Cold War, alliance with US, UK, West Germany, and Israel Overcoming anti-apartheid sanctions from the 1970s and 80s Developing a nuclear weapon Relationship with British services strained because of Afrikaner resentment, British concern of infiltration by Afrikaner nationalists. In the nineteenth century, the Boer republics (Transvaal Republic and Orange Free State) maintained limited intelligence organizations led by Cornelius Smidt and Willem Leyds (Transvaaler Secret Service).[^5] South African Police Detective Branch (SAP/DB) conducted counter-subversion and counterintelligence after 1899-1902 Anglo-Boer War and the establishment of the Union of South Africa in 1910. SAP/DB concentrated mostly on Nazi sympathizers in South West Africa until 1948, when the National Party (NP) was elected.[^6] MI5 conducted foreign intelligence but also watched radical Afrikaner groups such as Ossewa-Brandwag (OB), a paramilitary organization in competition with NP and with many sympathizers in SAP. Union Defense Forces, considered an anglophile institution, cooperated with MI5 on internal threats (Afrikaner nationalists and Republicans) and British Special Operations Executive (SOE), operating out of Durban. MI5 Director General Sir Percy Sillitoe served in South Africa and Rhodesia and was a key influence in shaping the South African intelligence structure. SAP Special Branch (SB) founded as Special Staff to hunt Nazis in 1939 before being redirected to investigating political crimes. Increasingly into the 1950s, became known as Security Branch. Union Defense Forces Department of Military Intelligence (DMI) created Feb 1940 but neglected after 1948. After independence from Britain in 1961, UDF became South African Defense Forces (SADF) and established a new Directorate of Military Intelligence (DMI) July 1962. Rivalry between SB and DMI survived administrative efforts to coordinate intelligence functions: State Security Committee (est. 1963) and State Security Advisory Board (est. 1966). SB\u2019s criminal emphasis hampered intelligence work, and a central intelligence agency was needed, sought first in Republican Intelligence (RI) spun off from SB 1963, then found in BOSS. Bureau for State Security (BOSS) founded 19690513 as a central intelligence apparatus to mitigate the rivalry between DMI and SAP. Reorganized as National Intelligence Service (NIS) in 1980. Grew from 500 personnel in 1969 to more than 1,000 by 1978. Six departments: Subversion, Counter-espionage, Political and Economic Intelligence, Military Intelligence, Administration, and National Evaluation, Research and Special Studies. Forged a relationship with Portuguese and Rhodesian intelligence services. Brought down by government\u2019s increased reliance on COIN strategies (vice counterintelligence and counter-espionage) and by the Information Scandal. African National Congress (ANC) , Umkhonto weSizwe (MK) , and South African Communist Party (SACP) established intelligence structures separate but parallel to that of the government during their armed struggle against the apartheid state. ANC established Department of Intelligence and Security (DIS). DMI gained control of \u201cTotal National Strategy\u201d and achieved dominance over BOSS. DMI strategists implemented various counter-insurgency (COIN) strategies they learned from abroad in ZA and SWA. Botha appointed Minister of Defence in 1966 and began a campaign to reinvigorate the SADF. By the beginning of Project SAVANNAH (South African intervention in Angola[^7] in 1975, Botha had begun a process of streamlining the SADF that would culminate years later in the incorporation of COIN principles in the South African Army. Area Defence Policy, later known as National Security Management System, was fully incorporated into the counter-insurgency forces of SADF. COIN forces were made up of part-time SADF personnel from the Citizen Force and Commandos and were divided into ten regional commands covering the countryside. SADF doubled in size between 1975 and 1990, reaching almost 100,000 with another 325,000 in the Citizen Force and Reserves ZA security confronted radical new challenges from 1975 to 1978. Key governments on ZA's periphery had turned hostile. Marxist MPLA took over Angola and hosted SWAPO bases on the border of South West Africa. Mozambique's new government was Marxist. Mozambique became a shelter for Rhodesian guerrillas, threatening ZA's last counter-revolutionary ally. Radicalization of blacks after 1976 uprising in Soweto overwhelmed internal security, which responded by introducing COIN concepts and reacting more harshly to protests. SADF COIN expertise was gained in collaboration with Rhodesian ISS and SOF during the 1960s and 1970s. ZA intelligence establishment absorbed members of Rhodesian Security Forces in the transition to majority rule in 1980 (Operation Winter).[^8] COIN Strategy Adopted Wholesale, New Units Established Opposition to apartheid state grew more radical while the state\u2019s response to opponents hardened. A sustained domestic protest movement called United Democratic Front ultimately gave ANC-MK their long sought-after internal subversion capability within ZA. Pretoria adopted COIN strategies in response to failing effort against MPLA in AO and SWAPO in SWA and deteriorating domestic security as MK stepped up attacks. K-Unit \u201cKoevoet\u201d founded by SAP/SB in January 1979, building on their decades of cooperation with Rhodesians in COIN operations. Formed after the Selous Scouts and Portuguese Flechas for the purpose of turning captured ANC, SWAPO, and PAC guerrillas called askaris. Turned SWAPO fighters who collaborated with Koevoet were known as makakunyanas \"blood-suckers\", and Koevoet collaborators were targeted for assassination by SWAPO. They were paid for each guerrilla killed. SAP/C1 \u201cVlakplaas\u201d named after the police farm outside Pretoria. Originally established 1979 when BG Johan Coetzee, C/SB, decided to use COIN activities for counter-revolutionary purposes within ZA. Revealed by Coetzee in the press in 1989 and disbanded in 1993. Koevoet elements including de Kock were withdrawn from SWA to form C1 in May 1983. Identification, tracking, and \"rehabilitation\" (turning) of ANC and PAC guerrillas. C1 also assassinated up to 65 people from 1980 to 1991. C1\u2019s operations in Swaziland were to disrupt the ANC/MK structures there. 22 ANC members were assassinated in Swaziland in the 1980s. C2 established concurrently to track activists leaving ZA and to interrogate arrested guerrillas SAP/G section , responsible for the penetration of ANC abroad, was resurrected after the reorganization of BOSS in 1980. G section attempted to assassinate ANC/MK strategist Joe Slovo, killing his wife instead. G section also blew up ANC\u2019s London office Directorate Covert Collection (Direktoraat von Koverte Insameling) established at an unclear date, but possible predecessor was Directorate of Covert Information, active in SWA by 1982. Established multiple front companies with the goal of duplicating successful COIN operations of AO and SWA within South Africa. Spread of liberation movements across Southern Africa removed governments traditionally friendly to Pretoria, which saw all black liberation movements as Communist-backed threats to regime. South African government resorted to policy of destabilizing neighbors and conducting covert action to remove safe havens for ANC/MK. Eventually South Africa\u2019s hand in regional politics became transparent. DMI integrated intelligence collection with covert action by taking control of South African Special Forces (SASF) in 1979. Rhodesian special forces integrated into SADF in 1980, further buttressing COIN capabilities. Directorate of Special Tasks (DST) formed in mid-1970s from a corps of DMI operators named Spesmagte, similar to Recces. DST was responsible for overseeing contra-mobilization and counter-revolutionary activities of DMI throughout southern Africa as part of a strategy to deny ANC-SACP safe havens in Frontline States. DST began operations in an office in Rundu, Namibia in 1976 in the wake of South African withdrawal from AO. First chief was COL Cornelius van Niekerk. DST terminated operations in the early 1990s. Two sections: - DST-1 (external operations) covered UNITA, RENAMO, and Zimbabwe - DST-2 (internal operations) covered Lesotho Liberation Army (LLA), and Operations MARION and KATZEN DST maintained logistical infrastructure throughout Southern Africa and conducted several operations, providing support to anti-Marxist proxies in neighboring countries: - DISA/SILWER : support to UNITA in Angola - DRAMA : support to Zimbabwean dissidents - PIKI/PUNDU MILIA/ALTAR : support to RENAMO and operations against FRELIMO in Mozambique. 5 Recce was principally responsible because many 5 Recce personnel had trained with the Selous Scouts. When RENAMO\u2019s headquarters moved from Phalaborwa, Transvaal (!) to Gorongosa, Mozambique, South African officers followed offering intelligence training. - PLATHOND : support to surrogate force in Zambia - CAPSIZE/LATSA : support to Lesotho proxy grou 5 Recce based out of Phalaborwa, Transvaal. Supported Op PIKI/PUNDU MILIA. Also used pseudo-operations against SWAPO in Namibia, sometimes cooperating with Koevoet. DCC mobilized contras in Namibia. Various operations: - ETANGO : DCC and other DMI units attempted to establish a conservative contra based in Ovambo tribalism to counter SWAPO. - EZUVA : Similar project to establish a contra among the Kavango. Many experienced contra-mobilizers from DCC moved into domestic contra-mobilization 1985-1986, setting up groups to foment black-on-black violence and undermine support of ANC and UDF: 23 such projects by 1986. Operations included: - Operation MARION provided security training and weapons to more than 200 Inkatha cadres 1986-1990. These units later conducted targeted killings. Inkatha had been supported by BOSS as an alternative to ANC from 1975, including funds and stage-managing internal political rivals to Chief Buthulezi. - Operation KATZEN was an attempt to organize a contra group among the Xhosa known as the Xhosa Resistance Movement (XWB, known as Iliso Lomzi). Cooperated with Army Intelligence Hammer units, which conducted special operations. Civil Cooperation Bureau (CCB) , also known as Burgerlike Samewerkingsburo (BSB), was formed in 1986 out of Operation BARNACLE. Formed by SASF to fulfill the requirement of domestic intelligence collection, which was used primarily for external operations. CCB was imagined to be fully functional only in the mid-nineties, possibly to conduct counter-revolutionary warfare after the transfer to black rule had been completed. CCB was organized as a corporation into Regions that coordinated covert activities in concert with other state bodies. CCB numbered up to 250-300 individuals. By the late 1980s, CCB had established numerous front-companies and businesses and was involved in lucrative criminal activities. Operations: - CCB conducted internal assassinations in line with the state\u2019s emphasis on counter-revolutionary warfare. As such, CCB was DMI\u2019s equivalent to SAP\u2019s C1. - CCB supported SASF in operations in the Frontline States by undertaking reconnaissance of ANC targets. 3 Recce (active 1980-1981) absorbed the remnants of Rhodesian special forces which fled Zimbabwe in 1980 as well as DMI\u2019s D-40 assassination unit (active 1979-1980) led by the Rhodesian Garth Barrett. 3 Recce operated against Zimbabwe, exploding very destructive bombs and assassinating ANC\u2019s representative in Harare.[^9] Operation BARNACLE , conducted by a group 30-40 mostly black ex-Rhodesians, was a project to use CBW to assassinate guerrillas, SWAPO prisoners of war, and members of South African security forces suspected of disloyalty with poison. BARNACLE was to be a completely independent resource at the disposal of the country\u2019s leaders, to serve as a hedge against the prospect of the government granting too much power to blacks. BARNACLE actors were not accountable to official security organs or to SADF commanders: they reported directly to General Office Commanding Special Forces (GOC-SF). Later reorganized into the Civil Cooperation Bureau. Notable People BG J.P. Tolletjie Botha ran Directorate Covert Collection. COL Jan Breytenbach founded the Reconnaissance Commandos, 32 Battalion, and the Directorate Special Tasks. Chief Buthulezi was leader of Inkatha and a BOSS stooge. COL Eugene de Kock was one of the most ruthless and effective of Koevoet's commanders. Eleven tours of duty between 1968 and 1973 in Rhodesia with Rhodesian SAS and Rhodesian African Rifles. Commanded Koevoet for four years before he requested a transfer to SAP/C1, where he assumed command in 1985. De Kock emphasized the assassination program and introduced paramilitary training for police members. During this time, De Kock became known as Prime Evil for his mercilessness. MAJ Craig Williamson was a counterintelligence operative in SAP/G Section from... to... Williamson infiltrated ANC by using the International University Exchange Fund (IUEF), but was exposed in 1980. Williamson served in SAP/G until December 1985. Williamson also established Longreach Pty Ltd in April 1986, which served as a front company for SAP/SB operations and also coordinated DMI and SASF operations. Sweden There was no specific institution in Sweden for intelligence before the 1930s. Navy intercepted communications and diplomats gather intelligence.[^10] Military Intelligence Before WW2 General Staff gathered intelligence against Norway during the war of secession Swedish Defense Staff established in 1937. Intelligence branch formed with 20 officers. Mostly OSINT collection and attache reporting from 15 attaches and SIGINT. Intelligence Organizations Formed in WW2 C-Bureau (central byr\u00e5a) under Defense Staff, but separate from Intelligence Division, established large intelligence network that was not fully documented. Cryptographic Department (CD) of Defense Staff took over SIGINT from Navy using civilians to succesfully break Soviet crypto and comms. Results were shared with Finnish. Decrypted machine crypto used over landlines. 150,000 cables over two years, until Germans changed codes. Swedish were unable to fully exploit this windfall of intelligence. CD reformed as F\u00e5ssvarets Radioanstalt (FRA), directly under Ministry of Defense. General Security Service (Allm\u00e4nna S\u00e4kerhetaj\u00e4nsten, GSS) formed out of a secret, extralegal government decision, established massive program of unlimited authority to monitor telephone, telegraph, and mail communications. Dissolved after scandal in 1946. Postwar Developments 2 of the 3 wartime intelligence agencies would be resurrected after dissolution 1947-1948. India Kautilya\u2019s Arthashastra[^11] - Constitutes a doctrine of statecraft using espionage as a basic means of governance. - Comprehensive textbook on statecraft, foreign diplomacy, and war emphasizing the collection of domestic and foreign intelligence. - Rediscovered and translated by Orientalist Rudrapatnam Shamasasty in 1909-1915. Written by Kautilya, trusted advisor to Chadragupta Maurya, founder of Mauryan Empire 321-185 BCE. Eight Institutes of Espionage Four were forms of religious cover (fraudulent disciple, recluse, ascetic, and mendicant woman), which were to take advantage of the intensely religious population of India. Religious class had access to other castes. Wandering female spies were to take religious cover, poor widows of Brahman caste were to target upper castes while Sudra-caste women willing to shave their head were to target lower caste communities. Classmate spies referred to recruitment pool, and the preferred choice for courier. Firebrands were to be used for covert action as assassins, agent provocateurs, and saboteurs. Result was a pervasive surveillance network covering the whole country. Treasury was also to have intelligence function Householder spies were to ascertain validity of assets Merchant spies were to monitor price changes and foreign goods Networks of spies under cover of bands of thieves would monitor the criminal underworld Provocation and entrapment are standard tactics in Arthashastra. Islamic Caliphate Pre-Islamic Arab tradition of intelligence predated a more developed intelligence culture in the Islamic age.[^12] Various words referring to spy or scout: - jasus : foreign spy - tajassasah : discouraged in the Qur\u2019an - \u2018ain : \u201ceye\u201d - suhhar : night sentinels who kept watch for strangers at approaches to market town or crossroads - rabi\u2019ah : lookout - other words Espionage in Early Arab States Lakhmid and Ghassanid buffer states between Sasanian Persia and Byzantium: - Story in 10th c. Kitab al-Aghani relates Lakhmid spies catching a would-be assassin and killing him in the 6th century - Scouts in Arabian peninsula Brigandage among Bedouin Skirmishes and raids ( suluk ) involved use of scouts - al-Basus War was remembered as the days of rabi\u2019ah - Abu Faraj al-Asbahani\u2019s anthology of Arabic verse - Muhammad b. al-Tabari on agents using disguises Muhammad and Intelligence Qur\u2019anic regulations on espionage reflect importance of clandestine activites in early Islam. The Prophet Muhammad\u2019s involvement in intelligence and espionage: - Muhammad gathered information on early converts, seeking candidates with honesty, trustworthiness, and the ability to keep a secret - Muhammad possessed detailed knowledge of clan loyalties and politics, and used this knowledge in negotiations with Bedouin - Abdullah b. Abu Bakr mingled with Quraishis of Mecca and report back to him at night in his cave. Abu Bakr\u2019s sister Asma also spied for the Prophet: first spies for Islam. - Abu al-Fadhl al-Abbas ran spy network in Mecca - Many \u2018ains from various corners of Arabia, from every town and tribe - Muhammad deliberately retreated during the Battle of Uhud to allow his lookouts to determine the size of the army, whether it had mounted camels (to retreat) or horses (to attack) - Muhammad debriefed two boys who were caught drawing water from a well before the Battle of Badr. They divulged key intelligence on the closing Quraishi army. Deception After the indecisive Battle of Uhud, Muhammad sent one of his \u2018ains to deceive the Quraishis into thinking a large host was approaching, causing them to retreat. Assassination of poets Poets had a complex role in Arab society and were highly influential - Asma of Marwan was stabbed in her sleep, but w\u2019o Prophet\u2019s prior approval - Abu Afak killed by fellow tribesmen - False prophet al-Aswad al-Ansi became influential and killed the governor of Yemen. Muhammad ordered Wabrak b. Yahmus to organize a plot. Wabrak recruited a circle of Persian Muslim converts and the governor\u2019s wife, who facilitated the operatives\u2019 infiltration into the castle where al-Ansi was murdered Intelligence by Islam\u2019s enemies Abu Sufyan determined Muslim spies were present by finding date seeds in camel dung, indicating the animals had Medinan fodder Multiple assassination attempts on Muhammad\u2019s life Umar b. al-Khattab in charge of counterintelligence and security Byzantines sent a monk who claimed to be a convert to Islam and established a mosque in Medina. Muslim agents surveilled the mosque after suspicious comings and goings and ultimately demolished the mosque. US Corporate espionage Vice reported that McDonald's had established an intelligence unit to monitor workers who supported increasing the minimum wage. The unit had targeted the Fight for $15 movement for increasing the wage to $15 an hour in particular. One intelligence report titled \"Ongoing FF$15 Activity Against McDonald's During the COVID-19 Crisis\" contained an analysis of the activities of labor activists. McDonald's had been using two different data collection software suites to collect open-source intelligence on the social networks of workers involved in the labor movement. COMINT War Department set up first organized cryptanalytic office in June 1917, numbering 3 people. By war\u2019s end, it would grow to 150. Navy In 1917 and 1918, Navy set up medium frequency DF stations along Atlantic coast to track U-boats. HFDF stations were researched and deployed by 1938. Strategic HFDF stations were established at Manila, Guam, Midway, Oahu, Dutch Harbor, Samoa, Canal Zone in Panama, San Juan Puerto Rico, and Greenland. US began tracking Japanese warships and merchant vessels in 1939, five years after the Japanese had begun tracking US vessels. Navy had established the Code and Signal Section of Naval Communications for producing codes and ciphers for use by Navy. Registered Publication Section, responsible for distribution of secret and confidential documents, was spun off in 1923. Navy funded development of Electric Cipher Machine from 1922. Communications Intelligence Organization (CIO) was established 1924. Intercept stations were established in the Pacific Area (Shanghai, Oahu, Peking, Guam, Manila, Bar Harbor, Astoria), and Washington DC. Cryptanalytic Units established in Manila and Pearl Harbor. Training was done with technical manuals, using the codes to send messages. Minor intercept activities were performed in strategic HFDF stations. In 1938, CIO became the Communications Security Group (CSG) and took over all Navy DF stations. By 1941, CSG had 730 total personnel. Arm Army Signal Corps founded 1860 by Albert James Myer, inventor of wig-wag flag signaling method. Signal Intelligence Service (SIS) founded 1930 as a secret part of Signal Corps for cryptanalysis. By 1939, SIS made use of 7 intercept sites from the Philippines and Hawaii in the West to the East Coast of the US. These were the sources of SIS intercepts until after Pearl Harbor. At SIS HQ in Arlington Hall, traffic was split between four analytic sections: - J: Japanese - G: German - I: Italian - M: Mexican and Latin American Although SIS intercepted tens of thousands of IJA messages from its station in Manila, these messages could not be fully exploited because IJA ciphers were not broken. However, SIS broke several diplomatic ciphers including Purple. After the war, SIS changed its name to the Army Security Agency (ASA) in 1945. In 1947, ASA and the Army Intelligence Agency were merged into the newly formed Intelligence and Security Command (INSCOM). Radio Free Europe Established 1949 by the National Committee for a Free Europe (NCFE), an anticommunist organization with Allen Dulles and Dwight D. Eisenhower as board members. Funded by CIA until 1972. Targeted Eastern European countries (as opposed to Radio Liberty). Radio Liberty Established by American Committee for the Liberation of the Peoples of Russia (Amcomlib) 1951. By 1954 was broadcasting in several other Central Asian languages. Foreign Broadcast Information Service Foreign Broadcast Monitoring Service, or FBMS, established 1941 under FCC to monitor Axis shortwave broadcasts to the US. Name changed to FBIS 1947 when it was made part of the new CIA. France (Andrew Orr) France took over Syria and Turkey\u2019s SE (Cilicia) after WW1 and perceived Turkish War of Independence as threat to its new imperial interests. Military intelligence services of Army and Navy monitored Kemalist movements and always saw the hand of German and Russian commies behind Turkey\u2019s developments. 3 reasons German closely involved in Ottoman military affairs from 1883 and German general Otto Liman von Sanders commanded the Ottoman Army Germans had also promoted pan-Islamic movement during the War Germans possibly still controlled Russia, according to the French, reasoning that they had sent Lenin to Russia inside of a sealed train car Colonial intelligence services lacked some of the checks on extreme predictions when reporting on events outside of Europe Military Intelligence French Army\u2019s Service de renseignments guerre ( SR Guerre ), which was part of a unified French intelligence organization during the War, but then reverted to the Army\u2019s II Bureau after its end. During the Turkish War of Independence, SRG deployed a small number of officers to the Middle East, stations opened in Constantinople in 1919, Algiers 1925, Rabat and Tangiers 1929. French Navy service ( SR Marine ) was similarly structured to Army, but with more familiarity with Mediterranean Sea and the Middle East SRM opened Constantinople station in 1919 SR sources included Europeans and Americans fleeing Turkey, human informants, and newspaper articles, as well as intercepted radio messages sometimes by way of the Brits Coincidence of treaty signings between USSR, Turkey, and Afghanistan and Persia led SR personnel to believe there was a plot brewing Poland From Anglo-Polish HUMINT, by PRJ Winter Histories of WW2 intelligence exalt COMINT successes of GCCS to the detriment of MI6 Historiography of British WW2 intelligence Agents - Paul Thummel (codename A-54), senior officer of German MI (Abwehr), recruited by Czechs 1936. SIS and Czechs ran him jointly from 1939 until he was arrested by the Gestapo and died in prison 1942. - Warlock, on staff of German High Command (Oberkommando der Wehrmacht, OKW), turned by 1941. - Knopf, reported on OKW intentions to take Malta, but records show that Germans were noncommital to the plan (Op Herkules) which would only support Italia High Command. Knopf may have been turned by this time. MI14\u2019s evaluation gave Knopf a mixed, but generally positive score. Polish government-in-exile settled in London and cooperated with British intelligence (II Bureau of Polish General Staff) SIS cooperation with Poles was driven out of desperation because aside from Warlock they had no useful penetrations of Nazi Germany CX reports were SIS, JX reports were from Poles. Some JX reports found in British National archives, including one report on Malta which was passed to Churchill himself. Poles ran sources reporting from the heart of OKW and OKH (Oberkommando des Heeres, Supreme High Command of the German Army) British interception of Polish communications confirmed Knopf\u2019s bona fides (as agent number 594) and the Poles\u2019 as well [^1]: Sawyer, Ralph D. \u201cSubversive Information: The Historical Thrust of Chinese Intelligence.\u201d Intelligence Elsewhere: Spies and Espionage Outside the Anglosphere . Ed. Philip H. J. Davies, Ed. Kristian C. Gustafson [^2]: [^3]: Homstr\u00f6m, Lauri. \u201cFinnish Security and Intelligence Service.\u201d Intelligence Elsewhere: Spies and Espionage Outside the Anglosphere .Ed. Philip H. J. Davies, Ed. Kristian C. Gustafson [^4]: O\u2019Brien, Kevin A. The South African Intelligence Services: From apartheid to democracy, 1948-2005 . Routledge: New York, NY 2011. [^5]: Blackburn, Douglas and Caddel, W. Waithman. Secret Service in South Africa . Honolulu: University Press of the Pacific, 2001. Swanepoel, P.C. Really Inside BOSS: A Tale of South Africa\u2019s Late Intelligence Service (And Something about the CIA) . Pretoria, 2008. [^6]: Ref. Kent Fedorowich, \u201cGerman espionage and British counter-intelligence in ZA and Mozambique, 1939-1944\u201d , The Historical Journal 48:1 [^7]: Robin Hallett, \u201cThe ZA Intervention in Angola,\u201d African Affairs 77:312 (July 1978) [^8]: Ngwabi Bhebe, Terence Ranger, Soldiers in Zimbabwe's Liberation War , 1995. [^9]: D-40 in turn was the reconstitution of the supposedly disbanded Z-squads used by BOSS for assassinations until its reorganization in 1979. [^10]: Agrell, Wilhelm. \u201cSweden: Intelligence the Middle Way.\u201d Intelligence Elsewhere: Spies and Espionage Outside the Anglosphere . Ed. Philip H. J. Davies, Ed. Kristian C. Gustafson [^11]: Davies, Philip H. J. \u201cThe Original Surveillance State: Kautilya\u2019s Arthashastra and Government by Espionage in Classical India. Intelligence Elsewhere: Spies and Espionage Outside the Anglosphere . Ed. Philip H. J. Davies, Ed. Kristian C. Gustafson [^12]: Al-Asmari, Abdulaziz A. \u201cOrigins of an Arab and Islamic Intelligence Culture.\u201d Intelligence Elsewhere: Spies and Espionage Outside the Anglosphere . Ed. Philip H. J. Davies, Ed. Kristian C. Gustafson United States Linda Zall established a program to analyze classified historical statellite imagery to analyze not foreign militaries but changes in the environment, in particular the extent of ice retreat in the polar regions of the Earth. The effort was sparked by then-Senator Al Gore whose letter to the CIA led to the establishment of the MEDEA program which declassified satellite imagery and oceanographic data. John Walker was a notorious spy who volunteered to the Soviet embassy in Washington in 1967. Especially after the North Koreans captured the USS Pueblo, Walker's information on the key list of the KL-47 cryptographic machine meant the Soviets were able to read US Navy communications until the entire system was replaced. John Walker was managed by former KGB general Oleg Kalugin. The acoustic characteristics of the Victor III submarine were made substantially less detectable as a result of Walker's revelation that the Soviet submarine fleet was easily tracked. The stealthy Akula-class submarines, launched in 1985, also benefited from the import of a Toshiba CNC milling machine , which in combination with Norwegian CNC machines , allowed propellors to be designed that were much quieter than before.","title":"\ud83d\udd0e Intelligence"},{"location":"Misc/Intelligence/#intelligence","text":"Inside the military's secret undercover army","title":"\ud83d\udd0e Intelligence"},{"location":"Misc/Intelligence/#china","text":"Chinese writers often wrote on intelligence matters in contrast with Western writers[^1] Historical writings on intelligence continue to be used by military institutions of PRC Fragments of earlier works \u201cprotobooks\u201d survive in later works. Sunzi wrote that warfare must be undertaken only after a comprehensive analysis of relative strengths and weaknesses of combatants, the world\u2019s first net assessment procedure. He also advocated deception on the battlefield. Later writers responded to Sunzi, updating and interpreting his precepts: Guan Zhong Li Quan, Li Jing. Sunzi condemned generals who failed to gather necessary intelligence and disparaged divination and consulting ancestors in favor of hard intelligence. Later writers like Shi Zimei also upbraided rulers who ignored these rules. Works advocating covert action - Toubi Fuban - Warring States period works on statecraft, e.g. Han Feizi Sunzi\u2019s Five Types of Spy Local Spy : Knowledgeable spies living outside their native habitat, including emigrants, travelers and exiles (e.g. defectors) Internal spy : Officials Turned spy : Double agent Living spy : Talented people dispatched abroad to observe and then report back Dead spy : Spies deliberately sent on suicide missions without their knowledge. Enemy agents could also be deceived and provided with false information, causing them to be executed when they report false information to their masters.","title":"China"},{"location":"Misc/Intelligence/#subversion","text":"By end of Spring and Autumn Period, subversion was an accepted goal of espionage. Assassination used most often in periods of fragmentation. Six Secret Teachings compiled two chapters on systematic programs of subversion. These two chapters, with Sunzi\u2019s \u201cEmploying Spies\u201d in Art of War formed the basis of espionage as late as the Ming Dynasty. Later military writers also devoted at least a few paragraphs to subversion: Le Quan\u2019s Techniques for Secret Plots, Huqian Jing, Bingfa Baiyan. These took historical and semihistorical episodes as examples (Wu Yue Chunqua). Covert action and subversion used by Qin in the western hinterland.","title":"Subversion"},{"location":"Misc/Intelligence/#debauching-kings","text":"Sunzi: \u201cYou must first know the names of the defensive commander, his assistants, staff, door guards, and attendants for any armies that you want to strike, cities you want to attack, and men you want to assassinate.\u201d","title":"\u201cDebauching\u201d kings"},{"location":"Misc/Intelligence/#egypt","text":"Egyptian intelligence had its origins in the urban policing organizations established during the British Khedive and occupation. Mamur Zapt s were the de facto chiefs of secret police in Cairo and Alexandria, operating networks of mukbireen informants, by the time of British occupation in 1882. The Central Special Office (CSO) was established in 1911 after the Coptic Prime Minister was assassinated by a young nationalist. When discontent exploded in 1919, and CSO couldn\u2019t respond effectively, the Interior Ministry formed the Special Section (SS) to centralize intelligence collection. CSO was closed and files transferred to SS in 1925. Egyptian Military Intelligence was being indigenized in the late 1930s. MI officers were complicit in the army coup of 23 July 1952. Once the Free Officers were in power, they set about creating a new intelligence community. The Military Intelligence Directorate (MID) as it was then called, became active in covert action and subversion across the Middle East and Africa. Fear of communist subversion fueled the creation of the General Intelligence Directorate (GID) , subordinate to the Interior Minister, which was led at first by Zakaria Muhi al-Din and stood up within months of the coup. GID inherited and took the place of the old political police organs. GID was renamed the State Security Investigations Service (SSIS) in 1971. As part of Gamal Abd al-Nasser\u2019s emphasis on covert action, the Egyptian General Intelligence Service (EGIS) was established by March 1954, also headed by Zakaria Muhi al-Din. EGIS was the capstone of the new Egyptian government\u2019s emphasis on projecting influence through covert action, and it was modeled after the CIA. During the heyday of Egyptian subversion abroad, Salah Nasr was EGIS Director. Egyptian covert action grew out of the effort to end the British occupation, remove British influence from government organs, and eject the British from the Canal Zone. In 1952, MID created a Special Resistance Office to form a guerrilla army against British military units based along the Suez Canal. The campaign became the template for Egyptian subversion abroad for the next decade and half. MID also sent officers to Sudan to recruit sources and build influence for possible unity with Sudan. While Egypt occupied the Gaza Strip, Palestinian infiltrators were a source of irritation for MID. However, MID sponsored infiltrations using Bedouins, Palestinians, and even its own scouts for intelligence-gathering. These infiltrations eventually were made from southern Lebanon as well. The death of an Israeli settler during one such raid sparked a cycle of reprisals that led to war. The four years after the British withdrawal from the Suez in 1956 marked Egypt at the height of its regional power. Nasser was the hero of the Arab world, and Cairo was the wellspring of a vigorous new Pan-Arab nationalism. However, Egyptian subversion against conservative Arab states and British protectorates was checked in many places by the United States. After Egypt and Syria joined the United Arab Republic (UAR), Nasser began covert action campaigns in Jordan and Lebanon, combining subversion with radio propaganda. In Lebanon, UAR meddling triggered a larger crisis that drew the attention of the US and ended Egyptian arming of Lebanese insurgents. A 1958 coup in Iraq initially drew feelersfrom the Egyptians, who hoped to draw Iraq into their circle of influence. However, old animosities arose again, and by 1959 Egypt was broadcasting radio propaganda, and senior UAR spymasters backed a short-lived army revolt in Mosul. Alienated by Nasser, Saudi Arabia fended off feeble Egypt\u2019s attempts to use its military presence and attache corps there for subversion. Nasser created African Affairs Branch, led by Mohamed Fa\u2019iq, to pursue Egyptian interests clandestinely across Africa. Key targets were Ethiopia and Congo. Egypt promoted Eritrean Liberation Front (ELF) after Suez Crisis and assisted Eritrean declaration of independence by 1960. Close relations with Somalia, which also had territorial disputes with Ethiopia. Egypt provided money, arms to friendly regime in Congo, flew Ghanaian and Egyptian troops to prop up regime. Also provided arms to resistance movements in Angola, Cameroon, Nigeria, and Portuguese Guiana, and training to Congo Brazzaville and Tanzania. MID broke down under pressure of war. Israelis had strong SIGINT and intercepted an insecure phone call between Nasser and King Hussein. Israel also captured a valuable cache of Egyptian intelligence documents in Gaza on MID\u2019s Palestinian infiltrators. Voice of the Arabs (VOA) served as a propaganda arm buttressing Egypt\u2019s foreign subversion policies. Daily radio programming was handled by Egyptian State Broadcasting (ESB) and used transmitters supplied by CIA. ESB turned to former Nazi propagandists, who must have felt at home with anti-Jewish programming. ESB grew out of EGIS promotion of VOA. EGIS had a representative on ESB\u2019s board of directors. EGIS was also responsible for running clandestine radio stations, reliant usually reliant on mobile transmitters. Voice of the Arab Nation targeted Iraq during the 1956 war when the British and French knocked out overt ESB transmitters. Egypt also operated a clandestine radio station called Voice of Free Iran in the early 1960s and gave support to anti-shah clerics. Egyptian intelligence had been preparing Yemen for subversion since 1950s. VOA broadcasted propaganda, and the signal to begin the coup in September 1962 was delivered by VOA. Egyptian army was overwhelmed by Yemeni rural insurgency, because Cairo didn\u2019t plan for a long war. No good cultural intelligence on Yemeni tribes. Intervention antagonized Great Britain and Saudi Arabia. MID provided training to National Liberation Front (NLF) which was set up by Egypt as an anti-British umbrella organization. NLF systematically kidnapped, tortured, and executed Aden SB officers and informants. Egypt still relied on broken Enigma-based encryption. GCHQ and NSA had access to military COMINT. Despite successful Egyptian-backed coup and energetic covert action campaign against British Protectorate in South Yemen, Egypt could not maintain political leverage over Yemen. South Yemen turned to GDR for intelligence training. The British were Egypt\u2019s first instructors in counterintelligence, and also some of its first victims. MI and Cairo SB busted a German spy ring in cooperation with British intelligence (The Key to Rebecca). Operation Susannah was an Israeli terror plot meant to undermine international faith in Cairo\u2019s ability to secure the Suez Canal. In 1954, an Israeli case officer activated sabotage cells among the remaining Egyptian Jewry. Egyptians doubled the Israeli case officer and rolled up the ring. Huge success for nascent Egyptian CI and was an impetus in formation of EGIS. GID rolled up a spy network led by the British expat James Swinburn in 1956. Unfortunately, this had the knock-on effect of forcing the British to rely on their formidable SIGINT collection. GCHQ had broken Egypt\u2019s diplomatic cipher and routinely intercepted Egyptian military communications. Egyptian CI penetrated several Israeli spy rings after the Suez War. The Goudswaard Ring of European and Egyptian nationals was recruited by Mossad. Eventually the GID turned them and were using them to pass disinformation. The Thomas Ring gathered military documents and passed them to Israel in microfilm hidden in furniture which was then exported. In 1961, GID rolled up this network. In the run-up to the Yom Kippur War, Egyptian denial and deception extended to their erstwhile allies the Soviet Union. Soviet photoreconnaissance satellites monitored the war. After the Yom Kippur War, CIA resumed its relationship with Egypt, and along with providing training and equipment, CIA began to aggressively recruit from throughout Egyptian society. Egypt joined the Safari Club (a coalition of intelligence agencies from France, Saudi Arabia, Iran, and Morocco), which provided aid to Zaire, Somalia, Djibouti and anti-Qadhafi groups in Libya and Egypt as well as the Afghan resistance. After Camp David, Egyptian intelligence had to contend with Libyan subversion and radio propaganda, mimicking Egyptian tactics of yore. Covert action escalated to a short border war in 1977. Libyan subversion continued, and in 1978 SSIS uncovered a clandestine organization for the \u201cliberation\u201d of Egypt through sabotage and assassination. Egypt also stopped an Iranian who planned to undermine the peace process by conducting and bomb attacks in Egypt. In 1979, Egyptian CI began a covert war with Soviet and East European services. SSIS claimed to have uncovered a Bulgarian intelligence network that year. In 1981, Soviet and Hungarian diplomats were expelled after Egyptian security discovered a spy ring allegedly inciting sectarian unrest. Israel attempted to subvert Egypt early on by using the Jewish population in Cairo and Alexandria as a base. IDF created sabotage cells which were activated in 1954 when Egypt and UK were negotiating a British withdrawal from the Canal Zone. Operation Susannah was rolled up, and the operatives brutally tortured in a MID prison. The case had political ripple effects within Israel that reached into the 1960s. Several Israeli spy rings were rolled up in the late 1950s and 1960s. The Goudswaard Ring, run by Mossad in a false-flag operation that posed it as a NATO service, was doubled and began passing disinformation. The Thomas Ring, which gathered military documents and passed them to Israel in microfilm hidden in exported furniture, was rolled up by GID in 1961. EGIS claimed to have run a \u201csuper-spy\u201d named Jack Bitton before and during the Six-Day War (1967). Bitton opened a travel agency in Tel Aviv in in 1954 or 1955. Some claim he was doubled by the Israelis, but Egypt claims he provided warning of Israel\u2019s attack through his contacts with senior Israelis. For Egyptians, Bitton is Egypt\u2019s master spy and the subject of a highly popular television serial in the 1980s. Israeli spy Wolfgang Lotz mapped out Egypt\u2019s SAM facilities and other important military infrastructure. Israel also operated an agent \u201cSulayman\u201d who reported the movements of individual Egyptian units during the war. Sulayman may have provided encryption material, because Israeli MI (Aman) broke Egypt\u2019s military cipher before the war. Israeli deception measures went undetected by the Egyptians. IDF created an entire fake military unit in Eilat, complete with encrypted and unencrypted radio traffic, which succeeded in tricking the Egyptians. In the run-up to the Yom Kippur War, Egypt suffered from COMSEC shortfalls which were aggravated by excellent Israeli SIGINT. After the 1967 war, Israeli SIGINT stations on the banks of the Suez could reach deeper into Egypt. Israel even planted listening devices on communications lines used by an Egyptian headquarters complex. Egyptian CI also discovered bugs in hollowed-out telephone poles which Israel was using to tap military communications between Red Sea bases and Cairo. Israel\u2019s \u201cTop Source\u201d, Ashraf Marwan, was turned by the Egyptians before the onset of war and gave false warnings of Egyptian attack that cost Israel money and lulled it into a state of complacency. Anwar Sadat and MID formulated a plan of strategic deception to accomplish diplomatic victory in a limited war (British deception before Battle of el-Alamein). EGIS planted deceptive media stories on military unpreparedness. Military coordinated movements to avoid US imagery satellites. Brinksmanship produced calm-alarm-calm cycle that bred complacency in Israelis. Egyptians improved intelligence collection by recruiting Bedouins and obtaining valuable intelligence on Israeli plans and codenames. Also leveraged OSINT. Ashraf Marwan, Sadat\u2019s new presidential gatekeeper, was recruited by Israel but doubled by Egyptians, gave false warnings of imminent attacks. After early success in the war, Israelis exploited weaknesses and were ready to pounce when ceasefire was declared. Egypt and Israel made peace and began sharing intelligence on Palestinians. Sources: Sirrs, Owen L. The Egyptian Intelligence Service: A History of the Mukhabarat, 1910-2009 (Studies in Intelligence) .","title":"Egypt"},{"location":"Misc/Intelligence/#finland","text":"Finnish economy\u2019s reliance on technology is growing and so is economic espionage. National innovation system is considered protected. Finland\u2019s intelligence community - Finnish Security Intelligence Service (Suojehpolisi, Supo) has been performing counterespionage, counterterrorism, and security since 1949. - Finnish Military Intelligence Command (FINMIC) is under Defense Command\u2019s Intelligence Division. - Finnish Intelligence Research Establishment (Viestikoelaitos) is Finnish Defense Force\u2019s (FDF) SIGINT unit, under Finnish Air Force (FAF). Predecessors to Supo - Detective Central Police (Etsiva keskuspoliisi) 1919-1937 - State Police (Vattriollinen poliisi, Valpo) 1937-1949 - Red Valpo, when dominated by Communists, 1945-1949 - Finnish Security Police est. 1949, name changed to Supo 2010.","title":"Finland"},{"location":"Misc/Intelligence/#organizational-structure","text":"Supo is subordinate to the Ministry of Interior, while other Nordic services are subordinate to the Ministry of Justice. Supo handles no foreign HUMINT sources. Four organizational reforms over past 20 years - In 1992, Supo had three directorates: Counterespionage, Security, and Development and Support - In 1998, operational and development matters were divided more clearly - In 2004, Supo was reorganized to develop research and analysis functions. Line organization was introduced. - In 2009, Supo was divided into Operational and Strategic Branches. - Operational Branch : Counterespionage Unit, Counterintelligence Unit, Security and Regional Unit, Field Surveillance Unit - Strategic Branch : Situational Awareness Unit, International Relations Unit, Internal Surveillance Unit - Communications Office directly subordinate to C/Supo","title":"Organizational structure"},{"location":"Misc/Intelligence/#personnel","text":"Supo employs 220 people and has a 17 million euro budget. - 55% are police personnel (30% command, 40% senior, 30% officers) - 1/3 of employees have a degree - Average age of 44 years","title":"Personnel"},{"location":"Misc/Intelligence/#iran","text":"In 1978-1979 (1357 H.S.) some former SAVAK officers revealed what they knew in a televised news conference. SAVAK had 9 or 10 chief directorates: First Chief Directorate : administration Second Chief Directorate : foreign intelligence Third Chief Directorate : internal security Fourth Chief Directorate : counterintelligence Fifth Chief Directorate : technical espionage, including telephone intercept and clandestine audio recordings, censorship, photography, and flaps and seals Sixth Chief Directorate : budgeting Seventh Chief Directorate : analysis, including compilation of daily \"bulletins\" Eighth Chief Directorate : counterespionage Ninth Chief Directorate : individual biographies and passport affairs Iraj Faridi, former chief of operations of the first section of the Third Chief Directorate, mentioned the Trident intelligence agreement but seemed to think the relationship with Turkey, not Israel, was sensitive. He claimed to have 17 years of service in SAVAK during the press conference. Mohsen Toluei","title":"Iran"},{"location":"Misc/Intelligence/#trident","text":"A three-way intelligence sharing agreement between Iran, Israel, and Turkey. This was prominently covered by the press when former Mossad officer Yossi Alpher published his book Periphery: Israel's Search for Allies in the Middle East .","title":"Trident"},{"location":"Misc/Intelligence/#russia","text":"Kouzminov, Alexander. Biological espionage: Special Operations . Directorate S, Department 12 Responsible for obtaining intelligence to aid the Soviet BW program, as well as new strains of pathogens. Requirements included Western governments, state commissions, civil defense and laboratories. Vladimir Kuzichkin worked as Illegals support officer in S/12 for 10 years Laboratory X founded in 1920s, transferred to NKVD in 1937. Soviet biological espionage - Morris and Leontina Cohen, controlled by Gordon Lonsdale, obtained information on British BW program. - Marcus Klinberg was director of Israeli BW program, spied for USSR.","title":"Russia"},{"location":"Misc/Intelligence/#communications-intelligence-and-tsarist-russia-by-thomas-r-hammant","text":"","title":"Communications Intelligence and Tsarist Russia by Thomas R. Hammant"},{"location":"Misc/Intelligence/#ministry-of-foreign-affairs","text":"Since Peter the Great, COMINT involving foreign governments and their representatives was the responsibility of the Ministry of Foreign Affairs (MID). Methods used included opening diplomatic letters (perlustration) and decrypting the contents, if encrypted, by cryptanalysis or purchasing codebooks. MID was aided by the \u201cBlack Cabinets\u201d of the Imperial Russian Postal Service. Post offices in major cities of the Russian Empire had Black Cabinets, which photographed contents of suspect correspondence and disseminating the information to the appropriate ministry. When the contents of a letter were encrypted, it was worked on not by a Black Cabinet but by a \u201csimilar establishment attached to the Ministry of Foreign Affairs.\u201d Little is known about MID\u2019s cryptographic organization, but it may have been brought under the control of the Minister of Foreign Affairs himself in the 1900s. Codebooks could be easily acquired on the open market.","title":"Ministry of Foreign Affairs"},{"location":"Misc/Intelligence/#russian-comint-in-world-war-i","text":"Army and Navy operated independent COMINT elements. At each Army HQ, radio intelligence operations were controlled by Chief of Army Communications through his assistant for technical matters. Each army\u2019s radio battalion had a radio intelligence squad or section which operated two stations: one monitored enemy communications, and the other station then recorded them once detected. Intercepts of encrypted German Army radiograms were sent to a \u201cspecial bureau\u201d of Chief Directorate of the General Staff in St. Petersburg for cryptanalysis. Generally, COMINT was poorly organized under the Russian Army. Black Sea and Baltic Sea Fleets established independent COMINT services in autumn 1914 after German naval codebooks were recovered from a sunken German cruiser. Copies of the codebook were shared with the British and French, and there was continued COMINT collaboration between the Allies throughout the war. The Baltic Sea Fleet\u2019s first radio intercept station was established close to Tallin. Intelligence was sent by underground cable to the Communications Service of the Southern Region. Each region (North, East, and South) had a Central Radio Station (CRS) that produced all-source intelligence and supported fleet communications. By 1916, Northern Region had 5 DF and 5 intercept stations, and Southern Region had 5 DF and 4 intercept stations. Southern Region also established a Radio Intelligence Center, probably to administer COMINT from these stations, which was subordinate to the CRS, and other Regions may have had similar units. The Baltic Sea Fleet\u2019s greatest debt to COMINT was accrued on 31 July 1915, when the Russians learned the German Navy planned to seize the city of Riga. Cryptanalysis of the messages, aerial reconnaissance, and shore-based observation posts allowed Russian ships to be ready for the attack when it came, and the Russians prevailed. The Black Sea Fleet\u2019s first radio intercept station was at Sevastopol, and although its Communications Service had two regions to administer, there are fewer details available about Black Sea Fleet COMINT. Black Sea Fleet was greatly helped by Turkey\u2019s use of German codes during the war. In December 1916, Black Sea Fleet decrypted information that indicated a German submarine was to return to Constantinople and included the location of the mine-swept channel by which it was to pass. Russian minelayers went to work, and within 48 hours the Russians learned the submarine had been sunk.","title":"Russian COMINT in World War I"},{"location":"Misc/Intelligence/#from-the-okhrana-to-the-kgb-by-christopher-andrew","text":"Okhrana est. 1881 Soviet intelligence MO and methods rooted in Tsarist secret service Okhrana active measures campaign to persuade French investors to invest in Russia. By 1914, a quarter of France\u2019s FDI was in Russia, three times as much as in its own empire, 80% of it in government loans. Peter Rachkowsky, head of Okhrana\u2019s Paris-based Foreign Agency from 1884 to 1982 may have been responsible for producing the Protocols of the Elders of Zion.","title":"From the Okhrana to the KGB, by Christopher Andrew"},{"location":"Misc/Intelligence/#humint","text":"Colonel Alfred Redl, senior Austrian MI officer. In winter 1901-2, Colonel Batyushin, head of Russian MI in Warsaw, discovered Redl was a homosexual. Redl sold Austria\u2019s mobilization plans against Russia and Serbia until his suicide in 1913. Roman Malinovsky, worker who became one of the most trusted Bolsheviks. One of 6 Bolshevik deputies to Duma (1912), then chair of Bolshevik faction when Mensheviks broke off. When Lenin smelled a rat and set up a committee to investigate the possibility of penetration, Malinovsky was a member of the committee. Okhrana sent him out of the country with a 6000 ruble payoff, and when Okhrana files were opened in 1917 Lenin couldn\u2019t believe Malinovsky had been a traitor.","title":"HUMINT"},{"location":"Misc/Intelligence/#sigint","text":"Okhrana began stealing cipher material to assist SIGINT at the beginning of the 20th century, decades before anyone but the French. Okhrana bribed embassy to services to make impressions of keys and smuggle them out, as well as papers to be photographed. SIGINT was used in diplomatic negotiations with Germany over the Bosphorus Canal. Bolshevik Revolution damaged SIGINT, dispersed codebreakers and cryptologists to other countries, where they sometimes joined those SIGINT services. For a decade after the Revolution, Soviet diplomatic traffic was easily decrypted. Soviets adopted the one-time pad in 1927. GB adopted the tactic of stealing cryptographic material. KGB and GRU operated a joint unit headed by Gleb Boki. SIGINT was responsible for a panic regarding a possible Japanese surprise attack 1931-1932. Intercepted attache cables were published in the Moscow press. Soviets provided edited excerpts of diplomatic intercepts to Germany to influence Germans into signing Molotov-Ribbentrop Pact. Maskirovka on Eastern Front achieved comparable results to British and Western denial and deception (XX system) Capture of hundreds of German signals personnel among tens of thousands of German POWs resulted in a windfall for Soviet SIGINT, 1943. Multiple penetrations of NSA, 1959-1963.","title":"SIGINT"},{"location":"Misc/Intelligence/#sword-and-the-shield-the-by-christopher-andrew","text":"","title":"Sword and the Shield, the by Christopher Andrew"},{"location":"Misc/Intelligence/#chapter-2-from-lenins-cheka-to-stalins-ogpu","text":"The paranoid instincts and shadowy methods of the Cheka and its successors were motivated by persecution of Bolshevik revolutionaries during the Tsarist period and provoked by agents provocateurs planted by the Tsarist Okhrana and foreign powers. - Cheka founded on 19171220 only weeks after Bolshevik Revolution: Feliks Dzerzhinsky - Foreign Intelligence Department (INO) established 19201220: Made use chiefly of Illegals because Soviet state had no legal residencies abroad - Foiled plots encouraged paranoia of young Cheka - Envoys\u2019 plot by naive young diplomats, caught in the net laid by the Cheka - Agents provocateurs Eduard Berzin and Yan Buikis: Berzin received Order of the Red Star, became Cheka officer, but then fell victim to Stalin\u2019s Terror and was shot in 1937; Buikis survived by changing his identity - Okhrana agents provocateurs - Bolshevik experience as an underground movement: Use of pseudonyms (\u2019Lenin\u2019, \u2018Stalin\u2019)","title":"Chapter 2: From Lenin\u2019s Cheka to Stalin\u2019s OGPU"},{"location":"Misc/Intelligence/#chapter-3-the-great-illegals","text":"The Great Illegals of the interwar period leveraged their personal flair and charisma to achieve remarkable successes against target countries with very weak security posture. Some of their earliest successes were in obtaining diplomatic cipher material, which was passed to a large SIGINT agency where diplomatic traffic was deciphered. Stalin didn\u2019t trust anyone to analyze the intelligence for him and acted as his own intelligence analyst. This reinforced his warped worldview as the secret services produced reports that catered to his paranoid suspicions. - Great Illegals were unique and remarkable spies: multilingual Central Europeans with great faith in Communist future; freer from bureaucracy, in comparison to post-war period - Target countries had very lax security - First successes were in obtaining diplomatic ciphers - Dmitri Aleksandrovich Bystroletov (HANS, ANDREI): very handsome, extroverted: portrait hangs in secret memory room of SVR Center in Yasenovo - Seduced female staff in foreign embassies: Prague, 1927: seduced 29 y.o. secretary in French embassy (LAROCHE) who provided British and Italian diplomatic ciphers and classified communiques for 2 years - Agents introduced ANDREI to other sources of information - Oldham , who provided British ciphers, provided introduction to Raymond Oake (SHELLEY) - De Ry provided Italian ciphers, provided introduction to Rodolphe Lemoine - Rodolphe Lemoine (JOSEPH) - Passion for espionage: began work for French Deuxi\u00e8me Bureau (DB) in 1918 - Recruited German cipher clerk in 1931 who was DB\u2019s most important source for a decade - Some intel fed into Enigma code breaking machines - Eventually passed to Ignace Reiss (RAYMOND), who defected in 1937 - Henri Christian Piecke (COOPER) - Flamboyant Dutch artist - John H. King (MAG) - Irish, hated English - Classified Foreign Office communications coroborated by DUNCAN (below) - Moisei Markovich Akselrod (OST, OSTO) - Jewish family, born 1898 - hired by INO in 1922 - Multilingual: Arabic, French, German, English, Italian - Francesco Constantini (DUNCAN) - Classified documents, ciphers from British Embassy in Rome - Also sold documents to Italian intelligence - Continued to provide intelligence after dismissal through brother Secondo Constantini (DUDLEY), also employed at embassy - Executed during Great Terror - Joint OGPU/Fourth Department SIGINT agency decrypted diplomatic traffic - largest SIGINT agency in the world at the time - No analysis of intelligence - Stalin considered analysis to be \u201cguesswork\u201d - Conspiracy theories of Stalin\u2019s continue to survive to this day","title":"Chapter 3: The Great Illegals"},{"location":"Misc/Intelligence/#chapter-4-the-magnificent-five","text":"Arnold Deutsch established the recruiting strategy for the Magnificent Five, young talents in Oxford and Cambridge Universities with Communist sympathies who became the most successful Soviet penetrations of Western governments during WW2. Arnold Deutsch (STEFAN, OTTO): True believer in Communism, chemistry PhD from Vienna University, five years after entering as undergraduate; began work for INO in 1932. Recruited 20 agents, including C5, over 4 years as controller Kim Philby (SOHNCHEN, SYNOK): heterosexual athlete; was not productive before 1937, when he was sent to Spain as war correspondent, wounded, and ultimately awarded medal by Franco, whom he was supposed to assassinate Donald MacLean (WAISE, SIROTA): bisexual, approached by Philby in 1934; Foreign Office Guy Burgess (MADCHEN): flamboyant homosexual and social butterfly Joined SIS in 1938, in newly founded Section D (covert action and influence) Anthony Blunt: homosexual, introduction by Burgess Talent spotter John Cairncross (MOLI\u00c8RE, LISZT): polygamist, spotted by Blunt, approached by Burgess, recruited by Klugmann; Foreign Office Norman Klugmann (MER): prominent Communist activist who acted as talent spotter for NKVD, recr. 1936. Given away by Ignace Poretsky in 1937 Teodor Maly (MANN) Hungarian POW during WW1, joined Bolsheviks during Revolution Head of London residency in 1936, where he completed recruitment of C5 with Deutsch Recalled to Moscow during Great Terror, shot in 1937 (Ch. 5) Internal turmoil in Soviet Union affected espionage Hunt for Trotskyites became priority by end of 1937 Great Terror: all 3 of Deutsch\u2019s residents during residency in London were executed","title":"Chapter 4: The Magnificent Five"},{"location":"Misc/Intelligence/#chapter-5-terror","text":"The fantasy of a Trotskyite conspiracy increasingly obsessed Stalin during the 1930s, who directed the NKVD and OGPU to penetrate Trotsky\u2019s organization. Trotskyites became targets of a cell of assassins called the Administration for Special Tasks, based out of the Paris residency. The Great Terror resulted in the liquidation of so many NKVD officers that tradecraft suffered. The Cambridge Five themselves, despite the quality of intelligence they provided, were suspected of being plants. \u2022 Mark Zborowski (MAKS, MAK, TULIP, KANT): Russian-born Polish Communist who deeply penetrated Trotsky\u2019s entourage \u25e6 Confidant of Lev Sedov, elder son of Trotsky \u25aa Entrusted with key to Sedov\u2019s letterbox and Trotsky\u2019s most confidential files and archives \u25aa Convinced Sedov to go to a Russian clinic for appendicitis while assassination was being planned \u25e6 After Sedov\u2019s death, encouraged internecine warfare between Trotskyites \u25e6 Orlov knew his first name and attempted to warn Trotsky after defection in 1938 \u2022 NKVD Administration for Special Tasks specialized in assassination and abduction, especially in France, headed by Yasha Serebryansky, resident in Paris \u25e6 Largest section of Soviet foreign intelligence by 1938, claiming to have 212 illegals in 16 countries \u25e6 Trained members of International Brigades in sabotage \u25e6 Main task was surveillance and destabilization of French Trotskyites \u25aa theft of Trotsky\u2019s papers from a Paris flat coordinated by Zborowski, who escaped suspicion \u25e6 Abduction of General Yevgeni Karlovich Miller \u25aa Entourage penetrated: Miller\u2019s deputy was NKVD agent \u25aa Another illegal was used to surveil Miller \u25aa Miller disappeared in broad daylight on a Paris street, drugged, packed in a heavy trunk, and sent to Moscow by Soviet freighter where he was interrogated and shot \u25e6 Assassination of Lev Sedov \u25aa Op was aborted after furor re. NKVD involvement in Miller\u2019s disappearance \u25aa Sedov developed appendicitis, died mysteriously a few days after a successful operation in Russian clinic (at Zborowski\u2019s insistence) \u25aa NKVD had a sophisticated medical section called the Kamera, experimented with lethal drugs \u25e6 Assassination of Rudolf Klement: secretary of Trotsky\u2019s Fourth International \u25e6 Assassination of Ignace Poretsky (Reiss, RAYMOND) using machine gun and chocolates laced with strychnine \u25e6 Assassination of Leon Trotsky: operation UTKA \u201cduck\u201d became chief Soviet foreign policy objective, to be effected by three groups \u25aa Penetration by illegal Ram\u00f3n Mercader (RAYMOND, alias Frank Jacson sic ), who seduced a Trotskyite secretary \u25aa Succeeded in killing Trotsky with icepick, caught and sentenced to 20 years imprisonment \u25aa Hero\u2019s welcome in Moscow 1960 \u25aa Assault on villa led by David Alfaro Siqueiros (KONE), Communist painter \u25aa Iosif Romualdovich Grigulevich (MAKS, FELIPE), member of Serebryanksy\u2019s cell, real leader of assault \u25aa Escaped to Argentina where he planted hundreds of mines in cargo ships bound for Germany \u2022 Spanish Civil War was training ground for saboteurs and battlefield against Franco\u2019s fascists as well as Trotskyites \u25e6 Orlov coordinated two-front war in Spain, ultimate goal was to build a secret police force under Soviet control \u25aa NKVD assassins murdered Andreu Nin, head of a Trotskyist workers\u2019 organization, as well as dozens of other Trotskyites in Spain \u25aa Orlov eventually defected to the US \u25e6 Stanislav Alekseyevich Vaupshasov: top assassin - Led raids on Polish and Lithuanian border villages dressed in Polish and Lithuanian army uniforms in the 1920s - Murdered a colleague in 1929 - Constructed and guarded secret crematorium which disposed of NKVD victims (SVR still considers this topic sensitive, gave hush money to female relative of the NKVD agent in charge of guarding this crematorium) - Great Terror sprung from Stalin\u2019s obsession with counterrevolutionaries - Leadership of NKVD liquidated and re-liquidated - Nikolai Ivanovich Yezhov, head of NKVD 1936-1938: author of Great Terror, replaced Yagoda who soon made absurd confessions - Replaced by Lavrenti Beria in December 1938 before accused of conspiracy - Abram Slutsky, chief of INO, poisoned by cyanide in 1938 - Slutsky\u2019s successors also shot before the end of the same year - NKVD officers were liquidated - Had to be careful even of body language or sighing - Officers most quick to denounce peers of imaginary crimes were most likely to survive Most of the Great Illegals were liquidated by 1938, except for: - Deutsch who was betrayed in 1937 by Ignace Poretsky (Reiss, RAYMOND) - Bystroletov brutally tortured before confession, imprisonment; wife sent to gulag where she cut her own throat with a kitchen knife; mother poisoned herself - Serebryansky himself recalled to Moscow and condemned in 1938 - Show trials depicted a vast, absurd conspiracy authored by Stalin, who proofread transcripts before publication - Great Terror and British operations - C5 transferred to legal residency, where controllers were much less experienced - MacLean seduced his new Soviet controller (NORMA, ADA) - Condemnation of C5\u2019s controllers and recruiters as enemies of the people placed their intelligence and reliability under question - Beria eventually disbanded the residency in 1940, Centre ordered all contact with Philby and Burgess to be broken off - Ideological commitment of C5 remained strong even after the Molotov-Ribbentropp pact - British agents were motivated out of revilement for fascism - Some agents ended their espionage - Histories of Stalin era still whitewash the emphasis on assassination of political opposition in Western Europe","title":"Chapter 5: Terror"},{"location":"Misc/Intelligence/#south-africa","text":"Intelligence's role in supporting counter-revolutionary operations of the apartheid state, as well as in the AND and SACP's efforts to overthrow the state. Five key uses of intelligence in counter-revolutionary struggle:[^4] Targeting enemies of the state, internal (including South West Africa) and external (including in Europe) ZA relationships with other states (Angola, Botswana, Mozambique, Tanzania, Zambia, and Zimbabwe) as anti colonial movement spread, isolating ZA Anti-Communist paradigm of Cold War, alliance with US, UK, West Germany, and Israel Overcoming anti-apartheid sanctions from the 1970s and 80s Developing a nuclear weapon Relationship with British services strained because of Afrikaner resentment, British concern of infiltration by Afrikaner nationalists. In the nineteenth century, the Boer republics (Transvaal Republic and Orange Free State) maintained limited intelligence organizations led by Cornelius Smidt and Willem Leyds (Transvaaler Secret Service).[^5] South African Police Detective Branch (SAP/DB) conducted counter-subversion and counterintelligence after 1899-1902 Anglo-Boer War and the establishment of the Union of South Africa in 1910. SAP/DB concentrated mostly on Nazi sympathizers in South West Africa until 1948, when the National Party (NP) was elected.[^6] MI5 conducted foreign intelligence but also watched radical Afrikaner groups such as Ossewa-Brandwag (OB), a paramilitary organization in competition with NP and with many sympathizers in SAP. Union Defense Forces, considered an anglophile institution, cooperated with MI5 on internal threats (Afrikaner nationalists and Republicans) and British Special Operations Executive (SOE), operating out of Durban. MI5 Director General Sir Percy Sillitoe served in South Africa and Rhodesia and was a key influence in shaping the South African intelligence structure. SAP Special Branch (SB) founded as Special Staff to hunt Nazis in 1939 before being redirected to investigating political crimes. Increasingly into the 1950s, became known as Security Branch. Union Defense Forces Department of Military Intelligence (DMI) created Feb 1940 but neglected after 1948. After independence from Britain in 1961, UDF became South African Defense Forces (SADF) and established a new Directorate of Military Intelligence (DMI) July 1962. Rivalry between SB and DMI survived administrative efforts to coordinate intelligence functions: State Security Committee (est. 1963) and State Security Advisory Board (est. 1966). SB\u2019s criminal emphasis hampered intelligence work, and a central intelligence agency was needed, sought first in Republican Intelligence (RI) spun off from SB 1963, then found in BOSS. Bureau for State Security (BOSS) founded 19690513 as a central intelligence apparatus to mitigate the rivalry between DMI and SAP. Reorganized as National Intelligence Service (NIS) in 1980. Grew from 500 personnel in 1969 to more than 1,000 by 1978. Six departments: Subversion, Counter-espionage, Political and Economic Intelligence, Military Intelligence, Administration, and National Evaluation, Research and Special Studies. Forged a relationship with Portuguese and Rhodesian intelligence services. Brought down by government\u2019s increased reliance on COIN strategies (vice counterintelligence and counter-espionage) and by the Information Scandal. African National Congress (ANC) , Umkhonto weSizwe (MK) , and South African Communist Party (SACP) established intelligence structures separate but parallel to that of the government during their armed struggle against the apartheid state. ANC established Department of Intelligence and Security (DIS). DMI gained control of \u201cTotal National Strategy\u201d and achieved dominance over BOSS. DMI strategists implemented various counter-insurgency (COIN) strategies they learned from abroad in ZA and SWA. Botha appointed Minister of Defence in 1966 and began a campaign to reinvigorate the SADF. By the beginning of Project SAVANNAH (South African intervention in Angola[^7] in 1975, Botha had begun a process of streamlining the SADF that would culminate years later in the incorporation of COIN principles in the South African Army. Area Defence Policy, later known as National Security Management System, was fully incorporated into the counter-insurgency forces of SADF. COIN forces were made up of part-time SADF personnel from the Citizen Force and Commandos and were divided into ten regional commands covering the countryside. SADF doubled in size between 1975 and 1990, reaching almost 100,000 with another 325,000 in the Citizen Force and Reserves ZA security confronted radical new challenges from 1975 to 1978. Key governments on ZA's periphery had turned hostile. Marxist MPLA took over Angola and hosted SWAPO bases on the border of South West Africa. Mozambique's new government was Marxist. Mozambique became a shelter for Rhodesian guerrillas, threatening ZA's last counter-revolutionary ally. Radicalization of blacks after 1976 uprising in Soweto overwhelmed internal security, which responded by introducing COIN concepts and reacting more harshly to protests. SADF COIN expertise was gained in collaboration with Rhodesian ISS and SOF during the 1960s and 1970s. ZA intelligence establishment absorbed members of Rhodesian Security Forces in the transition to majority rule in 1980 (Operation Winter).[^8] COIN Strategy Adopted Wholesale, New Units Established Opposition to apartheid state grew more radical while the state\u2019s response to opponents hardened. A sustained domestic protest movement called United Democratic Front ultimately gave ANC-MK their long sought-after internal subversion capability within ZA. Pretoria adopted COIN strategies in response to failing effort against MPLA in AO and SWAPO in SWA and deteriorating domestic security as MK stepped up attacks. K-Unit \u201cKoevoet\u201d founded by SAP/SB in January 1979, building on their decades of cooperation with Rhodesians in COIN operations. Formed after the Selous Scouts and Portuguese Flechas for the purpose of turning captured ANC, SWAPO, and PAC guerrillas called askaris. Turned SWAPO fighters who collaborated with Koevoet were known as makakunyanas \"blood-suckers\", and Koevoet collaborators were targeted for assassination by SWAPO. They were paid for each guerrilla killed. SAP/C1 \u201cVlakplaas\u201d named after the police farm outside Pretoria. Originally established 1979 when BG Johan Coetzee, C/SB, decided to use COIN activities for counter-revolutionary purposes within ZA. Revealed by Coetzee in the press in 1989 and disbanded in 1993. Koevoet elements including de Kock were withdrawn from SWA to form C1 in May 1983. Identification, tracking, and \"rehabilitation\" (turning) of ANC and PAC guerrillas. C1 also assassinated up to 65 people from 1980 to 1991. C1\u2019s operations in Swaziland were to disrupt the ANC/MK structures there. 22 ANC members were assassinated in Swaziland in the 1980s. C2 established concurrently to track activists leaving ZA and to interrogate arrested guerrillas SAP/G section , responsible for the penetration of ANC abroad, was resurrected after the reorganization of BOSS in 1980. G section attempted to assassinate ANC/MK strategist Joe Slovo, killing his wife instead. G section also blew up ANC\u2019s London office Directorate Covert Collection (Direktoraat von Koverte Insameling) established at an unclear date, but possible predecessor was Directorate of Covert Information, active in SWA by 1982. Established multiple front companies with the goal of duplicating successful COIN operations of AO and SWA within South Africa. Spread of liberation movements across Southern Africa removed governments traditionally friendly to Pretoria, which saw all black liberation movements as Communist-backed threats to regime. South African government resorted to policy of destabilizing neighbors and conducting covert action to remove safe havens for ANC/MK. Eventually South Africa\u2019s hand in regional politics became transparent. DMI integrated intelligence collection with covert action by taking control of South African Special Forces (SASF) in 1979. Rhodesian special forces integrated into SADF in 1980, further buttressing COIN capabilities. Directorate of Special Tasks (DST) formed in mid-1970s from a corps of DMI operators named Spesmagte, similar to Recces. DST was responsible for overseeing contra-mobilization and counter-revolutionary activities of DMI throughout southern Africa as part of a strategy to deny ANC-SACP safe havens in Frontline States. DST began operations in an office in Rundu, Namibia in 1976 in the wake of South African withdrawal from AO. First chief was COL Cornelius van Niekerk. DST terminated operations in the early 1990s. Two sections: - DST-1 (external operations) covered UNITA, RENAMO, and Zimbabwe - DST-2 (internal operations) covered Lesotho Liberation Army (LLA), and Operations MARION and KATZEN DST maintained logistical infrastructure throughout Southern Africa and conducted several operations, providing support to anti-Marxist proxies in neighboring countries: - DISA/SILWER : support to UNITA in Angola - DRAMA : support to Zimbabwean dissidents - PIKI/PUNDU MILIA/ALTAR : support to RENAMO and operations against FRELIMO in Mozambique. 5 Recce was principally responsible because many 5 Recce personnel had trained with the Selous Scouts. When RENAMO\u2019s headquarters moved from Phalaborwa, Transvaal (!) to Gorongosa, Mozambique, South African officers followed offering intelligence training. - PLATHOND : support to surrogate force in Zambia - CAPSIZE/LATSA : support to Lesotho proxy grou 5 Recce based out of Phalaborwa, Transvaal. Supported Op PIKI/PUNDU MILIA. Also used pseudo-operations against SWAPO in Namibia, sometimes cooperating with Koevoet. DCC mobilized contras in Namibia. Various operations: - ETANGO : DCC and other DMI units attempted to establish a conservative contra based in Ovambo tribalism to counter SWAPO. - EZUVA : Similar project to establish a contra among the Kavango. Many experienced contra-mobilizers from DCC moved into domestic contra-mobilization 1985-1986, setting up groups to foment black-on-black violence and undermine support of ANC and UDF: 23 such projects by 1986. Operations included: - Operation MARION provided security training and weapons to more than 200 Inkatha cadres 1986-1990. These units later conducted targeted killings. Inkatha had been supported by BOSS as an alternative to ANC from 1975, including funds and stage-managing internal political rivals to Chief Buthulezi. - Operation KATZEN was an attempt to organize a contra group among the Xhosa known as the Xhosa Resistance Movement (XWB, known as Iliso Lomzi). Cooperated with Army Intelligence Hammer units, which conducted special operations. Civil Cooperation Bureau (CCB) , also known as Burgerlike Samewerkingsburo (BSB), was formed in 1986 out of Operation BARNACLE. Formed by SASF to fulfill the requirement of domestic intelligence collection, which was used primarily for external operations. CCB was imagined to be fully functional only in the mid-nineties, possibly to conduct counter-revolutionary warfare after the transfer to black rule had been completed. CCB was organized as a corporation into Regions that coordinated covert activities in concert with other state bodies. CCB numbered up to 250-300 individuals. By the late 1980s, CCB had established numerous front-companies and businesses and was involved in lucrative criminal activities. Operations: - CCB conducted internal assassinations in line with the state\u2019s emphasis on counter-revolutionary warfare. As such, CCB was DMI\u2019s equivalent to SAP\u2019s C1. - CCB supported SASF in operations in the Frontline States by undertaking reconnaissance of ANC targets. 3 Recce (active 1980-1981) absorbed the remnants of Rhodesian special forces which fled Zimbabwe in 1980 as well as DMI\u2019s D-40 assassination unit (active 1979-1980) led by the Rhodesian Garth Barrett. 3 Recce operated against Zimbabwe, exploding very destructive bombs and assassinating ANC\u2019s representative in Harare.[^9] Operation BARNACLE , conducted by a group 30-40 mostly black ex-Rhodesians, was a project to use CBW to assassinate guerrillas, SWAPO prisoners of war, and members of South African security forces suspected of disloyalty with poison. BARNACLE was to be a completely independent resource at the disposal of the country\u2019s leaders, to serve as a hedge against the prospect of the government granting too much power to blacks. BARNACLE actors were not accountable to official security organs or to SADF commanders: they reported directly to General Office Commanding Special Forces (GOC-SF). Later reorganized into the Civil Cooperation Bureau.","title":"South Africa"},{"location":"Misc/Intelligence/#notable-people","text":"BG J.P. Tolletjie Botha ran Directorate Covert Collection. COL Jan Breytenbach founded the Reconnaissance Commandos, 32 Battalion, and the Directorate Special Tasks. Chief Buthulezi was leader of Inkatha and a BOSS stooge. COL Eugene de Kock was one of the most ruthless and effective of Koevoet's commanders. Eleven tours of duty between 1968 and 1973 in Rhodesia with Rhodesian SAS and Rhodesian African Rifles. Commanded Koevoet for four years before he requested a transfer to SAP/C1, where he assumed command in 1985. De Kock emphasized the assassination program and introduced paramilitary training for police members. During this time, De Kock became known as Prime Evil for his mercilessness. MAJ Craig Williamson was a counterintelligence operative in SAP/G Section from... to... Williamson infiltrated ANC by using the International University Exchange Fund (IUEF), but was exposed in 1980. Williamson served in SAP/G until December 1985. Williamson also established Longreach Pty Ltd in April 1986, which served as a front company for SAP/SB operations and also coordinated DMI and SASF operations.","title":"Notable People"},{"location":"Misc/Intelligence/#sweden","text":"There was no specific institution in Sweden for intelligence before the 1930s. Navy intercepted communications and diplomats gather intelligence.[^10]","title":"Sweden"},{"location":"Misc/Intelligence/#military-intelligence-before-ww2","text":"General Staff gathered intelligence against Norway during the war of secession Swedish Defense Staff established in 1937. Intelligence branch formed with 20 officers. Mostly OSINT collection and attache reporting from 15 attaches and SIGINT.","title":"Military Intelligence Before WW2"},{"location":"Misc/Intelligence/#intelligence-organizations-formed-in-ww2","text":"C-Bureau (central byr\u00e5a) under Defense Staff, but separate from Intelligence Division, established large intelligence network that was not fully documented. Cryptographic Department (CD) of Defense Staff took over SIGINT from Navy using civilians to succesfully break Soviet crypto and comms. Results were shared with Finnish. Decrypted machine crypto used over landlines. 150,000 cables over two years, until Germans changed codes. Swedish were unable to fully exploit this windfall of intelligence. CD reformed as F\u00e5ssvarets Radioanstalt (FRA), directly under Ministry of Defense. General Security Service (Allm\u00e4nna S\u00e4kerhetaj\u00e4nsten, GSS) formed out of a secret, extralegal government decision, established massive program of unlimited authority to monitor telephone, telegraph, and mail communications. Dissolved after scandal in 1946.","title":"Intelligence Organizations Formed in WW2"},{"location":"Misc/Intelligence/#postwar-developments","text":"2 of the 3 wartime intelligence agencies would be resurrected after dissolution 1947-1948.","title":"Postwar Developments"},{"location":"Misc/Intelligence/#india","text":"Kautilya\u2019s Arthashastra[^11] - Constitutes a doctrine of statecraft using espionage as a basic means of governance. - Comprehensive textbook on statecraft, foreign diplomacy, and war emphasizing the collection of domestic and foreign intelligence. - Rediscovered and translated by Orientalist Rudrapatnam Shamasasty in 1909-1915. Written by Kautilya, trusted advisor to Chadragupta Maurya, founder of Mauryan Empire 321-185 BCE.","title":"India"},{"location":"Misc/Intelligence/#eight-institutes-of-espionage","text":"Four were forms of religious cover (fraudulent disciple, recluse, ascetic, and mendicant woman), which were to take advantage of the intensely religious population of India. Religious class had access to other castes. Wandering female spies were to take religious cover, poor widows of Brahman caste were to target upper castes while Sudra-caste women willing to shave their head were to target lower caste communities. Classmate spies referred to recruitment pool, and the preferred choice for courier. Firebrands were to be used for covert action as assassins, agent provocateurs, and saboteurs. Result was a pervasive surveillance network covering the whole country. Treasury was also to have intelligence function Householder spies were to ascertain validity of assets Merchant spies were to monitor price changes and foreign goods Networks of spies under cover of bands of thieves would monitor the criminal underworld Provocation and entrapment are standard tactics in Arthashastra.","title":"Eight Institutes of Espionage"},{"location":"Misc/Intelligence/#islamic-caliphate","text":"Pre-Islamic Arab tradition of intelligence predated a more developed intelligence culture in the Islamic age.[^12] Various words referring to spy or scout: - jasus : foreign spy - tajassasah : discouraged in the Qur\u2019an - \u2018ain : \u201ceye\u201d - suhhar : night sentinels who kept watch for strangers at approaches to market town or crossroads - rabi\u2019ah : lookout - other words","title":"Islamic Caliphate"},{"location":"Misc/Intelligence/#espionage-in-early-arab-states","text":"Lakhmid and Ghassanid buffer states between Sasanian Persia and Byzantium: - Story in 10th c. Kitab al-Aghani relates Lakhmid spies catching a would-be assassin and killing him in the 6th century - Scouts in Arabian peninsula","title":"Espionage in Early Arab States"},{"location":"Misc/Intelligence/#brigandage-among-bedouin","text":"Skirmishes and raids ( suluk ) involved use of scouts - al-Basus War was remembered as the days of rabi\u2019ah - Abu Faraj al-Asbahani\u2019s anthology of Arabic verse - Muhammad b. al-Tabari on agents using disguises","title":"Brigandage among Bedouin"},{"location":"Misc/Intelligence/#muhammad-and-intelligence","text":"Qur\u2019anic regulations on espionage reflect importance of clandestine activites in early Islam. The Prophet Muhammad\u2019s involvement in intelligence and espionage: - Muhammad gathered information on early converts, seeking candidates with honesty, trustworthiness, and the ability to keep a secret - Muhammad possessed detailed knowledge of clan loyalties and politics, and used this knowledge in negotiations with Bedouin - Abdullah b. Abu Bakr mingled with Quraishis of Mecca and report back to him at night in his cave. Abu Bakr\u2019s sister Asma also spied for the Prophet: first spies for Islam. - Abu al-Fadhl al-Abbas ran spy network in Mecca - Many \u2018ains from various corners of Arabia, from every town and tribe - Muhammad deliberately retreated during the Battle of Uhud to allow his lookouts to determine the size of the army, whether it had mounted camels (to retreat) or horses (to attack) - Muhammad debriefed two boys who were caught drawing water from a well before the Battle of Badr. They divulged key intelligence on the closing Quraishi army.","title":"Muhammad and Intelligence"},{"location":"Misc/Intelligence/#deception","text":"After the indecisive Battle of Uhud, Muhammad sent one of his \u2018ains to deceive the Quraishis into thinking a large host was approaching, causing them to retreat.","title":"Deception"},{"location":"Misc/Intelligence/#assassination-of-poets","text":"Poets had a complex role in Arab society and were highly influential - Asma of Marwan was stabbed in her sleep, but w\u2019o Prophet\u2019s prior approval - Abu Afak killed by fellow tribesmen - False prophet al-Aswad al-Ansi became influential and killed the governor of Yemen. Muhammad ordered Wabrak b. Yahmus to organize a plot. Wabrak recruited a circle of Persian Muslim converts and the governor\u2019s wife, who facilitated the operatives\u2019 infiltration into the castle where al-Ansi was murdered","title":"Assassination of poets"},{"location":"Misc/Intelligence/#intelligence-by-islams-enemies","text":"Abu Sufyan determined Muslim spies were present by finding date seeds in camel dung, indicating the animals had Medinan fodder Multiple assassination attempts on Muhammad\u2019s life Umar b. al-Khattab in charge of counterintelligence and security Byzantines sent a monk who claimed to be a convert to Islam and established a mosque in Medina. Muslim agents surveilled the mosque after suspicious comings and goings and ultimately demolished the mosque.","title":"Intelligence by Islam\u2019s enemies"},{"location":"Misc/Intelligence/#us","text":"","title":"US"},{"location":"Misc/Intelligence/#corporate-espionage","text":"Vice reported that McDonald's had established an intelligence unit to monitor workers who supported increasing the minimum wage. The unit had targeted the Fight for $15 movement for increasing the wage to $15 an hour in particular. One intelligence report titled \"Ongoing FF$15 Activity Against McDonald's During the COVID-19 Crisis\" contained an analysis of the activities of labor activists. McDonald's had been using two different data collection software suites to collect open-source intelligence on the social networks of workers involved in the labor movement.","title":"Corporate espionage"},{"location":"Misc/Intelligence/#comint","text":"War Department set up first organized cryptanalytic office in June 1917, numbering 3 people. By war\u2019s end, it would grow to 150.","title":"COMINT"},{"location":"Misc/Intelligence/#navy","text":"In 1917 and 1918, Navy set up medium frequency DF stations along Atlantic coast to track U-boats. HFDF stations were researched and deployed by 1938. Strategic HFDF stations were established at Manila, Guam, Midway, Oahu, Dutch Harbor, Samoa, Canal Zone in Panama, San Juan Puerto Rico, and Greenland. US began tracking Japanese warships and merchant vessels in 1939, five years after the Japanese had begun tracking US vessels. Navy had established the Code and Signal Section of Naval Communications for producing codes and ciphers for use by Navy. Registered Publication Section, responsible for distribution of secret and confidential documents, was spun off in 1923. Navy funded development of Electric Cipher Machine from 1922. Communications Intelligence Organization (CIO) was established 1924. Intercept stations were established in the Pacific Area (Shanghai, Oahu, Peking, Guam, Manila, Bar Harbor, Astoria), and Washington DC. Cryptanalytic Units established in Manila and Pearl Harbor. Training was done with technical manuals, using the codes to send messages. Minor intercept activities were performed in strategic HFDF stations. In 1938, CIO became the Communications Security Group (CSG) and took over all Navy DF stations. By 1941, CSG had 730 total personnel.","title":"Navy"},{"location":"Misc/Intelligence/#arm","text":"Army Signal Corps founded 1860 by Albert James Myer, inventor of wig-wag flag signaling method. Signal Intelligence Service (SIS) founded 1930 as a secret part of Signal Corps for cryptanalysis. By 1939, SIS made use of 7 intercept sites from the Philippines and Hawaii in the West to the East Coast of the US. These were the sources of SIS intercepts until after Pearl Harbor. At SIS HQ in Arlington Hall, traffic was split between four analytic sections: - J: Japanese - G: German - I: Italian - M: Mexican and Latin American Although SIS intercepted tens of thousands of IJA messages from its station in Manila, these messages could not be fully exploited because IJA ciphers were not broken. However, SIS broke several diplomatic ciphers including Purple. After the war, SIS changed its name to the Army Security Agency (ASA) in 1945. In 1947, ASA and the Army Intelligence Agency were merged into the newly formed Intelligence and Security Command (INSCOM).","title":"Arm"},{"location":"Misc/Intelligence/#radio-free-europe","text":"Established 1949 by the National Committee for a Free Europe (NCFE), an anticommunist organization with Allen Dulles and Dwight D. Eisenhower as board members. Funded by CIA until 1972. Targeted Eastern European countries (as opposed to Radio Liberty).","title":"Radio Free Europe"},{"location":"Misc/Intelligence/#radio-liberty","text":"Established by American Committee for the Liberation of the Peoples of Russia (Amcomlib) 1951. By 1954 was broadcasting in several other Central Asian languages. Foreign Broadcast Information Service Foreign Broadcast Monitoring Service, or FBMS, established 1941 under FCC to monitor Axis shortwave broadcasts to the US. Name changed to FBIS 1947 when it was made part of the new CIA.","title":"Radio Liberty"},{"location":"Misc/Intelligence/#france-andrew-orr","text":"France took over Syria and Turkey\u2019s SE (Cilicia) after WW1 and perceived Turkish War of Independence as threat to its new imperial interests. Military intelligence services of Army and Navy monitored Kemalist movements and always saw the hand of German and Russian commies behind Turkey\u2019s developments. 3 reasons German closely involved in Ottoman military affairs from 1883 and German general Otto Liman von Sanders commanded the Ottoman Army Germans had also promoted pan-Islamic movement during the War Germans possibly still controlled Russia, according to the French, reasoning that they had sent Lenin to Russia inside of a sealed train car Colonial intelligence services lacked some of the checks on extreme predictions when reporting on events outside of Europe","title":"France (Andrew Orr)"},{"location":"Misc/Intelligence/#military-intelligence","text":"French Army\u2019s Service de renseignments guerre ( SR Guerre ), which was part of a unified French intelligence organization during the War, but then reverted to the Army\u2019s II Bureau after its end. During the Turkish War of Independence, SRG deployed a small number of officers to the Middle East, stations opened in Constantinople in 1919, Algiers 1925, Rabat and Tangiers 1929. French Navy service ( SR Marine ) was similarly structured to Army, but with more familiarity with Mediterranean Sea and the Middle East SRM opened Constantinople station in 1919 SR sources included Europeans and Americans fleeing Turkey, human informants, and newspaper articles, as well as intercepted radio messages sometimes by way of the Brits Coincidence of treaty signings between USSR, Turkey, and Afghanistan and Persia led SR personnel to believe there was a plot brewing","title":"Military Intelligence"},{"location":"Misc/Intelligence/#poland","text":"From Anglo-Polish HUMINT, by PRJ Winter Histories of WW2 intelligence exalt COMINT successes of GCCS to the detriment of MI6 Historiography of British WW2 intelligence Agents - Paul Thummel (codename A-54), senior officer of German MI (Abwehr), recruited by Czechs 1936. SIS and Czechs ran him jointly from 1939 until he was arrested by the Gestapo and died in prison 1942. - Warlock, on staff of German High Command (Oberkommando der Wehrmacht, OKW), turned by 1941. - Knopf, reported on OKW intentions to take Malta, but records show that Germans were noncommital to the plan (Op Herkules) which would only support Italia High Command. Knopf may have been turned by this time. MI14\u2019s evaluation gave Knopf a mixed, but generally positive score. Polish government-in-exile settled in London and cooperated with British intelligence (II Bureau of Polish General Staff) SIS cooperation with Poles was driven out of desperation because aside from Warlock they had no useful penetrations of Nazi Germany CX reports were SIS, JX reports were from Poles. Some JX reports found in British National archives, including one report on Malta which was passed to Churchill himself. Poles ran sources reporting from the heart of OKW and OKH (Oberkommando des Heeres, Supreme High Command of the German Army) British interception of Polish communications confirmed Knopf\u2019s bona fides (as agent number 594) and the Poles\u2019 as well [^1]: Sawyer, Ralph D. \u201cSubversive Information: The Historical Thrust of Chinese Intelligence.\u201d Intelligence Elsewhere: Spies and Espionage Outside the Anglosphere . Ed. Philip H. J. Davies, Ed. Kristian C. Gustafson [^2]: [^3]: Homstr\u00f6m, Lauri. \u201cFinnish Security and Intelligence Service.\u201d Intelligence Elsewhere: Spies and Espionage Outside the Anglosphere .Ed. Philip H. J. Davies, Ed. Kristian C. Gustafson [^4]: O\u2019Brien, Kevin A. The South African Intelligence Services: From apartheid to democracy, 1948-2005 . Routledge: New York, NY 2011. [^5]: Blackburn, Douglas and Caddel, W. Waithman. Secret Service in South Africa . Honolulu: University Press of the Pacific, 2001. Swanepoel, P.C. Really Inside BOSS: A Tale of South Africa\u2019s Late Intelligence Service (And Something about the CIA) . Pretoria, 2008. [^6]: Ref. Kent Fedorowich, \u201cGerman espionage and British counter-intelligence in ZA and Mozambique, 1939-1944\u201d , The Historical Journal 48:1 [^7]: Robin Hallett, \u201cThe ZA Intervention in Angola,\u201d African Affairs 77:312 (July 1978) [^8]: Ngwabi Bhebe, Terence Ranger, Soldiers in Zimbabwe's Liberation War , 1995. [^9]: D-40 in turn was the reconstitution of the supposedly disbanded Z-squads used by BOSS for assassinations until its reorganization in 1979. [^10]: Agrell, Wilhelm. \u201cSweden: Intelligence the Middle Way.\u201d Intelligence Elsewhere: Spies and Espionage Outside the Anglosphere . Ed. Philip H. J. Davies, Ed. Kristian C. Gustafson [^11]: Davies, Philip H. J. \u201cThe Original Surveillance State: Kautilya\u2019s Arthashastra and Government by Espionage in Classical India. Intelligence Elsewhere: Spies and Espionage Outside the Anglosphere . Ed. Philip H. J. Davies, Ed. Kristian C. Gustafson [^12]: Al-Asmari, Abdulaziz A. \u201cOrigins of an Arab and Islamic Intelligence Culture.\u201d Intelligence Elsewhere: Spies and Espionage Outside the Anglosphere . Ed. Philip H. J. Davies, Ed. Kristian C. Gustafson","title":"Poland"},{"location":"Misc/Intelligence/#united-states","text":"Linda Zall established a program to analyze classified historical statellite imagery to analyze not foreign militaries but changes in the environment, in particular the extent of ice retreat in the polar regions of the Earth. The effort was sparked by then-Senator Al Gore whose letter to the CIA led to the establishment of the MEDEA program which declassified satellite imagery and oceanographic data. John Walker was a notorious spy who volunteered to the Soviet embassy in Washington in 1967. Especially after the North Koreans captured the USS Pueblo, Walker's information on the key list of the KL-47 cryptographic machine meant the Soviets were able to read US Navy communications until the entire system was replaced. John Walker was managed by former KGB general Oleg Kalugin. The acoustic characteristics of the Victor III submarine were made substantially less detectable as a result of Walker's revelation that the Soviet submarine fleet was easily tracked. The stealthy Akula-class submarines, launched in 1985, also benefited from the import of a Toshiba CNC milling machine , which in combination with Norwegian CNC machines , allowed propellors to be designed that were much quieter than before.","title":"United States"},{"location":"Misc/Japanese/","text":"Japanese NO and KOTO are both used to nominalize clauses and verbs in different contexts. no \u53cb\u9054\u304c\u8a71\u3057\u3066\u3044\u308b\u306e\u304c\u805e\u3053\u3048\u307e\u3057\u305f\u3002# (1) \"I heard my friends talking.\" koto \u4eca\u5e74\u306e\u76ee\u6a19\u306f\u3001\u8a66\u9a13\u306b\u5408\u683c\u3059\u308b\u3053\u3068\u3067\u3059\u3002 # (1) \"My goal this year is to pass the exam.\" IME Typing in Japanese is done by applications known as IME (input method editor). The preedit or composition string refers to the string composed by the user using an IME, which can then be converted or rendered in the desired script. For example, a user may input the string \"watasinonamaehanakanodesu\", which is rendered as the following preedit: \u308f\u305f\u3057\u306e\u306a\u307e\u3048\u306f\u306a\u304b\u306e\u3067\u3059 By pressing the convert key Tab , the IME then opens a menu that allows the preedit to be converted to a mixture of hiragana and kanji: \u79c1\u306e\u540d\u524d\u306f\u4e2d\u91ce\u3067\u3059 Some conventions date from ancient practice and are silently assumed now. For example, the convention that F6 produces hiragana and F7 katakana is associated with ATOK , a Japanese IME with roots in the 1980s. Microsoft's IME is built-in to Windows, but on Linux there are multiple IMEs available. Mozc appears to be the most popular, but fcitx5 is installed by default on Garuda and offers Mozc-like functionality. Furigana In HTML, furigana can be placed with the ruby and rt HTML tags, which are supported by Anki: In order to render: \u76ee\u6a19 \u3082\u304f\u3072\u3087\u3046 (objective) < ruby > \u76ee\u6a19 < rt > \u3082\u304f\u3072\u3087\u3046 </ rt ></ ruby >","title":"Japanese"},{"location":"Misc/Japanese/#japanese","text":"NO and KOTO are both used to nominalize clauses and verbs in different contexts. no \u53cb\u9054\u304c\u8a71\u3057\u3066\u3044\u308b\u306e\u304c\u805e\u3053\u3048\u307e\u3057\u305f\u3002# (1) \"I heard my friends talking.\" koto \u4eca\u5e74\u306e\u76ee\u6a19\u306f\u3001\u8a66\u9a13\u306b\u5408\u683c\u3059\u308b\u3053\u3068\u3067\u3059\u3002 # (1) \"My goal this year is to pass the exam.\"","title":"Japanese"},{"location":"Misc/Japanese/#ime","text":"Typing in Japanese is done by applications known as IME (input method editor). The preedit or composition string refers to the string composed by the user using an IME, which can then be converted or rendered in the desired script. For example, a user may input the string \"watasinonamaehanakanodesu\", which is rendered as the following preedit: \u308f\u305f\u3057\u306e\u306a\u307e\u3048\u306f\u306a\u304b\u306e\u3067\u3059 By pressing the convert key Tab , the IME then opens a menu that allows the preedit to be converted to a mixture of hiragana and kanji: \u79c1\u306e\u540d\u524d\u306f\u4e2d\u91ce\u3067\u3059 Some conventions date from ancient practice and are silently assumed now. For example, the convention that F6 produces hiragana and F7 katakana is associated with ATOK , a Japanese IME with roots in the 1980s. Microsoft's IME is built-in to Windows, but on Linux there are multiple IMEs available. Mozc appears to be the most popular, but fcitx5 is installed by default on Garuda and offers Mozc-like functionality.","title":"IME"},{"location":"Misc/Japanese/#furigana","text":"In HTML, furigana can be placed with the ruby and rt HTML tags, which are supported by Anki: In order to render: \u76ee\u6a19 \u3082\u304f\u3072\u3087\u3046 (objective) < ruby > \u76ee\u6a19 < rt > \u3082\u304f\u3072\u3087\u3046 </ rt ></ ruby >","title":"Furigana"},{"location":"Misc/Lab/","text":"Home lab Media server tubearchivist , which describes itself as a self-hosted YouTube media server, can be used to store, index, and play YouTube videos. It uses yt-dlp to download from YouTube. It can be deployed using a docker-compose file . tubesync is not a complete media server, but a web application frontend that also allows channels and playlists to be added and downloaded to a file server. It supports Plex as a media server, but it has to be added separately. It is not meant to be used to download individual videos, but rather to scan specified channels or playlists for new content and to download them in the background.","title":"Home lab"},{"location":"Misc/Lab/#home-lab","text":"","title":"Home lab"},{"location":"Misc/Lab/#media-server","text":"tubearchivist , which describes itself as a self-hosted YouTube media server, can be used to store, index, and play YouTube videos. It uses yt-dlp to download from YouTube. It can be deployed using a docker-compose file . tubesync is not a complete media server, but a web application frontend that also allows channels and playlists to be added and downloaded to a file server. It supports Plex as a media server, but it has to be added separately. It is not meant to be used to download individual videos, but rather to scan specified channels or playlists for new content and to download them in the background.","title":"Media server"},{"location":"Misc/Nutanix/","text":"Nutanix Nutanix AOS is an offering that creates a storage fabric distributed across all nodes of an HCI cluster, intended for data center. Nutanix was the first to offer an HCI solution in 2011, and since then the market has exploded. AOS supports multiple hypervisors, including Nutanix's native AHV , as well as ESXi and Hyper-V. AOS nodes are able to run all the core services of the cluster. Nutanix has a CLI, known as NCLI . Prism is the name of the graphical management console. Architecture UVMs are controlled by the CVM or Controller VM which runs the Nutanix software. The CVM is a hardened Linux appliance and a \"User Mode VM\" which exists in only a single instance per node. AOS has a \"shared-nothing\" architecture, so in the event of one node's CVM failing, the cluster's other nodes take over management of that node's UVMs until the CVM has recovered. The CVM runs several services, including: Stargate the data I/O manager Cassandra metadata store Prism user interface Cerebro replication and \"DR\" Zookeeper distributed configuration store Curator MapReduce cluster manager and cleanup Acropolis AHV A block refers to a chassis that supports 1-4 nodes, providing power, cooling, and a shared backplane to all hosted nodes. However unlike a blade chassis it does not have shared networking. Storage Several blocks connected to a third-party switch form a Distributed Storage Fabric , which combines every block's storage into a single storage pool. Containers are logical storage policies and do not correspond with physical disks, and all are thinly provisioned. In ESXi a container is presented as a datastore .","title":"Nutanix"},{"location":"Misc/Nutanix/#nutanix","text":"Nutanix AOS is an offering that creates a storage fabric distributed across all nodes of an HCI cluster, intended for data center. Nutanix was the first to offer an HCI solution in 2011, and since then the market has exploded. AOS supports multiple hypervisors, including Nutanix's native AHV , as well as ESXi and Hyper-V. AOS nodes are able to run all the core services of the cluster. Nutanix has a CLI, known as NCLI . Prism is the name of the graphical management console.","title":"Nutanix"},{"location":"Misc/Nutanix/#architecture","text":"UVMs are controlled by the CVM or Controller VM which runs the Nutanix software. The CVM is a hardened Linux appliance and a \"User Mode VM\" which exists in only a single instance per node. AOS has a \"shared-nothing\" architecture, so in the event of one node's CVM failing, the cluster's other nodes take over management of that node's UVMs until the CVM has recovered. The CVM runs several services, including: Stargate the data I/O manager Cassandra metadata store Prism user interface Cerebro replication and \"DR\" Zookeeper distributed configuration store Curator MapReduce cluster manager and cleanup Acropolis AHV A block refers to a chassis that supports 1-4 nodes, providing power, cooling, and a shared backplane to all hosted nodes. However unlike a blade chassis it does not have shared networking.","title":"Architecture"},{"location":"Misc/Nutanix/#storage","text":"Several blocks connected to a third-party switch form a Distributed Storage Fabric , which combines every block's storage into a single storage pool. Containers are logical storage policies and do not correspond with physical disks, and all are thinly provisioned. In ESXi a container is presented as a datastore .","title":"Storage"},{"location":"Misc/Russia/","text":"Russia Aircraft graph LR A{<em>Su-27</em>} B[Su-30] C[Su-30SM] D[Su-33] E[Su-34] A --- B B --- C A --- D A --- E click A href \"#su-27\" click B href \"#su-30\" click C href \"#su-30sm\" click D href \"#su-33\" click E href \"#su-34\" Mig-35 Export variant of the MiG-29M2 with: New MFD displays instead of analog instruments RD-33MKB engines that implement thrust-vectoring AESA radar The ability to be armed with ground-attack weapons like bombs Su-27 Su-30 The Su-30 was originally called the Su-27PU and intended to be a long-range interceptor. The airframe of the Su-27PU, in turn, was based on that of the Su-27UB, another variant intended to be a two-seat trainer. It entered service in the 1990s as an all-weather multirole two-seat fighter. It achieved success in the export market: Su-30MKI , jointly produced with Hindustan Aeronautics Limited, featured thrust vectoring. Su-30MKM , designed for the Royal Malaysian Air Force, inherited the Su-30MKI's thrust vetoring and added avionics upgrades. Su-30MKK , intended for China, spawned derivatives of its own that were purchased by Vietnam and Venezuela. Su-30SM Like the Su-35, a modernized variant of the Su-27 that adds jamming pods and improved communications for the Russian Air Force. The Su-30SM2 will use the same engine as the Su-35 : the AL-41F-1S. Su-33 The Su-33 is a carrier-based variant of the Su-27 with reinforced undercarriage, rugged landing gear, canards, larger folding wings, and slightly more powerful engines. Although the concept dates from the late 1970s, when it was initially called the Su-27K, it was only introduced in 1998. It was developed to populate the fighter wings of \"heavy aviation cruisers\", a hybrid between an aircraft carrier and battleship that was developed by the Soviet Union late in the Cold War. These wings were originally intended to be filled by the Yak-38, which was a disappointment and quickly retired. Despite the changes made to accomodate carrier-based aviation, it still proved too big and did not have all the payload delivery features necessary. Efforts to export the Su-33 to China and India fell through, however China did reverse-engineer it to create the J-11B. India elected to purchase the more advanced MiG-29K. Russian Naval Aviation has been the Su-33's only operator. In 2009, the several dozen Su-33s in service began to be replaced by [MiG-29K][#mig-29k]s. Su-34 Fighter-bomber variant of the Su-27, designed by Rolan Martirosov. Ground Forces Battalions can be subordinate to brigades or regiments, depending on context. Brigades are more self-contained, whereas regiments are themselves subordinate to divisions. VDV Modern VDV units can be divided by mode of entry into airborne and air assault units. Historically, airborne troops were controlled by the Soviet General Staff for strategic objectives: capturing command and control nodes, political targets, etc deep in enemy territory. Air assault forces, in contrast, generally operated heavier ground equipment and had operational objectives to support ground forces. Some air assault forces had been under the control of military districts but since 2013 have been consolidated into the VDV . Both air assault and airborne platoons consist of 3 squads in their own IFVs, either the BMD-2 or the BMD-4M . Each IFV can only mount 7 personnel, of which two are the driver-mechanic and gunner-operator who remain with the vehicle after infantry dismount. Apparently the deputy platoon commander stays in the vehicle as well, providing a total of 14 dismounts per platoon. Airborne and air assault companies combine three such platoons with a Headquarters Section made of two additional IFVs. Air assault companies add a Grenade-Machine Gun Section in its own APC. Three such companies in combination with reconnaissance and support platoons and a medical section, form the core of a battalion. Air assault battalions add a mortar battery with six tubes. BMD-2 Introduced in 1985 , an IFV with a 30 mm cannon. BMD-4 Introduced in 2016 , an IFV with a 100 mm cannon.","title":"Russia"},{"location":"Misc/Russia/#russia","text":"","title":"Russia"},{"location":"Misc/Russia/#aircraft","text":"graph LR A{<em>Su-27</em>} B[Su-30] C[Su-30SM] D[Su-33] E[Su-34] A --- B B --- C A --- D A --- E click A href \"#su-27\" click B href \"#su-30\" click C href \"#su-30sm\" click D href \"#su-33\" click E href \"#su-34\"","title":"Aircraft"},{"location":"Misc/Russia/#mig-35","text":"Export variant of the MiG-29M2 with: New MFD displays instead of analog instruments RD-33MKB engines that implement thrust-vectoring AESA radar The ability to be armed with ground-attack weapons like bombs","title":"Mig-35"},{"location":"Misc/Russia/#su-27","text":"","title":"Su-27"},{"location":"Misc/Russia/#su-30","text":"The Su-30 was originally called the Su-27PU and intended to be a long-range interceptor. The airframe of the Su-27PU, in turn, was based on that of the Su-27UB, another variant intended to be a two-seat trainer. It entered service in the 1990s as an all-weather multirole two-seat fighter. It achieved success in the export market: Su-30MKI , jointly produced with Hindustan Aeronautics Limited, featured thrust vectoring. Su-30MKM , designed for the Royal Malaysian Air Force, inherited the Su-30MKI's thrust vetoring and added avionics upgrades. Su-30MKK , intended for China, spawned derivatives of its own that were purchased by Vietnam and Venezuela.","title":"Su-30"},{"location":"Misc/Russia/#su-30sm","text":"Like the Su-35, a modernized variant of the Su-27 that adds jamming pods and improved communications for the Russian Air Force. The Su-30SM2 will use the same engine as the Su-35 : the AL-41F-1S.","title":"Su-30SM"},{"location":"Misc/Russia/#su-33","text":"The Su-33 is a carrier-based variant of the Su-27 with reinforced undercarriage, rugged landing gear, canards, larger folding wings, and slightly more powerful engines. Although the concept dates from the late 1970s, when it was initially called the Su-27K, it was only introduced in 1998. It was developed to populate the fighter wings of \"heavy aviation cruisers\", a hybrid between an aircraft carrier and battleship that was developed by the Soviet Union late in the Cold War. These wings were originally intended to be filled by the Yak-38, which was a disappointment and quickly retired. Despite the changes made to accomodate carrier-based aviation, it still proved too big and did not have all the payload delivery features necessary. Efforts to export the Su-33 to China and India fell through, however China did reverse-engineer it to create the J-11B. India elected to purchase the more advanced MiG-29K. Russian Naval Aviation has been the Su-33's only operator. In 2009, the several dozen Su-33s in service began to be replaced by [MiG-29K][#mig-29k]s.","title":"Su-33"},{"location":"Misc/Russia/#su-34","text":"Fighter-bomber variant of the Su-27, designed by Rolan Martirosov.","title":"Su-34"},{"location":"Misc/Russia/#ground-forces","text":"Battalions can be subordinate to brigades or regiments, depending on context. Brigades are more self-contained, whereas regiments are themselves subordinate to divisions.","title":"Ground Forces"},{"location":"Misc/Russia/#vdv","text":"Modern VDV units can be divided by mode of entry into airborne and air assault units. Historically, airborne troops were controlled by the Soviet General Staff for strategic objectives: capturing command and control nodes, political targets, etc deep in enemy territory. Air assault forces, in contrast, generally operated heavier ground equipment and had operational objectives to support ground forces. Some air assault forces had been under the control of military districts but since 2013 have been consolidated into the VDV . Both air assault and airborne platoons consist of 3 squads in their own IFVs, either the BMD-2 or the BMD-4M . Each IFV can only mount 7 personnel, of which two are the driver-mechanic and gunner-operator who remain with the vehicle after infantry dismount. Apparently the deputy platoon commander stays in the vehicle as well, providing a total of 14 dismounts per platoon. Airborne and air assault companies combine three such platoons with a Headquarters Section made of two additional IFVs. Air assault companies add a Grenade-Machine Gun Section in its own APC. Three such companies in combination with reconnaissance and support platoons and a medical section, form the core of a battalion. Air assault battalions add a mortar battery with six tubes.","title":"VDV"},{"location":"Misc/Russia/#bmd-2","text":"Introduced in 1985 , an IFV with a 30 mm cannon.","title":"BMD-2"},{"location":"Misc/Russia/#bmd-4","text":"Introduced in 2016 , an IFV with a 100 mm cannon.","title":"BMD-4"},{"location":"Misc/Space/","text":"\ud83d\ude80 Space Missions to look forward to James Webb Space Telescope (JWST) to be launched ca. October 31, 2021 Artemis-1 to be launched in late 2021 , with first scientific observations after 6 months of testing Jupiter Icy Moon Explorer (JUICE) to be launched 2022 and arrive 2029 E-ELT first light expected November 2026 Dragonfly mission to Titan scheduled to launch in 2027 and to arrive in 2036","title":"\ud83d\ude80 Space"},{"location":"Misc/Space/#space","text":"Missions to look forward to James Webb Space Telescope (JWST) to be launched ca. October 31, 2021 Artemis-1 to be launched in late 2021 , with first scientific observations after 6 months of testing Jupiter Icy Moon Explorer (JUICE) to be launched 2022 and arrive 2029 E-ELT first light expected November 2026 Dragonfly mission to Titan scheduled to launch in 2027 and to arrive in 2036","title":"\ud83d\ude80 Space"},{"location":"Misc/Storage/","text":"Storage NAS The Dell PowerEdge R720 and R720XD use the H710 PERC RAID controller with 512 MB cache Helios64 is billed as the ultimate ARM-powered NAS. It comes with its own UPS which can power the unit for 15 minutes. It was reviewed by Alex from the Self-Hosted podcast, who took issue with poor build quality and value. Consumer-friendly products from Synology and QNap may perhaps be a better solution. Synology 1621+ was reviewed in Self-Hosted 43 . Synology offers DSM for easy management. Brian Moses has an annual blog series on making your own DIY NAS. Flash memory 3D NAND flash chips can be defined by how much data is stored in memory cells which are stacked vertically. SLC , MLC , TLC , and QLC represent progressively more memory-dense NAND technologies. They also represent progressively less long-lived devices in terms of P/E cycles.","title":"Storage"},{"location":"Misc/Storage/#storage","text":"","title":"Storage"},{"location":"Misc/Storage/#nas","text":"The Dell PowerEdge R720 and R720XD use the H710 PERC RAID controller with 512 MB cache Helios64 is billed as the ultimate ARM-powered NAS. It comes with its own UPS which can power the unit for 15 minutes. It was reviewed by Alex from the Self-Hosted podcast, who took issue with poor build quality and value. Consumer-friendly products from Synology and QNap may perhaps be a better solution. Synology 1621+ was reviewed in Self-Hosted 43 . Synology offers DSM for easy management. Brian Moses has an annual blog series on making your own DIY NAS.","title":"NAS"},{"location":"Misc/Storage/#flash-memory","text":"3D NAND flash chips can be defined by how much data is stored in memory cells which are stacked vertically. SLC , MLC , TLC , and QLC represent progressively more memory-dense NAND technologies. They also represent progressively less long-lived devices in terms of P/E cycles.","title":"Flash memory"},{"location":"Misc/TrueNAS/","text":"TrueNAS TrueNAS supports additional applications through catalogs , such as TrueCharts. z Glossary midclt middleware client TrueCharts A catalog of applications made available by veteran TrueNAS forum users . Applications are collected in several trains : stable, core, and incubator Applications are packaged as Helm packages.","title":"TrueNAS"},{"location":"Misc/TrueNAS/#truenas","text":"TrueNAS supports additional applications through catalogs , such as TrueCharts. z","title":"TrueNAS"},{"location":"Misc/TrueNAS/#glossary","text":"","title":"Glossary"},{"location":"Misc/TrueNAS/#midclt","text":"middleware client","title":"midclt"},{"location":"Misc/TrueNAS/#truecharts","text":"A catalog of applications made available by veteran TrueNAS forum users . Applications are collected in several trains : stable, core, and incubator Applications are packaged as Helm packages.","title":"TrueCharts  "},{"location":"Misc/bittorrent/","text":"BitTorrent Bram Cohen invented BitTorrent protocol in 2001 and wrote the first client in Python. It is a peer-to-peer file sharing protocol where those who share a file are called seeders and those who download are called peers . All seeders and peers related to a particular torrent comprise the swarm . The tracker server or tracker serves as a repository for information about peers associated with the same file. Files are downloaded in hashed pieces from multiple seeders to distribute the burden of seeding a file. ^ A Torrent Descriptor file is a hashmap file Torrent Descriptor property Description Announce URL of the tracker Info dictionary whose keys depend on whether one or more files are being shared, including: Files: list of dictionaries, only exists when multiple files are being shared, each dictionary has two keys and corresponds to a file Length: size of the file in bytes Path: list of strings corresponding to subdirectory names, the last of which is the actual filename length size of the file in bytes (when one file is being shared name suggested file or directory name Pieces length number of bytes per piece; must be a power of 2 and at least 16KiB Pieces list of SHA-1 160-bit hashes calculated on various parts of data { \"Announce\" : \"url of tracker\" , \"Info\" : { \"Files\" : [{ \"Length\" : 16 , \"path\" : \"/folder/to/path\" }, { \"length\" : 193 , \"path\" : \"/another/folder\" }] }, \"length\" : 192 , \"name\" : \" Ubuntu.iso\" , \"Pieces length\" : 262144 , \"Pieces\" : [ AAF 4 C 61 DDCC 5E8 A 2 DABEDE 0 F 3 B 482 CD 9 AEA 9434 D , CFEA 2496442 C 091 FDDD 1 BA 215 D 62 A 69E C 34E94 D 0 ] } BitTorrent clients Deluge ^ ^ command-line functionality open-source with expandable functionalities chosen as the best torrent client by Lifehacker qBittorrent open-source, ad-free alternative to uTorrent Tixati closed-source Transmission installed by default on Ubuntu (ca. 2017) Tribler Vuze has ads and is closed-source Frostwire : multiplatform, including Android WebTorrent Desktop","title":"BitTorrent"},{"location":"Misc/bittorrent/#bittorrent","text":"Bram Cohen invented BitTorrent protocol in 2001 and wrote the first client in Python. It is a peer-to-peer file sharing protocol where those who share a file are called seeders and those who download are called peers . All seeders and peers related to a particular torrent comprise the swarm . The tracker server or tracker serves as a repository for information about peers associated with the same file. Files are downloaded in hashed pieces from multiple seeders to distribute the burden of seeding a file. ^ A Torrent Descriptor file is a hashmap file Torrent Descriptor property Description Announce URL of the tracker Info dictionary whose keys depend on whether one or more files are being shared, including: Files: list of dictionaries, only exists when multiple files are being shared, each dictionary has two keys and corresponds to a file Length: size of the file in bytes Path: list of strings corresponding to subdirectory names, the last of which is the actual filename length size of the file in bytes (when one file is being shared name suggested file or directory name Pieces length number of bytes per piece; must be a power of 2 and at least 16KiB Pieces list of SHA-1 160-bit hashes calculated on various parts of data { \"Announce\" : \"url of tracker\" , \"Info\" : { \"Files\" : [{ \"Length\" : 16 , \"path\" : \"/folder/to/path\" }, { \"length\" : 193 , \"path\" : \"/another/folder\" }] }, \"length\" : 192 , \"name\" : \" Ubuntu.iso\" , \"Pieces length\" : 262144 , \"Pieces\" : [ AAF 4 C 61 DDCC 5E8 A 2 DABEDE 0 F 3 B 482 CD 9 AEA 9434 D , CFEA 2496442 C 091 FDDD 1 BA 215 D 62 A 69E C 34E94 D 0 ] }","title":"BitTorrent"},{"location":"Misc/bittorrent/#bittorrent-clients","text":"Deluge ^ ^ command-line functionality open-source with expandable functionalities chosen as the best torrent client by Lifehacker qBittorrent open-source, ad-free alternative to uTorrent Tixati closed-source Transmission installed by default on Ubuntu (ca. 2017) Tribler Vuze has ads and is closed-source Frostwire : multiplatform, including Android WebTorrent Desktop","title":"BitTorrent clients"},{"location":"Misc/cryptocurrency/","text":"\ud83e\ude99 Cryptocurrency The establishment of the ERC-20 protocol allows new tokens to launch on Ethereum's blockchain using Ethereum smart contracts. This resulted in an explosion of new cryptocurrencies in 2017. Since then, investors have switched focus to tokens that are exchanged on creditable exchanges. Initial Exchange Offerings (IEO) have become popular. ref Crypto tokens form one of the two categories of cryptocurrency and represent a tradable asset or utility that is found on a blockchain. Cryptocurrency is a standard currency used for the sole purpose of making or receivng payments on the blockchain. Crypto tokens represent an underlying asset, customer loyalty points for example. ref Blockchain Blockchain is a distributed digital ledger consisting of interlinked blocks, each of which stores information that cannot be retroactively tampered with or deleted. ref Blockchain is a database technology that uses hashes to ensure reliability and security of data stored across a network of computers, popularized by BitCoin. Records, containing information, are validated and then added to Blocks , or hashed containers, which are then concatenated in a chain by associating each block with the hash of both of its neighbors. ref There can only be a a maximum of 21 million BTC, and the reward for mining a new bitcoin halves every 210,000 blocks. This has already occurred twice in the 10-year history of Bitcoin until late 2019, and another blockhalving is expected to occur in May 2020. ref","title":"\ud83e\ude99 Cryptocurrency"},{"location":"Misc/cryptocurrency/#cryptocurrency","text":"The establishment of the ERC-20 protocol allows new tokens to launch on Ethereum's blockchain using Ethereum smart contracts. This resulted in an explosion of new cryptocurrencies in 2017. Since then, investors have switched focus to tokens that are exchanged on creditable exchanges. Initial Exchange Offerings (IEO) have become popular. ref Crypto tokens form one of the two categories of cryptocurrency and represent a tradable asset or utility that is found on a blockchain. Cryptocurrency is a standard currency used for the sole purpose of making or receivng payments on the blockchain. Crypto tokens represent an underlying asset, customer loyalty points for example. ref","title":"\ud83e\ude99 Cryptocurrency"},{"location":"Misc/cryptocurrency/#blockchain","text":"Blockchain is a distributed digital ledger consisting of interlinked blocks, each of which stores information that cannot be retroactively tampered with or deleted. ref Blockchain is a database technology that uses hashes to ensure reliability and security of data stored across a network of computers, popularized by BitCoin. Records, containing information, are validated and then added to Blocks , or hashed containers, which are then concatenated in a chain by associating each block with the hash of both of its neighbors. ref There can only be a a maximum of 21 million BTC, and the reward for mining a new bitcoin halves every 210,000 blocks. This has already occurred twice in the 10-year history of Bitcoin until late 2019, and another blockhalving is expected to occur in May 2020. ref","title":"Blockchain"},{"location":"Misc/languages/","text":"Languages Introductions English Spanish Portuguese Swahili How tall are you? Cuanto mides en altura? Qu\u00e3o alta eres? Spanish Portuguese Cuanto mides en altura? Qu\u00e3o alto eres? Politeness Swahili Spanish Habari ya asubuhi, umeamkaje? Nimeamka poa. Umeshindaje? Nimeshinda poa. Habari za mchana? Usiku mwema. Gracias por el cumplido. \"Thanks for the compliment\" Que tan la est\u00e1 pasando? Information Swahili Spanish Wewe ni mkabila gani? Tuve que quedarme despierto hasta tarde. Tom\u00e9 una siesta. Hair Spanish Portuguese Cabello rizado Alisas tu cabello? Corte rapado te quedar\u00e1 bien. Peluca Peruca Reconnecting Portuguese Spanish Te lembras de mim? Me recuerdas? Biography Portuguese Spanish Eu quis lan\u00e7ar a discuss\u00e3o falando pouco de mim . Tenho 39 anos e trabalho na tecnologia de informa\u00e7\u00e3o como engenheiro de servidores . Eu tenho uma filha de nove anos de um casamanto anterior . Gosto de document\u00e1rios e hist\u00f3ria. Especialmente tenho interesse na hist\u00f3ria do sul da \u00c1frica, especialmente as guerras que a \u00c1frica do Sul travava com os pa\u00edses vizinhos. Tu deves saber muito sobre esse assunto. Se puderes, fala de se. Eu gostar\u00eda de saber quantos irm\u00e3os tens, teus interesses, e tua hist\u00f3ria de relacionamentos. Gostas de estrangeiros, de Angolanos, outros Africanos.. Bueno amor, como te dije quiero intercambiar \u00e1udios contigo. Quise empezar con unos hechos de mi vida e podemos partir de ah\u00ed. Tengo 39 a\u00f1os, y trabajo en inform\u00e1tica como ingeniero de servidores . Tengo una ni\u00f1a de 9 a\u00f1os de un matrimonio anterior. Me gustan documentales y hist\u00f3ria. Y mucho me gusta aprender idiomas extranjeros. Lo que te pueda interesar es que trabajaba como traductor para el gobierno estadounidense. Nosotros nos casamos en 2011. Nosotros nos divorciamos en 2019. Tengo un hermano que es tr\u00e9s a\u00f1os menor que yo. Crec\u00ed en Texas. Sex French Faire la cave Angolan dialect O portugu\u00eas angolano soa diferente e n\u00e3o me acostumei. Est\u00e1s falando por enigmas. \"you're speaking in riddles\" Politeness Asante \"Thank you\" Karibu \"You're welcome\" Location Unaishi wapi? \"Where do you live?\" Ninaishi Marekani \"I live in America\" Ninaishi Ukunda. Ukunda wapi? Ukunda karibu na Mombasa. \"Ukunda is close to Mombasa\" Travel Naja Mombasa hivi karibuni \"I am coming to Mombasa very soon\" Nipo kunondon. \"I am here\" Sex Swahili Spanish Nataka kujibamba \"I want to have fun\" Sawa jibambe \"Enjoy\" Unataka kujibamba vipi \"How do you want to have fun?\" Ngono \"sex\" Hakuna ngono ya bure \"There is no free sex\" Tunaweza kulala wote? \"Can we sleep together?\" Ninataka kulamba uko wake Matako \"butt\" en cuatro \"doggystyle\" chocartelo to have sex forcefully On prostitution Spanish No tiene que andar en el calle para ser una prostituta. En estos d\u00edas con aplicaciones de citas, prostitutas ya no necesitan hacer eso y facilmente encontran clientes en l\u00ednea. Yo se sobre eso personalmente porque ya encontr\u00e9 muchas muchas chicas hiciendo eso especialmente en tu pa\u00eds. Y pienso k tuve un malentendido sobre lo que yo busco. Yo realmente quiero conocer a una chica y k estemos juntos, \u00edntimos, no una vez pero muchas vezes. Y quiero k salgamos juntos y nos divertamos y estemos juntos, con intimidad. Eso k deciste sobre tu relaci\u00f3n antiga, me parece k ese hombre no fue tu pareja pero un papi dulce.","title":"Languages"},{"location":"Misc/languages/#languages","text":"Introductions English Spanish Portuguese Swahili How tall are you? Cuanto mides en altura? Qu\u00e3o alta eres? Spanish Portuguese Cuanto mides en altura? Qu\u00e3o alto eres? Politeness Swahili Spanish Habari ya asubuhi, umeamkaje? Nimeamka poa. Umeshindaje? Nimeshinda poa. Habari za mchana? Usiku mwema. Gracias por el cumplido. \"Thanks for the compliment\" Que tan la est\u00e1 pasando? Information Swahili Spanish Wewe ni mkabila gani? Tuve que quedarme despierto hasta tarde. Tom\u00e9 una siesta. Hair Spanish Portuguese Cabello rizado Alisas tu cabello? Corte rapado te quedar\u00e1 bien. Peluca Peruca Reconnecting Portuguese Spanish Te lembras de mim? Me recuerdas? Biography Portuguese Spanish Eu quis lan\u00e7ar a discuss\u00e3o falando pouco de mim . Tenho 39 anos e trabalho na tecnologia de informa\u00e7\u00e3o como engenheiro de servidores . Eu tenho uma filha de nove anos de um casamanto anterior . Gosto de document\u00e1rios e hist\u00f3ria. Especialmente tenho interesse na hist\u00f3ria do sul da \u00c1frica, especialmente as guerras que a \u00c1frica do Sul travava com os pa\u00edses vizinhos. Tu deves saber muito sobre esse assunto. Se puderes, fala de se. Eu gostar\u00eda de saber quantos irm\u00e3os tens, teus interesses, e tua hist\u00f3ria de relacionamentos. Gostas de estrangeiros, de Angolanos, outros Africanos.. Bueno amor, como te dije quiero intercambiar \u00e1udios contigo. Quise empezar con unos hechos de mi vida e podemos partir de ah\u00ed. Tengo 39 a\u00f1os, y trabajo en inform\u00e1tica como ingeniero de servidores . Tengo una ni\u00f1a de 9 a\u00f1os de un matrimonio anterior. Me gustan documentales y hist\u00f3ria. Y mucho me gusta aprender idiomas extranjeros. Lo que te pueda interesar es que trabajaba como traductor para el gobierno estadounidense. Nosotros nos casamos en 2011. Nosotros nos divorciamos en 2019. Tengo un hermano que es tr\u00e9s a\u00f1os menor que yo. Crec\u00ed en Texas. Sex French Faire la cave","title":"Languages"},{"location":"Misc/languages/#angolan-dialect","text":"O portugu\u00eas angolano soa diferente e n\u00e3o me acostumei. Est\u00e1s falando por enigmas. \"you're speaking in riddles\"","title":"Angolan dialect"},{"location":"Misc/languages/#politeness","text":"Asante \"Thank you\" Karibu \"You're welcome\"","title":"Politeness"},{"location":"Misc/languages/#location","text":"Unaishi wapi? \"Where do you live?\" Ninaishi Marekani \"I live in America\" Ninaishi Ukunda. Ukunda wapi? Ukunda karibu na Mombasa. \"Ukunda is close to Mombasa\"","title":"Location"},{"location":"Misc/languages/#travel","text":"Naja Mombasa hivi karibuni \"I am coming to Mombasa very soon\" Nipo kunondon. \"I am here\" Sex Swahili Spanish Nataka kujibamba \"I want to have fun\" Sawa jibambe \"Enjoy\" Unataka kujibamba vipi \"How do you want to have fun?\" Ngono \"sex\" Hakuna ngono ya bure \"There is no free sex\" Tunaweza kulala wote? \"Can we sleep together?\" Ninataka kulamba uko wake Matako \"butt\" en cuatro \"doggystyle\" chocartelo to have sex forcefully On prostitution Spanish No tiene que andar en el calle para ser una prostituta. En estos d\u00edas con aplicaciones de citas, prostitutas ya no necesitan hacer eso y facilmente encontran clientes en l\u00ednea. Yo se sobre eso personalmente porque ya encontr\u00e9 muchas muchas chicas hiciendo eso especialmente en tu pa\u00eds. Y pienso k tuve un malentendido sobre lo que yo busco. Yo realmente quiero conocer a una chica y k estemos juntos, \u00edntimos, no una vez pero muchas vezes. Y quiero k salgamos juntos y nos divertamos y estemos juntos, con intimidad. Eso k deciste sobre tu relaci\u00f3n antiga, me parece k ese hombre no fue tu pareja pero un papi dulce.","title":"Travel"},{"location":"Misc/learn/","text":"Learning Retrieval Retrieval-based learning refers to the coupling of two ideas: Retrieval processes , those involved in using available cues to actively reconstruct knowledge, are most important in learning Active retrieval is most important for producing learning. Retrieval-based learning characterizes knowledge as something that is reconstructed at the time of recall. Knowledge reconstruction is affected by the presence or absence of retrieval cues . This is contrasted with the traditional analogy of knowledge as static storage. Retrieval-based learning is an outgrowth of recent cognitive science research that shows that the act of measuring knowledge - by recall, answering questions, or solving novel problems - actually aids learning. The testing effect refers to the observation that the act of taking a test, by itself and without any feedback or further study, results in improved learning. ( src ) Retrieval effort is a key concept in retrieval-based learning, which refers to the observation that activities that are difficult and require effort can be good for learning. In other words, the effort involved in retrieval is the key to learning. Learning activites that engage retrieval processes include group discussions, reciprocal teaching, and questioning techniques. ( src ) Free recall refers to a specific task provided to subjects whereby the experimenter presents a list of items to be remembered and the subject is free to recall them in any order. Paying forward What's the best way to organize information for newcomers? That is to say, what is the best way to organize notes on the sources of information that I stumble across, rather than merely the information itself? The information itself is practical, but the context of what type of material assisted me in learning it should also be preserved, somehow. For example, for someone new to vim , Chris Toomey's talk on YouTube might be a very good tool for learners who prefer to see lectures by impassioned and articulate people. Someone who is more mechanically inclined might benefit more from pacvim , or other hands-on activities. Maybe a combination of both? Some segments of PluralSight videos on esoteric technical topics appear to cover basic material concisely and effectively, in a way that made me wish I had access to those segments when I was learning it before. Gathering this type of information could be useful, not to me, but to others. How do I organize my thoughts and observations on the value of sources without doubling my effort? Solution Wikipedia-style links in a static site! Command-line syntax After several weeks of refining my note-taking technique with regard to syntax, I believe I have settled on an improved workflow. For all command-line syntax: 1. Note the command itself within \"Commands\" spreadsheet (separate from \"Terms\", which is for vocabulary) 2. If any options are encountered, document them in Options 3. If commands form command groups (like apt , docker , git , netsh , etc), those command groups need to be broken out separately (\"Group-style commands\") 4. If a command launches its own REPL ( bluetoothctl , diskpart , fdisk ...) those are broken out as well This turned out to be far too cumbersome. Better is the approach of learning syntax in the context of actual tasks . Finding magic numbers Before understanding the \"lay of the land\", or rather the best epistemology for a unit of information, you are first confronted with a list of information without context. This happened while studying for the Network+. On the topic of authentication, I learned a list of material, basically concepts associated with AAA. Authentication process of determining... Authorization identifying the resources... Accounting tracking methods used ... Authentication, Authorization, Accounting, and Auditing (AAAA) conceptual model... Remote Authentication Dial-In User Service (RADIUS) protocol that enables ... Terminal Access Control Access Control System (TACACS) security protocol designed ... Kerberos security system ... ticket security tokens issued to clients ... Local authentication subsystem (LASS) authenticates users ... But after further research, I found that once you understand the role of authentication, then really there are only three main systems that implement it (according to the material): Kerberos , TACACS , and RADIUS . Reducing a confusing mass of knowledge into a magic number (2, 3, 4, etc) helps in identifying interrelationships between concepts and entities Anki Made good progress incorporating task-based learning by simplifying procedures into command sequences with no parameters or (at most) one or two. Cloze notes with input comprise a low-level way of practicing the skill. The best strategy to pursue is to identify common patterns, and make the most common elements of those patterns into cloze cards. Basic templates: DSC C# Configuration DnsClient { Import-DscResource -ModuleName \"xNetworking\" Node ( \"ServerA\" , \"ServerB\" ) { xDnsServerAddress DnsServer { Address = 10 . 0 . 0 . 1 AddressFamily = \"Ipv4\" InterfaceAlias = \"Ethernet\" } } } namespace HelloWorld { class Program { static void Main ( string [] args ) { Console . WriteLine ( \"Hello world!\" ); } } } Two tasks with related forms # Get connection string of account constring =$( az storage account show-connection -string ) # Create file share with connection string az storage share create - -connection-string $constring Single task with different implementations (see tasks associated with cloud providers ) Creating an availability set Azure Powershell Azure CLI New-AzAvailabilitySet -PlatformUpdateDomainCount -PlatformFaultDomainCount -Sku \"{{c8::Aligned}}\" az vm availability-set create --platform-update-domain-count --platform-fault-domain-count Get - Set pattern in PowerShell Capture a managed VM image Create a subnet Create DNS record set Change VM size Add a route to a routing table $vm = Get-AzVM $image = New-AzImageConfig -SourceVirtualMachineId $vm . Id New-AzImage -Image $image $vnet2 = Get-AzVirtualNetwork -Name VNet2 -ResourceGroupName ExamRefRG $vnet2 . Subnets += New-AzVirtualNetworkSubnetConfig -Name GatewaySubnet -AddressPrefix 10 . 2 . 1 . 0 / 27 $vnet2 = Set-AzVirtualNetwork -VirtualNetwork $vnet2 $records = @() $records += New-AzDnsRecordConfig -IPv4Address \"1.2.3.4\" $records += New-AzDnsRecordConfig -IPv4Address \"5.6.7.8\" New-AzDnsRecordSet -Name \"@\" -RecordType A -ZoneName examref . com -ResourceGroupName ExamRefRG -Ttl 3600 -DnsRecords $records $vm = Get-AzVM $vm . HardwareProfile . VMSize = Standard_DS3_v2 Update-AzVM -VM $vm $rt = New-AzRouteTable Add-AzRouteConfig -RouteTable $rt Set-AzRouteTable -RouteTable $rt Structure Playing around with reference-style links and tooltips has me thinking that there really should be a more structured, flexible way of generating text reports from object-style hierarchical information. For example, whether a definition appears beside a word dictionary style or in a tooltip on hover is really an implementation detail. There should be an easy way of storing that data and specifying that presentation dynamically. What I have settled on is a multilayered note-taking strategy. Every lexeme is defined first in a slug or one-line description that establishes its epistemological context as well as its semantic significance. A stub further elaborates the lexeme, especially insofar as it encapsulates further lexemes or can be analyzed into components. These slugs and stubs can be presented in various ways. Most recently I have gotten into the habit of putting slugs into tooltips that appear when I hover over lexemes in my markdown notes. This is an especially elegant solution in tables, where I can provide a highly condensed and legible index of commands, each of which can be understood at a high level by hovering the mouse while still providing full details when clicked on. This is also an elegant solution in tables of contents, where I can use a tooltip to contain a synopsis of a chapter which still links to the full notes. It provides a way of rendering information of intermediary fidelity, between the mere title and fully developed notes. Actually, on second thought, notes themselves are not particularly useful once a topic has actually been learned...","title":"Learning"},{"location":"Misc/learn/#learning","text":"","title":"Learning"},{"location":"Misc/learn/#retrieval","text":"Retrieval-based learning refers to the coupling of two ideas: Retrieval processes , those involved in using available cues to actively reconstruct knowledge, are most important in learning Active retrieval is most important for producing learning. Retrieval-based learning characterizes knowledge as something that is reconstructed at the time of recall. Knowledge reconstruction is affected by the presence or absence of retrieval cues . This is contrasted with the traditional analogy of knowledge as static storage. Retrieval-based learning is an outgrowth of recent cognitive science research that shows that the act of measuring knowledge - by recall, answering questions, or solving novel problems - actually aids learning. The testing effect refers to the observation that the act of taking a test, by itself and without any feedback or further study, results in improved learning. ( src ) Retrieval effort is a key concept in retrieval-based learning, which refers to the observation that activities that are difficult and require effort can be good for learning. In other words, the effort involved in retrieval is the key to learning. Learning activites that engage retrieval processes include group discussions, reciprocal teaching, and questioning techniques. ( src ) Free recall refers to a specific task provided to subjects whereby the experimenter presents a list of items to be remembered and the subject is free to recall them in any order.","title":"Retrieval"},{"location":"Misc/learn/#paying-forward","text":"What's the best way to organize information for newcomers? That is to say, what is the best way to organize notes on the sources of information that I stumble across, rather than merely the information itself? The information itself is practical, but the context of what type of material assisted me in learning it should also be preserved, somehow. For example, for someone new to vim , Chris Toomey's talk on YouTube might be a very good tool for learners who prefer to see lectures by impassioned and articulate people. Someone who is more mechanically inclined might benefit more from pacvim , or other hands-on activities. Maybe a combination of both? Some segments of PluralSight videos on esoteric technical topics appear to cover basic material concisely and effectively, in a way that made me wish I had access to those segments when I was learning it before. Gathering this type of information could be useful, not to me, but to others. How do I organize my thoughts and observations on the value of sources without doubling my effort? Solution Wikipedia-style links in a static site!","title":"Paying forward"},{"location":"Misc/learn/#command-line-syntax","text":"After several weeks of refining my note-taking technique with regard to syntax, I believe I have settled on an improved workflow. For all command-line syntax: 1. Note the command itself within \"Commands\" spreadsheet (separate from \"Terms\", which is for vocabulary) 2. If any options are encountered, document them in Options 3. If commands form command groups (like apt , docker , git , netsh , etc), those command groups need to be broken out separately (\"Group-style commands\") 4. If a command launches its own REPL ( bluetoothctl , diskpart , fdisk ...) those are broken out as well This turned out to be far too cumbersome. Better is the approach of learning syntax in the context of actual tasks .","title":"Command-line syntax"},{"location":"Misc/learn/#finding-magic-numbers","text":"Before understanding the \"lay of the land\", or rather the best epistemology for a unit of information, you are first confronted with a list of information without context. This happened while studying for the Network+. On the topic of authentication, I learned a list of material, basically concepts associated with AAA. Authentication process of determining... Authorization identifying the resources... Accounting tracking methods used ... Authentication, Authorization, Accounting, and Auditing (AAAA) conceptual model... Remote Authentication Dial-In User Service (RADIUS) protocol that enables ... Terminal Access Control Access Control System (TACACS) security protocol designed ... Kerberos security system ... ticket security tokens issued to clients ... Local authentication subsystem (LASS) authenticates users ... But after further research, I found that once you understand the role of authentication, then really there are only three main systems that implement it (according to the material): Kerberos , TACACS , and RADIUS . Reducing a confusing mass of knowledge into a magic number (2, 3, 4, etc) helps in identifying interrelationships between concepts and entities","title":"Finding magic numbers"},{"location":"Misc/learn/#anki","text":"Made good progress incorporating task-based learning by simplifying procedures into command sequences with no parameters or (at most) one or two. Cloze notes with input comprise a low-level way of practicing the skill. The best strategy to pursue is to identify common patterns, and make the most common elements of those patterns into cloze cards. Basic templates: DSC C# Configuration DnsClient { Import-DscResource -ModuleName \"xNetworking\" Node ( \"ServerA\" , \"ServerB\" ) { xDnsServerAddress DnsServer { Address = 10 . 0 . 0 . 1 AddressFamily = \"Ipv4\" InterfaceAlias = \"Ethernet\" } } } namespace HelloWorld { class Program { static void Main ( string [] args ) { Console . WriteLine ( \"Hello world!\" ); } } } Two tasks with related forms # Get connection string of account constring =$( az storage account show-connection -string ) # Create file share with connection string az storage share create - -connection-string $constring Single task with different implementations (see tasks associated with cloud providers ) Creating an availability set Azure Powershell Azure CLI New-AzAvailabilitySet -PlatformUpdateDomainCount -PlatformFaultDomainCount -Sku \"{{c8::Aligned}}\" az vm availability-set create --platform-update-domain-count --platform-fault-domain-count Get - Set pattern in PowerShell Capture a managed VM image Create a subnet Create DNS record set Change VM size Add a route to a routing table $vm = Get-AzVM $image = New-AzImageConfig -SourceVirtualMachineId $vm . Id New-AzImage -Image $image $vnet2 = Get-AzVirtualNetwork -Name VNet2 -ResourceGroupName ExamRefRG $vnet2 . Subnets += New-AzVirtualNetworkSubnetConfig -Name GatewaySubnet -AddressPrefix 10 . 2 . 1 . 0 / 27 $vnet2 = Set-AzVirtualNetwork -VirtualNetwork $vnet2 $records = @() $records += New-AzDnsRecordConfig -IPv4Address \"1.2.3.4\" $records += New-AzDnsRecordConfig -IPv4Address \"5.6.7.8\" New-AzDnsRecordSet -Name \"@\" -RecordType A -ZoneName examref . com -ResourceGroupName ExamRefRG -Ttl 3600 -DnsRecords $records $vm = Get-AzVM $vm . HardwareProfile . VMSize = Standard_DS3_v2 Update-AzVM -VM $vm $rt = New-AzRouteTable Add-AzRouteConfig -RouteTable $rt Set-AzRouteTable -RouteTable $rt","title":"Anki"},{"location":"Misc/learn/#structure","text":"Playing around with reference-style links and tooltips has me thinking that there really should be a more structured, flexible way of generating text reports from object-style hierarchical information. For example, whether a definition appears beside a word dictionary style or in a tooltip on hover is really an implementation detail. There should be an easy way of storing that data and specifying that presentation dynamically. What I have settled on is a multilayered note-taking strategy. Every lexeme is defined first in a slug or one-line description that establishes its epistemological context as well as its semantic significance. A stub further elaborates the lexeme, especially insofar as it encapsulates further lexemes or can be analyzed into components. These slugs and stubs can be presented in various ways. Most recently I have gotten into the habit of putting slugs into tooltips that appear when I hover over lexemes in my markdown notes. This is an especially elegant solution in tables, where I can provide a highly condensed and legible index of commands, each of which can be understood at a high level by hovering the mouse while still providing full details when clicked on. This is also an elegant solution in tables of contents, where I can use a tooltip to contain a synopsis of a chapter which still links to the full notes. It provides a way of rendering information of intermediary fidelity, between the mere title and fully developed notes. Actually, on second thought, notes themselves are not particularly useful once a topic has actually been learned...","title":"Structure"},{"location":"Misc/mkdocs-material/","text":"Mkdocs Material mkdocs.yml site_name: Notes theme: name: material features: - content.code.annotate # (1) palette: scheme: default primary: white markdown_extensions: - admonition - md_in_html - pymdownx.snippets # (2) - pymdownx.details - pymdownx.tabbed: # (4) alternate_style: true - pymdownx.superfences: # (3) custom_fences: - name: mermaid class: mermaid format: !!python/name:pymdownx.superfences.fence_code_format This is a code annotation Snippets allow inclusions to be made from other files. Superfences allow code blocks to be palced inside tabs and admonitions. Tabbed allows tabs. Note that the alternate_style configuration is the only supported style and is required. Audio Audio clips require the md_in_html extension. Keep in mind that the filename of the audio will be appended to the route of the current page. '''md '''","title":"Mkdocs Material"},{"location":"Misc/mkdocs-material/#mkdocs-material","text":"","title":"Mkdocs Material"},{"location":"Misc/mkdocs-material/#mkdocsyml","text":"site_name: Notes theme: name: material features: - content.code.annotate # (1) palette: scheme: default primary: white markdown_extensions: - admonition - md_in_html - pymdownx.snippets # (2) - pymdownx.details - pymdownx.tabbed: # (4) alternate_style: true - pymdownx.superfences: # (3) custom_fences: - name: mermaid class: mermaid format: !!python/name:pymdownx.superfences.fence_code_format This is a code annotation Snippets allow inclusions to be made from other files. Superfences allow code blocks to be palced inside tabs and admonitions. Tabbed allows tabs. Note that the alternate_style configuration is the only supported style and is required.","title":"mkdocs.yml"},{"location":"Misc/mkdocs-material/#audio","text":"Audio clips require the md_in_html extension. Keep in mind that the filename of the audio will be appended to the route of the current page. '''md '''","title":"Audio"},{"location":"Misc/pins/","text":"Lapel pin ideas Zeon Weyland-Yutani Corp Tyrell Corporation OCP Homeworld Mass Effect Renegade/Paragon N7 Mirror Universe","title":"Lapel pin ideas"},{"location":"Misc/pins/#lapel-pin-ideas","text":"Zeon Weyland-Yutani Corp Tyrell Corporation OCP Homeworld Mass Effect Renegade/Paragon N7 Mirror Universe","title":"Lapel pin ideas"},{"location":"Misc/Languages/Spanish/","text":"Spanish Greetings graph TD F[Saludos] --> A; F --> C; F --> E; A[Qu\u00e9 lo que?] --> B[Tranquilo]; C[Dame luz?] --> B; A --> D[T\u00f3 frio]; C --> D; E[Como te sien?] --> B; E --> D; E --> G[Bacano]; A --> H[Normal]; C --> H; E --> H; A --> I[Manso]; C --> I; E --> I; Note that Qu\u00e9 lo que often appears in chat as 'klk' or 'qlok' Both men and women can be addressed as manito / manita , loco / loca , i.e. \"Qu\u00e9 lo que manito?\", etc. Patterns Spanish Dominican Example estoy t\u00ed est\u00e1s t\u00e1 est\u00e1 t\u00e1 est\u00e1mos tamo est\u00e1n tan Change Audio Transcription Definite article dropped van a los restaurantes finos Initial r become h cultura muy r efinada Initial r accepts interpolated h el dembow vuelva a h revivir con los tiempos de antes Interpolated h between n and l a divertirse en h las noches Vocabulary bajarle dos to lower the intensity or speed of something baquear Qui\u00e9n te t\u00e1 baqueando ese poloch\u00e9? brindar to present, offer chancear a alguien give somebody a chance , court someone chin a little bit chulo very handsome a man who is knowingly in a relationship with a woman who is a prostitute, and who is supported by her comer como una lima nueva describing someone who eats a lot chuqui to be irritable, angry (also chucky ) andar chuqu\u00ed en la calle to party in the streets cocotazo a smack on the head cualto money, payment ponerse los cuernos to cheat on a partner dar galleta slap the face flow swag, style, attractive manner of dress guayar to bump, grind hacer diligencia making money by exchanging sexual favors josear (also josiar , jociar ) to hustle jeva woman , partner laja something highly used and worn out \"Estoy buscando una laja de esas.\" Los Guandules a barrio on the right bank of the Ozama River in Santo Domingo, location of the ruins of the Monastery of San Francisco macorisano someone from SFM mangar to pinch, grab, obtain; have sexual relations with someone molote mob nama only, merely, just (preverbal) nina nothing more, nothing at all (sentence final) parar to end up in a place without intending to do so pariguayo lame guy who can't get girls pasar trote to pass a great ordeal pasola scooter pica pollo fried chicken planta compliment for a woman poloch\u00e9 t-shirt popola pussy rebajar to lose weight romper to exceed expectations, to draw attention (?) rulay loose se fue la luz end of discussion tigueraje slang; assertiveness tiguere a streetwise and capable guy ; guy, dude (usage is similar to vaina but with people) tirarse send a text message \"Hay que tirarme por privado\" toto pussy vaca \"ven ac\u00e1\" vaina stuff, thing; whatever; etc y eso? \"why?\" Resources Videos Qu\u00e9 caus\u00f3 la ca\u00edda del Imperio otomano, la superpotencia que se expandi\u00f3 por tres continentes Crisis en Hait\u00ed: un pa\u00eds perdido en su laberinto Mujeres del Bajo Mundo Audio Transcription A: Oye bro, estoy peleando con la mujer m\u00eda estoy buscando una laja de esas. Cu\u00e1l tu me dejas? B: Bueno tu sabes que yo tengo calidad en mi producto, ve a ver cual te gusta ... A: Bueno. B: Y dependiendo del presupuesto que cargas. B: Por dos mil pesos, cu\u00e1l tu me da de esas? A: Dos mil pesos? B: Claro viejo. A: T\u00fa sabes que eso es muy poco viejo dos mil pesos, y tu sabes que es planta de mujeres lo que yo tengo. A: Oye la regla m\u00eda. En media hora que tu me la traigas. Si en media hora tu no me la traes, yo salgo y te levanto por el pescuezo donde quiera que tu est\u00e9s. B: Pero viejo, media hora no da ni para quitarme el poloch\u00e9 . A: En media hora usted lo hace. C: Oye que lo que ... no he hecho nada, quiero estrenarme. Cons\u00edgueme una chamaquita de esas, viejo. B: Aqu\u00ed se habla con dinero, si usted no tiene dinero no disfruta de ese manjar. 42 Audio Transcription C: tu tiene hijo? Tu no tiene hijo verd\u00e1 E: ni un chin C: tiene pareja tu? E: ni un chin C: C\u00f3mo te gustan los hombre moreno, blanco, alto, flaco ... como? E: Como Dios me lo mande. C: Tu no tienes que ver como son? E: No como sea puede ser un gordo Popolo sin diente y en tembladera! C: C\u00f3mo te gustan la mujeres a ti ... blanca morena? P: A m\u00ed me gustan todas que sean bandidas todas que tiren al DM Capri la bandida, haitianita, me gustan m\u00e1s la haitianita me entiende? C: Mira, ven ac\u00e1.. si una gente lo quiere a ustedes en una discoteca cu\u00e1nto hay que pasar? E: Bueno a m\u00ed que tirame por privado yo no s\u00e9 cu\u00e1nto. C: Mira ... hasta que curso tu llegaste, hasta que curso? E: Yo a tercero. C: A tercero de bachillel - por qu\u00e9 te desacataste y cogiste la calle? Qu\u00e9 te hizo desacatarte? E: qu\u00e9 tu crees e adivina. C: Yo no s\u00e9. E: Oye yo me vua poner ahora en la escuela. P: No! Yo no ando en n\u00e1 dike que privado ni na . A mi lo que hay e que invitarme venga camina que te vua pas\u00e1 una vaina porque yo lo que ando e en mi diligencia C: Con una vaina ta bien P: Con una vaina ta bien y ello dike en privado C: Qu\u00e9 te hizo desacatarte, Perrote y cog\u00e9 la calle? P: t\u00fa no sabes ... no hab\u00eda ni pa el pasaje. Pa n\u00e1 ni pa ime t\u00fa sabes ya estaba bobo pila de bola pila de to Capri. C: Tu que erre el perrote dime P: Y t\u00fa no sabe hubo que ladr\u00e1 y embalase para la calle como un bandole! Truchitas Jenny de la Rosa Audio Transcription C: Tu nombre y tu edad. J: Jenny de la Rosa C: Que edad tu tienes? J: Dieciocho C: Hace que tiempo cumpliste los dieciocho? J: En octubre. C: Desde qu\u00e9 tiempo tu est\u00e1s en la calle - desde qu\u00e9 edad? J: Desde que sali\u00f3 la cuarentena. C: Qu\u00e9 hizo que tu entraras al mundo de la truchita? J: Mi mam\u00e1, como que yo la ve\u00eda trabajando, ella tiene cuatro muchacho, la ve\u00eda trabajando como que, me sent\u00eda mal vi\u00e9ndola como ella - tu sabes - manteni\u00e9ndome. Y me fui de la casa. Mi mam\u00e1 vive en Herrera y... C: Con qui\u00e9n tu te fuiste? Un tipo me llev\u00f3, un platanero. Vienen a vender vaina en Pintura, en la Plaza y la Bandera, y me dej\u00f3 all\u00e1 y despu\u00e9s yo fui alquilando pensiones hasta que me qued\u00e9 ah\u00ed. C: C\u00f3mo tu te busca el dinero en la calle? J: Joseando . C: Tu le cobra a los hombres y vaina? J: Claro! Malo es robar. C: Malo es robar? J: Yo toy dando mi ... C: O sea que si un hombre le da lo de \u00e9l y \u00e9l tiene una vaina f\u00e1cil ah\u00ed se se se fue con t\u00f3... J: No, la bacaner\u00eda vale m\u00e1s y para qu\u00e9. C: Cu\u00e1nto es lo m\u00e1s que te ha pagado un hombre? J: Oh...depende. Hay veces que te dan tu 5 mil, tu 10 mil pesos en bacaner\u00eda depende como tu trates al hombre. C: Tu trabaja en un cabaret , es? J: Si C: En ese cabaret cu\u00e1nto te pagan a ti por estar con un hombre? J: Depende ... oh el hombre tiene que dar 400. Un ejemplo si tu la coge de all\u00e1, tienes que dar 400 pa llevartela. C: Y a ti te gusta eso? J: No. C: Qu\u00e9 es lo m\u00e1s fuerte que te ha pasado a ti con un cliente? J: Hay en veces que tu encuentras tu palomo pero uno no es pariguayo tampoco, tu le pide tu cuarto adelante. C: Y hay cliente que tienen un mal olor o algo? J: A veces, y uno no se le quiere ni pegar. C: Y como hacen con eso entonce? Le devuelven lo cuarto? J: No, pero yo tengo mi cuarto ganado ya. Yo no soy una mujer, ser\u00e1 yo un [omitido] C: O sea que despu\u00e9s ya que la vaina est\u00e1 ah\u00ed ya tu le haces cualquier diligencia y te vas? J: No porque si es robar es robar, si es ganado es ganado. Me entiendes? Si ust\u00e9d me llama a mi, ust\u00e9d no me puede perder mi trabajo dizque para venir a llamarme, ust\u00e9d viene a relajar conmigo, es? Hay que cobrarle por el tiempo yo soy una mujer. C: La calle trae enemistad, tu tiene mujeres que son enemigas tuyas? J: No C: Si tu est\u00e1s desde pandemia, cuando tu entraste a la calle tu era menor de edad. En ese tiempo... la fam\u00edlia tuya no andaba detr\u00e1s de ti buscandote ni nada de eso? J: Mi mam\u00e1 me amarraba con la cadena y lo tiguere vini\u00e9n y me solt\u00e1n. Claro. Y yo romp\u00eda hasta la ventana .. hasta la ventana romp\u00ed yo. C: Tu cres que tu situaci\u00f3n actual de que tu est\u00e1s en la calle, es por tu familia realmente o es porque tu eres rulai ? J: No a veces es la fam\u00edlia, que no me gustar\u00eda como yo estoy haciendo esto, como que mi familia me est\u00e9 - tu me entiende? Ah\u00ed mismo viendo que mi fam\u00edlia no fuman ni nada de eso, son gente decente me entiende? Gente de su casa y no me gustar\u00eda ve fumando - me entiende? - y que mi hermano levanten con esa misma costumbre no me gusta y me decid\u00ed y lejos porqu\u00e9 mi mam\u00e1 vive en la 25 donde el maestro - ust\u00e9d sabe donde es? C: No. J: Donde el mec\u00e1 - donde el maestro el del caf\u00e9. C: Ya si si, \u00e9l que arregla la pasola y la vaina. J: Aj\u00e1! Ah\u00ed mimo. En el Caf\u00e9. C: Mira...cuanto tu gastas? En ti en ti en ti en tu cuerpo, cuanto tu gastas? J: Eh... pero loco uno come todos los d\u00edas, uno gasta todos los d\u00edas. C: A n\u00edvel de flow y vaina, qu\u00e9 lo que? J: Oh, su 5 mil as\u00ed. Su un vaina de 1500, 1800 porque los pantalones de hombre estan caros ahora, para tu mangar un poloch\u00e9 dizque vaina tienes que dar 1500 y 1800 en una boutique. C: Qu\u00e9 [drogas] tu has consumido? Cu\u00e1les? J: Yo lo que [fumo] ya, gracias a Dios. C: Tu nama usas eso, tu nama fumas y ya? J: Y ya. C: Y nunca te han querido brindar otro tipo de droga? J: No, gracias a Dios. C: Mira, tu en alg\u00fan momento ha pensado tu ... tener un noviecito asi ... est\u00e1 estable en una casa as\u00ed, tener tus hijos y toda esa vaina? J: No, si es un gringuito s\u00ed, pero hay en veces ya no hay hombre ya, eso era antes ... si es un gringo s\u00ed, que yo s\u00e9 que no va a pasar familia miseria nina que yo s\u00e9 que yo me muera y no va coger lucha nina . C: Pero ya ... \"no hay hombre ya\", por qu\u00e9 tu dices que no hay hombre? Son malo toditos ? J: No que yo he visto pel\u00edcula aqu\u00ed que no quiero que me ... pase tambi\u00e9n. C: C\u00f3mo cuales? J: Yo he visto mujer que ha ten\u00eddo tres muchachos y los hijos no le dan n\u00e1 ! C: Son malos? J: Malos. C: Que tu vas a hacer el 31 de diciembre? J: Oh no puedo dec\u00edr ahora nada porque yo no se si yo hasta vivo el 25, Dios es que sabe ahora si yo estoy ah\u00ed - me entiende - y yo mango lo m\u00edo porque yo no mango dizque flow para guardarlo, si mango un flow es para pon\u00e9rmelo hoy. Gerson Gerson Audio Transcription Y m\u00e1s que todo con la cultura de la m\u00fasica urbana, la m\u00fasica que se escucha en la Rep\u00fablica Dominican se llama \"dembow\". El popi en la Rep\u00fablica Dominicana son las personas que tienen una cultura muy refinada. Van a los restaurantes finos a comer langosta, beben vino, .. muy formal, son personas que tienen que hacer... Pronuncian el espa\u00f1ol muy correctamente, y regularmente son ese tipo de persona. Pero los wawawa son las personas que viven en los barrios y los ghettos de la Rep\u00fablica Dominicana. Entonces esos son los que comen en la calle, comen la comida callejera, toman cerveza, son las personas que bailan en las calles.. y tienen un estilo de hablar no tan refinado. As\u00ed es que esas personas que son la mayoria de las personas de la Rep\u00fablica Dominicana que viven en los barrios pertenecen a la cultura wawawa. sin embargo tambi\u00e9n yo creo que no soy tan wawawa en la vida, soy un poco medio popi Ese barrio tiene una calle muy t\u00edpica, muy conocida, se llama la calle 42, y es donde se unen todas las personas, donde van todas las personas a divertirse en las noches. Hay mucha m\u00fasica, pero tambi\u00e9n hay muchas cosas que pasan all\u00ed que no son tan buenas. Otras son buenas, otras son malas. All\u00ed se juntan los buenos y los malos... Entonces es una locura total. O manito, sabes que estamos aqu\u00ed para eso, pa ayudarte, y pa que rompe cuando llega a la RD. Counterfactuals Me habr\u00eda gustado salir contigo. \"I would have liked to go with you...\" If it weren't for you, I wouldn't have had a vacation If I went out with you I would have wanted to be with your friend too Podr\u00eda haberte pagado por tus salarios perdidos Si te hubiera robado yo ser\u00eda para ti una chica mala o buena?\ud83e\udd14 C\u00f3mo quisiera salir de ac\u00e1 y recostarme en tus brazos","title":"Spanish"},{"location":"Misc/Languages/Spanish/#spanish","text":"","title":"Spanish"},{"location":"Misc/Languages/Spanish/#greetings","text":"graph TD F[Saludos] --> A; F --> C; F --> E; A[Qu\u00e9 lo que?] --> B[Tranquilo]; C[Dame luz?] --> B; A --> D[T\u00f3 frio]; C --> D; E[Como te sien?] --> B; E --> D; E --> G[Bacano]; A --> H[Normal]; C --> H; E --> H; A --> I[Manso]; C --> I; E --> I; Note that Qu\u00e9 lo que often appears in chat as 'klk' or 'qlok' Both men and women can be addressed as manito / manita , loco / loca , i.e. \"Qu\u00e9 lo que manito?\", etc. Patterns Spanish Dominican Example estoy t\u00ed est\u00e1s t\u00e1 est\u00e1 t\u00e1 est\u00e1mos tamo est\u00e1n tan Change Audio Transcription Definite article dropped van a los restaurantes finos Initial r become h cultura muy r efinada Initial r accepts interpolated h el dembow vuelva a h revivir con los tiempos de antes Interpolated h between n and l a divertirse en h las noches","title":"Greetings"},{"location":"Misc/Languages/Spanish/#vocabulary","text":"bajarle dos to lower the intensity or speed of something baquear Qui\u00e9n te t\u00e1 baqueando ese poloch\u00e9? brindar to present, offer chancear a alguien give somebody a chance , court someone chin a little bit chulo very handsome a man who is knowingly in a relationship with a woman who is a prostitute, and who is supported by her comer como una lima nueva describing someone who eats a lot chuqui to be irritable, angry (also chucky ) andar chuqu\u00ed en la calle to party in the streets cocotazo a smack on the head cualto money, payment ponerse los cuernos to cheat on a partner dar galleta slap the face flow swag, style, attractive manner of dress guayar to bump, grind hacer diligencia making money by exchanging sexual favors josear (also josiar , jociar ) to hustle jeva woman , partner laja something highly used and worn out \"Estoy buscando una laja de esas.\" Los Guandules a barrio on the right bank of the Ozama River in Santo Domingo, location of the ruins of the Monastery of San Francisco macorisano someone from SFM mangar to pinch, grab, obtain; have sexual relations with someone molote mob nama only, merely, just (preverbal) nina nothing more, nothing at all (sentence final) parar to end up in a place without intending to do so pariguayo lame guy who can't get girls pasar trote to pass a great ordeal pasola scooter pica pollo fried chicken planta compliment for a woman poloch\u00e9 t-shirt popola pussy rebajar to lose weight romper to exceed expectations, to draw attention (?) rulay loose se fue la luz end of discussion tigueraje slang; assertiveness tiguere a streetwise and capable guy ; guy, dude (usage is similar to vaina but with people) tirarse send a text message \"Hay que tirarme por privado\" toto pussy vaca \"ven ac\u00e1\" vaina stuff, thing; whatever; etc y eso? \"why?\"","title":"Vocabulary"},{"location":"Misc/Languages/Spanish/#resources","text":"Videos Qu\u00e9 caus\u00f3 la ca\u00edda del Imperio otomano, la superpotencia que se expandi\u00f3 por tres continentes Crisis en Hait\u00ed: un pa\u00eds perdido en su laberinto","title":"Resources"},{"location":"Misc/Languages/Spanish/#mujeres-del-bajo-mundo","text":"Audio Transcription A: Oye bro, estoy peleando con la mujer m\u00eda estoy buscando una laja de esas. Cu\u00e1l tu me dejas? B: Bueno tu sabes que yo tengo calidad en mi producto, ve a ver cual te gusta ... A: Bueno. B: Y dependiendo del presupuesto que cargas. B: Por dos mil pesos, cu\u00e1l tu me da de esas? A: Dos mil pesos? B: Claro viejo. A: T\u00fa sabes que eso es muy poco viejo dos mil pesos, y tu sabes que es planta de mujeres lo que yo tengo. A: Oye la regla m\u00eda. En media hora que tu me la traigas. Si en media hora tu no me la traes, yo salgo y te levanto por el pescuezo donde quiera que tu est\u00e9s. B: Pero viejo, media hora no da ni para quitarme el poloch\u00e9 . A: En media hora usted lo hace. C: Oye que lo que ... no he hecho nada, quiero estrenarme. Cons\u00edgueme una chamaquita de esas, viejo. B: Aqu\u00ed se habla con dinero, si usted no tiene dinero no disfruta de ese manjar.","title":"Mujeres del Bajo Mundo"},{"location":"Misc/Languages/Spanish/#42","text":"Audio Transcription C: tu tiene hijo? Tu no tiene hijo verd\u00e1 E: ni un chin C: tiene pareja tu? E: ni un chin C: C\u00f3mo te gustan los hombre moreno, blanco, alto, flaco ... como? E: Como Dios me lo mande. C: Tu no tienes que ver como son? E: No como sea puede ser un gordo Popolo sin diente y en tembladera! C: C\u00f3mo te gustan la mujeres a ti ... blanca morena? P: A m\u00ed me gustan todas que sean bandidas todas que tiren al DM Capri la bandida, haitianita, me gustan m\u00e1s la haitianita me entiende? C: Mira, ven ac\u00e1.. si una gente lo quiere a ustedes en una discoteca cu\u00e1nto hay que pasar? E: Bueno a m\u00ed que tirame por privado yo no s\u00e9 cu\u00e1nto. C: Mira ... hasta que curso tu llegaste, hasta que curso? E: Yo a tercero. C: A tercero de bachillel - por qu\u00e9 te desacataste y cogiste la calle? Qu\u00e9 te hizo desacatarte? E: qu\u00e9 tu crees e adivina. C: Yo no s\u00e9. E: Oye yo me vua poner ahora en la escuela. P: No! Yo no ando en n\u00e1 dike que privado ni na . A mi lo que hay e que invitarme venga camina que te vua pas\u00e1 una vaina porque yo lo que ando e en mi diligencia C: Con una vaina ta bien P: Con una vaina ta bien y ello dike en privado C: Qu\u00e9 te hizo desacatarte, Perrote y cog\u00e9 la calle? P: t\u00fa no sabes ... no hab\u00eda ni pa el pasaje. Pa n\u00e1 ni pa ime t\u00fa sabes ya estaba bobo pila de bola pila de to Capri. C: Tu que erre el perrote dime P: Y t\u00fa no sabe hubo que ladr\u00e1 y embalase para la calle como un bandole!","title":"42"},{"location":"Misc/Languages/Spanish/#truchitas","text":"Jenny de la Rosa Audio Transcription C: Tu nombre y tu edad. J: Jenny de la Rosa C: Que edad tu tienes? J: Dieciocho C: Hace que tiempo cumpliste los dieciocho? J: En octubre. C: Desde qu\u00e9 tiempo tu est\u00e1s en la calle - desde qu\u00e9 edad? J: Desde que sali\u00f3 la cuarentena. C: Qu\u00e9 hizo que tu entraras al mundo de la truchita? J: Mi mam\u00e1, como que yo la ve\u00eda trabajando, ella tiene cuatro muchacho, la ve\u00eda trabajando como que, me sent\u00eda mal vi\u00e9ndola como ella - tu sabes - manteni\u00e9ndome. Y me fui de la casa. Mi mam\u00e1 vive en Herrera y... C: Con qui\u00e9n tu te fuiste? Un tipo me llev\u00f3, un platanero. Vienen a vender vaina en Pintura, en la Plaza y la Bandera, y me dej\u00f3 all\u00e1 y despu\u00e9s yo fui alquilando pensiones hasta que me qued\u00e9 ah\u00ed. C: C\u00f3mo tu te busca el dinero en la calle? J: Joseando . C: Tu le cobra a los hombres y vaina? J: Claro! Malo es robar. C: Malo es robar? J: Yo toy dando mi ... C: O sea que si un hombre le da lo de \u00e9l y \u00e9l tiene una vaina f\u00e1cil ah\u00ed se se se fue con t\u00f3... J: No, la bacaner\u00eda vale m\u00e1s y para qu\u00e9. C: Cu\u00e1nto es lo m\u00e1s que te ha pagado un hombre? J: Oh...depende. Hay veces que te dan tu 5 mil, tu 10 mil pesos en bacaner\u00eda depende como tu trates al hombre. C: Tu trabaja en un cabaret , es? J: Si C: En ese cabaret cu\u00e1nto te pagan a ti por estar con un hombre? J: Depende ... oh el hombre tiene que dar 400. Un ejemplo si tu la coge de all\u00e1, tienes que dar 400 pa llevartela. C: Y a ti te gusta eso? J: No. C: Qu\u00e9 es lo m\u00e1s fuerte que te ha pasado a ti con un cliente? J: Hay en veces que tu encuentras tu palomo pero uno no es pariguayo tampoco, tu le pide tu cuarto adelante. C: Y hay cliente que tienen un mal olor o algo? J: A veces, y uno no se le quiere ni pegar. C: Y como hacen con eso entonce? Le devuelven lo cuarto? J: No, pero yo tengo mi cuarto ganado ya. Yo no soy una mujer, ser\u00e1 yo un [omitido] C: O sea que despu\u00e9s ya que la vaina est\u00e1 ah\u00ed ya tu le haces cualquier diligencia y te vas? J: No porque si es robar es robar, si es ganado es ganado. Me entiendes? Si ust\u00e9d me llama a mi, ust\u00e9d no me puede perder mi trabajo dizque para venir a llamarme, ust\u00e9d viene a relajar conmigo, es? Hay que cobrarle por el tiempo yo soy una mujer. C: La calle trae enemistad, tu tiene mujeres que son enemigas tuyas? J: No C: Si tu est\u00e1s desde pandemia, cuando tu entraste a la calle tu era menor de edad. En ese tiempo... la fam\u00edlia tuya no andaba detr\u00e1s de ti buscandote ni nada de eso? J: Mi mam\u00e1 me amarraba con la cadena y lo tiguere vini\u00e9n y me solt\u00e1n. Claro. Y yo romp\u00eda hasta la ventana .. hasta la ventana romp\u00ed yo. C: Tu cres que tu situaci\u00f3n actual de que tu est\u00e1s en la calle, es por tu familia realmente o es porque tu eres rulai ? J: No a veces es la fam\u00edlia, que no me gustar\u00eda como yo estoy haciendo esto, como que mi familia me est\u00e9 - tu me entiende? Ah\u00ed mismo viendo que mi fam\u00edlia no fuman ni nada de eso, son gente decente me entiende? Gente de su casa y no me gustar\u00eda ve fumando - me entiende? - y que mi hermano levanten con esa misma costumbre no me gusta y me decid\u00ed y lejos porqu\u00e9 mi mam\u00e1 vive en la 25 donde el maestro - ust\u00e9d sabe donde es? C: No. J: Donde el mec\u00e1 - donde el maestro el del caf\u00e9. C: Ya si si, \u00e9l que arregla la pasola y la vaina. J: Aj\u00e1! Ah\u00ed mimo. En el Caf\u00e9. C: Mira...cuanto tu gastas? En ti en ti en ti en tu cuerpo, cuanto tu gastas? J: Eh... pero loco uno come todos los d\u00edas, uno gasta todos los d\u00edas. C: A n\u00edvel de flow y vaina, qu\u00e9 lo que? J: Oh, su 5 mil as\u00ed. Su un vaina de 1500, 1800 porque los pantalones de hombre estan caros ahora, para tu mangar un poloch\u00e9 dizque vaina tienes que dar 1500 y 1800 en una boutique. C: Qu\u00e9 [drogas] tu has consumido? Cu\u00e1les? J: Yo lo que [fumo] ya, gracias a Dios. C: Tu nama usas eso, tu nama fumas y ya? J: Y ya. C: Y nunca te han querido brindar otro tipo de droga? J: No, gracias a Dios. C: Mira, tu en alg\u00fan momento ha pensado tu ... tener un noviecito asi ... est\u00e1 estable en una casa as\u00ed, tener tus hijos y toda esa vaina? J: No, si es un gringuito s\u00ed, pero hay en veces ya no hay hombre ya, eso era antes ... si es un gringo s\u00ed, que yo s\u00e9 que no va a pasar familia miseria nina que yo s\u00e9 que yo me muera y no va coger lucha nina . C: Pero ya ... \"no hay hombre ya\", por qu\u00e9 tu dices que no hay hombre? Son malo toditos ? J: No que yo he visto pel\u00edcula aqu\u00ed que no quiero que me ... pase tambi\u00e9n. C: C\u00f3mo cuales? J: Yo he visto mujer que ha ten\u00eddo tres muchachos y los hijos no le dan n\u00e1 ! C: Son malos? J: Malos. C: Que tu vas a hacer el 31 de diciembre? J: Oh no puedo dec\u00edr ahora nada porque yo no se si yo hasta vivo el 25, Dios es que sabe ahora si yo estoy ah\u00ed - me entiende - y yo mango lo m\u00edo porque yo no mango dizque flow para guardarlo, si mango un flow es para pon\u00e9rmelo hoy.","title":"Truchitas"},{"location":"Misc/Languages/Spanish/#gerson","text":"Gerson Audio Transcription Y m\u00e1s que todo con la cultura de la m\u00fasica urbana, la m\u00fasica que se escucha en la Rep\u00fablica Dominican se llama \"dembow\". El popi en la Rep\u00fablica Dominicana son las personas que tienen una cultura muy refinada. Van a los restaurantes finos a comer langosta, beben vino, .. muy formal, son personas que tienen que hacer... Pronuncian el espa\u00f1ol muy correctamente, y regularmente son ese tipo de persona. Pero los wawawa son las personas que viven en los barrios y los ghettos de la Rep\u00fablica Dominicana. Entonces esos son los que comen en la calle, comen la comida callejera, toman cerveza, son las personas que bailan en las calles.. y tienen un estilo de hablar no tan refinado. As\u00ed es que esas personas que son la mayoria de las personas de la Rep\u00fablica Dominicana que viven en los barrios pertenecen a la cultura wawawa. sin embargo tambi\u00e9n yo creo que no soy tan wawawa en la vida, soy un poco medio popi Ese barrio tiene una calle muy t\u00edpica, muy conocida, se llama la calle 42, y es donde se unen todas las personas, donde van todas las personas a divertirse en las noches. Hay mucha m\u00fasica, pero tambi\u00e9n hay muchas cosas que pasan all\u00ed que no son tan buenas. Otras son buenas, otras son malas. All\u00ed se juntan los buenos y los malos... Entonces es una locura total. O manito, sabes que estamos aqu\u00ed para eso, pa ayudarte, y pa que rompe cuando llega a la RD.","title":"Gerson"},{"location":"Misc/Languages/Spanish/#counterfactuals","text":"Me habr\u00eda gustado salir contigo. \"I would have liked to go with you...\" If it weren't for you, I wouldn't have had a vacation If I went out with you I would have wanted to be with your friend too Podr\u00eda haberte pagado por tus salarios perdidos Si te hubiera robado yo ser\u00eda para ti una chica mala o buena?\ud83e\udd14 C\u00f3mo quisiera salir de ac\u00e1 y recostarme en tus brazos","title":"Counterfactuals"},{"location":"Misc/Text-editors/code/","text":"VS Code Code can be folded by placing markers in comments Markdown C# Python <!-- #region --> ... <!-- #endregion --> #region ... #endregion #region ... #endregion Snippets \"Editor group\" refers to window panes. Keyboard shortcut Setting Description ++Ctrl+\\++ workbench.action.splitEditor Split Editor ++Ctrl+k+Ctrl+UpArrow++ workbench.action.focusAboveGroup View: Focus Above Editor Group ++Ctrl+k+Ctrl+RightArrow++ workbench.action.focusRightGroup View: Focus Right Editor Group ++Ctrl+k+Ctrl+DownArrow++ workbench.action.focusBelowGroup View: Focus Below Editor Group ++Ctrl+k+Ctrl+LeftArrow++ workbench.action.focusLeftGroup View: Focus Left Editor Group ++Ctrl+k+UpArrow++ workbench.action.moveActiveEditorGroupUp View: Move Editor Group Up ++Ctrl+k+RightArrow++ workbench.action.moveActiveEditorGroupRight View: Move Editor Group Right ++Ctrl+k+DownArrow++ workbench.action.moveActiveEditorGroupDown View: Move Editor Group Down ++Ctrl+k+LeftArrow++ workbench.action.moveActiveEditorGroupLeft View: Move Editor Group Left ++Alt+UpArrow++ editor.action.moveLinesUpAction Move line up ++Alt+DownArrow++ editor.action.moveLinesDownAction Move line down ++Option+Command+DownArrow++ Add a cursor down ++Option+Command+UpArrow++ Add a cursor up ++Option+Shift+Left Click++ Click and drag to add cursors ++Ctrl+Shift+5++ Terminal: Split terminal ++Ctrl+h++ Replace ++Ctrl+l++ Expand line selection ++Ctrl+j++ workbench.action.togglePanel View: Toggle Panel ++Ctrl+b++ View: Toggle Side Bar Visibility Terminal The shells available in the integrated terminal for any OS can be adjusted using terminal.integrated.profiles Developer PowerShell for VS 2022 \"terminal.integrated.profiles.windows\" : { \"Developer PowerShell for VS 2022\" : { \"source\" : \"PowerShell\" , \"args\" : [ \"-NoExit\" , \"-Command\" , \"&{Import-Module \\\"C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\Common7\\\\Tools\\\\Microsoft.VisualStudio.DevShell.dll\\\"; Enter-VsDevShell 1916cd63}\" ], \"icon\" : \"terminal-powershell\" , }","title":"VS Code"},{"location":"Misc/Text-editors/code/#vs-code","text":"Code can be folded by placing markers in comments Markdown C# Python <!-- #region --> ... <!-- #endregion --> #region ... #endregion #region ... #endregion Snippets \"Editor group\" refers to window panes. Keyboard shortcut Setting Description ++Ctrl+\\++ workbench.action.splitEditor Split Editor ++Ctrl+k+Ctrl+UpArrow++ workbench.action.focusAboveGroup View: Focus Above Editor Group ++Ctrl+k+Ctrl+RightArrow++ workbench.action.focusRightGroup View: Focus Right Editor Group ++Ctrl+k+Ctrl+DownArrow++ workbench.action.focusBelowGroup View: Focus Below Editor Group ++Ctrl+k+Ctrl+LeftArrow++ workbench.action.focusLeftGroup View: Focus Left Editor Group ++Ctrl+k+UpArrow++ workbench.action.moveActiveEditorGroupUp View: Move Editor Group Up ++Ctrl+k+RightArrow++ workbench.action.moveActiveEditorGroupRight View: Move Editor Group Right ++Ctrl+k+DownArrow++ workbench.action.moveActiveEditorGroupDown View: Move Editor Group Down ++Ctrl+k+LeftArrow++ workbench.action.moveActiveEditorGroupLeft View: Move Editor Group Left ++Alt+UpArrow++ editor.action.moveLinesUpAction Move line up ++Alt+DownArrow++ editor.action.moveLinesDownAction Move line down ++Option+Command+DownArrow++ Add a cursor down ++Option+Command+UpArrow++ Add a cursor up ++Option+Shift+Left Click++ Click and drag to add cursors ++Ctrl+Shift+5++ Terminal: Split terminal ++Ctrl+h++ Replace ++Ctrl+l++ Expand line selection ++Ctrl+j++ workbench.action.togglePanel View: Toggle Panel ++Ctrl+b++ View: Toggle Side Bar Visibility","title":"VS Code"},{"location":"Misc/Text-editors/code/#terminal","text":"The shells available in the integrated terminal for any OS can be adjusted using terminal.integrated.profiles Developer PowerShell for VS 2022 \"terminal.integrated.profiles.windows\" : { \"Developer PowerShell for VS 2022\" : { \"source\" : \"PowerShell\" , \"args\" : [ \"-NoExit\" , \"-Command\" , \"&{Import-Module \\\"C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\Common7\\\\Tools\\\\Microsoft.VisualStudio.DevShell.dll\\\"; Enter-VsDevShell 1916cd63}\" ], \"icon\" : \"terminal-powershell\" , }","title":"Terminal"},{"location":"Misc/Web/","text":"\ud83c\udf0e Web Emoji The codepoint U+FE0F (\"variation select 16\") is placed immediately after an emoji to indicate it should indeed be rendered as an emoji. This is used for some ambiguous codepoints that can be rendered either as emoji or as older-style glyphs, such as the heart, arrows, etc. \u2665&#xFE0F; Firefox about:config font.name-list.emoji Font used for emoji","title":"\ud83c\udf0e Web"},{"location":"Misc/Web/#web","text":"","title":"\ud83c\udf0e Web"},{"location":"Misc/Web/#emoji","text":"The codepoint U+FE0F (\"variation select 16\") is placed immediately after an emoji to indicate it should indeed be rendered as an emoji. This is used for some ambiguous codepoints that can be rendered either as emoji or as older-style glyphs, such as the heart, arrows, etc. \u2665&#xFE0F;","title":"Emoji"},{"location":"Misc/Web/#firefox","text":"","title":"Firefox"},{"location":"Misc/Web/#aboutconfig","text":"font.name-list.emoji Font used for emoji","title":"about:config"},{"location":"Misc/Web/CSS/","text":"CSS Browser compatibility User agent stylesheets are bundled with the browser and contain default CSS rules in the absence of an external stylesheet. Their styles may be removed using a reset stylesheet. Attribute selectors Selectors enclosed in square brackets \u201c[\u201d are called attribute selectors, and define selectors by the presence of an attribute - a[href] selects anchor tags with an href attribute - a[href=\"#\"] selects anchor tags with an href attribute defined as \u201c#\u201d - a[href^=\"http\"] all anchors with href attribute that starts with \u201chttp\u201d - a[href$=\".com\"] all anchors with href attribute that ends with \u201c.com\u201d. - a[href*=\"volusion\"] all anchors with href attribute that contains the word volusion. Miscellaneous properties overflow controls how content overflows its container. Three possible values: - visible by default - hidden from view - scroll producing a scroll bar within the container visibility allows the content of elements to be hidden. The space reserved for the element will still be there. - visible by default - hidden display an element to be completely removed from the rendered webpage, unlike Visibility. - none - unset will reset the value, for example in responsive web design box-sizing defines how width and height of elements are calculated - content-box by default, width and height properties includes only content - border-box includes content, padding, and border BEM naming convention In the BEM (\u201cblock, element, modifier\u201d) naming convention a double hyphen \"--\" separates an element from its modifier while a double underscore \u201c__\u201d separates an element from its subelement. For example: - .person - .person__hand - .person--female - .person--female__hand - .person__hand--left Pseudo classes Dynamic selectors :link unvisited link :visited visited link :hover mouse hover :active when a user activates an element (between click and release of mouse button) Structural selectors :first-child first child of the parent element :last-child last child :nth-child(an+b) where a represents every second, third element if a was 2 or 3 and b represents the first element to start the subset Media Queries @media only screen and (max-width: 480px) \\{ ... \\} - @media begins a media query - only screen instructs the CSS compiler to apply the rules only to display devices (other options are print and handheld ) - and (max-width: 480px) called a media feature , CSS compiler is instructed to apply the rules to screens of less than 480 pixels - CSS rules follow in between curly braces, which will be applied if the media query is satisfied @media only screen and (max-width: 480px) and (min-resolution: 300dpi) \\{ ...\\} - and can be used to chain multiple conditions together @media only screen and (min-width: 480px), (orientation: landscape) \\{ ... \\} - comma (,) separates criteria either of which may be satisfied for the rules to take effect Positioning Adjusting the position of HTML elements with the following 5 properties: 1. position which changes the default position of an element - static by default - relative which places the element relative to its default static position by means of the 4 offset properties: top , bottom , left , and right - absolute all other elements will ignore the element, and the element will be positioned to its closest parent element, and will scroll with the rest of the page - fixed ignores user scrolling and places the element at a fixed location on the page (useful for navbars, for example). max-width:100% must be specified 2. display with 3 values: - inline - block - inline-block Some elements, such as a , strong , em , and a are inline by default, and their height and width are defined by their content. And some elements, such as p and h1 are block by default. But these values can be manipulated in CSS. Inline-block combines features of both inline and block. They appear next to each other but their width and height can be manipulated, such as images. 3. z-index accepts integer values to control the depth of elements: a higher value will bring the element to the front 4. float which will move elements as far left or right as possible. Width must be specified . 5. clear specifies how elements behave when they bump into each other on the left , right , both , or none sides Flexbox A new tool developed for CSS3 that simplifies the layout and positioning of some elements. the display property of the container must be set to : flex or inline-flex There are then 10 properties that specify how its children can behave. 1. justify-content (parent) with 5 values: flex-start , flex-end , center , space-around , space-between 2. align-items (parent) vertical alignment, similar to vertical justification: - stretch by default, child elements will stretch from top to bottom of parent, unless height is defined. min-height is permissible - flex-start , flex-end , center , baseline 3. flex-grow (child) allows items to grow. This property takes an integer value: higher values allow the element to grow larger. max-width takes precedence 0 by default 4. flex-shrink (child) allows items to shrink, similar to flex-grow. min-width takes precedence - 1 by default 5. flex-basis (child) allows the parent to be sized 6. flex (child) allows flex-grow, flex-shrink, and flex-basis to be defined in a single line, in that order 7. flex-wrap (parent) similar to text-wrapping, causing child elements to overflow to another line. 3 values: nowrap by default, wrap , wrap-reverse 8. align-content (parent) affects the behavior of child elements while wrapping similar to align-items: flex-start , flex-end , center , space-between , space-around , stretch 9. flex-direction (parent) allows flex containers to define how children are populated and which directions are assigned to the major and cross axes: row , row-reverse , column , column-reverse 10. flex-flow (parent) allows both flex-wrap and flex-direction properties to be declared on a single line Animation Simple transitions - transition-property specifies the property which is to be animated - transition-duration specifies animation length, in seconds ( s ) or milliseconds ( ms ) - transition-delay Timing function Default value is ease - ease-in starts slow, accelerates quickly, stops abruptly - ease-out begins abruptly, slows down, and ends slowly - ease-in-out starts slow, gets fast, ends slow - linear constant speed Transition declaration combines multiple declarations for brevity transition: transition-property transition-duration transition-timing-function (optional: transition-delay) Accessibility Accessible Rich Internet Applications (ARIA) describe guidelines to make webpages accessible to a broad audience. - Use semantic elements: article , aside , details , figcaption , figure , footer , header , main , mark , nav , section , summary , time . - role=\"complementary\" attribute and value help a screen reader to understand that the element contains information supporting other text - role=\"presentation\" suppresses a screen reader\u2019s reading of the name of the element when unnecessary (for ordered lists, for example) - ARIA properties such as aria-label=\"...\" identify the significance of ambiguous elements - alt attributes of some elements is built in (i.e. img). It can be empty but should always be present. Limit of 150 characters. Bootstrap Most popular front-end framework. CSS library used by developers to quickly build responsive websites - .navbar applied to top-level nav element - themes: .navbar-default , .navbar-inverse (dark), .navbar-brand applied to anchor element Positioning: .navbar-fixed-top , .navbar-fixed-bottom , .navbar-left , .navbar-right Jumbotron: showcase area directly beneath header, drawing attention to important site content - .jumbotron , .jumbotron-fluid Grid system All grid content must be wrapped in a .container or .container-fluid div - .row direct children of the .container have 12 columns - .col-md-\\# direct children of .row , spans # columns; .col-md-4 4 columns, .col-md-6 6 columns, etc - .col-xs-\\# applied to img , spans # columns Responsive design .col-xs-x when viewport is less than 768px .col-sm-x 768-991px .col-md-x 992-1199px .col-lg-x greater than 1199px Buttons .btn Color schemes: .btn-primary , .btn-secondary , .btn-success , .btn-info , .btn-warning , .btn-danger , .btn-link Low-Quality Image Placeholders (LQIP) in React devtips (a-74Zy9EfMQ) Wrap the image in a div <div className=\"imageWrapper\"> Use padding-bottom to define height in terms of width padding-bottom: 150%; Create logic to replace the placeholder image with the high-quality image upon load const image = { original: './path/original.jpg', optimized: './path/optimized.jpg', svg: './path/svg.svg' } componentDidMount() { const img img.src = image.original img.onload = function() { // Query the DOM document .querySelector('imageWrapper') .style['background-image'] = `url('${img.src}')` } } Ensure proper scaling background-size: 100%;","title":"CSS"},{"location":"Misc/Web/CSS/#css","text":"","title":"CSS"},{"location":"Misc/Web/CSS/#browser-compatibility","text":"User agent stylesheets are bundled with the browser and contain default CSS rules in the absence of an external stylesheet. Their styles may be removed using a reset stylesheet.","title":"Browser compatibility"},{"location":"Misc/Web/CSS/#attribute-selectors","text":"Selectors enclosed in square brackets \u201c[\u201d are called attribute selectors, and define selectors by the presence of an attribute - a[href] selects anchor tags with an href attribute - a[href=\"#\"] selects anchor tags with an href attribute defined as \u201c#\u201d - a[href^=\"http\"] all anchors with href attribute that starts with \u201chttp\u201d - a[href$=\".com\"] all anchors with href attribute that ends with \u201c.com\u201d. - a[href*=\"volusion\"] all anchors with href attribute that contains the word volusion.","title":"Attribute selectors"},{"location":"Misc/Web/CSS/#miscellaneous-properties","text":"overflow controls how content overflows its container. Three possible values: - visible by default - hidden from view - scroll producing a scroll bar within the container visibility allows the content of elements to be hidden. The space reserved for the element will still be there. - visible by default - hidden display an element to be completely removed from the rendered webpage, unlike Visibility. - none - unset will reset the value, for example in responsive web design box-sizing defines how width and height of elements are calculated - content-box by default, width and height properties includes only content - border-box includes content, padding, and border","title":"Miscellaneous properties"},{"location":"Misc/Web/CSS/#bem-naming-convention","text":"In the BEM (\u201cblock, element, modifier\u201d) naming convention a double hyphen \"--\" separates an element from its modifier while a double underscore \u201c__\u201d separates an element from its subelement. For example: - .person - .person__hand - .person--female - .person--female__hand - .person__hand--left","title":"BEM naming convention"},{"location":"Misc/Web/CSS/#pseudo-classes","text":"","title":"Pseudo classes"},{"location":"Misc/Web/CSS/#dynamic-selectors","text":":link unvisited link :visited visited link :hover mouse hover :active when a user activates an element (between click and release of mouse button)","title":"Dynamic selectors"},{"location":"Misc/Web/CSS/#structural-selectors","text":":first-child first child of the parent element :last-child last child :nth-child(an+b) where a represents every second, third element if a was 2 or 3 and b represents the first element to start the subset","title":"Structural selectors"},{"location":"Misc/Web/CSS/#media-queries","text":"@media only screen and (max-width: 480px) \\{ ... \\} - @media begins a media query - only screen instructs the CSS compiler to apply the rules only to display devices (other options are print and handheld ) - and (max-width: 480px) called a media feature , CSS compiler is instructed to apply the rules to screens of less than 480 pixels - CSS rules follow in between curly braces, which will be applied if the media query is satisfied @media only screen and (max-width: 480px) and (min-resolution: 300dpi) \\{ ...\\} - and can be used to chain multiple conditions together @media only screen and (min-width: 480px), (orientation: landscape) \\{ ... \\} - comma (,) separates criteria either of which may be satisfied for the rules to take effect","title":"Media Queries"},{"location":"Misc/Web/CSS/#positioning","text":"Adjusting the position of HTML elements with the following 5 properties: 1. position which changes the default position of an element - static by default - relative which places the element relative to its default static position by means of the 4 offset properties: top , bottom , left , and right - absolute all other elements will ignore the element, and the element will be positioned to its closest parent element, and will scroll with the rest of the page - fixed ignores user scrolling and places the element at a fixed location on the page (useful for navbars, for example). max-width:100% must be specified 2. display with 3 values: - inline - block - inline-block Some elements, such as a , strong , em , and a are inline by default, and their height and width are defined by their content. And some elements, such as p and h1 are block by default. But these values can be manipulated in CSS. Inline-block combines features of both inline and block. They appear next to each other but their width and height can be manipulated, such as images. 3. z-index accepts integer values to control the depth of elements: a higher value will bring the element to the front 4. float which will move elements as far left or right as possible. Width must be specified . 5. clear specifies how elements behave when they bump into each other on the left , right , both , or none sides","title":"Positioning"},{"location":"Misc/Web/CSS/#flexbox","text":"A new tool developed for CSS3 that simplifies the layout and positioning of some elements. the display property of the container must be set to : flex or inline-flex There are then 10 properties that specify how its children can behave. 1. justify-content (parent) with 5 values: flex-start , flex-end , center , space-around , space-between 2. align-items (parent) vertical alignment, similar to vertical justification: - stretch by default, child elements will stretch from top to bottom of parent, unless height is defined. min-height is permissible - flex-start , flex-end , center , baseline 3. flex-grow (child) allows items to grow. This property takes an integer value: higher values allow the element to grow larger. max-width takes precedence 0 by default 4. flex-shrink (child) allows items to shrink, similar to flex-grow. min-width takes precedence - 1 by default 5. flex-basis (child) allows the parent to be sized 6. flex (child) allows flex-grow, flex-shrink, and flex-basis to be defined in a single line, in that order 7. flex-wrap (parent) similar to text-wrapping, causing child elements to overflow to another line. 3 values: nowrap by default, wrap , wrap-reverse 8. align-content (parent) affects the behavior of child elements while wrapping similar to align-items: flex-start , flex-end , center , space-between , space-around , stretch 9. flex-direction (parent) allows flex containers to define how children are populated and which directions are assigned to the major and cross axes: row , row-reverse , column , column-reverse 10. flex-flow (parent) allows both flex-wrap and flex-direction properties to be declared on a single line","title":"Flexbox"},{"location":"Misc/Web/CSS/#animation","text":"Simple transitions - transition-property specifies the property which is to be animated - transition-duration specifies animation length, in seconds ( s ) or milliseconds ( ms ) - transition-delay Timing function Default value is ease - ease-in starts slow, accelerates quickly, stops abruptly - ease-out begins abruptly, slows down, and ends slowly - ease-in-out starts slow, gets fast, ends slow - linear constant speed Transition declaration combines multiple declarations for brevity transition: transition-property transition-duration transition-timing-function (optional: transition-delay)","title":"Animation"},{"location":"Misc/Web/CSS/#accessibility","text":"Accessible Rich Internet Applications (ARIA) describe guidelines to make webpages accessible to a broad audience. - Use semantic elements: article , aside , details , figcaption , figure , footer , header , main , mark , nav , section , summary , time . - role=\"complementary\" attribute and value help a screen reader to understand that the element contains information supporting other text - role=\"presentation\" suppresses a screen reader\u2019s reading of the name of the element when unnecessary (for ordered lists, for example) - ARIA properties such as aria-label=\"...\" identify the significance of ambiguous elements - alt attributes of some elements is built in (i.e. img). It can be empty but should always be present. Limit of 150 characters.","title":"Accessibility"},{"location":"Misc/Web/CSS/#bootstrap","text":"Most popular front-end framework. CSS library used by developers to quickly build responsive websites - .navbar applied to top-level nav element - themes: .navbar-default , .navbar-inverse (dark), .navbar-brand applied to anchor element Positioning: .navbar-fixed-top , .navbar-fixed-bottom , .navbar-left , .navbar-right Jumbotron: showcase area directly beneath header, drawing attention to important site content - .jumbotron , .jumbotron-fluid","title":"Bootstrap"},{"location":"Misc/Web/CSS/#grid-system","text":"All grid content must be wrapped in a .container or .container-fluid div - .row direct children of the .container have 12 columns - .col-md-\\# direct children of .row , spans # columns; .col-md-4 4 columns, .col-md-6 6 columns, etc - .col-xs-\\# applied to img , spans # columns","title":"Grid system"},{"location":"Misc/Web/CSS/#responsive-design","text":".col-xs-x when viewport is less than 768px .col-sm-x 768-991px .col-md-x 992-1199px .col-lg-x greater than 1199px","title":"Responsive design"},{"location":"Misc/Web/CSS/#buttons","text":".btn Color schemes: .btn-primary , .btn-secondary , .btn-success , .btn-info , .btn-warning , .btn-danger , .btn-link","title":"Buttons"},{"location":"Misc/Web/CSS/#low-quality-image-placeholders-lqip-in-react","text":"devtips (a-74Zy9EfMQ) Wrap the image in a div <div className=\"imageWrapper\"> Use padding-bottom to define height in terms of width padding-bottom: 150%; Create logic to replace the placeholder image with the high-quality image upon load const image = { original: './path/original.jpg', optimized: './path/optimized.jpg', svg: './path/svg.svg' } componentDidMount() { const img img.src = image.original img.onload = function() { // Query the DOM document .querySelector('imageWrapper') .style['background-image'] = `url('${img.src}')` } } Ensure proper scaling background-size: 100%;","title":"Low-Quality Image Placeholders (LQIP) in React"},{"location":"Misc/Web/mkdocs/","text":"Supporting emoji ( src ) mkdocs.yml markdown_extensions: - pymdownx.emoji: # from https://squidfunk.github.io/mkdocs-material/reference/icons-emojis/#emoji emoji_index: !!python/name:materialx.emoji.twemoji emoji_generator: !!python/name:materialx.emoji.to_svg","title":"Mkdocs"},{"location":"Misc/Web/JavaScript/","text":"Axios Accessing RESTful web services and HTTP APIs in JavaScript. Inspired by the $http service in Angular, and development of Axios was an effort to provide a similar standalone service for use outside of Angular. axios .get(url) .then(response => console.log(response)) .catch(error => console.log(error)) Installation $ npm install axios cdn link: https://unpkg.com/axios/dist/axios.min.js Tutorial GET request for a given user ID: const axios = require ( 'axios' ) axios . get ( '/user?ID=12345' ) // Request user with given ID . then ( function ( res ) { console . log ( res ) // Handle success }) . catch ( function ( err ) { console . log ( err ) // Handle error } ) . then ( function () {}) // Always executed Alternatively axios . get ( 'user' , { params : { ID : 12345 } } ) . then ( function ( response ) { console . log ( response ) }) . catch ( function ( error ) { console . log ( error ) }) . then ( function () {}) Blade Templating engine used in Laravel Directives @section('component') Used to define a section of content @yield('variable') Used to display the contents of a given section @endsection @parent append content, rather than overwrite Gulp Toolkit for automating time-consuming tasks in development workflow - const gulp = require('gulp'); - gulp-uglify const uglify = require('gulp-uglify'); gulp . task ( 'scripts' , function () { gulp . src ( 'src/*.js' ) . pipe ( uglify ()) . pipe ( gulp . dest ( 'dist' )); } ); Laravel PHP framework for the development of web applications following the model-view-controller architectural pattern. Developed as a more advanced alternative to the CodeIgniter framework. Installation XAMPP, MAMP (Mac), or WAMP (Windows) to ensure the Apache server environment Composer dependency manager for PHP, similar to npm . It will want to know where the PHP.exe file is, which will be in the directory installed of the server environment installed above. VS Code, Git Bash, Git Initialization composer create-project laravel/laravel lsapp cd lsapp This project will be accessible at localhost/lsapp/ . This is a security issue and must be fixed: Edit C:/xampp/apache/conf/extra/httpd-vhosts.conf (if using XAMPP) Uncomment the opening and closing tags for the VirtualHost tag Uncomment the DocumentRoot line, changing the path to the public directory Uncomment the ServerName line, which can be named arbitrarily, e.g. lsapp.dev Add another VirtualHost pointing to the htdocs directory (within which lsapp was created) and set ServerName to localhost Open Notepad as an administrator Navigate to C:/Windows/System32/drivers/etc , view All files , and open hosts Add 127.0.0.1 localhost Add 127.0.0.1 lsapp.dev Stop and then start the Apache server Folder structure public/ frontend of the application app/ all models go in this folder, including User.php Http/Controllers/ contains all controllers resources/views/ all Laravel views use the Blade template engine routes routing middleware can be installed here config/database.php contains settings Syntax namespace keyword assigns an identifier Illuminate refers to Laravel core TypeScript Programming language developed and maintained by Microsoft, a strict syntactical superset of JavaScript which supports generic programming static typing using annotations, including number , boolean , string , and any . These declarations can be exported to a declarations file. function add ( left : number , right : number ) : number { return left + right ; } Promises A promise is the eventual result of an asynchronous operation. Stages: Wrapping then -ing Catching Chaining Promises can be in one of 4 states: fulfilled : action relating to the promise succeeded reject : action relating to the promise failed pending : has not fulfilled or rejected yet settled : has fulfilled or rejected Promises vs. events an event listener registered after an event has occurred will never fire an action set for resolution can fire after a promise has already resolved an event can fire many times, but promises can only settle once Promise is a try/catch wrapper around code that will finish at an unpredictable time A Promise takes a function as argument. That function takes two arguments: resolve and reject . They are both callbacks that will execute when the promise has succeeded or failed any argument passed to resolve or reject , will then be received by then or catch methods, respectively resolve leads to the next then in the chain, and reject leads to the next catch Glossary Babel JavaScript preprocessor","title":"Axios"},{"location":"Misc/Web/JavaScript/#axios","text":"Accessing RESTful web services and HTTP APIs in JavaScript. Inspired by the $http service in Angular, and development of Axios was an effort to provide a similar standalone service for use outside of Angular. axios .get(url) .then(response => console.log(response)) .catch(error => console.log(error))","title":"Axios"},{"location":"Misc/Web/JavaScript/#installation","text":"$ npm install axios cdn link: https://unpkg.com/axios/dist/axios.min.js","title":"Installation"},{"location":"Misc/Web/JavaScript/#tutorial","text":"GET request for a given user ID: const axios = require ( 'axios' ) axios . get ( '/user?ID=12345' ) // Request user with given ID . then ( function ( res ) { console . log ( res ) // Handle success }) . catch ( function ( err ) { console . log ( err ) // Handle error } ) . then ( function () {}) // Always executed Alternatively axios . get ( 'user' , { params : { ID : 12345 } } ) . then ( function ( response ) { console . log ( response ) }) . catch ( function ( error ) { console . log ( error ) }) . then ( function () {})","title":"Tutorial"},{"location":"Misc/Web/JavaScript/#blade","text":"Templating engine used in Laravel","title":"Blade"},{"location":"Misc/Web/JavaScript/#directives","text":"@section('component') Used to define a section of content @yield('variable') Used to display the contents of a given section @endsection @parent append content, rather than overwrite","title":"Directives"},{"location":"Misc/Web/JavaScript/#gulp","text":"Toolkit for automating time-consuming tasks in development workflow - const gulp = require('gulp'); - gulp-uglify const uglify = require('gulp-uglify'); gulp . task ( 'scripts' , function () { gulp . src ( 'src/*.js' ) . pipe ( uglify ()) . pipe ( gulp . dest ( 'dist' )); } );","title":"Gulp"},{"location":"Misc/Web/JavaScript/#laravel","text":"PHP framework for the development of web applications following the model-view-controller architectural pattern. Developed as a more advanced alternative to the CodeIgniter framework. Installation XAMPP, MAMP (Mac), or WAMP (Windows) to ensure the Apache server environment Composer dependency manager for PHP, similar to npm . It will want to know where the PHP.exe file is, which will be in the directory installed of the server environment installed above. VS Code, Git Bash, Git Initialization composer create-project laravel/laravel lsapp cd lsapp This project will be accessible at localhost/lsapp/ . This is a security issue and must be fixed: Edit C:/xampp/apache/conf/extra/httpd-vhosts.conf (if using XAMPP) Uncomment the opening and closing tags for the VirtualHost tag Uncomment the DocumentRoot line, changing the path to the public directory Uncomment the ServerName line, which can be named arbitrarily, e.g. lsapp.dev Add another VirtualHost pointing to the htdocs directory (within which lsapp was created) and set ServerName to localhost Open Notepad as an administrator Navigate to C:/Windows/System32/drivers/etc , view All files , and open hosts Add 127.0.0.1 localhost Add 127.0.0.1 lsapp.dev Stop and then start the Apache server","title":"Laravel"},{"location":"Misc/Web/JavaScript/#folder-structure","text":"public/ frontend of the application app/ all models go in this folder, including User.php Http/Controllers/ contains all controllers resources/views/ all Laravel views use the Blade template engine routes routing middleware can be installed here config/database.php contains settings","title":"Folder structure"},{"location":"Misc/Web/JavaScript/#syntax","text":"namespace keyword assigns an identifier Illuminate refers to Laravel core","title":"Syntax"},{"location":"Misc/Web/JavaScript/#typescript","text":"Programming language developed and maintained by Microsoft, a strict syntactical superset of JavaScript which supports generic programming static typing using annotations, including number , boolean , string , and any . These declarations can be exported to a declarations file. function add ( left : number , right : number ) : number { return left + right ; }","title":"TypeScript"},{"location":"Misc/Web/JavaScript/#promises","text":"A promise is the eventual result of an asynchronous operation. Stages: Wrapping then -ing Catching Chaining Promises can be in one of 4 states: fulfilled : action relating to the promise succeeded reject : action relating to the promise failed pending : has not fulfilled or rejected yet settled : has fulfilled or rejected Promises vs. events an event listener registered after an event has occurred will never fire an action set for resolution can fire after a promise has already resolved an event can fire many times, but promises can only settle once Promise is a try/catch wrapper around code that will finish at an unpredictable time A Promise takes a function as argument. That function takes two arguments: resolve and reject . They are both callbacks that will execute when the promise has succeeded or failed any argument passed to resolve or reject , will then be received by then or catch methods, respectively resolve leads to the next then in the chain, and reject leads to the next catch","title":"Promises"},{"location":"Misc/Web/JavaScript/#glossary","text":"","title":"Glossary"},{"location":"Misc/Web/JavaScript/#babel","text":"JavaScript preprocessor","title":"Babel"},{"location":"Misc/Web/JavaScript/Express/","text":"const express = require ( 'express' ) const app = express () app . get ( '/' , ( req , res ) => res . send ( 'Hello world!' )) app . listen ( 3000 , () => console . log ( 'Server ready' ))","title":"Express"},{"location":"Misc/Web/JavaScript/React/","text":"React Most popular JavaScript library for building user interfaces, built by Facebook in 2011, competing with Angular and Vue . Every React application is a tree of components : independent, isolated, and reusable pieces of UI. React Elements map to DOM elements maintained in memory ( Virtual DOM ). React state changes are then updated to the real DOM. React is a library Angular is a framework sudo npm i -g create-react-app@1.5.2 # Create ./src/App.js create-react-app $APPNAME # Launch development server at http://localhost:3000/ npm start // index.js import React from 'react' ; import ReactDOM from 'react-dom' ; const element = < h1 > Hello World < /h1>; console . log ( element ); Counter app create-react-app counter-app Create counter-app Ctrl+` to open terminal within Code npm i bootstrap@4.1.1 Install Bootstrap into the app Cmd+P to open file explorer Find index.js Append import 'bootstrap/dist/css/bootstrap.css' to import statements at top Creating a component By convention, components are added to the components directory Using the Simple React Snippets extension for Code allows convenient expansion: - imrc : Import React/Component - cc : Class Component React is being imported because the JSX expression will be compiled into a React object. counter.jsx import React, { Component } from 'react'; class Counter extends Component { render() { return ( <h1>Hello World</h1> ); }} export default Counter; index.js ... import Counter from './components/counter'; ReactDOM.render(<Counter />, document.getElementById('root')); ... Embedding expressions A button is added into the HTML within counter.jsx counter.jsx import React, { Component } from 'react'; class Counter extends Component { render() { return ( <div><h1>Hello World</h1><button>Increment</button></div> ); } } export default Counter; Upon compile, the JSX expression within render() is converted to a statement using the React.createElement() method, which takes two arguments. Replacing <div> with <React.Fragment> - Cmd+D to select all instances - Shift+Option+F to format document Instead of hard-coding \u201cHello World\u201d, we can display a dynamic value. We do this at the component level using the state property, which is an object. counter.jsx import React, { Component } from \"react\"; class Counter extends Component { state = { count: 0 }; render() { return ( <React.Fragment> <span>{this.state.count}</span> <button>Increment</button> </React.Fragment> ); }} export default Counter; In between those curly braces, we can write any valid JavaScript expression, even arithmetic. Adding a formatCount() method which will return the string \u2018Zero\u2019 if count is zero. counter.jsx import React, { Component } from \"react\"; class Counter extends Component { state = { count: 0 }; render() { return ( <React.Fragment> <span>{this.formatCount()}</span> <button>Increment</button> </React.Fragment> ); } formatCount() { const { count } = this.state; return count === 0 ? 'Zero' : count; }} export default Counter; Alternatively, instead of returning plaintext, we can return a JSX expression with h1: return count === 0 ? <h1>Zero</h1> : count; Setting attributes HTML attributes can also render dynamically imported data: counter.jsx import React, { Component } from \"react\"; class Counter extends Component { state = { count: 0, imageUrl: 'https://picsum.photos/200' }; render() { return ( <React.Fragment> <img src={ this.state.imageUrl } alt=\"\"/> <span>{this.formatCount()}</span> <button>Increment</button> </React.Fragment> ); } formatCount() { const { count } = this.state; return count === 0 ? 'Zero' : count; } } export default Counter; Specifying HTML class requires className because Class is a reserved word in JavaScript. ... render() { return ( <React.Fragment> <span className=\"badge badge-primary m-2\">{this.formatCount()}</span> <button className=\"btn btn-secondary btn-sm\">Increment</button> </React.Fragment> ); } CSS styling can be specified in the same way, passing an object in as an HTML attribute ... styles = { fontSize: 50, fontWeight: 'bold' } render() { return ( <React.Fragment> <span style = {this.styles} className=\"badge badge-primary m-2\">{this.formatCount()}</span> <button className=\"btn btn-secondary btn-sm\">Increment</button> </React.Fragment> ); } ... Alternatively, CSS can be defined inline by using double curly braces: ...<span style={{fontSize: 30}}>... Rendering classes dynamically CSS classes can be edited using simple string concatenation techniques that key off of conditional statements ... render() { let classes = \"badge m-2 badge-\"; classes += this.state.count === 0 ? \"warning\" : \"primary\"; return ( <React.Fragment> <span className={classes}>{this.formatCount()}</span> ... The lines associated with classes cause the render() method to become bloated. We can refactor within Code using Ctrl+Shift+R, which extracts the two lines to a new method. counter.jsx ... getBadgeClasses() { let classes = \"badge m-2 badge-\"; classes += this.state.count === 0 ? \"warning\" : \"primary\"; return classes; } ... Rendering lists Let\u2019s see how to render a list of tags. Loops do not exist as a concept within JSX, but we can use the .map() method with arrays. counter.jsx ... state = { count: 0, tags: ['tag1','tag2','tag3'] }; ... <ul> { this.state.tags.map(tag => <li>{tag}</li>)} </ul> ... This produces an error in the console, because React wants each DOM element to have a distinct key value. For now, we can use the same tag as a key. <ul> { this.state.tags.map(tag => <li key ={tag}>{ tag}</li>)} </ul> Conditional rendering Unlike Angular, JSX is not a templating engine, so there are no conditionals. We can use JavaScript outside of the JSX expression and pass that into the expression. counter.jsx Another technique is to take advantage of truthy expressions and how JavaScript evaluates the logical AND operator. Handling events Properties based on standard DOM events placed within JSX expressions (note camelcase): - onClick() - onDoubleClick - onKeyDown - onKeyUp - onKeyPress counter.jsx ... handleIncrement() { console.log('Increment clicked') } ... <button onClick={this.handleIncrement} ... Attempting to log this.state.count produces an error, revealing that count is not available to the method. Binding event handlers JavaScript will return undefined if this is used in strict mode in a standalone function without an object reference. To solve this, we use the bind method. We have to add a constructor for every event handler constructor() { super(); this.handleIncrement = this.handleIncrement.bind(this); } Alternatively, an arrow function can be used because they inherit the this object. counter.jsx handleIncrement = () => { console.log(\"Increment Clicked\", this\") }; Updating state State cannot be updated directly, e.g. this.state.count++; is ineffective. React must be told explicitly what part of the DOM has been changed, unlike Angular. counter.jsx handleIncrement = () => { this.state.count++; this.setState({ count: this.state.count + 1 }); } To solve this problem we use of the setstate() method available in the Component class (from which Counter inherits). handleIncrement = () => { this.setState( { count: this.state.count + 1 }); }; What happens when state changes When state changes, an asynchronous call is made to the render method. React compares the updated virtual DOM with the old, real DOM and updates only the DOM nodes which need updating. Pass arguments using arrow functions Earlier, we saw that the onClick() method only takes a function reference, not a full function call with parameters. We can declare a new method named .doHandleIncrement() which acts as a wrapper for .handleIncrement() , passing an argument. counter.jsx doHandleIncrement = () => { this.handleIncrement({ id: 1}) } But this is wasteful. Better is defining an inline function. ... <button onClick={ () => this.handleIncrement(product)} className=\"btn btn-secondary btn-sm\" > ... Summary JSX (JavaScript XML) Rendering lists Conditional rendering Handling events Updating state Composing components Changing Counter component to Counters Every React component has a property called props Index of React commands and methods .setstate() JavaScript for React Developers let vs var vs const Variables declared with let are block-scoped. example function sayHello() { for (let i = 0; i < 5; i++) { console.log(i); }>) console.log(i) // undefined } sayHello(); Objects example const person = { name: 'Mosh', walk: function() {}, talk() {} // Alternative way of declaring a method } person.talk(); const targetMember = 'name'; person[targetMember.value] = 'John' this keyword this can be made explicit by using the bind method, which will avoid complications of using this in top-level function calls. Functions are objects in JavaScript. example const person = { name: 'Mosh', walk() { console.log(this); }, } person.walk(); // person {} const walk = person.walk.bind(person) In React, strict mode is enabled by default, so the this keyword returns undefined . example const person = { name: 'Mosh', walk() { console.log(this); }, } person.walk(); // person {} const walk = person.walk(); walk(); // undefined Arrow functions Arrow functions allow a good way to clean up code for the simplest functions. const square = function(number) { return number * number } const square = number => number * number; But they do not rebind the this object for callback functions. Array.map() One of the new array methods defined in ECMAScript 6, .map() is very useful in rendering lists. const colors = ['red','green','blue'] const items = colors.map(color => <li>${color}</li> ) }) Object destructuring const address = { street: '', city: '', country: '' } const street = address.street; const city = address.city; const country = address.country; The repetitive use of the object name address is a good object to use destructuring syntax . const {street, city, country } = address; const {street: st } = address; Classes When we have an object with at least one method, we use classes to ensure that child objects do not reduplicate code unnecessarily. const person = { name: 'Mosh', walk() { constole.log('walk') // console is mispelled }} const person2 = { name: 'Mosh', walk() { constole.log('walk') // copying reduplicates the error }} class Person { constructor(name) { this.name = name; } walk() { console.log('walk'); }} const person = new Person('Mosh'); Inheritance class Person { constructor(name) { this.name = name; } walk() { console.log('walk'); } } class Teacher { // Teacher should also be able to walk, teach() { // but we don't want to reduplicate code... console.log('teach'); } } class Teacher extends Person { constructor(name, degree) { super(name) // references parent class this.degree = degree } teach() { console.log('teach') } } const teacher = new Teacher ('Mosh', 'MSc') Modules Splitting code across multiple files, each of which is called a module . Class keyword must be prefixed by export person.js export class Person { constructor(name) { this.name = name; } walk() { console.log('walk'); } } teacher.js import { Person } from './person' // extension is not added export class Teacher extends Person { constructor(name, degree) { super(name) this.degree = degree } teach() { console.log('teach') } } index.js import Teacher from './teacher' const teacher = new Teacher ('Mosh', 'MSc') Named and default exports default keyword to export an entire object, after export and before class named exports to export functions using the export keyword named exports must be individually named and placed within braces React Native React Native is like react, but it uses native components instead of web components as building blocks, allowing true mobile apps to be programmed. import React , { Component } from 'react' ; import { Text , View } from 'react-native' ; export default class HelloWorldApp extends Component { render () { return ( < View > < Text > Hello world !< /Text> < /View> ); } } Parameters associated with customizing components are called props that appear similar to HTML attributes: < Greeting name = 'Jaina' /> < Greeting name = 'Sanjay' /> ` Two types of data control a component: 1. props set by the parent are fixed throughout the lifetime of a component 2. state data that changes","title":"React"},{"location":"Misc/Web/JavaScript/React/#react","text":"Most popular JavaScript library for building user interfaces, built by Facebook in 2011, competing with Angular and Vue . Every React application is a tree of components : independent, isolated, and reusable pieces of UI. React Elements map to DOM elements maintained in memory ( Virtual DOM ). React state changes are then updated to the real DOM. React is a library Angular is a framework sudo npm i -g create-react-app@1.5.2 # Create ./src/App.js create-react-app $APPNAME # Launch development server at http://localhost:3000/ npm start // index.js import React from 'react' ; import ReactDOM from 'react-dom' ; const element = < h1 > Hello World < /h1>; console . log ( element );","title":"React"},{"location":"Misc/Web/JavaScript/React/#counter-app","text":"create-react-app counter-app Create counter-app Ctrl+` to open terminal within Code npm i bootstrap@4.1.1 Install Bootstrap into the app Cmd+P to open file explorer Find index.js Append import 'bootstrap/dist/css/bootstrap.css' to import statements at top","title":"Counter app"},{"location":"Misc/Web/JavaScript/React/#creating-a-component","text":"By convention, components are added to the components directory Using the Simple React Snippets extension for Code allows convenient expansion: - imrc : Import React/Component - cc : Class Component React is being imported because the JSX expression will be compiled into a React object.","title":"Creating a component"},{"location":"Misc/Web/JavaScript/React/#counterjsx","text":"import React, { Component } from 'react'; class Counter extends Component { render() { return ( <h1>Hello World</h1> ); }} export default Counter;","title":"counter.jsx"},{"location":"Misc/Web/JavaScript/React/#indexjs","text":"... import Counter from './components/counter'; ReactDOM.render(<Counter />, document.getElementById('root')); ...","title":"index.js"},{"location":"Misc/Web/JavaScript/React/#embedding-expressions","text":"A button is added into the HTML within counter.jsx","title":"Embedding expressions"},{"location":"Misc/Web/JavaScript/React/#counterjsx_1","text":"import React, { Component } from 'react'; class Counter extends Component { render() { return ( <div><h1>Hello World</h1><button>Increment</button></div> ); } } export default Counter; Upon compile, the JSX expression within render() is converted to a statement using the React.createElement() method, which takes two arguments. Replacing <div> with <React.Fragment> - Cmd+D to select all instances - Shift+Option+F to format document Instead of hard-coding \u201cHello World\u201d, we can display a dynamic value. We do this at the component level using the state property, which is an object.","title":"counter.jsx"},{"location":"Misc/Web/JavaScript/React/#counterjsx_2","text":"import React, { Component } from \"react\"; class Counter extends Component { state = { count: 0 }; render() { return ( <React.Fragment> <span>{this.state.count}</span> <button>Increment</button> </React.Fragment> ); }} export default Counter; In between those curly braces, we can write any valid JavaScript expression, even arithmetic. Adding a formatCount() method which will return the string \u2018Zero\u2019 if count is zero.","title":"counter.jsx"},{"location":"Misc/Web/JavaScript/React/#counterjsx_3","text":"import React, { Component } from \"react\"; class Counter extends Component { state = { count: 0 }; render() { return ( <React.Fragment> <span>{this.formatCount()}</span> <button>Increment</button> </React.Fragment> ); } formatCount() { const { count } = this.state; return count === 0 ? 'Zero' : count; }} export default Counter; Alternatively, instead of returning plaintext, we can return a JSX expression with h1: return count === 0 ? <h1>Zero</h1> : count;","title":"counter.jsx"},{"location":"Misc/Web/JavaScript/React/#setting-attributes","text":"HTML attributes can also render dynamically imported data:","title":"Setting attributes"},{"location":"Misc/Web/JavaScript/React/#counterjsx_4","text":"import React, { Component } from \"react\"; class Counter extends Component { state = { count: 0, imageUrl: 'https://picsum.photos/200' }; render() { return ( <React.Fragment> <img src={ this.state.imageUrl } alt=\"\"/> <span>{this.formatCount()}</span> <button>Increment</button> </React.Fragment> ); } formatCount() { const { count } = this.state; return count === 0 ? 'Zero' : count; } } export default Counter; Specifying HTML class requires className because Class is a reserved word in JavaScript. ... render() { return ( <React.Fragment> <span className=\"badge badge-primary m-2\">{this.formatCount()}</span> <button className=\"btn btn-secondary btn-sm\">Increment</button> </React.Fragment> ); } CSS styling can be specified in the same way, passing an object in as an HTML attribute ... styles = { fontSize: 50, fontWeight: 'bold' } render() { return ( <React.Fragment> <span style = {this.styles} className=\"badge badge-primary m-2\">{this.formatCount()}</span> <button className=\"btn btn-secondary btn-sm\">Increment</button> </React.Fragment> ); } ... Alternatively, CSS can be defined inline by using double curly braces: ...<span style={{fontSize: 30}}>...","title":"counter.jsx"},{"location":"Misc/Web/JavaScript/React/#rendering-classes-dynamically","text":"CSS classes can be edited using simple string concatenation techniques that key off of conditional statements ... render() { let classes = \"badge m-2 badge-\"; classes += this.state.count === 0 ? \"warning\" : \"primary\"; return ( <React.Fragment> <span className={classes}>{this.formatCount()}</span> ... The lines associated with classes cause the render() method to become bloated. We can refactor within Code using Ctrl+Shift+R, which extracts the two lines to a new method.","title":"Rendering classes dynamically"},{"location":"Misc/Web/JavaScript/React/#counterjsx_5","text":"... getBadgeClasses() { let classes = \"badge m-2 badge-\"; classes += this.state.count === 0 ? \"warning\" : \"primary\"; return classes; } ...","title":"counter.jsx"},{"location":"Misc/Web/JavaScript/React/#rendering-lists","text":"Let\u2019s see how to render a list of tags. Loops do not exist as a concept within JSX, but we can use the .map() method with arrays.","title":"Rendering lists"},{"location":"Misc/Web/JavaScript/React/#counterjsx_6","text":"... state = { count: 0, tags: ['tag1','tag2','tag3'] }; ... <ul> { this.state.tags.map(tag => <li>{tag}</li>)} </ul> ... This produces an error in the console, because React wants each DOM element to have a distinct key value. For now, we can use the same tag as a key. <ul> { this.state.tags.map(tag => <li key ={tag}>{ tag}</li>)} </ul>","title":"counter.jsx"},{"location":"Misc/Web/JavaScript/React/#conditional-rendering","text":"Unlike Angular, JSX is not a templating engine, so there are no conditionals. We can use JavaScript outside of the JSX expression and pass that into the expression.","title":"Conditional rendering"},{"location":"Misc/Web/JavaScript/React/#counterjsx_7","text":"Another technique is to take advantage of truthy expressions and how JavaScript evaluates the logical AND operator.","title":"counter.jsx"},{"location":"Misc/Web/JavaScript/React/#handling-events","text":"Properties based on standard DOM events placed within JSX expressions (note camelcase): - onClick() - onDoubleClick - onKeyDown - onKeyUp - onKeyPress","title":"Handling events"},{"location":"Misc/Web/JavaScript/React/#counterjsx_8","text":"... handleIncrement() { console.log('Increment clicked') } ... <button onClick={this.handleIncrement} ... Attempting to log this.state.count produces an error, revealing that count is not available to the method.","title":"counter.jsx"},{"location":"Misc/Web/JavaScript/React/#binding-event-handlers","text":"JavaScript will return undefined if this is used in strict mode in a standalone function without an object reference. To solve this, we use the bind method. We have to add a constructor for every event handler constructor() { super(); this.handleIncrement = this.handleIncrement.bind(this); } Alternatively, an arrow function can be used because they inherit the this object.","title":"Binding event handlers"},{"location":"Misc/Web/JavaScript/React/#counterjsx_9","text":"handleIncrement = () => { console.log(\"Increment Clicked\", this\") };","title":"counter.jsx"},{"location":"Misc/Web/JavaScript/React/#updating-state","text":"State cannot be updated directly, e.g. this.state.count++; is ineffective. React must be told explicitly what part of the DOM has been changed, unlike Angular.","title":"Updating state"},{"location":"Misc/Web/JavaScript/React/#counterjsx_10","text":"handleIncrement = () => { this.state.count++; this.setState({ count: this.state.count + 1 }); } To solve this problem we use of the setstate() method available in the Component class (from which Counter inherits). handleIncrement = () => { this.setState( { count: this.state.count + 1 }); };","title":"counter.jsx"},{"location":"Misc/Web/JavaScript/React/#what-happens-when-state-changes","text":"When state changes, an asynchronous call is made to the render method. React compares the updated virtual DOM with the old, real DOM and updates only the DOM nodes which need updating.","title":"What happens when state changes"},{"location":"Misc/Web/JavaScript/React/#pass-arguments-using-arrow-functions","text":"Earlier, we saw that the onClick() method only takes a function reference, not a full function call with parameters. We can declare a new method named .doHandleIncrement() which acts as a wrapper for .handleIncrement() , passing an argument.","title":"Pass arguments using arrow functions"},{"location":"Misc/Web/JavaScript/React/#counterjsx_11","text":"doHandleIncrement = () => { this.handleIncrement({ id: 1}) } But this is wasteful. Better is defining an inline function. ... <button onClick={ () => this.handleIncrement(product)} className=\"btn btn-secondary btn-sm\" > ...","title":"counter.jsx"},{"location":"Misc/Web/JavaScript/React/#summary","text":"JSX (JavaScript XML) Rendering lists Conditional rendering Handling events Updating state","title":"Summary"},{"location":"Misc/Web/JavaScript/React/#composing-components","text":"Changing Counter component to Counters Every React component has a property called props","title":"Composing components"},{"location":"Misc/Web/JavaScript/React/#index-of-react-commands-and-methods","text":".setstate()","title":"Index of React commands and methods"},{"location":"Misc/Web/JavaScript/React/#javascript-for-react-developers","text":"","title":"JavaScript for React Developers"},{"location":"Misc/Web/JavaScript/React/#let-vs-var-vs-const","text":"Variables declared with let are block-scoped.","title":"let vs var vs const"},{"location":"Misc/Web/JavaScript/React/#example","text":"function sayHello() { for (let i = 0; i < 5; i++) { console.log(i); }>) console.log(i) // undefined } sayHello();","title":"example"},{"location":"Misc/Web/JavaScript/React/#objects","text":"","title":"Objects"},{"location":"Misc/Web/JavaScript/React/#example_1","text":"const person = { name: 'Mosh', walk: function() {}, talk() {} // Alternative way of declaring a method } person.talk(); const targetMember = 'name'; person[targetMember.value] = 'John'","title":"example"},{"location":"Misc/Web/JavaScript/React/#this-keyword","text":"this can be made explicit by using the bind method, which will avoid complications of using this in top-level function calls. Functions are objects in JavaScript.","title":"this keyword"},{"location":"Misc/Web/JavaScript/React/#example_2","text":"const person = { name: 'Mosh', walk() { console.log(this); }, } person.walk(); // person {} const walk = person.walk.bind(person) In React, strict mode is enabled by default, so the this keyword returns undefined .","title":"example"},{"location":"Misc/Web/JavaScript/React/#example_3","text":"const person = { name: 'Mosh', walk() { console.log(this); }, } person.walk(); // person {} const walk = person.walk(); walk(); // undefined","title":"example"},{"location":"Misc/Web/JavaScript/React/#arrow-functions","text":"Arrow functions allow a good way to clean up code for the simplest functions. const square = function(number) { return number * number } const square = number => number * number; But they do not rebind the this object for callback functions.","title":"Arrow functions"},{"location":"Misc/Web/JavaScript/React/#arraymap","text":"One of the new array methods defined in ECMAScript 6, .map() is very useful in rendering lists. const colors = ['red','green','blue'] const items = colors.map(color => <li>${color}</li> ) })","title":"Array.map()"},{"location":"Misc/Web/JavaScript/React/#object-destructuring","text":"const address = { street: '', city: '', country: '' } const street = address.street; const city = address.city; const country = address.country; The repetitive use of the object name address is a good object to use destructuring syntax . const {street, city, country } = address; const {street: st } = address;","title":"Object destructuring"},{"location":"Misc/Web/JavaScript/React/#classes","text":"When we have an object with at least one method, we use classes to ensure that child objects do not reduplicate code unnecessarily. const person = { name: 'Mosh', walk() { constole.log('walk') // console is mispelled }} const person2 = { name: 'Mosh', walk() { constole.log('walk') // copying reduplicates the error }} class Person { constructor(name) { this.name = name; } walk() { console.log('walk'); }} const person = new Person('Mosh');","title":"Classes"},{"location":"Misc/Web/JavaScript/React/#inheritance","text":"class Person { constructor(name) { this.name = name; } walk() { console.log('walk'); } } class Teacher { // Teacher should also be able to walk, teach() { // but we don't want to reduplicate code... console.log('teach'); } } class Teacher extends Person { constructor(name, degree) { super(name) // references parent class this.degree = degree } teach() { console.log('teach') } } const teacher = new Teacher ('Mosh', 'MSc')","title":"Inheritance"},{"location":"Misc/Web/JavaScript/React/#modules","text":"Splitting code across multiple files, each of which is called a module . Class keyword must be prefixed by export person.js export class Person { constructor(name) { this.name = name; } walk() { console.log('walk'); } } teacher.js import { Person } from './person' // extension is not added export class Teacher extends Person { constructor(name, degree) { super(name) this.degree = degree } teach() { console.log('teach') } }","title":"Modules"},{"location":"Misc/Web/JavaScript/React/#indexjs_1","text":"import Teacher from './teacher' const teacher = new Teacher ('Mosh', 'MSc')","title":"index.js"},{"location":"Misc/Web/JavaScript/React/#named-and-default-exports","text":"default keyword to export an entire object, after export and before class named exports to export functions using the export keyword named exports must be individually named and placed within braces","title":"Named and default exports"},{"location":"Misc/Web/JavaScript/React/#react-native","text":"React Native is like react, but it uses native components instead of web components as building blocks, allowing true mobile apps to be programmed. import React , { Component } from 'react' ; import { Text , View } from 'react-native' ; export default class HelloWorldApp extends Component { render () { return ( < View > < Text > Hello world !< /Text> < /View> ); } } Parameters associated with customizing components are called props that appear similar to HTML attributes: < Greeting name = 'Jaina' /> < Greeting name = 'Sanjay' /> ` Two types of data control a component: 1. props set by the parent are fixed throughout the lifetime of a component 2. state data that changes","title":"React Native"},{"location":"Misc/Web/JavaScript/Web%20Components/","text":"Web Components is a suite of technologies allowing you to create reusable custom elements 3 main technologies: Custom elements set of APIs that allow you to define custom elements and their behavior Shadow DOM set of APIs for attaching an encapsulated shadow DOM tree to an element, rendered separately from the main document DOM, to keep an element's features private. HTML templates write markup templates using template and slot elements that are not rendered in the rendered page. Basic approach: Create a class or function in which you specify web component functionality must use super() command at the top must contain a constructor() function that defines structure of Shadow DOM using appendChild() methods Use CustomElementRegistry.define() to link the component class and the custom element Attach a Shadow DOM to custom element using Element.attachShadow() and add children, event listeners, using regular DOM methods: const shadow = this.attachShadow({mode: 'open'}) Define an HTML template using template and slot Use custom element at will on page. 2 types of custom elements: Autonomous custom elements are standalone and don't inherit from standard HTML elements, e.g. <popup-info> or document.createElement('popup-info') . They almost always extend HTMLElement Customized built-in elements inherit from basic HTML elements, e.g. <p is=\"word-count\"> or document.createElement(\"p\",{ is: \"word-count\"}) Lifecycle callbacks adoptedCallback invoked each time the custom element is moved to a new document attributeChangedCallback invoked when one of the custom element's attributed is added, removed, or changed connectedCallback invoked every time the custom element is appended into a document-connected element disconnectedCallback invoked each time the custom element is disconnected from the document's DOM","title":"Web Components"},{"location":"Misc/Web/JavaScript/Web%20Components/#3-main-technologies","text":"Custom elements set of APIs that allow you to define custom elements and their behavior Shadow DOM set of APIs for attaching an encapsulated shadow DOM tree to an element, rendered separately from the main document DOM, to keep an element's features private. HTML templates write markup templates using template and slot elements that are not rendered in the rendered page.","title":"3 main technologies:"},{"location":"Misc/Web/JavaScript/Web%20Components/#basic-approach","text":"Create a class or function in which you specify web component functionality must use super() command at the top must contain a constructor() function that defines structure of Shadow DOM using appendChild() methods Use CustomElementRegistry.define() to link the component class and the custom element Attach a Shadow DOM to custom element using Element.attachShadow() and add children, event listeners, using regular DOM methods: const shadow = this.attachShadow({mode: 'open'}) Define an HTML template using template and slot Use custom element at will on page.","title":"Basic approach:"},{"location":"Misc/Web/JavaScript/Web%20Components/#2-types-of-custom-elements","text":"Autonomous custom elements are standalone and don't inherit from standard HTML elements, e.g. <popup-info> or document.createElement('popup-info') . They almost always extend HTMLElement Customized built-in elements inherit from basic HTML elements, e.g. <p is=\"word-count\"> or document.createElement(\"p\",{ is: \"word-count\"})","title":"2 types of custom elements:"},{"location":"Misc/Web/JavaScript/Web%20Components/#lifecycle-callbacks","text":"adoptedCallback invoked each time the custom element is moved to a new document attributeChangedCallback invoked when one of the custom element's attributed is added, removed, or changed connectedCallback invoked every time the custom element is appended into a document-connected element disconnectedCallback invoked each time the custom element is disconnected from the document's DOM","title":"Lifecycle callbacks"},{"location":"Misc/Windows/","text":"Windows Like other multiuser OSes, Windows architecture draws a distinction between kernel mode and user mode . When user-mode threads call a system service, a special instruction is executed that switches the calling thread to kernel mode. When the call completes, the thread context is switched back to user mode. executive The Windows executive contains base OS services like memory management, process and thread management, security, I/O, networking, and IPC heterogeneous multi-processing A type of SMP -based design where some processor cores have less capability but higher efficiency than others. This allows power consumption to be reduced by allocating appropriate work to slower cores while power managing the faster ones. kernel The Windows kernel consists of low-level OS functions, like thread scheduling, interrupt, and exception dispatching, and multiprocessor synchronization. It also provides a set of routines and basic objects that the rest of the executive uses to implement higher-level constructs. kernel mode privileged processor mode with access to system data and hardware user mode non-privileged processor mode with limited access to system data and no direct access to hardware Windows core files condrv.sys Console driver, which spawns conhost.exe dwm.exe Desktop Window Manager ntdll.dll Special system support library primarily for the use of subsystem DLLs and native applications (meaning images that are not tied to any particular subsystem) ntoskrnl.exe contains the Windows executive and kernel ssmss.exe Session Manager","title":"Windows"},{"location":"Misc/Windows/#windows","text":"Like other multiuser OSes, Windows architecture draws a distinction between kernel mode and user mode . When user-mode threads call a system service, a special instruction is executed that switches the calling thread to kernel mode. When the call completes, the thread context is switched back to user mode. executive The Windows executive contains base OS services like memory management, process and thread management, security, I/O, networking, and IPC heterogeneous multi-processing A type of SMP -based design where some processor cores have less capability but higher efficiency than others. This allows power consumption to be reduced by allocating appropriate work to slower cores while power managing the faster ones. kernel The Windows kernel consists of low-level OS functions, like thread scheduling, interrupt, and exception dispatching, and multiprocessor synchronization. It also provides a set of routines and basic objects that the rest of the executive uses to implement higher-level constructs. kernel mode privileged processor mode with access to system data and hardware user mode non-privileged processor mode with limited access to system data and no direct access to hardware","title":"Windows"},{"location":"Misc/Windows/#windows-core-files","text":"condrv.sys Console driver, which spawns conhost.exe dwm.exe Desktop Window Manager ntdll.dll Special system support library primarily for the use of subsystem DLLs and native applications (meaning images that are not tied to any particular subsystem) ntoskrnl.exe contains the Windows executive and kernel ssmss.exe Session Manager","title":"Windows core files"},{"location":"Misc/Windows/Active%20Directory/","text":"Active Directory Table of Contents 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 \\ 21 22 23 24 25 26 27 28 29 30 31 32 33 Terms Active Directory components Active Directory Lightweight Directory Services (AD LDS)) Instance Configuration set Replica Partition/naming context Application partition Configuration partition Schema partition Bindable object Bindable proxy object Active Directory Federated Services (ADFS) Directory Information Tree (DIT) Key tables: Data table Link table Hidden table Security descriptor table Extensible Storage Engine (ESE) Identity Management for Unix (IMU) Security Accounts Manager (SAM) Global Catalog Schema attribute syntax Active Directory concepts Domain Controller (DC) FSMO roles: Schema Master Domain Naming Master PDC Emulator RID Master Infrastructure Master Distinguished Name (DN) domain tree forest organizational unit Trust forest trust simple trust transitive trust Domain models: Multimaster Single-domain Single-master Complete trust Groups: Scopes [Domain local][domain local group] [Domain global][domain global group] [Universal][universal group] Types: Distribution Security Site topology site subnet site link connection object Windows Management Interface (WMI) CIM infrastructure CIM Repository CIMOM WMI providers Fundamentals History Active Directory has its origins in 1990 when Microsoft released Windows NT 3.0, its first Network Operating System (NOS). Limitations of NT led Microsoft to rearchitect their solution based on LDAP, a directory service that originated in 1993 as a lighter-weight alternative to X.500 . Feature NT AD Database SAM ESE Trust Simple Transitive Domain models Multimaster Single-domain Single-master Complete trust Complete trust Name resolution WINS DNS Schemas Not extensible Extensible Major components AD objects, which can be containers or non-containers (leaf nodes), are stored in a DIT file. Each object is identified by a GUID but also commonly referred to by distinguished name (i.e. dc=mycorp,dc=com ) Active Directory's structure is based on the concept of a domain , based on the following components: - Hierarchical structure of containers and objects based on X.500 - DNS domain name - Security service to provide AAA - Policies to restrict functionality for users or machines Domains can be organized into domain trees , and domain trees can be organized into forests . The most common container type is the OU . Global Catalog can be used to search for AD objects. Because Kerberos, which underlies AD, is sensitive to time differences all computers on a domain must have clocks synchronized to within 5 minutes. NTP can be useful for this. Naming contexts Predefined NCs within AD: - Domain Naming Context - Schema Naming Context - Configuration Naming Context Schema Each object in AD is an instance of a class defined in the schema . The schema version can be queried from the command-line with [ adfind ][adfind] OID SID A Windows SID is generally composed of 2 fixed fields and up to 15 additional fields, all separated by dashes: S-v- id - s1 - s2 - s3 - s4 - s5 - s6 - s7 - s8 - s9 - s10 - s11 - s12 - s13 - s14 - s15 AD LDS AD LDS offers a pared-down version of AD that is easy to set up and tear down. It was first released in November 2003 as Active Directory Application Mode (ADAM) V1.0 and offers security benefits because it doesn't enable so many services by default. It was renamed AD LDS with the release of Windows Server 2008. Differences between AD and AD LDS - AD LDS is a standalone application run from a dsamain.exe process (rather than lsass.exe ), which means it can be started or stopped on demand without rebooting and multiple instances can be run. - AD LDS lacks the global catalog functionality (removing NSPI and AB as well) Site topology A site topology is a map of the sites , subnets , site links , site link bridges, and connection objects as it relates to a forest. WMI An industry effort to develop a model for managing systems and devices for vendor use arose in the 1990s which resulted in CIM , which provides the basis for WMI . The WMI architecture is composed of two main layers: the CIM infrastructure ( CIMOM and CIM Repository ) and the WMI providers Each provider is associated with a namespace, which is similar in concept to a filesystem. .NET The .NET Framework was developed with the intention of replacing the old Win32 and COM APIs. It has two major components: - Common Language Runtime (CLR) - .NET Framework class library Searching Active Directory","title":"Active Directory"},{"location":"Misc/Windows/Active%20Directory/#active-directory","text":"","title":"Active Directory"},{"location":"Misc/Windows/Active%20Directory/#table-of-contents","text":"01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 \\ 21 22 23 24 25 26 27 28 29 30 31 32 33","title":"Table of Contents"},{"location":"Misc/Windows/Active%20Directory/#terms","text":"Active Directory components Active Directory Lightweight Directory Services (AD LDS)) Instance Configuration set Replica Partition/naming context Application partition Configuration partition Schema partition Bindable object Bindable proxy object Active Directory Federated Services (ADFS) Directory Information Tree (DIT) Key tables: Data table Link table Hidden table Security descriptor table Extensible Storage Engine (ESE) Identity Management for Unix (IMU) Security Accounts Manager (SAM) Global Catalog Schema attribute syntax Active Directory concepts Domain Controller (DC) FSMO roles: Schema Master Domain Naming Master PDC Emulator RID Master Infrastructure Master Distinguished Name (DN) domain tree forest organizational unit Trust forest trust simple trust transitive trust Domain models: Multimaster Single-domain Single-master Complete trust Groups: Scopes [Domain local][domain local group] [Domain global][domain global group] [Universal][universal group] Types: Distribution Security Site topology site subnet site link connection object Windows Management Interface (WMI) CIM infrastructure CIM Repository CIMOM WMI providers","title":"Terms"},{"location":"Misc/Windows/Active%20Directory/#fundamentals","text":"","title":"Fundamentals"},{"location":"Misc/Windows/Active%20Directory/#history","text":"Active Directory has its origins in 1990 when Microsoft released Windows NT 3.0, its first Network Operating System (NOS). Limitations of NT led Microsoft to rearchitect their solution based on LDAP, a directory service that originated in 1993 as a lighter-weight alternative to X.500 . Feature NT AD Database SAM ESE Trust Simple Transitive Domain models Multimaster Single-domain Single-master Complete trust Complete trust Name resolution WINS DNS Schemas Not extensible Extensible","title":"History"},{"location":"Misc/Windows/Active%20Directory/#major-components","text":"AD objects, which can be containers or non-containers (leaf nodes), are stored in a DIT file. Each object is identified by a GUID but also commonly referred to by distinguished name (i.e. dc=mycorp,dc=com ) Active Directory's structure is based on the concept of a domain , based on the following components: - Hierarchical structure of containers and objects based on X.500 - DNS domain name - Security service to provide AAA - Policies to restrict functionality for users or machines Domains can be organized into domain trees , and domain trees can be organized into forests . The most common container type is the OU . Global Catalog can be used to search for AD objects. Because Kerberos, which underlies AD, is sensitive to time differences all computers on a domain must have clocks synchronized to within 5 minutes. NTP can be useful for this.","title":"Major components"},{"location":"Misc/Windows/Active%20Directory/#naming-contexts","text":"Predefined NCs within AD: - Domain Naming Context - Schema Naming Context - Configuration Naming Context","title":"Naming contexts"},{"location":"Misc/Windows/Active%20Directory/#schema","text":"Each object in AD is an instance of a class defined in the schema . The schema version can be queried from the command-line with [ adfind ][adfind] OID","title":"Schema"},{"location":"Misc/Windows/Active%20Directory/#sid","text":"A Windows SID is generally composed of 2 fixed fields and up to 15 additional fields, all separated by dashes: S-v- id - s1 - s2 - s3 - s4 - s5 - s6 - s7 - s8 - s9 - s10 - s11 - s12 - s13 - s14 - s15","title":"SID"},{"location":"Misc/Windows/Active%20Directory/#ad-lds","text":"AD LDS offers a pared-down version of AD that is easy to set up and tear down. It was first released in November 2003 as Active Directory Application Mode (ADAM) V1.0 and offers security benefits because it doesn't enable so many services by default. It was renamed AD LDS with the release of Windows Server 2008. Differences between AD and AD LDS - AD LDS is a standalone application run from a dsamain.exe process (rather than lsass.exe ), which means it can be started or stopped on demand without rebooting and multiple instances can be run. - AD LDS lacks the global catalog functionality (removing NSPI and AB as well)","title":"AD LDS"},{"location":"Misc/Windows/Active%20Directory/#site-topology","text":"A site topology is a map of the sites , subnets , site links , site link bridges, and connection objects as it relates to a forest.","title":"Site topology"},{"location":"Misc/Windows/Active%20Directory/#wmi","text":"An industry effort to develop a model for managing systems and devices for vendor use arose in the 1990s which resulted in CIM , which provides the basis for WMI . The WMI architecture is composed of two main layers: the CIM infrastructure ( CIMOM and CIM Repository ) and the WMI providers Each provider is associated with a namespace, which is similar in concept to a filesystem.","title":"WMI"},{"location":"Misc/Windows/Active%20Directory/#net","text":"The .NET Framework was developed with the intention of replacing the old Win32 and COM APIs. It has two major components: - Common Language Runtime (CLR) - .NET Framework class library","title":".NET"},{"location":"Misc/Windows/Active%20Directory/#searching-active-directory","text":"","title":"Searching Active Directory"},{"location":"Misc/Windows/DNS-labs/","text":"Configuring DNS servers Hostname IP Address PLABDC01 192.168.0.1 PLABDM01 192.168.0.2 PLABSA01 192.168.0.4 Create a Domain Forest Create a new domain and forest on PLABSA01 named \"PRACTICEIT.CO.UK\". Configure PLABDC01 to forward for the new domain, and PLABSA01 to forward for the old one. Create a new DNS zone Set up alternate DNS server Configure DNS forwarders New AD forest Create a new domain and forest on PLABSA01 named \"PRACTICEIT.CO.UK\" # PLABSA01 Add-WindowsFeature dns , ad-domain-services -IncludeManagementTools $pw = ConvertTo-SecureString -Force -AsPlainText 'Passw0rd' Install-ADDSForest -DomainName PRACTICEIT . CO . UK -SafeModeAdministratorPassword $pw Forwarding Forward DNS queries for the new domain to PLABSA01 # PLABDC01 # Equivalent to `netsh interface ipv4 add dns Ethernet 192.168.0.4 index=2` Set-DNSClientServerAddress -InterfaceAlias Ethernet -ServerAddresses ( '192.168.0.1' , '192.168.0.4' ) Add-DnsServerConditionalForwarderZone -Name 'practiceit.co.uk' -MasterServers '192.168.0.4' Add-DnsServerForwarder -IpAddress '192.168.0.4' Test-NetConnection plabsa01 . practiceit . co . uk Forward DNS queries for the old domain to PLABDC01 # PLABSA01 # Equivalent to `netsh interface ipv4 add dns Ethernet 192.168.0.1 index=2` Set-DnsClientServerAddress -InterfaceIndex 13 -ServerAddresses ( '127.0.0.1' , '192.168.0.1' ) Add-DnsServerConditionalForwarderZone -Name 'practicelabs.com' -MasterServers '192.168.0.1' Test-NetConnection plabdc01 . practicelabs . com Create new zone # PLABDM01 Add-WindowsFeature dns -IncludeManagementTools Add-DnsServerPrimaryZone -ZoneFile 'apac.practicelabs.com.dns' -ZoneName 'apac.practicelabs.com' -DynamicUpdate NonsecureAndSecure Delegate to the new zone # PLABDC01 Add-DnsServerZoneDelegation -Name 'practicelabs.com' -ChildZoneName 'apac' -NameServer 'plabdm01.practicelabs.com' -IPAddress '192.168.0.2' Add-DnsServerPrimaryZone -ZoneName PLTEST . com -ReplicationScope Domain Add-DnsServerResourceRecordA -Name www -ZoneName pltest . com -IPv4Address 192 . 168 . 0 . 1 # Copying PowerShell commands from a script Add-DnsServerClientSubnet -Name EMEA -IPv4Subnet '192.168.0.0/24' Add-DnsServerClientSubnet -Name APAC -IPv4Subnet '192.168.1.0/24' Add-DnsServerZoneScope -ZoneName \"PLTEST.com\" -Name \"EMEAZoneScope\" Add-DnsServerZoneScope -ZoneName \"PLTEST.com\" -Name \"APACZoneScope\" Add-DnsServerResourceRecord -A -Name www -ZoneName 'PLTEST.com' -IPv4Address '192.168.1.1' -ZoneScope APACZoneScope Add-DnsServerResourceRecord -A -Name www -ZoneName 'PLTEST.com' -IPv4Address '192.168.0.1' -ZoneScope EMEAZoneScope Add-DnsServerQueryResolutionPolicy -Name EMEAPolicy -Action ALLOW -ClientSubnet 'eq,EMEA' -ZoneScope 'EMEAZoneScope,1' -ZoneName 'PLTEST.com' Add-DnsServerQueryResolutionPolicy -Name APACPolicy -Action ALLOW -ClientSubnet 'eq,APAC' -ZoneScope 'APACZoneScope,1' -ZoneName 'PLTEST.com' # PLABWIN10 ipconfig / flushdns nslookup www . pltest . com # PLABDC01 Add-DnsServerQueryResolutionPolicy -Name Blocklist -Action IGNORE -Fqdn 'eq,*.pltest.com' -PassThru # PLABWIN10 # Now the query does not work nslookup www . pltest . com # PLABDC01 Remove-DnsServerQueryResolutionPolicy -Name Blocklist -Confirm Configuration Configure DNS socket pool ::PLABDC01 dnscmd /info /socketpoolsize dnscmd /config /socketpoolsize 3500 net stop dns net start dns dnscmd /info /socketpoolsize Manage DNS cache locking dnscmd /info /cachelockingpercent dnscmd /config /cachelockingpercent 90 dnscmd /info /cachelockingpercent net stop dns net start dns dnscmd /info /cachelockingpercent Create a GlobalNames zone Set-DnsServerGlobalNameZone -AlwaysQueryServer $true Add-DnsServerPrimaryZone -Name GlobalNames -ReplicationScope Forest dnscmd . / config / enableglobalnamessupport 1 Create a resource record in the GlobalNames zone Add-DnsServerResourceRecordA -Name PLABDC01 -ZoneName GlobalNames -IPv4Address 192 . 168 . 0 . 1 Enable response rate limiting Get-DNSServerResponseRateLimiting Set-DNSServerResponseRateLimiting -Mode Enable -Confirm Get-DNSServerResponseRateLimiting # confirming Add-DNSServerResponseRateLimitingExceptionList -Name Whitelist1 -Fqdn 'eq, *.practicelabs.com' Set-DNSServerResponseRateLimiting -Mode Enable -Confirm Manage DNS logging Add-WindowsFeature dns -IncludeManagementTools -ComputerName PLABDM01 Set-DnsServerDiagnostics -LogFilePath C :\\ dnslog . txt # Some additional settings appear necessary in order to begin logging Enable zone transfers Set-DnsServerPrimaryZone -SecureSecondaries TransferAnyServer Create a secondary DNS zone # PLABDM01 Add-DnsServerSecondaryZone -ComputerNAme PLABDM01 -MasterServers 192 . 168 . 0 . 1 -ZoneName practicelabs . com -ZoneFile practicelabs . com . dns Check DNS logs at text file and event viewer. Manage DNS zones and resource records Convert AD-integrated zone to file-based zone Get-DNSServerZone -ZoneName practicelabs . com | ConvertTo-DnsServerPrimaryZone -ZoneFile practicelabs . com . dns Manually add an A record Add-Content -Path C :\\ Windows \\ System32 \\ dns \\ practicelabs . com . dns -Value \"PLABSVR 1200 A 192.168.0.105\" Re-integrate zone to AD ConvertTo-DnsServerPrimaryZone -ZoneName practicelabs . com -ReplicationScope Domain Promote a new DC # PLABDM01 Add-WindowsFeature dns , ad-domain-services -IncludeManagementTools Install-ADDSDomainController -Domain practicelabs . com Create a secondary zone # PLABSA01 Add-WindowsFeature dns -IncludeManagementTools Add-DnsServerSecondaryZone -ZoneName practicelabs . com -ZoneFile \"practicelabs.com.dns\" -MasterServers 192 . 168 . 0 . 1 Enable zone transfer from master # PLABDC01 Set-DnsServerPrimaryZone -Name practicelabs . com -SecureSecondaries TransferAnyServer Manually initiate a zone transfer if needed # PLABSA01 Start-DnsServerZoneTransfer -ZoneName practicelabs . com Remove secondary zone # PLABSA01 Remove-DnsServerZone -ZoneName practicelabs . com Add-DnsServerStubZone -ZoneName practicelabs . com -ZoneFile practicelabs . com . dns -MasterServers 192 . 168 . 0 . 1 Create new resource records # PLABDC01 Add-DnsServerResourceRecord -A -Name plabsa05 -IPv4Address 192 . 168 . 0 . 50 -ZoneName practicelabs . com Add-DnsServerResourceRecord -CName -Name plabdmsrv01 -ZoneName practicelabs . com -HostNameAlias plabdm01 . practicelabs . com Add-DnsServerResourceRecord -MX -Name \".\" -ZoneName practicelabs . com -MailExchange mobilemail . practicelabs . com -Preference 10 Add-DnsServerResourceRecord -A -Name mobilemail -IPv4Address 192 . 168 . 0 . 21 -ZoneName practicelabs . com","title":"DNS labs"},{"location":"Misc/Windows/DNS-labs/#configuring-dns-servers","text":"Hostname IP Address PLABDC01 192.168.0.1 PLABDM01 192.168.0.2 PLABSA01 192.168.0.4 Create a Domain Forest Create a new domain and forest on PLABSA01 named \"PRACTICEIT.CO.UK\". Configure PLABDC01 to forward for the new domain, and PLABSA01 to forward for the old one. Create a new DNS zone Set up alternate DNS server Configure DNS forwarders","title":"Configuring DNS servers"},{"location":"Misc/Windows/DNS-labs/#new-ad-forest","text":"Create a new domain and forest on PLABSA01 named \"PRACTICEIT.CO.UK\" # PLABSA01 Add-WindowsFeature dns , ad-domain-services -IncludeManagementTools $pw = ConvertTo-SecureString -Force -AsPlainText 'Passw0rd' Install-ADDSForest -DomainName PRACTICEIT . CO . UK -SafeModeAdministratorPassword $pw","title":"New AD forest"},{"location":"Misc/Windows/DNS-labs/#forwarding","text":"Forward DNS queries for the new domain to PLABSA01 # PLABDC01 # Equivalent to `netsh interface ipv4 add dns Ethernet 192.168.0.4 index=2` Set-DNSClientServerAddress -InterfaceAlias Ethernet -ServerAddresses ( '192.168.0.1' , '192.168.0.4' ) Add-DnsServerConditionalForwarderZone -Name 'practiceit.co.uk' -MasterServers '192.168.0.4' Add-DnsServerForwarder -IpAddress '192.168.0.4' Test-NetConnection plabsa01 . practiceit . co . uk Forward DNS queries for the old domain to PLABDC01 # PLABSA01 # Equivalent to `netsh interface ipv4 add dns Ethernet 192.168.0.1 index=2` Set-DnsClientServerAddress -InterfaceIndex 13 -ServerAddresses ( '127.0.0.1' , '192.168.0.1' ) Add-DnsServerConditionalForwarderZone -Name 'practicelabs.com' -MasterServers '192.168.0.1' Test-NetConnection plabdc01 . practicelabs . com","title":"Forwarding"},{"location":"Misc/Windows/DNS-labs/#create-new-zone","text":"# PLABDM01 Add-WindowsFeature dns -IncludeManagementTools Add-DnsServerPrimaryZone -ZoneFile 'apac.practicelabs.com.dns' -ZoneName 'apac.practicelabs.com' -DynamicUpdate NonsecureAndSecure Delegate to the new zone # PLABDC01 Add-DnsServerZoneDelegation -Name 'practicelabs.com' -ChildZoneName 'apac' -NameServer 'plabdm01.practicelabs.com' -IPAddress '192.168.0.2' Add-DnsServerPrimaryZone -ZoneName PLTEST . com -ReplicationScope Domain Add-DnsServerResourceRecordA -Name www -ZoneName pltest . com -IPv4Address 192 . 168 . 0 . 1 # Copying PowerShell commands from a script Add-DnsServerClientSubnet -Name EMEA -IPv4Subnet '192.168.0.0/24' Add-DnsServerClientSubnet -Name APAC -IPv4Subnet '192.168.1.0/24' Add-DnsServerZoneScope -ZoneName \"PLTEST.com\" -Name \"EMEAZoneScope\" Add-DnsServerZoneScope -ZoneName \"PLTEST.com\" -Name \"APACZoneScope\" Add-DnsServerResourceRecord -A -Name www -ZoneName 'PLTEST.com' -IPv4Address '192.168.1.1' -ZoneScope APACZoneScope Add-DnsServerResourceRecord -A -Name www -ZoneName 'PLTEST.com' -IPv4Address '192.168.0.1' -ZoneScope EMEAZoneScope Add-DnsServerQueryResolutionPolicy -Name EMEAPolicy -Action ALLOW -ClientSubnet 'eq,EMEA' -ZoneScope 'EMEAZoneScope,1' -ZoneName 'PLTEST.com' Add-DnsServerQueryResolutionPolicy -Name APACPolicy -Action ALLOW -ClientSubnet 'eq,APAC' -ZoneScope 'APACZoneScope,1' -ZoneName 'PLTEST.com' # PLABWIN10 ipconfig / flushdns nslookup www . pltest . com # PLABDC01 Add-DnsServerQueryResolutionPolicy -Name Blocklist -Action IGNORE -Fqdn 'eq,*.pltest.com' -PassThru # PLABWIN10 # Now the query does not work nslookup www . pltest . com # PLABDC01 Remove-DnsServerQueryResolutionPolicy -Name Blocklist -Confirm","title":"Create new zone"},{"location":"Misc/Windows/DNS-labs/#configuration","text":"Configure DNS socket pool ::PLABDC01 dnscmd /info /socketpoolsize dnscmd /config /socketpoolsize 3500 net stop dns net start dns dnscmd /info /socketpoolsize Manage DNS cache locking dnscmd /info /cachelockingpercent dnscmd /config /cachelockingpercent 90 dnscmd /info /cachelockingpercent net stop dns net start dns dnscmd /info /cachelockingpercent Create a GlobalNames zone Set-DnsServerGlobalNameZone -AlwaysQueryServer $true Add-DnsServerPrimaryZone -Name GlobalNames -ReplicationScope Forest dnscmd . / config / enableglobalnamessupport 1 Create a resource record in the GlobalNames zone Add-DnsServerResourceRecordA -Name PLABDC01 -ZoneName GlobalNames -IPv4Address 192 . 168 . 0 . 1 Enable response rate limiting Get-DNSServerResponseRateLimiting Set-DNSServerResponseRateLimiting -Mode Enable -Confirm Get-DNSServerResponseRateLimiting # confirming Add-DNSServerResponseRateLimitingExceptionList -Name Whitelist1 -Fqdn 'eq, *.practicelabs.com' Set-DNSServerResponseRateLimiting -Mode Enable -Confirm Manage DNS logging Add-WindowsFeature dns -IncludeManagementTools -ComputerName PLABDM01 Set-DnsServerDiagnostics -LogFilePath C :\\ dnslog . txt # Some additional settings appear necessary in order to begin logging Enable zone transfers Set-DnsServerPrimaryZone -SecureSecondaries TransferAnyServer Create a secondary DNS zone # PLABDM01 Add-DnsServerSecondaryZone -ComputerNAme PLABDM01 -MasterServers 192 . 168 . 0 . 1 -ZoneName practicelabs . com -ZoneFile practicelabs . com . dns Check DNS logs at text file and event viewer.","title":"Configuration"},{"location":"Misc/Windows/DNS-labs/#manage-dns-zones-and-resource-records","text":"Convert AD-integrated zone to file-based zone Get-DNSServerZone -ZoneName practicelabs . com | ConvertTo-DnsServerPrimaryZone -ZoneFile practicelabs . com . dns Manually add an A record Add-Content -Path C :\\ Windows \\ System32 \\ dns \\ practicelabs . com . dns -Value \"PLABSVR 1200 A 192.168.0.105\" Re-integrate zone to AD ConvertTo-DnsServerPrimaryZone -ZoneName practicelabs . com -ReplicationScope Domain Promote a new DC # PLABDM01 Add-WindowsFeature dns , ad-domain-services -IncludeManagementTools Install-ADDSDomainController -Domain practicelabs . com Create a secondary zone # PLABSA01 Add-WindowsFeature dns -IncludeManagementTools Add-DnsServerSecondaryZone -ZoneName practicelabs . com -ZoneFile \"practicelabs.com.dns\" -MasterServers 192 . 168 . 0 . 1 Enable zone transfer from master # PLABDC01 Set-DnsServerPrimaryZone -Name practicelabs . com -SecureSecondaries TransferAnyServer Manually initiate a zone transfer if needed # PLABSA01 Start-DnsServerZoneTransfer -ZoneName practicelabs . com Remove secondary zone # PLABSA01 Remove-DnsServerZone -ZoneName practicelabs . com Add-DnsServerStubZone -ZoneName practicelabs . com -ZoneFile practicelabs . com . dns -MasterServers 192 . 168 . 0 . 1 Create new resource records # PLABDC01 Add-DnsServerResourceRecord -A -Name plabsa05 -IPv4Address 192 . 168 . 0 . 50 -ZoneName practicelabs . com Add-DnsServerResourceRecord -CName -Name plabdmsrv01 -ZoneName practicelabs . com -HostNameAlias plabdm01 . practicelabs . com Add-DnsServerResourceRecord -MX -Name \".\" -ZoneName practicelabs . com -MailExchange mobilemail . practicelabs . com -Preference 10 Add-DnsServerResourceRecord -A -Name mobilemail -IPv4Address 192 . 168 . 0 . 21 -ZoneName practicelabs . com","title":"Manage DNS zones and resource records"},{"location":"Misc/Windows/Hyper-V/","text":"Hyper-V Configure VM NIC HDD CPU RAM Create Read List Update Delete Add-VMNetworkAdapter -VMName $vmName -SwitchName $switchName command get command list command set command remove Add-VMHardDiskDrive -VMName $vmName -ControllerType IDE -ControllerNumber 0 -Path $vhdPath","title":"Hyper-V"},{"location":"Misc/Windows/Hyper-V/#hyper-v","text":"","title":"Hyper-V"},{"location":"Misc/Windows/Hyper-V/#configure-vm","text":"NIC HDD CPU RAM Create Read List Update Delete Add-VMNetworkAdapter -VMName $vmName -SwitchName $switchName command get command list command set command remove Add-VMHardDiskDrive -VMName $vmName -ControllerType IDE -ControllerNumber 0 -Path $vhdPath","title":"Configure VM"},{"location":"Misc/Windows/Labs/","text":"Labs Server Core Configure VM Rename-VMSwitch Intel * External New-VM PLABDMCORE02 1024 1 -SwitchName External -NewVHDPath 'D:\\VHD\\PLABDMCORE02\\Virtual Hard Disks\\PLABDMCORE02.vhdx' -NewVHDSize 127gb Set-VMDvdDrive ... # Can't find way to pass through host drive Z:\\ Add DVD drive Z: of host to VM's DVD drive (no Powershell equivalent found yet). Start-VM PLABDMCORE02 Install Windows, then continue configuring VM # PLABDMCORE02 Rename-Computer PLABDMCORE02 New-NetIpAddress 192 . 168 . 0 . 7 -PrefixLength 24 -InterfaceAlias Ethernet Set-DnsClientServerAddress -InterfaceAlias Ethernet -ServerAddresses 192 . 168 . 0 . 1 Restart-Computer Join computer to domain Add-Computer -DomainName practicelabs . com -DomainCredential practicelabs \\ administrator Restart-Computer Install packages remotely # PLABDC01 Enter-PSSession PLABDMCORE02 Add-WindowsFeature -Name dns , rsat-dns-server -IncludeManagementTools Add PLABDMCORE02 as a DNS server while remotely connected to DC (no Powershell equivalent found yet). Add a secondary lookup zone to PLABDMCORE02 (doesn't work; asks for Zonefile) Add-DnsServerSecondaryZone -Name practicelabs . com -MasterServers 192 . 168 . 0 . 1 Nano Server # PLABDM01 Copy-Item Z :\\ NanoServer \\ NanoServerImageGenerator \\*. ps * C :\\ nanoserver Import-Module C :\\ nNanoserver \\ NanoServerImageGenerator . psm1 Create the VHD image New-NanoServerImage -Edition Standard -MediaPath z :\\ -Basepath c :\\ nanoserver -Targetpath c :\\ nanoserver \\ PLABNANOSRV01 . vhdx -DeploymentType Guest -ComputerName PLABNANOSRV01 -Storage -Package Microsoft-NanoServer-DNS-Package Set-VMHardDiskDrive -VMName PLABNANOSRV01 -Path C :\\ nanoserver \\ PLABNANOSRV01 . vhdx DISM # PLABSA01 Set-Location C :\\ mkdir updates , images , mount , drivers Copy-Item \\\\ plabdm01 \\ Win10 \\ sources \\ install . wim C :\\ updates ( Get-Item C :\\ updates \\ install . wim ). IsReadOnly = $false Move specified .msu installers to C:\\updates and uncompress device driver to C:\\drivers Using Powershell: Mount-WindowsImage -Path C :\\ mount -ImagePath C :\\ images \\ install . wim -Index 1 Add-WindowsPackage -Path C :\\ mount -PackagePath C :\\ updates Add-WindowsDriver -Path C :\\ mount -Driver C :\\ drivers \\ display . driver \\ nv_dispi . inf Save-WindowsImage -Path C :\\ mount Dismount-WindowsImage -Path C :\\ mount -Save Using dism.exe : dism /mount-wim /wimfile:c:\\images\\install.wim /index:1 /mountdir:c:\\mount dism /image:c:\\mount /add-package /packagepath:c:\\updates dism /image:c:\\mount /add-driver /driver:c:\\drivers\\display.driver\\nv_dispi.inf dism /commit-wim /mountdir:c:\\mount dism /unmount-wim /commit /mountdir:c:\\mount NLB lab Configure a Network Load Balancing cluster. [Practice Lab][pl:70-740] # PLABDM01 and PLABSA01 Install-WindowsFeature web-server , web-webserver , web-mgmt-tools -IncludeManagementTools # PLABDM01 New-NetIpAddress 192 . 168 . 0 . 20 -PrefixLength 24 -InterfaceAlias Ethernet1 # PLABSA01 New-NetIpAddress 192 . 168 . 0 . 40 -PrefixLength 24 -InterfaceAlias Ethernet1 Install-WindowsFeature -Name nlb -IncludeAllSubFeature -IncludeManagementTools Using the Network Load Balancing Manager , create a new cluster named PLAB-NLB1 specifying multicast operation mode at 192.168.0.25, and create an appropriate DNS record on PLABDC01 . Install NLB on PLABDM01 and add it to the cluster through the NLB Manager. Edit the image located at C:\\inetpub\\wwwroot\\iisstart.png on either server. Once saved, these edits will now be visible when that server's website is visited at its hostname. Copy the contents of C:\\inetpub\\wwwroot to new directory C:\\nlbport on PLABDM01 . Host that directory as a website on port 6789, opening the firewall appropriately. New-Website -Name nlbport -PhysicalPath \"c:\\nlbport\" -Port 6789 New-NetFirewallRule -DisplayName NLBPort -Protocol TCP -LocalPort 6789 Returning to the NLB Manager, open Cluster Properties > Port Rules and remove the single defined port rule. Add a new port rule that specifies port 80 for \"multiple host\" filtering mode, and another that specifies port 6789 for \"single host\" filtering mode. Then from the host-specific port rule dialog box, select a handling priority of 10. iSCSI Storage Network topology # PLABDC01 New-NetIpAddress 192 . 168 . 0 . 10 -PrefixLength 24 -InterfaceAlias Ethernet2 # PLABDM01 New-NetIpAddress 192 . 168 . 0 . 20 -PrefixLength 24 -InterfaceAlias Ethernet1 New-NetIpAddress 192 . 168 . 1 . 20 -PrefixLength 24 -InterfaceAlias Ethernet2 Set-DnsClientServerAddress -InterfaceAlias Ethernet1 , Ethernet2 -ServerAddresses 192 . 168 . 0 . 1 iSCSI target # PLABDC01 Add-WindowsFeature fs-iscsitarget-server -IncludeManagementTools New-iSCSIVirtualDisk -SizeBytes 5gb -Path 'C:\\CorporateHD.vhdx' New-iSCSIServerTarget -TargetName plabdc01 -InitiatorId @( 'ipaddress:192.168.0.20' , 'ipaddress:192.168.1.20' ) Add-iSCSIVirtualDiskTargetMapping -Path 'C:\\CorporateHD.vhdx' -TargetName plabdc01 iSCSI initiator # PLABDM01 Start-Service msiscsi Get-NetFirewallServiceFilter -Service msiscsi | Get-NetFirewallRule | Enable-NetFirewallRule Connect-iSCSITarget -NodeAddress ( Get-iSCSITarget ). NodeAddress Multipath ... Finally, mount the drive New-Volume -DiskNumber 5 -FriendlyName iSCSI-Volume1 -FileSystem NTFS -DriveLetter R Data deduplication # PLABDM01 Add-WindowsFeature fs-data-deduplication -IncludeManagementTools Get-DedupVolume Get-DedupStatus Enable-DedupVolume -Volume 'D:' Set-DedupVolume -Volume 'D:' -ExcludeFolder 'D:\\win81' , 'D:\\virtual machines' New-DedupSchedule $name -Days Sunday , Monday , Tuesday , Wednesday , Thursday , Friday , Saturday -DurationHours 6 -Enabled $true -Type Optimization -Memory 50 Start-DedupJob D : -Type Optimization -Memory 50 Failover cluster lab Network topology # PLABDM01 New-NetIpAddress 192 . 168 . 0 . 20 -PrefixLength 24 -InterfaceAlias Ethernet1 New-NetIpAddress 192 . 168 . 0 . 21 -PrefixLength 24 -InterfaceAlias \"vEthernet (Internal network 1)\" New-NetIpAddress 192 . 168 . 1 . 20 -PrefixLength 24 -InterfaceAlias Ethernet2 Set-DnsClientServerAddresses Ethernet1 , Ethernet2 -ServerAddresses 192 . 168 . 0 . 1 Disable-NetAdapter \"vEthernet (Internal network 1)\" # PLABSA01 New-NetIpAddress 192 . 168 . 0 . 40 -PrefixLength 24 -InterfaceAlias Ethernet1 New-NetIpAddress 192 . 168 . 0 . 41 -PrefixLength 24 -InterfaceAlias \"vEthernet (Internal network 1)\" New-NetIpAddress 192 . 168 . 1 . 40 -PrefixLength 24 -InterfaceAlias Ethernet2 Set-DnsClientServerAddress Ethernet1 , Ethernet2 -Serveraddresses 192 . 168 . 0 . 1 Disable-NetAdapter \"vEthernet (Internal network 1)\" iSCSI target # PLABDC01 Add-WindowsFeature FS-iSCSITarget-Server -IncludeManagementTools Then go into Server Manager > File and Storage Services > iSCSI and start the New iSCSI Virtual Disk Wizard . Create a fixed 500 MB iSCSI virtual disk on C:\\ named \"QuorumHD\" and assign it to the PLABDC01 iSCSI target server. Add plabdm01.practicelabs.com and plabsa01.practicelabs.com as initiators, and keep authentication disabled, and confirm the disk has been created. Create another virtual disk, 30 GB dynamic on D:\\ names \"CSVHD\", and assign it to the same PLABDC01 iSCSI target server. Configure iSCSI initiators to iSCSI Target Server On PLABDM01 , open Server Manager and click on Tools > iSCSI Initiator and connect to plabdc01.practicelabs.com . Bring disks 5 and 6 online and initialize them with [MBR][MBR] partition style. This can be done within diskmgmt.msc , where the shared volumes that have been created will appear beneath the local disks, or using PowerShell: # PLABDM01 New-Volume -DiskNumber 5 -DriveLetter Q -FileSystem NTFS -FriendlyName 'Quorum' New-Volume -DiskNumber 6 -DriveLetter V -FileSystem NTFS -FriendlyName 'CSV' Only bring the disks online on the other client # PLABSA01 Set-Disk -Number 4 , 5 -IsOffline $false Install failover clusters On both PLABDM01 and PLABSA01 : Install-WindowsFeature failover-clustering -IncludeManagementTools Then validate the installation through the Failover Cluster Manager (cluadmin.msc). Open the Validate Configuration wizard and enter plabdm01.practicelabs.com and plabsa01.practicelabs.com into the list of selected servers. Open the Create Cluster Wizard and create cluster \"PLHYPVCL01\" using those same two nodes, choosing 192.168. 0 .25 and 192.168. 1 .25 for the administrative access points. Open the Configure Cluster Quorum Wizard and configure a disk witness using the Quorum shared volume. Open Hyper-V Manager (virtmgmt.msc) and move PLABDC02 and PLABWIN811 to V:\\PLABDC02 and V:\\PLABWIN811 respectively, specifying the VM's storage . Confirm the move has taken place by inspecting each VM's hard drive in the settings. Add the Virtual Machine role to the Failover Cluster Manager , and select PLABDC02 and PLABWIN811 . This will create the \"PLABWIN811\" Role, which actually includes both selected VMs. Open PLABWIN811 Properties , then the Failover tab, and select \"Allow failback\" specifying between 1 and 2 hours. Open Hyper-V Manager and start PLABDC02 . Wait until it has booted completely, then return to Failover Cluster Manager , open Nodes, right-click on PLABDM01 and then Stop Cluster Service. This option forces a failover of the VM, which drains to PLABSA01 . This can be confirmed by returning to PLABSA01 and opening Hyper-V Manager , where it is revealed that PLABDC02 is running. Scale-Out Fileserver lab Plan network topology # PLABDM01 New-NetIpAddress 192 . 168 . 0 . 20 -PrefixLength 24 -InterfaceAlias Ethernet1 New-NetIpAddress 192 . 168 . 1 . 20 -PrefixLength 24 -InterfaceAlias Ethernet2 Set-DnsClientServerAddress -InterfaceAlias Ethernet1 , Ethernet2 -ServerAddresses 192 . 168 . 0 . 1 Disable-NetAdapter -InterfaceAlias 'vethernet (internal network 1)' # PLABSA01 New-NetIpAddress 192 . 168 . 0 . 40 -PrefixLength 24 -InterfaceAlias Ethernet1 New-NetIpAddress 192 . 168 . 1 . 40 -PrefixLength 24 -InterfaceAlias Ethernet2 Set-DnsClientServerAddress -InterfaceAlias Ethernet1 , Ethernet2 -ServerAddresses 192 . 168 . 0 . 1 Configure iSCSI target # PLABDC01 Install-WindowsFeature fs-iscsitarget-server -IncludeManagementTools New-IscsiVirtualDisk -Fixed -SizeBytes 500mb -Path 'C:\\QuorumHD.vhdx' New-IscsiVirtualDisk -SizeBytes 30gb -Path 'D:\\CSVHD.vhdx' New-IscsiServerTarget -TargetName PLABDC01 -InitiatorId @( 'iqn:iqn.1991-05.com.microsoft:plabsa01.practicelabs.com' , 'iqn:iqn.1991-05.com.microsoft:plabdm01.practicelabs.com' ) Add-IscsiVirtualDiskTargetMapping -Path 'C:\\QuorumHD.vhdx' -TargetName plabdc01 Add-IscsiVirtualDiskTargetMapping -Path 'D:\\CSVHD.vhdx' -TargetName plabdc01 Configure iSCSI initiator on both clients # PLABDM01 and PLABSA01 Start-Service msiscsi Get-NetFirewallServiceFilter -Service msiscsi | Get-NetFirewallRule | Enable-NetFirewallRule Connect-iSCSITarget -NodeAddress ( Get-iSCSITarget ). NodeAddress Install Failover Clustering role on nodes of the cluster # PLABDM01 and PLABSA01 Install-WindowsFeature failover-clustering -IncludeManagementTools Create a failover cluster through the GUI (the Powershell commands for cluster creation appear not to support adding multiple network networks) # PLABDM01 New-Cluster -Name PLABSCL01 -Node PLABDM01 , PLABSA01 -StaticAddress 192 . 168 . 0 . 25 -IgnoreNetwork 192 . 168 . 1 . 0 / 24 Set-ClusterQuorum -DiskWitness 'Cluster Disk 1' Hyper-V storage lab Create a VM with VHDX disks New-VM -Name PLABWIN102 -Generation 1 -MemoryStartupBytes 1536mb -SwitchName 'Private network 1' -NewVHDPath 'C:\\Users\\Public\\Documents\\Hyper-V\\Virtual hard disks\\PLABWIN102.vhdx' -NewVHDSizeBytes 127gb Set-VMDvdDrive -VMName PLABWIN102 -Path C :\\ Users \\ Administrator . PRACTICELABS \\ Documents \\ Eval81 . iso New-VHD -Path 'D:\\Virtual Machines\\PLAB1.vhdx' -Dynamic -SizeBytes 127gb Add-VMHardDiskDrive -VMName PLABWIN102 -ControllerType IDE -ControllerNumber 0 -Path 'D:\\Virtual Machines\\PLAB1.vhdx' Add a differencing disk New-VHD -Path 'D:\\Virtual Machines\\PLAB2.vhdx' -Differencing -ParentPath 'C:\\Users\\Public\\Documents\\Hyper-V\\Virtual hard disks\\PLABWIN102.vhdx' Add-VMHardDrive PLABWIN102 SCSI -Path 'D:\\Virtual Machines\\PLAB2.vhdx' Add a passthrough disk Set-Disk -Number 1 -IsOffline $true Add-VMHardDiskDrive PLABWIN102 IDE 1 -DiskNumber 1 Create a SAN New-VMSan -Name \"PLABS-Fc\" Add-VMFibreChannelHBA -VMName PLABDC02 -SanName \"PLABS-Fc\" Hyper-V replica lab Emphasizes VMReplication cmdlets. # PLABDM01 Enable-NetFirewallRule VIRT-HVRHTTPL-In-TCP-NoScope , VIRT-HVRHTTPSL-In-TCP-NoScope Enable incoming replication on the receiving server #PLABSA01 Enable-NetFirewallRule VIRT-HVRHTTPL-In-TCP-NoScope , VIRT-HVRHTTPSL-In-TCP-NoScope Set-Disk -Number 1 , 3 , 4 -IsOffline $false -IsReadOnly $false New-Volume -DiskNumber 1 -FileSystem NTFS -FriendlyName VMReplica -DriveLetter E Set-VmReplicationServer -ReplicationEnabled $true -AllowedAuthenticationType kerberos -ReplicationAllowedFromAnyServer $true -DefaultStorageLocation E :\\ VHD Configure one VM and initiate initial replication # PLABDM01 Test-VMReplicationConnection PLABDC02 plabsa01 . practicelabs . com 80 Kerberos Enable-VMReplication PLABDC02 plabsa01 . practicelabs . com 80 Kerberos Start-VMInitialReplication PLABDC02 Repeat for another VM Enable-VMReplication PLABWIN811 plabsa01 . practicelabs . com 80 Kerberos Start-VMInitialReplication PLABWIN811 Initiate a planned failover Start-VMFailover PLABDC02 Start-VM PLABDC02 WSUS lab Almost totally GUI configuration of WSUS server after installation # PLABDM01 Add-WindowsFeature updateservices , updateservices-widdb , updateservices-services -IncludeManagementTools mkdir C :\\ updates 'C:\\Program Files\\Update Services\\Tools\\wsusutil.exe' postinstall content_dir = C :\\ updates WSUS (Sec+) # PLABDM01 Set-Disk -Number 1 -IsOffline $false Set-Disk -Number 1 -IsReadOnly $false New-Volume -DiskNumber 1 -Filesystem NTFS - FriendlyName WSUS -DriveLetter E Add-WindowsFeature updateservices , updateservices-widdb , updateservices-services -IncludeManagementTools 'C:\\Program Files\\Update Services\\Tools\\wsusutil.exe' postinstall content_dir = E :\\ updates $wsus = Get-WsusServer $config = $wsus . GetConfiguration () Proxy $config . UseProxy = $true $config . ProxyName = 'proxy' $config . ProxyServerPort = 80 English language $config . AllUpdateLanguagesEnabled = $false $config . SetEnabledUpdateLanguages ( 'en' ) $config . Save () Upstream server is Microsoft Update Set-WsusServerSynchronization \u2013 SyncFromMU Select products and classifications Get-WSUSProduct | Where-Object { $_ . Product . Title -In ( 'Windows 10' , 'Windows 8.1' )} | Set-WSUSProduct Get-WSUSClassification | Where-Object { $_ . Classification . Title -eq 'Critical Updates' } | Set-WSUSClassification Get WSUS Subscription and perform initial synchronization $sub = $wsus . GetSubscription () $sub . StartSynchronization () $sub . SynchronizeAutomatically = $false Storage replica lab Actually from the [Practice test][mu:70-740]. # On both origin and replica servers Install-WindowsFeature storage-replica -Restart Test-SRTopology New-SRGroup -ComputerName server1 New-SRPartnership -SourceComputerName server1 -SourceRGName rg1 -SourceVolumeName D : -SourceLogVolumeName E : -DestinationComputerName server2 -DestinationRGName rg2 -DestinationVolumeName D : -DestinationLogVolumeName E : Certificates ADCSCertificationAuthority Install ? ADCSWebEnrollment [ Install ][Install-AdcsWebEnrollment] [?][msdocs:Install-AdcsWebEnrollment] WindowsFeature Add ? Add-WindowsFeature -Name ADCS-Cert-Authority , ADCS-Web-Enrollment -IncludeManagementTools Install-ADCSCertificationAuthority -CAType EnterpriseRootCA Install-ADCSWebEnrollment Make a duplicate of the \"User\" template named \"SecureUser\", with the following changes - In the Request Handling tab, select \"Prompt the user during enrollment\" - In the Security tab ensure that Authenticated Users has Autoenroll allowed - In the Superseded Templates tab, select User - In the Subject Name tab, clear checkboxes for \"Include e-mail name in subject name\" and \"E-mail name\" Add-CATemplate SecureUser Hyperconverged failover cluster [MeasureUp][mu:70-740] New-Cluster -Name HC-CLU1 -Node node1 , node2 , node3 , node4 -NoStorage Enable-ClusterStorageSpacesDirect -CacheMode Disabled -AutoConfig : 0 -SkipEligibilityChecks New-StoragePool -StorageSubSystemFriendlyName * Cluster * -FriendlyName S2DPool -ProvisioningTyupeDefault Fixed -PhysicalDisk ( Get-PhysicalDisk | Where-Object -Property CanPool -eq $true ) $pool = Get-StoragePool S2DPool New-StorageTier -StoragePoolUniqueID ( $pool ). UniqueID -FriendlyName Performance -MediaType HDD -ResiliencySettingName Mirror New-StorageTier -StoragePoolUniqueID ( $pool ). UniqueID -FriendlyName Capacity -MediaType HDD -ResiliencySettingName Parity New-Volume -StoragePool $pool -FriendlyName SharedVol1 -FileSystem CSVFS_REFS -StorageTierFriendlyNames Performance , Capacity -StorageTierSizes 2gb , 10gb","title":"Labs"},{"location":"Misc/Windows/Labs/#labs","text":"","title":"Labs"},{"location":"Misc/Windows/Labs/#server-core","text":"Configure VM Rename-VMSwitch Intel * External New-VM PLABDMCORE02 1024 1 -SwitchName External -NewVHDPath 'D:\\VHD\\PLABDMCORE02\\Virtual Hard Disks\\PLABDMCORE02.vhdx' -NewVHDSize 127gb Set-VMDvdDrive ... # Can't find way to pass through host drive Z:\\ Add DVD drive Z: of host to VM's DVD drive (no Powershell equivalent found yet). Start-VM PLABDMCORE02 Install Windows, then continue configuring VM # PLABDMCORE02 Rename-Computer PLABDMCORE02 New-NetIpAddress 192 . 168 . 0 . 7 -PrefixLength 24 -InterfaceAlias Ethernet Set-DnsClientServerAddress -InterfaceAlias Ethernet -ServerAddresses 192 . 168 . 0 . 1 Restart-Computer Join computer to domain Add-Computer -DomainName practicelabs . com -DomainCredential practicelabs \\ administrator Restart-Computer Install packages remotely # PLABDC01 Enter-PSSession PLABDMCORE02 Add-WindowsFeature -Name dns , rsat-dns-server -IncludeManagementTools Add PLABDMCORE02 as a DNS server while remotely connected to DC (no Powershell equivalent found yet). Add a secondary lookup zone to PLABDMCORE02 (doesn't work; asks for Zonefile) Add-DnsServerSecondaryZone -Name practicelabs . com -MasterServers 192 . 168 . 0 . 1","title":"Server Core"},{"location":"Misc/Windows/Labs/#nano-server","text":"# PLABDM01 Copy-Item Z :\\ NanoServer \\ NanoServerImageGenerator \\*. ps * C :\\ nanoserver Import-Module C :\\ nNanoserver \\ NanoServerImageGenerator . psm1 Create the VHD image New-NanoServerImage -Edition Standard -MediaPath z :\\ -Basepath c :\\ nanoserver -Targetpath c :\\ nanoserver \\ PLABNANOSRV01 . vhdx -DeploymentType Guest -ComputerName PLABNANOSRV01 -Storage -Package Microsoft-NanoServer-DNS-Package Set-VMHardDiskDrive -VMName PLABNANOSRV01 -Path C :\\ nanoserver \\ PLABNANOSRV01 . vhdx","title":"Nano Server"},{"location":"Misc/Windows/Labs/#dism","text":"# PLABSA01 Set-Location C :\\ mkdir updates , images , mount , drivers Copy-Item \\\\ plabdm01 \\ Win10 \\ sources \\ install . wim C :\\ updates ( Get-Item C :\\ updates \\ install . wim ). IsReadOnly = $false Move specified .msu installers to C:\\updates and uncompress device driver to C:\\drivers Using Powershell: Mount-WindowsImage -Path C :\\ mount -ImagePath C :\\ images \\ install . wim -Index 1 Add-WindowsPackage -Path C :\\ mount -PackagePath C :\\ updates Add-WindowsDriver -Path C :\\ mount -Driver C :\\ drivers \\ display . driver \\ nv_dispi . inf Save-WindowsImage -Path C :\\ mount Dismount-WindowsImage -Path C :\\ mount -Save Using dism.exe : dism /mount-wim /wimfile:c:\\images\\install.wim /index:1 /mountdir:c:\\mount dism /image:c:\\mount /add-package /packagepath:c:\\updates dism /image:c:\\mount /add-driver /driver:c:\\drivers\\display.driver\\nv_dispi.inf dism /commit-wim /mountdir:c:\\mount dism /unmount-wim /commit /mountdir:c:\\mount","title":"DISM"},{"location":"Misc/Windows/Labs/#nlb-lab","text":"Configure a Network Load Balancing cluster. [Practice Lab][pl:70-740] # PLABDM01 and PLABSA01 Install-WindowsFeature web-server , web-webserver , web-mgmt-tools -IncludeManagementTools # PLABDM01 New-NetIpAddress 192 . 168 . 0 . 20 -PrefixLength 24 -InterfaceAlias Ethernet1 # PLABSA01 New-NetIpAddress 192 . 168 . 0 . 40 -PrefixLength 24 -InterfaceAlias Ethernet1 Install-WindowsFeature -Name nlb -IncludeAllSubFeature -IncludeManagementTools Using the Network Load Balancing Manager , create a new cluster named PLAB-NLB1 specifying multicast operation mode at 192.168.0.25, and create an appropriate DNS record on PLABDC01 . Install NLB on PLABDM01 and add it to the cluster through the NLB Manager. Edit the image located at C:\\inetpub\\wwwroot\\iisstart.png on either server. Once saved, these edits will now be visible when that server's website is visited at its hostname. Copy the contents of C:\\inetpub\\wwwroot to new directory C:\\nlbport on PLABDM01 . Host that directory as a website on port 6789, opening the firewall appropriately. New-Website -Name nlbport -PhysicalPath \"c:\\nlbport\" -Port 6789 New-NetFirewallRule -DisplayName NLBPort -Protocol TCP -LocalPort 6789 Returning to the NLB Manager, open Cluster Properties > Port Rules and remove the single defined port rule. Add a new port rule that specifies port 80 for \"multiple host\" filtering mode, and another that specifies port 6789 for \"single host\" filtering mode. Then from the host-specific port rule dialog box, select a handling priority of 10.","title":"NLB lab"},{"location":"Misc/Windows/Labs/#iscsi-storage","text":"Network topology # PLABDC01 New-NetIpAddress 192 . 168 . 0 . 10 -PrefixLength 24 -InterfaceAlias Ethernet2 # PLABDM01 New-NetIpAddress 192 . 168 . 0 . 20 -PrefixLength 24 -InterfaceAlias Ethernet1 New-NetIpAddress 192 . 168 . 1 . 20 -PrefixLength 24 -InterfaceAlias Ethernet2 Set-DnsClientServerAddress -InterfaceAlias Ethernet1 , Ethernet2 -ServerAddresses 192 . 168 . 0 . 1 iSCSI target # PLABDC01 Add-WindowsFeature fs-iscsitarget-server -IncludeManagementTools New-iSCSIVirtualDisk -SizeBytes 5gb -Path 'C:\\CorporateHD.vhdx' New-iSCSIServerTarget -TargetName plabdc01 -InitiatorId @( 'ipaddress:192.168.0.20' , 'ipaddress:192.168.1.20' ) Add-iSCSIVirtualDiskTargetMapping -Path 'C:\\CorporateHD.vhdx' -TargetName plabdc01 iSCSI initiator # PLABDM01 Start-Service msiscsi Get-NetFirewallServiceFilter -Service msiscsi | Get-NetFirewallRule | Enable-NetFirewallRule Connect-iSCSITarget -NodeAddress ( Get-iSCSITarget ). NodeAddress Multipath ... Finally, mount the drive New-Volume -DiskNumber 5 -FriendlyName iSCSI-Volume1 -FileSystem NTFS -DriveLetter R","title":"iSCSI Storage"},{"location":"Misc/Windows/Labs/#data-deduplication","text":"# PLABDM01 Add-WindowsFeature fs-data-deduplication -IncludeManagementTools Get-DedupVolume Get-DedupStatus Enable-DedupVolume -Volume 'D:' Set-DedupVolume -Volume 'D:' -ExcludeFolder 'D:\\win81' , 'D:\\virtual machines' New-DedupSchedule $name -Days Sunday , Monday , Tuesday , Wednesday , Thursday , Friday , Saturday -DurationHours 6 -Enabled $true -Type Optimization -Memory 50 Start-DedupJob D : -Type Optimization -Memory 50","title":"Data deduplication"},{"location":"Misc/Windows/Labs/#failover-cluster-lab","text":"Network topology # PLABDM01 New-NetIpAddress 192 . 168 . 0 . 20 -PrefixLength 24 -InterfaceAlias Ethernet1 New-NetIpAddress 192 . 168 . 0 . 21 -PrefixLength 24 -InterfaceAlias \"vEthernet (Internal network 1)\" New-NetIpAddress 192 . 168 . 1 . 20 -PrefixLength 24 -InterfaceAlias Ethernet2 Set-DnsClientServerAddresses Ethernet1 , Ethernet2 -ServerAddresses 192 . 168 . 0 . 1 Disable-NetAdapter \"vEthernet (Internal network 1)\" # PLABSA01 New-NetIpAddress 192 . 168 . 0 . 40 -PrefixLength 24 -InterfaceAlias Ethernet1 New-NetIpAddress 192 . 168 . 0 . 41 -PrefixLength 24 -InterfaceAlias \"vEthernet (Internal network 1)\" New-NetIpAddress 192 . 168 . 1 . 40 -PrefixLength 24 -InterfaceAlias Ethernet2 Set-DnsClientServerAddress Ethernet1 , Ethernet2 -Serveraddresses 192 . 168 . 0 . 1 Disable-NetAdapter \"vEthernet (Internal network 1)\" iSCSI target # PLABDC01 Add-WindowsFeature FS-iSCSITarget-Server -IncludeManagementTools Then go into Server Manager > File and Storage Services > iSCSI and start the New iSCSI Virtual Disk Wizard . Create a fixed 500 MB iSCSI virtual disk on C:\\ named \"QuorumHD\" and assign it to the PLABDC01 iSCSI target server. Add plabdm01.practicelabs.com and plabsa01.practicelabs.com as initiators, and keep authentication disabled, and confirm the disk has been created. Create another virtual disk, 30 GB dynamic on D:\\ names \"CSVHD\", and assign it to the same PLABDC01 iSCSI target server.","title":"Failover cluster lab"},{"location":"Misc/Windows/Labs/#configure-iscsi-initiators-to-iscsi-target-server","text":"On PLABDM01 , open Server Manager and click on Tools > iSCSI Initiator and connect to plabdc01.practicelabs.com . Bring disks 5 and 6 online and initialize them with [MBR][MBR] partition style. This can be done within diskmgmt.msc , where the shared volumes that have been created will appear beneath the local disks, or using PowerShell: # PLABDM01 New-Volume -DiskNumber 5 -DriveLetter Q -FileSystem NTFS -FriendlyName 'Quorum' New-Volume -DiskNumber 6 -DriveLetter V -FileSystem NTFS -FriendlyName 'CSV' Only bring the disks online on the other client # PLABSA01 Set-Disk -Number 4 , 5 -IsOffline $false","title":"Configure iSCSI initiators to iSCSI Target Server"},{"location":"Misc/Windows/Labs/#install-failover-clusters","text":"On both PLABDM01 and PLABSA01 : Install-WindowsFeature failover-clustering -IncludeManagementTools Then validate the installation through the Failover Cluster Manager (cluadmin.msc). Open the Validate Configuration wizard and enter plabdm01.practicelabs.com and plabsa01.practicelabs.com into the list of selected servers. Open the Create Cluster Wizard and create cluster \"PLHYPVCL01\" using those same two nodes, choosing 192.168. 0 .25 and 192.168. 1 .25 for the administrative access points. Open the Configure Cluster Quorum Wizard and configure a disk witness using the Quorum shared volume. Open Hyper-V Manager (virtmgmt.msc) and move PLABDC02 and PLABWIN811 to V:\\PLABDC02 and V:\\PLABWIN811 respectively, specifying the VM's storage . Confirm the move has taken place by inspecting each VM's hard drive in the settings. Add the Virtual Machine role to the Failover Cluster Manager , and select PLABDC02 and PLABWIN811 . This will create the \"PLABWIN811\" Role, which actually includes both selected VMs. Open PLABWIN811 Properties , then the Failover tab, and select \"Allow failback\" specifying between 1 and 2 hours. Open Hyper-V Manager and start PLABDC02 . Wait until it has booted completely, then return to Failover Cluster Manager , open Nodes, right-click on PLABDM01 and then Stop Cluster Service. This option forces a failover of the VM, which drains to PLABSA01 . This can be confirmed by returning to PLABSA01 and opening Hyper-V Manager , where it is revealed that PLABDC02 is running.","title":"Install failover clusters"},{"location":"Misc/Windows/Labs/#scale-out-fileserver-lab","text":"Plan network topology # PLABDM01 New-NetIpAddress 192 . 168 . 0 . 20 -PrefixLength 24 -InterfaceAlias Ethernet1 New-NetIpAddress 192 . 168 . 1 . 20 -PrefixLength 24 -InterfaceAlias Ethernet2 Set-DnsClientServerAddress -InterfaceAlias Ethernet1 , Ethernet2 -ServerAddresses 192 . 168 . 0 . 1 Disable-NetAdapter -InterfaceAlias 'vethernet (internal network 1)' # PLABSA01 New-NetIpAddress 192 . 168 . 0 . 40 -PrefixLength 24 -InterfaceAlias Ethernet1 New-NetIpAddress 192 . 168 . 1 . 40 -PrefixLength 24 -InterfaceAlias Ethernet2 Set-DnsClientServerAddress -InterfaceAlias Ethernet1 , Ethernet2 -ServerAddresses 192 . 168 . 0 . 1 Configure iSCSI target # PLABDC01 Install-WindowsFeature fs-iscsitarget-server -IncludeManagementTools New-IscsiVirtualDisk -Fixed -SizeBytes 500mb -Path 'C:\\QuorumHD.vhdx' New-IscsiVirtualDisk -SizeBytes 30gb -Path 'D:\\CSVHD.vhdx' New-IscsiServerTarget -TargetName PLABDC01 -InitiatorId @( 'iqn:iqn.1991-05.com.microsoft:plabsa01.practicelabs.com' , 'iqn:iqn.1991-05.com.microsoft:plabdm01.practicelabs.com' ) Add-IscsiVirtualDiskTargetMapping -Path 'C:\\QuorumHD.vhdx' -TargetName plabdc01 Add-IscsiVirtualDiskTargetMapping -Path 'D:\\CSVHD.vhdx' -TargetName plabdc01 Configure iSCSI initiator on both clients # PLABDM01 and PLABSA01 Start-Service msiscsi Get-NetFirewallServiceFilter -Service msiscsi | Get-NetFirewallRule | Enable-NetFirewallRule Connect-iSCSITarget -NodeAddress ( Get-iSCSITarget ). NodeAddress Install Failover Clustering role on nodes of the cluster # PLABDM01 and PLABSA01 Install-WindowsFeature failover-clustering -IncludeManagementTools Create a failover cluster through the GUI (the Powershell commands for cluster creation appear not to support adding multiple network networks) # PLABDM01 New-Cluster -Name PLABSCL01 -Node PLABDM01 , PLABSA01 -StaticAddress 192 . 168 . 0 . 25 -IgnoreNetwork 192 . 168 . 1 . 0 / 24 Set-ClusterQuorum -DiskWitness 'Cluster Disk 1'","title":"Scale-Out Fileserver lab"},{"location":"Misc/Windows/Labs/#hyper-v-storage-lab","text":"Create a VM with VHDX disks New-VM -Name PLABWIN102 -Generation 1 -MemoryStartupBytes 1536mb -SwitchName 'Private network 1' -NewVHDPath 'C:\\Users\\Public\\Documents\\Hyper-V\\Virtual hard disks\\PLABWIN102.vhdx' -NewVHDSizeBytes 127gb Set-VMDvdDrive -VMName PLABWIN102 -Path C :\\ Users \\ Administrator . PRACTICELABS \\ Documents \\ Eval81 . iso New-VHD -Path 'D:\\Virtual Machines\\PLAB1.vhdx' -Dynamic -SizeBytes 127gb Add-VMHardDiskDrive -VMName PLABWIN102 -ControllerType IDE -ControllerNumber 0 -Path 'D:\\Virtual Machines\\PLAB1.vhdx' Add a differencing disk New-VHD -Path 'D:\\Virtual Machines\\PLAB2.vhdx' -Differencing -ParentPath 'C:\\Users\\Public\\Documents\\Hyper-V\\Virtual hard disks\\PLABWIN102.vhdx' Add-VMHardDrive PLABWIN102 SCSI -Path 'D:\\Virtual Machines\\PLAB2.vhdx' Add a passthrough disk Set-Disk -Number 1 -IsOffline $true Add-VMHardDiskDrive PLABWIN102 IDE 1 -DiskNumber 1 Create a SAN New-VMSan -Name \"PLABS-Fc\" Add-VMFibreChannelHBA -VMName PLABDC02 -SanName \"PLABS-Fc\"","title":"Hyper-V storage lab"},{"location":"Misc/Windows/Labs/#hyper-v-replica-lab","text":"Emphasizes VMReplication cmdlets. # PLABDM01 Enable-NetFirewallRule VIRT-HVRHTTPL-In-TCP-NoScope , VIRT-HVRHTTPSL-In-TCP-NoScope Enable incoming replication on the receiving server #PLABSA01 Enable-NetFirewallRule VIRT-HVRHTTPL-In-TCP-NoScope , VIRT-HVRHTTPSL-In-TCP-NoScope Set-Disk -Number 1 , 3 , 4 -IsOffline $false -IsReadOnly $false New-Volume -DiskNumber 1 -FileSystem NTFS -FriendlyName VMReplica -DriveLetter E Set-VmReplicationServer -ReplicationEnabled $true -AllowedAuthenticationType kerberos -ReplicationAllowedFromAnyServer $true -DefaultStorageLocation E :\\ VHD Configure one VM and initiate initial replication # PLABDM01 Test-VMReplicationConnection PLABDC02 plabsa01 . practicelabs . com 80 Kerberos Enable-VMReplication PLABDC02 plabsa01 . practicelabs . com 80 Kerberos Start-VMInitialReplication PLABDC02 Repeat for another VM Enable-VMReplication PLABWIN811 plabsa01 . practicelabs . com 80 Kerberos Start-VMInitialReplication PLABWIN811 Initiate a planned failover Start-VMFailover PLABDC02 Start-VM PLABDC02","title":"Hyper-V replica lab"},{"location":"Misc/Windows/Labs/#wsus-lab","text":"Almost totally GUI configuration of WSUS server after installation # PLABDM01 Add-WindowsFeature updateservices , updateservices-widdb , updateservices-services -IncludeManagementTools mkdir C :\\ updates 'C:\\Program Files\\Update Services\\Tools\\wsusutil.exe' postinstall content_dir = C :\\ updates","title":"WSUS lab"},{"location":"Misc/Windows/Labs/#wsus-sec","text":"# PLABDM01 Set-Disk -Number 1 -IsOffline $false Set-Disk -Number 1 -IsReadOnly $false New-Volume -DiskNumber 1 -Filesystem NTFS - FriendlyName WSUS -DriveLetter E Add-WindowsFeature updateservices , updateservices-widdb , updateservices-services -IncludeManagementTools 'C:\\Program Files\\Update Services\\Tools\\wsusutil.exe' postinstall content_dir = E :\\ updates $wsus = Get-WsusServer $config = $wsus . GetConfiguration () Proxy $config . UseProxy = $true $config . ProxyName = 'proxy' $config . ProxyServerPort = 80 English language $config . AllUpdateLanguagesEnabled = $false $config . SetEnabledUpdateLanguages ( 'en' ) $config . Save () Upstream server is Microsoft Update Set-WsusServerSynchronization \u2013 SyncFromMU Select products and classifications Get-WSUSProduct | Where-Object { $_ . Product . Title -In ( 'Windows 10' , 'Windows 8.1' )} | Set-WSUSProduct Get-WSUSClassification | Where-Object { $_ . Classification . Title -eq 'Critical Updates' } | Set-WSUSClassification Get WSUS Subscription and perform initial synchronization $sub = $wsus . GetSubscription () $sub . StartSynchronization () $sub . SynchronizeAutomatically = $false","title":"WSUS (Sec+)"},{"location":"Misc/Windows/Labs/#storage-replica-lab","text":"Actually from the [Practice test][mu:70-740]. # On both origin and replica servers Install-WindowsFeature storage-replica -Restart Test-SRTopology New-SRGroup -ComputerName server1 New-SRPartnership -SourceComputerName server1 -SourceRGName rg1 -SourceVolumeName D : -SourceLogVolumeName E : -DestinationComputerName server2 -DestinationRGName rg2 -DestinationVolumeName D : -DestinationLogVolumeName E :","title":"Storage replica lab"},{"location":"Misc/Windows/Labs/#certificates","text":"ADCSCertificationAuthority Install ? ADCSWebEnrollment [ Install ][Install-AdcsWebEnrollment] [?][msdocs:Install-AdcsWebEnrollment] WindowsFeature Add ? Add-WindowsFeature -Name ADCS-Cert-Authority , ADCS-Web-Enrollment -IncludeManagementTools Install-ADCSCertificationAuthority -CAType EnterpriseRootCA Install-ADCSWebEnrollment Make a duplicate of the \"User\" template named \"SecureUser\", with the following changes - In the Request Handling tab, select \"Prompt the user during enrollment\" - In the Security tab ensure that Authenticated Users has Autoenroll allowed - In the Superseded Templates tab, select User - In the Subject Name tab, clear checkboxes for \"Include e-mail name in subject name\" and \"E-mail name\" Add-CATemplate SecureUser","title":"Certificates"},{"location":"Misc/Windows/Labs/#hyperconverged-failover-cluster","text":"[MeasureUp][mu:70-740] New-Cluster -Name HC-CLU1 -Node node1 , node2 , node3 , node4 -NoStorage Enable-ClusterStorageSpacesDirect -CacheMode Disabled -AutoConfig : 0 -SkipEligibilityChecks New-StoragePool -StorageSubSystemFriendlyName * Cluster * -FriendlyName S2DPool -ProvisioningTyupeDefault Fixed -PhysicalDisk ( Get-PhysicalDisk | Where-Object -Property CanPool -eq $true ) $pool = Get-StoragePool S2DPool New-StorageTier -StoragePoolUniqueID ( $pool ). UniqueID -FriendlyName Performance -MediaType HDD -ResiliencySettingName Mirror New-StorageTier -StoragePoolUniqueID ( $pool ). UniqueID -FriendlyName Capacity -MediaType HDD -ResiliencySettingName Parity New-Volume -StoragePool $pool -FriendlyName SharedVol1 -FileSystem CSVFS_REFS -StorageTierFriendlyNames Performance , Capacity -StorageTierSizes 2gb , 10gb","title":"Hyperconverged failover cluster"},{"location":"Misc/Windows/PowerShell/","text":"\ud83d\udc1a PowerShell Control flow if switch while do while if ( $condition ) { # ... } switch ( $reference ) { $value1 { ... } $value2 { ... } } $Values = while ( $true ) { (++ $Tick ) if ( $Tick -gt 2 ) { break } } # => @(1,2,3) $Values = do { 'Hello, world!' } while ( $false ) # => @('Hello, world!') Loops are implemented with ForEach-Object . 1 .. 5 | ForEach -Object { $_ + 2 } # => @(3,4,5,6,7) When values are stored in a variable at the end of a pipeline, it will create an array. while and do while loops are available, as well as until and do until loops which operate so long as their condition is false . Variables Variables are accessed by prefixing the identifier with $ . Automatic variables ( PSVersionTable , $IsLinux , etc) are PowerShell-specific. Windows environment variables are actually accessed through the Env virtual drive the syntax $Env:VARIABLE . APPDATA LOCALAPPDATA USERNAME USERPROFILE WINDIR Typing Variables can be typed by preceding their identifier with the datatype in brackets [double] $Price [int] $Quantity [string] $Description Compatible data can be cast or converted by simply specifying the type in an assignment, but when the data cannot be converted the interpreter will throw an error. $Number = [int] '04' $FailedCast = [int] 'Hello' Filtering results can be done with 5 commands: Where-Object (aliased to where and ? ): the most commonly used such command Select-Object (aliased to sc ed to specify specific columns of information to be displayed Select-String (aliased to sls ) ForEach-Object (aliased to foreach and % ) There are two different ways to construct a ForEach-Object statement: Script block , within which the variable $_ represents the current object Operation statement , more naturalistic, where you specify a property value or call a method. Hashtable Build a hash table using literals or the Add method Literal Literal (inline) Add $fruit = @{ Apple = 'red' Orange = 'orange' Eggplant = 'purple' } $fruit = @{ Apple = 'red' ; Orange = 'orange' ; Eggplant = 'purple' } $fruit = @{} $fruit . Add ( 'Apple' , 'red' ) $fruit . Add ( 'Orange' , 'orange' ) $fruit . Add ( 'Kiwi' , 'green' ) Hashtable methods Add Clone Keys Values Count Remove $fruit = @{} $fruit . Add ( 'Apple' , 'red' ) $fruit . Add ( 'Orange' , 'orange' ) $fruit . Add ( 'Kiwi' , 'green' ) # Deep copy or \"clone\" of a hashtable. $fruitclone = $fruit . Clone () $fruit . Keys # => @('Apple','Orange','Kiwi') $fruit . Values # => @('red','orange','green') $fruit . Count $fruit . Remove ( 'One' ) Unlike Python, a hash table can be made ordered, changing its data type: $fruit = [ordered] @{ Apple = 'red' ; Orange = 'orange' ; Eggplant = 'purple' } $fruit . GetType (). Name # => OrderedDictionary Documentation Single-line comments are preceded by # and block quotes are enclosed between <# and #> . Such a block comment will be parsed by PowerShell when running Get-Help . <# .SYNOPSIS This script coordinates the process of creating new employees .DESCRIPTION This script creates new users in Active Directory... .PARAMETER UserName The official logon name for the user... .PARAMETER HomeServer The server name where the user's home folder will live... #> <# .EXAMPLE New-CorpEmployee -UserName John-Doe -HomeServer HOMESERVER1 This example creates a single new employee... #> Functions Functions are declared with the function keyword and the body in braces, following this syntax: function Verb-Noun { # ... } Positional parameters can be referenced using the $args array, which contains all arguments passed to the function on invocation. Named parameters can be declared in one of two ways. Within the function body using the param keyword, followed by the name of the variable representing the parameter's value, enclosed in $(...) : Directly after the function name in parentheses, with each parameter separated by a comma. The name of the variable becomes the named parameter used when invoking the function. Default values for parameters can be specified by placing them within the parentheses. Parameters can be made mandatory by preceding the parameter name with [Parameter(Mandatory=$true)] . Parameters can be static typed by preceding the parameter identifier with the data type in brackets. Positional parameter Named parameter Using param Default value Typed Invocation function Get-LargeFiles { Get-ChildItem C :\\ Users \\ Michael \\ Documents | where { $_ . Length -gt $args [ 0 ] and ! $_PSIscontainer } | Sort-Object Length -Descending } function Get-LargeFiles ( $Size ) { # param ($Size) Get-ChildItem C :\\ Users \\ Michael \\ Documents | where { $_ . Length -gt $Size -and ! $_ . PSIsContainer } | Sort-Object Length -Descending } function Get-LargeFiles { param ( $Size ) Get-ChildItem C :\\ Users \\ Michael \\ Documents | where { $_ . Length -gt $Size -and ! $_ . PSIsContainer } | Sort-Object Length -Descending } function Get-LargeFiles { param ( $Size = 2000 ) Get-ChildItem C :\\ Users \\ Michael \\ Documents | where { $_ . Length -gt $Size -and ! $_ . PSIsContainer } | Sort-Object Length -Descending } function Get-LargeFiles { param ( [int] $Size = 2000 ) Get-ChildItem C :\\ Users \\ Michael \\ Documents | where { $_ . Length -gt $Size -and ! $_ . PSIsContainer } | Sort-Object Length -Descending } Get-LargeFiles -Size 2000 Switch parameters are typed as a switch data type. Boolean values can be explicitly set upon invocation using this syntax: Switch-Item Invocation function Switch -Item { param ( [switch] $on ) if ( $on ) { \"Switch on\" } else { \"Switch off\" } } Switch -Item # => Switch off Switch -Item -On # => Switch on Switch -Item -On : $false # => Switch off Attach common parameters to a custom function by placing the [CmdletBinding()] within the body of a function. This allows use of options like -Verbose or -Debug with custom functions. Now, using Write-Verbose and Write-Debug within the function body serve the dual purpose of outputting additional information at the time of execution, when needed, as well as documentation. Remote administration Powershell remoting can be done explicitly or implicitly . Remoting relies on WinRM , which is Microsoft's implementation of WSMAN. Explicit remoting is also 1-to-1 remoting, where an interactive Powershell prompt is brought up on a remote computer. One-to-many or fan-out remoting is possible with implicit remoting, where a command is transmitted to many computers. Testing Pester tests are organized in a hierarchy of blocks and run with Invoke-Pester : Describe { Context # optional { It { Should # assertion statements accept a value passed in via pipe and **must** be called within a `Describe` block } } } New-Fixture deploy Foo function Foo { # ... } Describe 'Foo' { $true | Should -Be $true } The block in braces is actually an argument pass to the -Fixture parameter. Describe \"Best airports in the USA\" -Fixture { It -Name \"RDU is one of the best airports\" -Test { $Output = Get-Airport -City \"Raleigh\" $Output | Should -BeOfType System . Collections . Hashtable } } Tasks Download files URIs and filenames are built up as formatted strings 1 .. 24 | ForEach -Object { Invoke-WebRequest -OutFile ( \"TGC_3466_Lect{0:d2}_FallPagansOriginsMedievalChristianity.m4v\" -f $_ ) ( \"https://securedownloads.teach12.com/anon.eastbaymedia-drm/courses/3466/m4v/TGC_3466_Lect{0:d2}_FallPagansOriginsMedievalChristianity.m4v?userid=$USERID&orderid=$ORDERID&courseid=$COURSEID&FName=TGC_3466_Lect{0:d2}_FallPagansOriginsMedievalChristianity\" -f $_ )} Display computer name Cmdlet $Env Get-ComputerInfo -Property CsName gin . CsName $Env:computername Generate password Generate a random password 20 characters long ( src ) Add-Type -AssemblyName 'System.Web' [System.Web.Security.Membership] :: GeneratePassword ( 20 , 3 ) Store credential Interactive Cmdlet $cred = Get-Credential $pw = ConvertTo-SecureString \"Password\" -AsPlainText -Force $cred = New-Object System . Management . Automation . PSCredential ( \"FullerP\" , $pw ) Create a new file in the current working directory named filename New-Item -ItemType File -Name filename Append content to file Add-Content C :\\ path \\ to \\ file $content New domain controller [Jones][Jones] Install-WindowsFeature AD-Domain-Services , DHCP -IncludeManagementTools Install-ADDSForest -DomainName corp . packtlab . com Add-DhcpServerv4Scope -Name \"PacktLabNet\" -StartRange 10 . 0 . 0 . 50 -EndRange 10 . 0 . 0 . 100 -SubnetMask 255 . 255 . 255 . 0 Set-DhcpServerv4OptionValue -DnsDomain corp . packtlab . com Add-DhcpServerInDC -DnsName dc . corp . packtlab . com New-AdUser -SamAccountName SysAdmin -AccountPassword ( Read-Host \"Set user password\" -AsSecureString ) -Name \"SysAdmin\" -Enabled $true -PasswordNeverExpires $true -ChangePasswordAtLogon $false Add-ADPrincipalGroupMembership -Identity \"CN=SysAdmin,CN=Users,DC=corp,DC=packtlab,DC=com\" , \"CN=Domain Admins,CN=Users,DC=corp,DC=packtlab,DC=com\" Get-ADPrincipalGroupMembership sysadmin Text-to-speech Initialize text-to-speech object scriptinglibrary.com Add-Type \u2013 AssemblyName System . Speech $tts = New-Object \u2013 TypeName System . Speech . Synthesis . SpeechSynthesizer $tts . Speak ( 'Hello, World!' ) List available voices Foreach ( $voice in $SpeechSynthesizer . GetInstalledVoices ()){ $Voice . VoiceInfo | Select-Object Gender , Name , Culture , Description } Change voice $tts . SelectVoice ( \"Microsoft Zira Desktop\" ) $tts . Speak ( 'Hello, World!' ) Set output to WAV file thinkpowershell.com $WavFileOut = Join-Path -Path $env:USERPROFILE -ChildPath \"Desktop\\thinkpowershell-demo.wav\" $SpeechSynthesizer . SetOutputToWaveFile ( $WavFileOut ) VHDX file Create a new 256 GB dynamic VHDX file, mount it, initialize it, and create and format the partition [Zacker][Zacker]: 91 New-VHD -Path C :\\ Data \\ disk1 . vhdx -SizeBytes 256GB -Dynamic | Mount-VHD -Passthru | Initialize-Disk -PassThru | New-Partition -DriveLetter X -UseMaximumSize | Format-Volume -Filesystem ntfs -FileSystemLabel data1 -Confirm : $False -Force Restart Wi-Fi adapter $adaptor = Get-WmiObject -Class Win32_NetworkAdapter | Where-Object { $_ . Name -like \"*Wireless*\" } $adaptor . Disable () $adaptor . Enable () Add a member to a group Add-ADGroupMember -Identity $group -Members $user1 , $user2 Add a new local admin nlu ansible Add-LocalGroupMember Administrators ansible Configure secure remoting using a self-signed certificate Create a virtual switch with SET enabled Create a virtual switch with SET enabled. [Zacker][Zacker]: 254 New-VMSwitch -Name SETSwitch -NetAdapterName \"nic1\" , \"nic2\" -EnableEmbeddedTeaming $true Add new virtual network adapters to VMs Add-VMNetworkAdapter -VMName server1 -SwitchName setswitch -Name set1 Enable RDMA with [ Get- ][Get-NetAdapterRdma] and [ Enable-NetAdapterRdma ][Enable-NetAdapterRdma]. Implement nested virtualization Both the physical host and the nested virtual host must be running Windows Server 2016, but before installing Hyper-V on the nested host, the following configurations must be made. [Zacker][Zacker]: 181 Provide nested host's processor with access to virtualization technology on the physical host Set-VMProcessor -VMName server1 -ExposeVirtualizationExtensions $true Disable dynamic memory Set-VMMemory -VMName SRV01 -DynamicMemoryEnabled $false Configure 2 virtual processors Set-VMProcessor -VMName SVR01 -Count 2 Turn on MAC address spoofing Set-VMNetworkAdapter -VMName SVR01 -Name \"NetworkAdapter\" -MACAddressSpoofing On Enable CredSSP On the remote (managed) server [Zacker][Zacker]: 176 Enable-PSRemoting Enable-WSManCredSSP Add the fully-qualified domain name of the Hyper-V server to be managed to the local system's WSMan trusted hosts list Set-Item WSMan :\\ localhost \\ client \\ trustedhosts -Value \"hypervserver.domain.com\" Enable the use of CredSSP on the client Enable-WSManCredSSP -Role client -DelegateComputer \"hypervserver.domain.com\" Configure Server Core Manually configure network interface, if a DHCP server is unavailable [Zacker][Zacker]: 19 New-NetIPAddress 10 . 0 . 0 . 3 -InterfaceAlias \"Ethernet' -PrefixLength 24 Configure the DNS server addresses for the adapter Set-DnsClientServerAddress -InterfaceIndex 6 -ServerAddresses ( \"192.168.0.1\" , \"192.168.0.2\" ) Rename the computer and join it to a domain Add-Computer -DomainName adatum . com -NewName Server8 -Credential adatum \\ administrator Update Server Core image Mount-WindowsImage -ImagePath .\\ CoreServer . vhdx -Path .\\ MountDir -Index 1 Add-WindowsPackage -Path .\\ MountDir -PackagePath C :\\ ServicingPackages_cabs Dismount-WindowsImage -Path .\\ MountDir -Save Implement DDA Discrete Device Assignment (DDA) begins with finding the Instance ID of the device needed to be passed through. [Zacker][Zacker]: 212 Get-PnpDevice -PresentOnly Disable-PnpDevice -InstanceId # Remove host-installed drivers Get-PnpDeviceProperty # Provide `InstanceId` and `KeyName` values in order to get value for `LocationPath` parameter in next command Dismount-VmHostAssignableDevice -LocationPath # Remove the device from host control Add-VMAssignableDevice -VM -LocationPath # Attach the device to a guest Configure live migration Live migration is possible between Hyper-V hosts that are not clustered, but they must be within the same (or trusted) domains. [Zacker][Zacker]: 306 Enable-VMMigration Set-VMMigrationNetwork 192 . 168 . 4 . 0 Set-VMHost -VirtualMachineMigrationAuthenticationType Kerberos Set-VMHost -VirtualMachineMigrationPerformanceOption smbtransport Configure S2D cluster New-Cluster -Name cluster1 -node server1 , server2 , server3 , server4 -NoStorage Enable-ClusterStorageSpacesDirect Install Docker Enterprise [Zacker][Zacker]: 266 Install-Module -Name dockermsftprovider -repository psgallery -force Install-Package -Name docker -ProviderName dockermsftprovider Handle XML files Find a sample XML file here Assign the output of [ gc ][Get-Content] to a variable [xml] $xdoc = gc $xmlfile The XML tree can be viewed in VS Code using the XML Tools extension. The object itself can be treated as a first-class Powershell object using dot notation. red-gate.com $xdoc . catalog . book | Format-Table -Autosize Arrays of elements can be accessed by their index $xdoc . catalog . book [ 0 ] Nodes in the XML object can also be navigated using XPath notation with the SelectNodes and SelectSingleNode methods. $xdoc . SelectNodes ( '//author' ) This produces the same output as the command above ( in XPath nodes are 1-indexed ). ``` powershell $xdoc . SelectSingleNode ( '//book[1]' ) [ Select-Xml ][Select-Xml] wraps the returned XML node with additional metadata, including the pattern searched. However, it can accept piped input. ( Select-Xml -Xml $xdoc -Xpath '//book[1]' ). Node ( $xml | Select-Xml -Xpath '//book[1]' ). Node Update Server Core images [MeasureUp Lab][pl:70-740] Mount-WindowsImage -ImagePath .\\ CoreServer . vhdx -Path .\\ MountDir -Index 1 Add-WindowsPackage -Path .\\ MountDir PackagePath C :\\ ServicingPackages_cabs Dismount-WindowsImage -Path .\\ MountDir -Save Pass-through disk [Zacker][Zacker]: 226 Set-Disk -Number 2 -IsOffline $true Add-VMHardDiskDrive -VMName server1 -ControllerType scsi -DiskNumber 2 Site-aware failover cluster Configure failover clusters for two offices [Zacker][Zacker]: 366 New-ClusterFaultDomain -Name ny -Type site -Description \"Primary\" -Location \"New York, NY\" New-ClusterFaultDomain -Name sf -Type site -Description \"Secondary\" -Location \"San Francisco, CA\" Set-ClusterFaultDomain -Name node1 -Parent ny Set-ClusterFaultDomain -Name node2 -Parent ny Set-ClusterFaultDomain -Name node3 -Parent sf Set-ClusterFaultDomain -Name node4 -Parent sf Filter AD account information Get-aduser -filter {( SamAccountName -like \"*CA0*\" )} -properties Displayname , SaMaccountName , Enabled , EmailAddress , proxyaddresses | Where {( $_ . EmailAddress -notlike \"*@*\" )} | Where {( $_ . Enabled -eq $True )} | Select Displayname , SaMaccountName , Enabled , EmailAddress , @{ L = \u2019 ProxyAddress_1 '; E={$_.proxyaddresses[0]}}, @{L=\u2019ProxyAddress_2' ; E ={ $_ . ProxyAddresses [ 1 ]}} | Export-csv .\\ usersnoemail2 . csv -notypeinformation Create VM with installation media [Practice Lab][pl:70-740] New-VM PLABWIN102 1536mb 1 -SwitchName 'Private network 1' -NewVHDPath 'C:\\Users\\Public\\Documents\\Hyper-V\\Virtual hard disks\\PLABWIN102.vhdx' -NewVHDSizeBytes 127gb Set-VMDvdDrive -VMName PLABWIN102 -Path C :\\ Users \\ Administrator . PRACTICELABS \\ Documents \\ Eval81 . iso Registry Description Affected key Fix Windows Search bar docs.microsoft.com HKCU:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Search Remove 3D Objects howtogeek.com HKLM:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Explorer\\MyComputer\\NameSpace Display seconds in system clock howtogeek.com HKCU:\\Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\Advanced Disable Aero Shake howtogeek.com HKCU:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Explorer\\Advanced Remove 3D Objects from This PC howtogeek.com Remove-Item 'HKLM:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Explorer\\MyComputer\\NameSpace\\{0DB7E03F-FC29-4DC6-9020-FF41B59E513A}' Add seconds to clock howtogeek.com New-Item -Path HKCU :\\ Software \\ Microsoft \\ Windows \\ CurrentVersion \\ Explorer \\ Advanced -Name ShowSecondsInSystemClock -Value 1 Restart-Computer New-Item -Path HKCU :\\ SOFTWARE \\ Microsoft \\ Windows \\ CurrentVersion \\ Search -Name BingSearchEnabled -Value 0 New-Item -Path HKCU :\\ SOFTWARE \\ Microsoft \\ Windows \\ CurrentVersion \\ Search -Name CortanaConsent -Value 0 Safely combine related registry modifications using [ Start-Transaction ][Start-Transaction] and [ Complete-Transaction ][Complete-Transaction] [Holmes][Holmes]: 604 Start-Transaction New-Item TempKey -UseTransaction Complete-Transaction Remove User UAC for local users. ref Set-ItemProperty -Path HKLM :\\ Software \\ Microsoft \\ Windows \\ CurrentVersion \\ Policies \\ System -Name EnableLUA -Value 0 WinForms Pastebin # Load required assemblies [void] [System.Reflection.Assembly] :: LoadWithPartialName ( \"System.Windows.Forms\" ) # Drawing form and controls $Form_HelloWorld = New-Object System . Windows . Forms . Form $Form_HelloWorld . Text = \"Hello World\" $Form_HelloWorld . Size = New-Object System . Drawing . Size ( 272 , 160 ) $Form_HelloWorld . FormBorderStyle = \"FixedDialog\" $Form_HelloWorld . TopMost = $true $Form_HelloWorld . MaximizeBox = $false $Form_HelloWorld . MinimizeBox = $false $Form_HelloWorld . ControlBox = $true $Form_HelloWorld . StartPosition = \"CenterScreen\" $Form_HelloWorld . Font = \"Segoe UI\" # adding a label to my form $label_HelloWorld = New-Object System . Windows . Forms . Label $label_HelloWorld . Location = New-Object System . Drawing . Size ( 8 , 8 ) $label_HelloWorld . Size = New-Object System . Drawing . Size ( 240 , 32 ) $label_HelloWorld . TextAlign = \"MiddleCenter\" $label_HelloWorld . Text = \"Hello World\" $Form_HelloWorld . Controls . Add ( $label_HelloWorld ) # add a button $button_ClickMe = New-Object System . Windows . Forms . Button $button_ClickMe . Location = New-Object System . Drawing . Size ( 8 , 80 ) $button_ClickMe . Size = New-Object System . Drawing . Size ( 240 , 32 ) $button_ClickMe . TextAlign = \"MiddleCenter\" $button_ClickMe . Text = \"Click Me!\" $button_ClickMe . Add_Click ({ $button_ClickMe . Text = \"You did click me!\" Start-Process calc . exe }) $Form_HelloWorld . Controls . Add ( $button_ClickMe ) # show form $Form_HelloWorld . Add_Shown ({ $Form_HelloWorld . Activate ()}) [void] $Form_HelloWorld . ShowDialog () Modules Create a new module by placing a .psm1 file in a directory of the same name .\\Starship\\Starship.psm1 Functions defined within the module can be loaded with [ Import-Module ][Import-Module] (execution policy must allow this). ipmo .\\ Starship To import classes, a different syntax must be used source Using module .\\ Starship Sample enumeration PowerShellMagazine Add-Type -AssemblyName System . Drawing $count = [Enum] :: GetValues ( [System.Drawing.KnownColor] ). Count [System.Drawing.KnownColor] ( Get-Random -Minimum 1 -Maximum $count ) Migrate a VM Enable-VMMigration Set-VMMigrationNetwork 192 . 168 . 10 . 1 Set-VMHost -VirtualMachineMigrationAuthenticationType Kerberos Set-VMHost -VirtualMachineMigrationPerformanceOption SMBTransport Storage Spaces Direct [MeasureUp][mu:70-740] New-Cluster -Name HC-CLU1 -Node node1 , node2 , node3 , node4 -NoStorage Enable-ClusterStorageSpacesDirect -CacheMode Disabled -AutoConfig : 0 -SkipEligibilityChecks New-StoragePool -StorageSubSystemFriendlyName * Cluster * -FriendlyName S2DPool -ProvisioningTypeDefault Fixed -PhysicalDisk ( Get-PhysicalDisk | Where-Object -Property CanPool -eq $true ) $pool = Get-StoragePool S2DPool New-StorageTier -StoragePoolUniqueID ( $pool ). UniqueID -FriendlyName Performance -MediaType HDD -ResiliencySettingName Mirror New-StorageTier -StoragePoolUniqueID ( $pool ). UniqueID -FriendlyName Capacity -MediaType HDD -ResiliencySettingName Parity The next step would be the creation of a new volume New-Volume -StoragePool $pool -FriendlyName SharedVol1 -FileSystem CSVFS_REFS -StorageTiersFriendlyNames Performance , Capacity -StorageTierSizes 2GB , 10GB Scheduled task Automatically run SSH server in WSL on system start $action = New-ScheduledTaskAction -Execute C :\\ WINDOWS \\ System32 \\ bash . exe -Argument '-c sudo service ssh start' $trigger = New-ScheduledTaskTrigger -AtLogon Register-ScheduledTask -TaskName 'SSH server' -Trigger $trigger -Action $action Network connection alert Play a tone when network connection has been (re)-established. while ( $true ) { if (( Test-NetConnection 8 . 8 . 8 . 8 -WarningAction SilentlyContinue ). PingSucceeded -eq $true ) { [System.Console] :: Beep ( 1000 , 100 ) break } } while ( Test-NetConnection 8 . 8 . 8 . 8 -WarningAction SilentlyContinue ). PingSucceeded -eq $false ) { continue }","title":"\ud83d\udc1a PowerShell"},{"location":"Misc/Windows/PowerShell/#powershell","text":"","title":"\ud83d\udc1a PowerShell"},{"location":"Misc/Windows/PowerShell/#control-flow","text":"if switch while do while if ( $condition ) { # ... } switch ( $reference ) { $value1 { ... } $value2 { ... } } $Values = while ( $true ) { (++ $Tick ) if ( $Tick -gt 2 ) { break } } # => @(1,2,3) $Values = do { 'Hello, world!' } while ( $false ) # => @('Hello, world!') Loops are implemented with ForEach-Object . 1 .. 5 | ForEach -Object { $_ + 2 } # => @(3,4,5,6,7) When values are stored in a variable at the end of a pipeline, it will create an array. while and do while loops are available, as well as until and do until loops which operate so long as their condition is false .","title":"Control flow"},{"location":"Misc/Windows/PowerShell/#variables","text":"Variables are accessed by prefixing the identifier with $ . Automatic variables ( PSVersionTable , $IsLinux , etc) are PowerShell-specific. Windows environment variables are actually accessed through the Env virtual drive the syntax $Env:VARIABLE . APPDATA LOCALAPPDATA USERNAME USERPROFILE WINDIR","title":"Variables"},{"location":"Misc/Windows/PowerShell/#typing","text":"Variables can be typed by preceding their identifier with the datatype in brackets [double] $Price [int] $Quantity [string] $Description Compatible data can be cast or converted by simply specifying the type in an assignment, but when the data cannot be converted the interpreter will throw an error. $Number = [int] '04' $FailedCast = [int] 'Hello' Filtering results can be done with 5 commands: Where-Object (aliased to where and ? ): the most commonly used such command Select-Object (aliased to sc ed to specify specific columns of information to be displayed Select-String (aliased to sls ) ForEach-Object (aliased to foreach and % ) There are two different ways to construct a ForEach-Object statement: Script block , within which the variable $_ represents the current object Operation statement , more naturalistic, where you specify a property value or call a method.","title":"Typing"},{"location":"Misc/Windows/PowerShell/#hashtable","text":"Build a hash table using literals or the Add method Literal Literal (inline) Add $fruit = @{ Apple = 'red' Orange = 'orange' Eggplant = 'purple' } $fruit = @{ Apple = 'red' ; Orange = 'orange' ; Eggplant = 'purple' } $fruit = @{} $fruit . Add ( 'Apple' , 'red' ) $fruit . Add ( 'Orange' , 'orange' ) $fruit . Add ( 'Kiwi' , 'green' ) Hashtable methods Add Clone Keys Values Count Remove $fruit = @{} $fruit . Add ( 'Apple' , 'red' ) $fruit . Add ( 'Orange' , 'orange' ) $fruit . Add ( 'Kiwi' , 'green' ) # Deep copy or \"clone\" of a hashtable. $fruitclone = $fruit . Clone () $fruit . Keys # => @('Apple','Orange','Kiwi') $fruit . Values # => @('red','orange','green') $fruit . Count $fruit . Remove ( 'One' ) Unlike Python, a hash table can be made ordered, changing its data type: $fruit = [ordered] @{ Apple = 'red' ; Orange = 'orange' ; Eggplant = 'purple' } $fruit . GetType (). Name # => OrderedDictionary","title":"Hashtable"},{"location":"Misc/Windows/PowerShell/#documentation","text":"Single-line comments are preceded by # and block quotes are enclosed between <# and #> . Such a block comment will be parsed by PowerShell when running Get-Help . <# .SYNOPSIS This script coordinates the process of creating new employees .DESCRIPTION This script creates new users in Active Directory... .PARAMETER UserName The official logon name for the user... .PARAMETER HomeServer The server name where the user's home folder will live... #> <# .EXAMPLE New-CorpEmployee -UserName John-Doe -HomeServer HOMESERVER1 This example creates a single new employee... #>","title":"Documentation"},{"location":"Misc/Windows/PowerShell/#functions","text":"Functions are declared with the function keyword and the body in braces, following this syntax: function Verb-Noun { # ... } Positional parameters can be referenced using the $args array, which contains all arguments passed to the function on invocation. Named parameters can be declared in one of two ways. Within the function body using the param keyword, followed by the name of the variable representing the parameter's value, enclosed in $(...) : Directly after the function name in parentheses, with each parameter separated by a comma. The name of the variable becomes the named parameter used when invoking the function. Default values for parameters can be specified by placing them within the parentheses. Parameters can be made mandatory by preceding the parameter name with [Parameter(Mandatory=$true)] . Parameters can be static typed by preceding the parameter identifier with the data type in brackets. Positional parameter Named parameter Using param Default value Typed Invocation function Get-LargeFiles { Get-ChildItem C :\\ Users \\ Michael \\ Documents | where { $_ . Length -gt $args [ 0 ] and ! $_PSIscontainer } | Sort-Object Length -Descending } function Get-LargeFiles ( $Size ) { # param ($Size) Get-ChildItem C :\\ Users \\ Michael \\ Documents | where { $_ . Length -gt $Size -and ! $_ . PSIsContainer } | Sort-Object Length -Descending } function Get-LargeFiles { param ( $Size ) Get-ChildItem C :\\ Users \\ Michael \\ Documents | where { $_ . Length -gt $Size -and ! $_ . PSIsContainer } | Sort-Object Length -Descending } function Get-LargeFiles { param ( $Size = 2000 ) Get-ChildItem C :\\ Users \\ Michael \\ Documents | where { $_ . Length -gt $Size -and ! $_ . PSIsContainer } | Sort-Object Length -Descending } function Get-LargeFiles { param ( [int] $Size = 2000 ) Get-ChildItem C :\\ Users \\ Michael \\ Documents | where { $_ . Length -gt $Size -and ! $_ . PSIsContainer } | Sort-Object Length -Descending } Get-LargeFiles -Size 2000 Switch parameters are typed as a switch data type. Boolean values can be explicitly set upon invocation using this syntax: Switch-Item Invocation function Switch -Item { param ( [switch] $on ) if ( $on ) { \"Switch on\" } else { \"Switch off\" } } Switch -Item # => Switch off Switch -Item -On # => Switch on Switch -Item -On : $false # => Switch off Attach common parameters to a custom function by placing the [CmdletBinding()] within the body of a function. This allows use of options like -Verbose or -Debug with custom functions. Now, using Write-Verbose and Write-Debug within the function body serve the dual purpose of outputting additional information at the time of execution, when needed, as well as documentation.","title":"Functions"},{"location":"Misc/Windows/PowerShell/#remote-administration","text":"Powershell remoting can be done explicitly or implicitly . Remoting relies on WinRM , which is Microsoft's implementation of WSMAN. Explicit remoting is also 1-to-1 remoting, where an interactive Powershell prompt is brought up on a remote computer. One-to-many or fan-out remoting is possible with implicit remoting, where a command is transmitted to many computers.","title":"Remote administration"},{"location":"Misc/Windows/PowerShell/#testing","text":"Pester tests are organized in a hierarchy of blocks and run with Invoke-Pester : Describe { Context # optional { It { Should # assertion statements accept a value passed in via pipe and **must** be called within a `Describe` block } } } New-Fixture deploy Foo function Foo { # ... } Describe 'Foo' { $true | Should -Be $true } The block in braces is actually an argument pass to the -Fixture parameter. Describe \"Best airports in the USA\" -Fixture { It -Name \"RDU is one of the best airports\" -Test { $Output = Get-Airport -City \"Raleigh\" $Output | Should -BeOfType System . Collections . Hashtable } }","title":"Testing"},{"location":"Misc/Windows/PowerShell/#tasks","text":"","title":"Tasks"},{"location":"Misc/Windows/PowerShell/#download-files","text":"URIs and filenames are built up as formatted strings 1 .. 24 | ForEach -Object { Invoke-WebRequest -OutFile ( \"TGC_3466_Lect{0:d2}_FallPagansOriginsMedievalChristianity.m4v\" -f $_ ) ( \"https://securedownloads.teach12.com/anon.eastbaymedia-drm/courses/3466/m4v/TGC_3466_Lect{0:d2}_FallPagansOriginsMedievalChristianity.m4v?userid=$USERID&orderid=$ORDERID&courseid=$COURSEID&FName=TGC_3466_Lect{0:d2}_FallPagansOriginsMedievalChristianity\" -f $_ )}","title":"Download files"},{"location":"Misc/Windows/PowerShell/#display-computer-name","text":"Cmdlet $Env Get-ComputerInfo -Property CsName gin . CsName $Env:computername","title":"Display computer name"},{"location":"Misc/Windows/PowerShell/#generate-password","text":"Generate a random password 20 characters long ( src ) Add-Type -AssemblyName 'System.Web' [System.Web.Security.Membership] :: GeneratePassword ( 20 , 3 ) Store credential Interactive Cmdlet $cred = Get-Credential $pw = ConvertTo-SecureString \"Password\" -AsPlainText -Force $cred = New-Object System . Management . Automation . PSCredential ( \"FullerP\" , $pw ) Create a new file in the current working directory named filename New-Item -ItemType File -Name filename Append content to file Add-Content C :\\ path \\ to \\ file $content","title":"Generate password"},{"location":"Misc/Windows/PowerShell/#new-domain-controller","text":"[Jones][Jones] Install-WindowsFeature AD-Domain-Services , DHCP -IncludeManagementTools Install-ADDSForest -DomainName corp . packtlab . com Add-DhcpServerv4Scope -Name \"PacktLabNet\" -StartRange 10 . 0 . 0 . 50 -EndRange 10 . 0 . 0 . 100 -SubnetMask 255 . 255 . 255 . 0 Set-DhcpServerv4OptionValue -DnsDomain corp . packtlab . com Add-DhcpServerInDC -DnsName dc . corp . packtlab . com New-AdUser -SamAccountName SysAdmin -AccountPassword ( Read-Host \"Set user password\" -AsSecureString ) -Name \"SysAdmin\" -Enabled $true -PasswordNeverExpires $true -ChangePasswordAtLogon $false Add-ADPrincipalGroupMembership -Identity \"CN=SysAdmin,CN=Users,DC=corp,DC=packtlab,DC=com\" , \"CN=Domain Admins,CN=Users,DC=corp,DC=packtlab,DC=com\" Get-ADPrincipalGroupMembership sysadmin","title":"New domain controller"},{"location":"Misc/Windows/PowerShell/#text-to-speech","text":"Initialize text-to-speech object scriptinglibrary.com Add-Type \u2013 AssemblyName System . Speech $tts = New-Object \u2013 TypeName System . Speech . Synthesis . SpeechSynthesizer $tts . Speak ( 'Hello, World!' ) List available voices Foreach ( $voice in $SpeechSynthesizer . GetInstalledVoices ()){ $Voice . VoiceInfo | Select-Object Gender , Name , Culture , Description } Change voice $tts . SelectVoice ( \"Microsoft Zira Desktop\" ) $tts . Speak ( 'Hello, World!' ) Set output to WAV file thinkpowershell.com $WavFileOut = Join-Path -Path $env:USERPROFILE -ChildPath \"Desktop\\thinkpowershell-demo.wav\" $SpeechSynthesizer . SetOutputToWaveFile ( $WavFileOut )","title":"Text-to-speech"},{"location":"Misc/Windows/PowerShell/#vhdx-file","text":"Create a new 256 GB dynamic VHDX file, mount it, initialize it, and create and format the partition [Zacker][Zacker]: 91 New-VHD -Path C :\\ Data \\ disk1 . vhdx -SizeBytes 256GB -Dynamic | Mount-VHD -Passthru | Initialize-Disk -PassThru | New-Partition -DriveLetter X -UseMaximumSize | Format-Volume -Filesystem ntfs -FileSystemLabel data1 -Confirm : $False -Force","title":"VHDX file"},{"location":"Misc/Windows/PowerShell/#restart-wi-fi-adapter","text":"$adaptor = Get-WmiObject -Class Win32_NetworkAdapter | Where-Object { $_ . Name -like \"*Wireless*\" } $adaptor . Disable () $adaptor . Enable ()","title":"Restart Wi-Fi adapter"},{"location":"Misc/Windows/PowerShell/#add-a-member-to-a-group","text":"Add-ADGroupMember -Identity $group -Members $user1 , $user2","title":"Add a member to a group"},{"location":"Misc/Windows/PowerShell/#add-a-new-local-admin","text":"nlu ansible Add-LocalGroupMember Administrators ansible","title":"Add a new local admin"},{"location":"Misc/Windows/PowerShell/#configure-secure-remoting-using-a-self-signed-certificate","text":"","title":"Configure secure remoting using a self-signed certificate"},{"location":"Misc/Windows/PowerShell/#create-a-virtual-switch-with-set-enabled","text":"Create a virtual switch with SET enabled. [Zacker][Zacker]: 254 New-VMSwitch -Name SETSwitch -NetAdapterName \"nic1\" , \"nic2\" -EnableEmbeddedTeaming $true Add new virtual network adapters to VMs Add-VMNetworkAdapter -VMName server1 -SwitchName setswitch -Name set1 Enable RDMA with [ Get- ][Get-NetAdapterRdma] and [ Enable-NetAdapterRdma ][Enable-NetAdapterRdma].","title":"Create a virtual switch with SET enabled"},{"location":"Misc/Windows/PowerShell/#implement-nested-virtualization","text":"Both the physical host and the nested virtual host must be running Windows Server 2016, but before installing Hyper-V on the nested host, the following configurations must be made. [Zacker][Zacker]: 181 Provide nested host's processor with access to virtualization technology on the physical host Set-VMProcessor -VMName server1 -ExposeVirtualizationExtensions $true Disable dynamic memory Set-VMMemory -VMName SRV01 -DynamicMemoryEnabled $false Configure 2 virtual processors Set-VMProcessor -VMName SVR01 -Count 2 Turn on MAC address spoofing Set-VMNetworkAdapter -VMName SVR01 -Name \"NetworkAdapter\" -MACAddressSpoofing On","title":"Implement nested virtualization"},{"location":"Misc/Windows/PowerShell/#enable-credssp","text":"On the remote (managed) server [Zacker][Zacker]: 176 Enable-PSRemoting Enable-WSManCredSSP Add the fully-qualified domain name of the Hyper-V server to be managed to the local system's WSMan trusted hosts list Set-Item WSMan :\\ localhost \\ client \\ trustedhosts -Value \"hypervserver.domain.com\" Enable the use of CredSSP on the client Enable-WSManCredSSP -Role client -DelegateComputer \"hypervserver.domain.com\"","title":"Enable CredSSP"},{"location":"Misc/Windows/PowerShell/#configure-server-core","text":"Manually configure network interface, if a DHCP server is unavailable [Zacker][Zacker]: 19 New-NetIPAddress 10 . 0 . 0 . 3 -InterfaceAlias \"Ethernet' -PrefixLength 24 Configure the DNS server addresses for the adapter Set-DnsClientServerAddress -InterfaceIndex 6 -ServerAddresses ( \"192.168.0.1\" , \"192.168.0.2\" ) Rename the computer and join it to a domain Add-Computer -DomainName adatum . com -NewName Server8 -Credential adatum \\ administrator","title":"Configure Server Core"},{"location":"Misc/Windows/PowerShell/#update-server-core-image","text":"Mount-WindowsImage -ImagePath .\\ CoreServer . vhdx -Path .\\ MountDir -Index 1 Add-WindowsPackage -Path .\\ MountDir -PackagePath C :\\ ServicingPackages_cabs Dismount-WindowsImage -Path .\\ MountDir -Save","title":"Update Server Core image"},{"location":"Misc/Windows/PowerShell/#implement-dda","text":"Discrete Device Assignment (DDA) begins with finding the Instance ID of the device needed to be passed through. [Zacker][Zacker]: 212 Get-PnpDevice -PresentOnly Disable-PnpDevice -InstanceId # Remove host-installed drivers Get-PnpDeviceProperty # Provide `InstanceId` and `KeyName` values in order to get value for `LocationPath` parameter in next command Dismount-VmHostAssignableDevice -LocationPath # Remove the device from host control Add-VMAssignableDevice -VM -LocationPath # Attach the device to a guest","title":"Implement DDA"},{"location":"Misc/Windows/PowerShell/#configure-live-migration","text":"Live migration is possible between Hyper-V hosts that are not clustered, but they must be within the same (or trusted) domains. [Zacker][Zacker]: 306 Enable-VMMigration Set-VMMigrationNetwork 192 . 168 . 4 . 0 Set-VMHost -VirtualMachineMigrationAuthenticationType Kerberos Set-VMHost -VirtualMachineMigrationPerformanceOption smbtransport","title":"Configure live migration"},{"location":"Misc/Windows/PowerShell/#configure-s2d-cluster","text":"New-Cluster -Name cluster1 -node server1 , server2 , server3 , server4 -NoStorage Enable-ClusterStorageSpacesDirect","title":"Configure S2D cluster"},{"location":"Misc/Windows/PowerShell/#install-docker-enterprise","text":"[Zacker][Zacker]: 266 Install-Module -Name dockermsftprovider -repository psgallery -force Install-Package -Name docker -ProviderName dockermsftprovider","title":"Install Docker Enterprise"},{"location":"Misc/Windows/PowerShell/#handle-xml-files","text":"Find a sample XML file here Assign the output of [ gc ][Get-Content] to a variable [xml] $xdoc = gc $xmlfile The XML tree can be viewed in VS Code using the XML Tools extension. The object itself can be treated as a first-class Powershell object using dot notation. red-gate.com $xdoc . catalog . book | Format-Table -Autosize Arrays of elements can be accessed by their index $xdoc . catalog . book [ 0 ] Nodes in the XML object can also be navigated using XPath notation with the SelectNodes and SelectSingleNode methods. $xdoc . SelectNodes ( '//author' ) This produces the same output as the command above ( in XPath nodes are 1-indexed ). ``` powershell $xdoc . SelectSingleNode ( '//book[1]' ) [ Select-Xml ][Select-Xml] wraps the returned XML node with additional metadata, including the pattern searched. However, it can accept piped input. ( Select-Xml -Xml $xdoc -Xpath '//book[1]' ). Node ( $xml | Select-Xml -Xpath '//book[1]' ). Node","title":"Handle XML files"},{"location":"Misc/Windows/PowerShell/#update-server-core-images","text":"[MeasureUp Lab][pl:70-740] Mount-WindowsImage -ImagePath .\\ CoreServer . vhdx -Path .\\ MountDir -Index 1 Add-WindowsPackage -Path .\\ MountDir PackagePath C :\\ ServicingPackages_cabs Dismount-WindowsImage -Path .\\ MountDir -Save","title":"Update Server Core images"},{"location":"Misc/Windows/PowerShell/#pass-through-disk","text":"[Zacker][Zacker]: 226 Set-Disk -Number 2 -IsOffline $true Add-VMHardDiskDrive -VMName server1 -ControllerType scsi -DiskNumber 2","title":"Pass-through disk"},{"location":"Misc/Windows/PowerShell/#site-aware-failover-cluster","text":"Configure failover clusters for two offices [Zacker][Zacker]: 366 New-ClusterFaultDomain -Name ny -Type site -Description \"Primary\" -Location \"New York, NY\" New-ClusterFaultDomain -Name sf -Type site -Description \"Secondary\" -Location \"San Francisco, CA\" Set-ClusterFaultDomain -Name node1 -Parent ny Set-ClusterFaultDomain -Name node2 -Parent ny Set-ClusterFaultDomain -Name node3 -Parent sf Set-ClusterFaultDomain -Name node4 -Parent sf","title":"Site-aware failover cluster"},{"location":"Misc/Windows/PowerShell/#filter-ad-account-information","text":"Get-aduser -filter {( SamAccountName -like \"*CA0*\" )} -properties Displayname , SaMaccountName , Enabled , EmailAddress , proxyaddresses | Where {( $_ . EmailAddress -notlike \"*@*\" )} | Where {( $_ . Enabled -eq $True )} | Select Displayname , SaMaccountName , Enabled , EmailAddress , @{ L = \u2019 ProxyAddress_1 '; E={$_.proxyaddresses[0]}}, @{L=\u2019ProxyAddress_2' ; E ={ $_ . ProxyAddresses [ 1 ]}} | Export-csv .\\ usersnoemail2 . csv -notypeinformation","title":"Filter AD account information"},{"location":"Misc/Windows/PowerShell/#create-vm-with-installation-media","text":"[Practice Lab][pl:70-740] New-VM PLABWIN102 1536mb 1 -SwitchName 'Private network 1' -NewVHDPath 'C:\\Users\\Public\\Documents\\Hyper-V\\Virtual hard disks\\PLABWIN102.vhdx' -NewVHDSizeBytes 127gb Set-VMDvdDrive -VMName PLABWIN102 -Path C :\\ Users \\ Administrator . PRACTICELABS \\ Documents \\ Eval81 . iso","title":"Create VM with installation media"},{"location":"Misc/Windows/PowerShell/#registry","text":"Description Affected key Fix Windows Search bar docs.microsoft.com HKCU:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Search Remove 3D Objects howtogeek.com HKLM:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Explorer\\MyComputer\\NameSpace Display seconds in system clock howtogeek.com HKCU:\\Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\Advanced Disable Aero Shake howtogeek.com HKCU:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Explorer\\Advanced Remove 3D Objects from This PC howtogeek.com Remove-Item 'HKLM:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Explorer\\MyComputer\\NameSpace\\{0DB7E03F-FC29-4DC6-9020-FF41B59E513A}' Add seconds to clock howtogeek.com New-Item -Path HKCU :\\ Software \\ Microsoft \\ Windows \\ CurrentVersion \\ Explorer \\ Advanced -Name ShowSecondsInSystemClock -Value 1 Restart-Computer New-Item -Path HKCU :\\ SOFTWARE \\ Microsoft \\ Windows \\ CurrentVersion \\ Search -Name BingSearchEnabled -Value 0 New-Item -Path HKCU :\\ SOFTWARE \\ Microsoft \\ Windows \\ CurrentVersion \\ Search -Name CortanaConsent -Value 0 Safely combine related registry modifications using [ Start-Transaction ][Start-Transaction] and [ Complete-Transaction ][Complete-Transaction] [Holmes][Holmes]: 604 Start-Transaction New-Item TempKey -UseTransaction Complete-Transaction Remove User UAC for local users. ref Set-ItemProperty -Path HKLM :\\ Software \\ Microsoft \\ Windows \\ CurrentVersion \\ Policies \\ System -Name EnableLUA -Value 0","title":"Registry"},{"location":"Misc/Windows/PowerShell/#winforms","text":"Pastebin # Load required assemblies [void] [System.Reflection.Assembly] :: LoadWithPartialName ( \"System.Windows.Forms\" ) # Drawing form and controls $Form_HelloWorld = New-Object System . Windows . Forms . Form $Form_HelloWorld . Text = \"Hello World\" $Form_HelloWorld . Size = New-Object System . Drawing . Size ( 272 , 160 ) $Form_HelloWorld . FormBorderStyle = \"FixedDialog\" $Form_HelloWorld . TopMost = $true $Form_HelloWorld . MaximizeBox = $false $Form_HelloWorld . MinimizeBox = $false $Form_HelloWorld . ControlBox = $true $Form_HelloWorld . StartPosition = \"CenterScreen\" $Form_HelloWorld . Font = \"Segoe UI\" # adding a label to my form $label_HelloWorld = New-Object System . Windows . Forms . Label $label_HelloWorld . Location = New-Object System . Drawing . Size ( 8 , 8 ) $label_HelloWorld . Size = New-Object System . Drawing . Size ( 240 , 32 ) $label_HelloWorld . TextAlign = \"MiddleCenter\" $label_HelloWorld . Text = \"Hello World\" $Form_HelloWorld . Controls . Add ( $label_HelloWorld ) # add a button $button_ClickMe = New-Object System . Windows . Forms . Button $button_ClickMe . Location = New-Object System . Drawing . Size ( 8 , 80 ) $button_ClickMe . Size = New-Object System . Drawing . Size ( 240 , 32 ) $button_ClickMe . TextAlign = \"MiddleCenter\" $button_ClickMe . Text = \"Click Me!\" $button_ClickMe . Add_Click ({ $button_ClickMe . Text = \"You did click me!\" Start-Process calc . exe }) $Form_HelloWorld . Controls . Add ( $button_ClickMe ) # show form $Form_HelloWorld . Add_Shown ({ $Form_HelloWorld . Activate ()}) [void] $Form_HelloWorld . ShowDialog ()","title":"WinForms"},{"location":"Misc/Windows/PowerShell/#modules","text":"Create a new module by placing a .psm1 file in a directory of the same name .\\Starship\\Starship.psm1 Functions defined within the module can be loaded with [ Import-Module ][Import-Module] (execution policy must allow this). ipmo .\\ Starship To import classes, a different syntax must be used source Using module .\\ Starship","title":"Modules"},{"location":"Misc/Windows/PowerShell/#sample-enumeration","text":"PowerShellMagazine Add-Type -AssemblyName System . Drawing $count = [Enum] :: GetValues ( [System.Drawing.KnownColor] ). Count [System.Drawing.KnownColor] ( Get-Random -Minimum 1 -Maximum $count )","title":"Sample enumeration"},{"location":"Misc/Windows/PowerShell/#migrate-a-vm","text":"Enable-VMMigration Set-VMMigrationNetwork 192 . 168 . 10 . 1 Set-VMHost -VirtualMachineMigrationAuthenticationType Kerberos Set-VMHost -VirtualMachineMigrationPerformanceOption SMBTransport","title":"Migrate a VM"},{"location":"Misc/Windows/PowerShell/#storage-spaces-direct","text":"[MeasureUp][mu:70-740] New-Cluster -Name HC-CLU1 -Node node1 , node2 , node3 , node4 -NoStorage Enable-ClusterStorageSpacesDirect -CacheMode Disabled -AutoConfig : 0 -SkipEligibilityChecks New-StoragePool -StorageSubSystemFriendlyName * Cluster * -FriendlyName S2DPool -ProvisioningTypeDefault Fixed -PhysicalDisk ( Get-PhysicalDisk | Where-Object -Property CanPool -eq $true ) $pool = Get-StoragePool S2DPool New-StorageTier -StoragePoolUniqueID ( $pool ). UniqueID -FriendlyName Performance -MediaType HDD -ResiliencySettingName Mirror New-StorageTier -StoragePoolUniqueID ( $pool ). UniqueID -FriendlyName Capacity -MediaType HDD -ResiliencySettingName Parity The next step would be the creation of a new volume New-Volume -StoragePool $pool -FriendlyName SharedVol1 -FileSystem CSVFS_REFS -StorageTiersFriendlyNames Performance , Capacity -StorageTierSizes 2GB , 10GB","title":"Storage Spaces Direct"},{"location":"Misc/Windows/PowerShell/#scheduled-task","text":"Automatically run SSH server in WSL on system start $action = New-ScheduledTaskAction -Execute C :\\ WINDOWS \\ System32 \\ bash . exe -Argument '-c sudo service ssh start' $trigger = New-ScheduledTaskTrigger -AtLogon Register-ScheduledTask -TaskName 'SSH server' -Trigger $trigger -Action $action","title":"Scheduled task"},{"location":"Misc/Windows/PowerShell/#network-connection-alert","text":"Play a tone when network connection has been (re)-established. while ( $true ) { if (( Test-NetConnection 8 . 8 . 8 . 8 -WarningAction SilentlyContinue ). PingSucceeded -eq $true ) { [System.Console] :: Beep ( 1000 , 100 ) break } } while ( Test-NetConnection 8 . 8 . 8 . 8 -WarningAction SilentlyContinue ). PingSucceeded -eq $false ) { continue }","title":"Network connection alert"},{"location":"Misc/Windows/WDS-lab/","text":"# PLABDC01 Set-DHCPServerv4Scope -ScopeId '192.168.0.0' -Type Both -State Active wdsutil . exe / initialize-server / remInst : \"D:\\RemoteInstall\" wdsutil . exe / enable-server wdsutil . exe / start-server Import install and boot images New-WDSInstallImageGroup -Name Win10-DVDImage Get-WindowsImage -ImagePath \\\\ plabdm01 \\ win10 \\ sources \\ install . wim Import-WDSInstallImage -Path \\\\ plabdm01 \\ win10 \\ sources \\ install . wim -ImageGroup Win10-DVDImage -ImageName 'Windows 10 Enterprise' Import-WdsBootImage -Path \\\\ plabdm01 \\ win10 \\ sources \\ boot . wim -NewImageName 'Microsoft Windows Setup (x64)' -NewDescription 'Microsoft Windows Setup (x64)' Configure server to accept clients ::wdsutil.exe /start-transportserver wdsutil.exe /set-server /answerclients:all wdsutil.exe /set-server /authorize:yes wdsutil.exe /set-server /NewMachineNamingPolicy:PLABSA%#03 Configure VM to boot from PXE # PLABDM01 Rename-VMSwitch Intel * External Add-VMNetworkAdapter -IsLegacy $true -VMName PLABSA02 -SwitchName External Remove-VMAssignableDevice -VMName PLABSA02 Start-VM PLABSA02 # While connected to VM, when prompted to \"press F12 for network service boot\", press F12.","title":"WDS lab"},{"location":"Misc/Windows/WS2016/","text":"Windows Server Certification exams Number Title 70-740 Installation, storage and Compute with Windows Server 2016 70-741 Networking with Windows Server 2016 70-742 Identity with Windows Server 2016 Find notes on labs here . Installation Windows Server 2016 installations are determined by the most suitable installation method, option, and edition. Installation methods : An upgrade is an installation performed in-place with existing data intact and is opposed to a clean installation . A migration is a clean installation with old data transferred over. Migrations are facilitated by Powershell and [command prompt][SmigDeploy.exe] tools Installation options include Desktop Experience, [Server Core][Server Core], and Nano Server . The most important installation edition is Windows Server 2016 Datacenter edition , which is the only edition to have several important features that figure prominently in the exam. Storage Spaces Direct , Storage Replica Shielded VMs Network controller . Various other installation options exist, including: Windows Server 2016 Standard , Essentials , Multipoint Premium , Storage , and Hyper-V editions. Server installations are also influenced by choice of activation model . Licensing Servicing channels provide a way of separating users into deployment groups for feature and quality updates. Semi-Annual Channel - previously known as Current Branch for Business (CBB) - features updates twice a year. It is more appropriate for non-infrastructure workloads that can be deployed through automation. Long Term Servicing Channel (LTSC) has a minimum servicing lifetime of 10 years and was designed to be used only for specialized devices such as those that control medical equipment or ATM machines, receiving new feature releases every 2-3 years Server Core Installing the Windows Server 2016 Server Core foregoes the possibility of later switching back to Desktop Experience, as had been possible in previous editions. Notably, WDS is incompatible with Server Core installations. Server Core installations can be managed with a GUI with the use of MMC snap-ins . Because MMC is reliant on Distributed Component Object Model (DCOM) technology, firewall rules have to be enabled to allow DCOM traffic (ref. Set-NetFirewallRule ). Nano Server Nano Server , a new installation option introduced in Windows Server 2016, provides a much smaller footprint and attack surface than even Server Core, but supports only some roles and features. Installation is done by building a VHD image via PowerShell on another computer. That VHD is then deployed as a VM or used as a boot drive for a physical server. Booting a Nano Server VM produces a text-based interface called the Nano Server Recovery Console , a menu system that allows configuration of static network options (DHCP is enabled by default). The DNS server may not be configured interactively, but must be specified when building the image with the Ipv4Dns parameter. If a Nano Server is domain-joined a remote Powershell session will authenticate via Kerberos. If not, its name or IP address must be added to the Trusted Hosts list. The Windows Server 2016 installation media contains a NanoServer directory, from which the NanoServerImageGenerator Powershell module must be imported. It also contains a Packages subdirectory, with CAB files containing roles and features that correspond to named parameters or packages that are specified as values to the Packages named parameter when building a Nano Server image. Cmdlet Description Edit-NanoServerImage Add a role or feature to an existing Nano Server VHD file New-NanoServerImage Used to create a Nano Server VHD file for Nano Server installation Activation Server installations are influenced by choice of activation model. MAK is suitable for small networks, but large enterprises may opt for KMS . - MAK activations are subdivided into Independent and Proxy , based on whether or not a VAMT is used. - KMS activations, which distribute GVLK s, are valid for a period of time and require the installation of a role and management tools . KMS operates on TCP port 1688. - AVMA simplifies the process of activating Hyper-V VMs running Windows Server 2012 or 2016. Active Directory-based activation is an alternative for enterprises who opt to activate licenses through the existing AD DS domain infrastructure. Any domain-joined computers running a supported OS with a GVLK will be activated automatically and transparently. The domain must be extended to the Windows Server 2012 R2 or higher schema level, and a KMS host key must be added using the VAMT. After Microsoft verifies the KMS host key, client computers are activated by receiving an activation object from the DC. MS Docs Images Many enterprises have begun virtualizing their server environments to take advantage of the many cost, reliability, and performance benefits that this change creates. Migrations should start with systems that are peripheral to main business interests before moving on to those that are more vital. A carefully documented protocol should be developed to facilitate the conversion of physical hard disks to VHDs for use in Hyper-V guests. Supported guest OSes include Linux and FreeBSD. The Microsoft Assessment and Planning (MAP) Toolkit is a free software tool that intelligently constructs a database of the hardware, software, and performance of computers on a network to plan for an operating system upgrade or virtualization. MAP supports the following discovery methods: Active Directory Domain Services Windows networking protocols System Center Configuration Manager IP address range scanning Computer names entered manually or imported from a file Server Core relies on the command-line for system maintenance, including updates which must be installed directly to the image using dism.exe or equivalent Powershell commands. Containers DNS Installation DNS server role requiremenets: - Statically assigned IP - Signed-in user must be member of local Administrators group There are several recommended DNS deployment scenarios, all of which involve installing DNS on a Server Core or Nano Server instance. This is because these installation options offer a reduced attack surface, a reduced resource footprint, and reduced patching requirements. - DNS on DC : All DNS features are available and supports AD-integrated, primary, secondary, and stub zones. - DNS on RODC : Passes DNS zone updates to a writeable DC - DNS on standalone member server : Supports file-based primary, secondary, and stub zones but requiring zone replication because there is no integration over AD. Nano Server Installing DNS on a running Nano Server image requires running Install-NanoServerPackage as well as enabling the \"DNS-Server-Full-Role\" optional feature using Enable-WindowsOptionalFeature . As of early 2017, Nano Server only supported a few roles, including DNS , but was only able to do so with some limitations - Nano Server can only support file-based DNS and cannot host AD-integrated zones. - Nano Server only supports the Semi-Annual servicing channel license. - Nano Server is not suitable for primary zones, only caching-only, forwarder, or secondary zone DNS servers Zones Zones can be considered one or more DNS domains or subdomains, associated with zone files , which compose the DNS database itself and contain two types of entries: Parser commands , which provide shorthand ways to enter records: $ORIGIN , $INCLUDE , and $TTL Resource records are whitespace delimited text files with columns for name, time to live, class, type, and data The copies of zone files local to individual DNS servers can be primary (read/write) or secondary (read-only). A primary zone is a writable copy of a DNS zone that exists on a DNS server. A secondary zone is a read-only replica of a primary zone and necessitates the presence of a primary zone for the same zone. Defining a secondary zone via PowerShell requires specifying that zone's MasterServers . In Windows Server, zone files can also be integrated with Active Directory, making what is called an Active Directory Integrated Zone . These allow multi-master zones , meaning any DC can process zone updates and the zone can be replicated to any DC in the domain or forest. An AD-integrated zone can be specified by passing the ReplicationScope parameter to the Add-DnsServerPrimaryZone cmdlet. Stub zones contains only name server (NS) records of another zone, but unlike a forwarder is able to update when name servers in a target zone change. Reverse Lookup zones are used to resolve IP addresses to FQDNs. Reverse lookup zones for public IP address space are often administered by ISPs, and they are useful in spam filtering to double-check the source domain name with the IP address. GlobalNames zones provide \"single label name resolution\" (as opposed to a FQDN) and are intended to replace WINS servers. Query traffic The process of resolving a query by querying other DNS servers is called recursion . Recursion can be disabled outright but Windows Server 2016 supports recursion scopes which will allow recursion to be disabled unless certain conditions are met (such as receiving the request on a particular interface). There are two types of query in the context of recursion: - Recursive query sent by the petitioner: that is, the original query which begins recursion. - Iterative query : individual queries sent out to authoritative name servers in order to resolve a recursive query. Root hints are preconfigured root servers that are necessary to begin the recursion process. The DNS Server service stores root hints in %systemroot%\\System32\\dns\\CACHE.DNS . These can be edited through the GUI or by using the PowerShell commands Add- , Import- , Remove- , and Set-DnsServerRootHint . Forwarding of a request occurs when a petitioned DNS server is unable to resolve the query because it is both: - Non-authoritative for the specified zone, and - Does not have the response cached. Two actions are possible when forwarding: - Configure a DNS server only to respond to queries it can satisfy by referencing locally-stored zone information, forwarding all other requests. - Configure forwarding for specific zones through conditional forwarding A secondary zone is not to be confused with delegation , where a DNS server delegates authority over part of its namespace (i.e. a subdomain) to one or more other servers. Windows Server 2016 supports a DNS GlobalNames zone meant to supercede WINS, which served a role similar to DNS for the old NetBIOS naming standard. NetBIOS names use a nonhierarchical structure (i.e. are a single name and not divisible into sub-domains) based on a name up to 16 characters long (although the 16th character defines a particular service running on the host defined by the previous 15). An organization must share a single GlobalNames zone, which must be created in PowerShell manually. Resource records Zone scavenging allows servers with stale records to remove them. This feature is disabled by default, but can be set at the server or zone level. Type Description A IPv4 address record AAAA IPv6 address record CNAME Hostname or alias for hosts in the domain MX Where mail for the domain should be delivered NS Name servers PTR Reverse lookup SOA Each zone contains a single SOA record SRV Generalized service location record, used for newer protocols instead of protocol-specific records TXT Typically holds machine-readable data Security DNSSEC offers security features using public key certificates. A socket pool can be used to configure the DNS server to use a random source port when issuing DNS queries. Response rate limiting can pose a defense against DNS DoS attacks by ignoring potentially malicious, repetitive requests. DNS-based Authentication of Named Entities (DANE) is supported by Windows Server 2016 to reduce man-in-the-middle attacks. DANE works by informing DNS clients requesting records from the domain which Certification Authoority they must expect digital certificates to be issued from. Policies Zone transfer policies can prevent or allow zone transfers to any server, to name servers, or to servers specified by FQDN or IP address. DNS Policy is a new feature in Windows Server 2016 that can control DNS server behavior depending on certain criteria. These criteria include: Client subnet Recursion scope Zone scope DNSSEC DNSSEC is a security setting for DNS that enables all DNS records in a zone to be digitally signed by a trust anchor which validates DNSKEY resource records. Root and top-level domain zones already have trust anchors configured and merely have to have it enabled. To implement trust anchors: - A TrustAnchors zone must be created, which will store public keys associated with specific zones. A trust anchor from the secured zone must be created on every DNS server that hosts the zone. - A Name Resolution Policy Table (NRPT) GPO must be created (Windows Settings\\Name Resolution Policy) This option can require DNSSEC based on computer name prefix or suffix, FQDN, or subnet. - DNSSEC key master is a special DNS server that generates and manages signing keys for DNSSEC protected zones. DANE allows you to publish certificate information within the DNS zone, rather than one of the thousands of trusted CAs. This protects against rogue/compromised CAs issuing illegitimate TLS certificates. Two cryptographic keys: - Zone Signing Key (ZSK) signs zone data including individual resource records other than DNSKEY. It is also used to create the KSK. - Key Signing Key (KSK) is used to sign all DNSKEY records at the zone root. DNSSEC record types: - RRSIG \"resource record signature\" each of which matches and provides a signature for an existing record in a zone - NSEC proves nonexistence of a record - NSEC3 NSEC replacement that prevents zone walking - NSEC3PARAM specifies the NSEC3 records included in response for DNS names that don't exist - DNSKEY stores public key used to verify a signature - DS delegation signer records secure delegations DSC IaaS management of servers is possible with Desired State Configuration (DSC) , a feature of Windows PowerShell where script files stored on a central server can apply specific a specific configuration to nodes. These scripts are idempotent , meaning that they can be applied repeatedly without generating errors. The DSC model is composed of phases 1. Authoring Phase, where MOF definitions are created 2. Staging Phase, where declarative MOFs are staged and a Configuration calculated per node 3. \"Make It So\" Phase, where declarative Configurations are implemented through imperative providers Components of DSC scripts include: - Local Configuration Manager : engine running on the client system that received configurations from the DSC server and applies them to the target. - Node block specifies the names of target computers. - Resource block specifies settings or components and the values that the configuration script should assign to them. DSC configurations can be deployed in two different refresh modes Pull architecture : target LCM periodically retrieves configuration from a Pull Server , which consolidates MOF files. Push architecture : configuration is sent to target in response to explicit invocation of Start-DSCConfiguration on the server. LCM has to be configured to accept Configurations of either refresh mode. Tasks Set LCM to push mode [ DSCLocalConfigurationManager ()] Configuration LCMConfig { Node localhost { Settings { RefreshMode = 'Push' } } } Install Telnet client Configuration InstallTelnetLocal { Import-DscResource -ModuleName 'PSDesiredStateConfiguration' Node localhost { WindowsOptionalFeature InstallTelnet { Name = \"Telnet-Client\" Ensure = \"Present\" } } } Install WSL Configuration InstallWSLLocal { Import-DscResource -ModuleName 'PSDesiredStateConfiguration' Node localhost { WindowsOptionalFeature InstallWSL { Name = \"Microsoft-Windows-Subsystem-Linux\" Ensure = \"Present\" } } } Failover clusters Failover clusters are composed of computers called nodes and can be created using New-Cluster . which typically possess a secondary network adapter, used for cluster communications. Before Windows Server 2016, all cluster nodes had to belong to the same domain, but now this is but one of several possible cluster types called a single-domain cluster. A failover cluster can also be multi-domain , or workgroup , depending on how or if the servers are joined to domains. A cluster can also be detached from AD, even though its nodes are joined. A cluster whose servers are joined to a single domain is typically associated with a cluster name object in Active Directory, which serves as its administrative access point . A workgroup cluster or a detached cluster need to have the cluster's network name registered in DNS as its administrative access point, which can be specified in Powershell with the AdministrativeAccessPoint named parameter. Additionally, on a workgroup cluster the same local administrator account must be created on every node, preferably the builtin Administrator account, although a different account can be configured if a particular Registry key is [created][New-ItemProperty] on each node. Nodes that are domain-joined support CredSSP or Kerberos authentication, but workgroup nodes support NTLM authentication only. Three types of witness resources can help to ensure a quorum takes place in clusters. This is necessary to prevent a split-brain situation, where communication failures between nodes cause separate segments of the clusters to continue operating independently of each other. A witness is created when a cluster has an even number of nodes, and only one can be configured. [pwsh][Set-ClusterQuorum] - Disk witness : dedicated disk in shared storage that contains a copy of the cluster database - File Share witness : SMB file share containing a Witness.log file with information about the cluster - Cloud witness : blob stored in Azure Scale-out File Server (SoFS) is a clustered role providing highly available storage to cluster nodes. SoFS ensures continuous availability in the case of a node failure. Using SoFS, multiple nodes can also access the same block of storage at the same time, and for this reason is is an active/active or dual active system, as opposed to one where only one node provides accessible shares, or an active/passive system. SoFS is specifically recommended for use on Hyper-V and SQL Server clusters and can be installed with Add-ClusterScaleOutFileServer . SoFS shares are created with the [ New-SmbShare ][New-SmbShare] PowerShell cmdlet. SoFS shares are located on Cluster Shared Volumes (CSV) , a shared disk containing an NTFS or ReFS volume that is made accessible for read and write operations by all nodes within a failover cluster. CSVs solved a historical problem with using NTFS volumes with VMs in previous versions of Windows Server. NTFS is designed to be accessed by only one operating system instance at a time. In Windows Server 2008 and earlier, this meant that only one node could access a disk at a time, which had to be mounted and dismounted for every VM. The solution was to create a pseudo-file system called CSVFS , sitting on top of NTFS, that enables multiple drives to modify a disk's content at the same time, but restricting access to the metada to the owner or coordinator . The coordinator node refers to the cluster node where NTFS for the clustered CSV disk is mounted, any other node is called a Data Server (DS) . VM resiliency can be configured by adjusting settings in response to changes in VM state: - Unmonitored : VM owning a role is not being monitored by the Cluster Service - Isolated : Node is not currently an active member of the cluster, but still possess the role - Quarantine : Node has been drained of its roles and removed from the cluster for a specified length of time. Cluster Operating System Rolling Upgrade is a new feature that reduces downtime by making it possible for a cluster to have nodes running both Windows Server 2012 R2 and Window Server 2016. Using this feature, nodes can be brought down for an upgrade. When [Storage Spaces][Storage Spaces] is combined with a failover cluster, the solution is known as Clustered Storage Spaces . High availability Hyper-V Replica allows simple failover to occur between Hyper-V hosts, without the need for a cluster. To configure a simple one-way failover solution using Hyper-V Replica, configure the destination VM as a replica server , either in Hyper-V Manager or PowerShell. [ Set-VMReplicationServer ][Set-VMReplicationServer] lab The destination host must also have firewall ports opened corresponding to the authentication method chosen. The source VM, which is to be replicated, must have its options configured through the Enable Replication wizard . [ Enable-VMReplication ][Enable-VMReplication] To use Hyper-V Replica as a (two-way) failover solution, configure both VMs as replica servers. Migrations can take place one of three methods: - Live Migration moves only the system state and live memory contents, not data files. Live migration requires that the hosts be, if not clustered, at least part of the same (or a trusted) domain. Live Migration requires that VHD files be placed on shared storage and both hosts have appropriate permissions to access said storage. An unpopulated VM is created on the destination with the same resources as the source before transferring memory pages. Once the servers have an identical memory state, the source VM is suspended and the destination takes over. Hyper-V notifies the network switch of the change, diverting network traffic to the destination. Authentication can be made by [CredSSP][CredSSP] or Kerberos. When a Hyper-V cluster is created, the Failover Cluster Manager launches the High Availability Wizard, which configures the VM to support Live Migration. The same thing can be done with the PowerShell cmdlet Add-ClusterVirtualMachineRole . Additionally, using Kerberos authentication for live migration requires constrained delegation , which enables a server to act on behalf of a user for only certain defined services. This must be configured within Active Directory Users and Computers , by opening the Properties of the source Computer object, and changing the setting under the Delegation tab. - An additional, outdated method of migration is quick migration , which was present in Windows Server prior to the introduction of live migration and persists in Windows Server 2016 for backward compatibility. A quick migration involves pausing the VM, saving its state, moving the VM to the new owner, and starting it again. A quick migration always involves a short period of VM downtime. Shared Nothing Live Migration requires that source and destination VMs be members of the same (or trusted) domain, and source and domain servers must be running the same processor family (Intel or AMD) and linked by an Ethernet network running a minimum of 1 Gbps. Additionally, both Hyper-V hosts must be running idential virtual switches that use the same name; otherwise the migration process will be interrupted to prompt the operator to select a switch on the destination server. The process of migrating is almost identical to a Live Migration, except that you select the \"Move the Virtual Machine's Data To a Single Location\" option on the Choose Move Options page of the Move Wizard. Storage Migration works by first creating new VHDs on the destination corresponding to those on the source server. While the source server continues to operate using local files, Hyper-V begins mirroring disk writes to the destination server and begins a single-pass copy of the source disks to the destination begins, skipping blocks that have already been copied. Once the copy has completed, the VM begins working from the destination server and the source files are deleted. For a VM that is shut off, storage migration is equivalent to simply copying files from source to destination. Site-aware clusters have failover affinity . Node fairnes evalutes memory and CPU loads on cluster nodes over time. Cluster management VM Monitoring allows specific services to be restarted or failed-over when a problem occurs. To use VM Monitoring: - The guest must be joined to the same domain as the host - The host administrator must be a member of the guest's local Administrators group - And Windows Firewall rules in the Virtual Machine Monitoring group must be enabled. The service can then be monitored using [ Add-ClusterVMMonitoredItem ][Add-ClusterVMMonitoredItem]. Migration VMs can be moved from node to node of a cluster using live , storage , or quick migrations. VM network health protection is a feature (enabled by default) that detects whether a VM on a cluster node has a functional connection to a designated network. If not, the cluster live migrates the VM role to another node that does have such a connection. This setting can be controlled in Hyper-V Manager > VM Settings > Advanced Features > Protected network GPO Group Policy Objects (GPO) facilitate the uniform administration of large numbers of users and computers. GPOs can be local or domain-based . Local GPOs come in several varieties, applied in the following order (last takes highest precedence): - Local Group Policy applied to computers - Administrators and Non-Administrators Local Group Policy applied to users based on their membership in local Administrators group. - User-specific Local Group Policy : Domain-based GPOs consist of two components a [ container ][Group Policy container] and a [ template ][Group Policy template]. These are stored in different locations and replicated by different means. - Containers define the fundamental attributes of a GPO, each of which is assigned a GUID, and are stored in the AD DS database and replicated to other domain controllers using intrasite or intersite AD DS replication schedule. - Templates, a collection of files and folders that define the actual GPO settings, are stored in the SYSVOL shared folder ( %SystemRoot%\\SYSVOL\\Domain\\Poligicies\\{GUID} ) on all DCs. SYSVOL replication is handled by the DFS Replication Agent since Windows Server 2008. A GPO consists of 2 top-level nodes: - Computer Configuration contains settings that are applied to computer objects to which the GPO is linked - User Configuration containers user-related settings, applied when a user signs in and thereafter and automatically refreshed every 90-120 minutes Beneath each of these nodes are folders that group settings - Software Settings - Windows Settings allows basic configuration for computers or users - Administrative Templates contains Registry settings that control user, computer, and app behavior and settings, grouped logically into folders Although domain controllers store and serve GPOs, the client computer itself must request and apply the GPOs using the Group Policy Client service. Client-side extensions process the GPOs once downloaded Starter GPOs are intended for use in large organizations with a proliferation of GPOs that share settings. Starter GPOs can be imported from, and exported to, a .CAB file. Once a GPO is created it must be linked to a container object in AD DS for it to apply to objects, a process known as scoping . GPOs can be linked to Sites, Domains, and OUs. If multiple GPOs are linked to the same container, the link order must be configured. There are 2 default GPOs in an AD DS domain, which can be reset using arguments to the dcgpofix command. - Default Domain Policy, linked to the domain object - Default Domain Controllers Policy, linked to the Domain Controllers OU Although it is possible to link the same GPO to multiple containers, it is recommended to import (i.e. copy) a GPO from another domain. This process effecitvely restores the settings of another GPO into a newly created GPO, which is then linked to another container. Hyper-V [Hyper-V][Hyper-V] is a Type I hypervisor and role that allows a Windows Server 2016 host to create VMs, called guests . In Type I virtualization, the hypervisor forms an abstraction layer that interacts directly with the host hardware. In this model, the individual environments created by the hypervisor, including the host operating system and guest VMs, are called [partitions][partition]. Hyper-V Server, a free product available for download is limited to the command-line Server Core interface, however it does include Sconfig to aid configuration. Hyper-V can be managed remotely using the GUI (Hyper-V Manager, hyper-v-tools ), or Powershell ( hyper-v-powershell ). Authentication can be via Kerberos or Credential Security Support Provider (CredSSP) , which must be enabled on both server and client. PowerShell remoting - Explicit remoting involves opening a PowerShell session to a remote session - Implicit remoting involves running a cmdlet specifying the ComputerName parameter. PowerShell Direct allows easy remoting to VMs by using the -VmName Powershell parameter using a PowerShell session. [Nested virtualization][Nested virtualization] is a new capability where a virtual host running Windows Server 2016 on a physical host also running Windows Server 2016 can host nested VMs. Host configuration [Hyper-V][Hyper-V] is a Type I hypervisor and role that allows a Windows Server 2016 host to create VMs, called guests . In Type I virtualization, the hypervisor forms an abstraction layer that interacts directly with the host hardware. In this model, the individual environments created by the hypervisor, including the host operating system and guest VMs, are called [partitions][partition]. Hyper-V Server, a free product available for download is limited to the command-line Server Core interface, however it does include Sconfig to aid configuration. Hyper-V can be managed remotely using the GUI (Hyper-V Manager, hyper-v-tools ), or Powershell ( hyper-v-powershell ). Authentication can be via Kerberos or Credential Security Support Provider (CredSSP) , which must be enabled on both server and client. PowerShell remoting - Explicit remoting involves opening a PowerShell session to a remote session - Implicit remoting involves running a cmdlet specifying the ComputerName parameter. PowerShell Direct allows easy remoting to VMs by using the -VmName Powershell parameter using a PowerShell session. [Nested virtualization][Nested virtualization] is a new capability where a virtual host running Windows Server 2016 on a physical host also running Windows Server 2016 can host nested VMs. Networking Virtual switches can be external , internal , or private (in order of decreasing access). Up to 8 network adapters can be [added][Add-VMNetworkAdapter] to a Windows Server 2016 Hyper-V VM. Hyper-V maintains a pool of MAC addresses which are assigned to virtual network adapters as they are created. Hyper-V MAC addresses begin with 00-15-5D , followed by the last two bytes of the IP address assigned to the server's physical network adapter (i.e. last two octets), then a final byte. Generation 1 VMs supported synthetic and legacy virtual network adapters, but in Generation 2 VMs only synthetic adapters are used. Generation 1 VMs can only boot from network (PXE) when using a legacy adapter. Physical hosts running Windows Server 2016 can support teams of up to 32 NICs, but Hyper-V VMs are limited to teams of two. The team must first be configured in the host operating system and appears as a single interface in the Virtual Switch Manager. High-performance embedded teaming , reliant on RDMA , can only be configured with Powershell . - Teaming Mode - Switch Independent : switch is unaware of presence of NIC team and does not load balance to members; Windows is performing the teaming - Switch Dependent : switch determines how to distribute inbound network traffic; only supported by specialty hardware - Static Teaming : switch and host are manually configured (typically supported by server-class switches) - Link Aggregation Control Protocol (LACP) : dynamically identifies links that are connected between the host and the switch - Load Balancing Mode - Address Hash : a hash is created based on address components of the packet, which is used to reasonably balance adapters - Hyper-V Port : NIC teams configured on Hyper-V hosts give VMs independent MAC addresses - Dynamic : outbound loads are distributed based on a hash of the TCP ports and IP addresses Virtual machine queuing will enhance performance if a physical host supports it and it is enabled . Bandwidth management is achieved by setting limits on the virtual network adapter, in the GUI or in [Powershell][Set-VMNetworkAdapter]. Storage The New Virtual Machine Wizard presents different options for Generation 1 vs. Generation 2 VMs. - Generation 1 VMs provide two IDE controllers, which host the hard drive and a DVD drive, and an unpopulated SCSI controller which can host additional disks. - Generation 2 VMs, however, have only a single SCSI controller, which hosts all virtual drives. A new VHD can be created using - Hyper-V Manager through the New Virtual Hard Disk Wizard - Disk Management ( diskmgmt.msc ), however the option to create a differencing disk is not available, nor can specific block or sector size be specified - PowerShell Shared virtual disk files are preferably created as VHD sets . Pass-through disks make exclusive use of a physical disk. pwsh Standard checkpoints (previously known as \"snapshots\" in Windows Server 2012 and before) with the extensions AVHD or AVHDX save the state, data, and hardware configuration of a VM. They are recommended for development and testing but are not a replacement for backup software nor recommended for production environmentsj, because restoring them in a production environment will interrupt running services. Production checkpoints do not save memory state, but use Volume Shadow Copy Service (Windows) or File System Freeze (Linux) inside the guest to create \"point in time\" images of the VM. Shielded VMs Shielded VMs are a feature exclusive to the Datacenter Edition of Windows Server 2016. As a result of increased virtualization, physical servers that were once secured physically were migrated to Hyper-V hosts that are less secure because they are accessible to fabric administrators . Shielded VMs were introduced to protect tenant workloads from inspection, theft, and tampering as a result of being run on potentially compromised hosts. A security concept closely associated to shielded VMs is the guarded fabric , which is a collection of nodes cooperating to protect shielded Hyper-V guests. The guarded fabric consists of: - Host Guardian Service (HGS) utilizes remote attestation to confirm that a node is trusted; if so, it releases a key enabling the shielded VM to be started. HGS is typically a cluster of 3 nodes. - Guarded hosts : Windows Server 2016 Datacenter edition Hyper-V hosts that can run shielded VMs only if they can prove they are running in a known, trusted state to the Host Guardian Service. - Shielded VMs In a production environment, a fabric manager like Virtual Machine Manager would be used to deploy shielded VMs (which are signified by a shield icon). Shielded VMs must run Windows (8+) or Windows Server (2012+), although Linux shielded VMs are now also supported since version Windows Server version 1709. Shielded VMs are produced by a three-stage process (VHD -> Shielded template -> Shielded VMs) 1. Preparation : Install and configure an OS onto a virtual disk file 2. Templatization : Convert virtual disk file into a shielded template 3. Provisioning : Create one or more shielded VMs from the shielded template Configure HGS in its own new forest YouTube Install-WindowsFeature HostGuardianServiceRole -Restart Install-HgsServer -HgsDomainName 'savtechhgs.net' -SafeModeAdministratorPassword $adminPassword -Restart Shielding Data is created and owned by tenant VM owners and contains secrets needed to create shielded VMs that must be protected from the fabric admin. Resources: Intro to shielded VMs Create a shielded VM using Powershell Linux Shielded VM How To Shielded VM Demonstration and Quick Setup Guarded Fabric Deployment Guide for Windows Server 2016 Deploying Shielded VMs and a Guarded Fabric with Windows Server 2016 Attestation There are two modes of attestation supported by HGS: Hardware-trusted attestation Hardware-trusted attestation mode requires : Measured boot : TPMv2 to seal software and hardware configuration details measured at boot Code integrity enforcement to strictly define permissible software Platform Identity Verification : Active Directory is not sufficient to identify the host. Rather, an identity key rooted in the host TPM is used for identity. Remote attestation based on asymmetric key pairs Admin-trusted attestation was previously based on guarded host membership in a designated AD DS security group, but is deprecated beginning with Windows Server 2019. Host identity is [verified]](https://youtu.be/B2vFrdXd5jg?t=525) by checking security group permission No Measured Boot or Code Integrity Validation Intended to aid transition to Hardware-trusted attestation mode for hosts produced before TPMv2 VM configuration VMs are associated with a variety of file types: Extension Description .vmc XML-format VM configuration .vhd, .vhdx Virtual hard disks .vsv Saved-state files VMs can be created in Hyper-V, and a machine's RAM can even be changed dynamically. Hyper-V guests can take advantage of a suite of features to enhance performance and functionality. - Virtualization of NUMA architecture - Smart paging for when VMs that use dynamic memory restart and temporarily need more memory than is available on the host, for example at boot - Monitoring resource usage, to minimize cost overruns when guests run in the cloud - Disk and GPU passthrough, and other PCI-x devices, with DDA pwsh - Increased performance of interactive sessions that use [VMConnect][VMConnect.exe] Microsoft supports some Linux distributions, like Ubuntu, with built-in Linux Integration Services , which improve performance by providing custom drivers to interface with Hyper-V. Some distributions like CentOS and Oracle come with integrated LIS packages, but free LIS packages provided by Microsoft for download from the Microsoft Download Center support additional features and come with the additional benefit of being versioned. These packages are provided as tarballs or ISO images, and must be loaded directly into the running guest operating system. FreeBSD has included full support for FreeBSD Integration Services (BIS) since version 10. Secure Boot has to be disabled when loading Hyper-V VMs running Linux distributions, since UEFI does not have certificates for non-Windows operating systems by default. Some distributions supported by Microsoft do have certificates in the UEFI Certificate Authority . Different versions of Hyper-V create VMs associated with that version (Windows Server 2016 uses Hyper-V 8.0). VMs created by older versions of Hyper-V can be [updated][Update-VMVersion], but once updated they may no longer run on a host of a previous version. Importing an exported VM can be done in three ways: - Register : exported files are left as-is and the guest's ID is maintained; - Restore : exported files copied to the host's default locations or ones that are otherwise specified; ID is maintained - Copy : exported files are copied; new ID generated PXE boot is supported in two scenarios: - Generation 1 VMs with a legacy virtual network adapter support PXE boot (but not synthetic ). Generation 1 VMs are limited to 2 TB in size and do not support many of the advanced features that Generation 2 VMs do. But PXE Boot remains one of the primary reasons to continue using a Generation 1 VM. - Generation 2 VMs with a synthetic network adapter also support PXE boot. would also support bandwidth management and VMQ. Generation 2 VMs also do not support 32-bit OSes, including: - Windows Server 2008, R2 - Windows 7 - Older Linux distros - FreeBSD (all) VMs cannot be upgraded from Generation 1 to Generation 2 easily, although a script named Convert-VMGeneration was once provided by Microsoft and can still be found. But the VM's version , referring to the version of Hyper-V used to create it, can be upgraded with Upgrade-VMVersion . Monitoring Performance Monitor is a program that allows realtime monitoring of hundreds of different system performance statistics, called performance counters . Counters can be viewed in several ways, including line graph, histogram bar graph, and report views. Every counter added to a graph is associated with a computer, a performance object (hardware or software component to be monitored), a performance counter (statistic), and an instance. A data collector set captures counter statistics for later review. A single data collector set can gather performance counter data from multiple VMs. Event trace data cannot be combined with performance data in the same data collector set. Expiration dates can be set for data collector sets, but if actively collecting data the expiration date will not stop collection. A performance alert is a type of data collector set that can track system performance and log events in the application event log. Alerts can be triggered when a performance counter value exceeds a certain threshold. Only members of the local groups Administrators and Performance Log Users can create alerts, but the Log on as a batch user right must be granted to members of Performance Log Users. A hard fault occurs when data is swapped between memory and disk. Performance counters Counter Acceptable values Processor: % Processor Time <85% Processor: Interrupts/Sec cf. baseline System: Processor Queue Length <2 Server Work Queues: Queue Length <4 Memory: Page Faults/Sec <5 Memory: Pages/Sec <20 Memory: Available MBytes >5% of physical memory Memory: Committed Bytes < physical memory Memory: Pool Non-Paged Bytes Stable PhysicalDisk: Disk Bytes/Sec cf. baseline PhysicalDisk: Avg. Disk Bytes/Transfer cf. baseline PhysicalDisk: Current Disk Queue Length <2 per spindle PhysicalDisk: % Disk Time <90% LogicalDisk: % Free Space >20% Network Interface: Bytes Total/Sec cf. baseline Network Interface: Output Queue Length <2 Server: Bytes Total/Sec 50% of total bandwidth Network Load Balancing Cluster VMs can be configured to drain their workloads to other nodes when being shutdown using Suspend-ClusterNode NLB Clusters are made of hosts , while Failover Clusters are made of nodes . NLB port rules control how the cluster functions and are defined by two operational parameters: Affinity : associate client requests to cluster hosts. When no affinity is specified, all network requests are load-balanced across the cluster without regard to their source. Filtering mode : specify how the cluster handles traffic described by port range and protocols; can be single or multiple hosts. When a port rule is not configured, the default host will receive all network traffic. Windows Server NLB Clusters can be upgraded to Windows Server 2016 in two ways: - Rolling upgrade brings only a single host down at a time, upgrading it before adding it and proceeding to the next one - Simultaneous upgrade brings the entire NLB cluster goes down NLB clusters have a Cluster Operation Mode setting specifying what kind of TCP/IP traffic the cluster hosts should use - Unicast : NLB replaces the MAC address on the interface with the cluster's virtual MAC address, causing traffic to go to all hosts. Cluster hosts are prevented from communicating with each other in this mode. In this case, a second network adapter must be installed in order to facilitate normal communication between NLB cluster hosts. - Multicast : NLB adds a multicast MAC address to the network interface on each host that does not replace the original. Storage Every track of a hard drive platter is split into disk sectors , traditionally 512 bytes. A block is commonly called an \"allocation unit\" in Windows, but also commonly called a cluster. Storage left over unused in partially unused blocks is known as slack space . A new disk must first be initialized , that is, a partition table style must be chosen: - GPT : 128 partitions per disk, maximum volume size of 18 exabytes (2 60 bytes). Booting from a GPT drive is not possible unless the computer architecture supports EFI-based boot partitions. - MBR : older format that is commonly used for removable media, supporting volumes up to 2 TB with up to 4 primary partitions , although a common workaround is to make one of these partitions an extended partition , which can be be further subdivided into logical drives Mounting a partition as a single filesystem produces a volume , although the distinction can often be lost. The exception would be a case where a volume spans multiple partitions or physical disks, as is possible with software RAID. Virtual hard disks can be created with [Powershell][New-VHD] or in diskmgmt.msc and come in two formats: - VHD - VHDX Only 2 filesystem options are available for modern servers: - NTFS supports volumes up to 16 TB with the default 4 KB allocation unit size (but 256 TB with the 64 KB allocation unit size) and is required by some Windows Server services like AD DS , File Replication Service , Volume Shadow Copy Service , and Distributed File System - ReFS uses the same system of permissions as NTFS and offers error checking and repair capabilities that NTFS does not, but it does not support NTFS features like file compression, Encrypted File System , and disk quotas. ReFS supports a maximum file size of 16 exabytes and volumes up to 1 yobibyte (2 80 bytes) Software RAID can be implemented by creating Spanned , Striped , or RAID-5 volumes in diskmgmt.msc . A more modern and preferred technique is to create storage pools in [Storage Spaces][Storage Spaces]. Dedup Data deduplication (\"dedup\") is a role service that conserves storage space by storing only one copy of redundant chunks of files. Data duplication is appropriate to specific workloads, like backup volumes and file servers. It is not appropriate for database storage or operating system data or boot volumes. Data deduplication had required NTFS , although ReFS is supported since 1709. Data deduplication runs as a low-priority background process when the system is idle, by default; however its behavior can be configured based on its intended usage. Deduplication works by scanning files, and breaking them into unique chunks of various sizes that are collected in a chunk store . The original locations of chunks are replaced by reparse points . When a file is recently written, it is written in the standard, unoptimized form; the accumulation of such files is known as churn . Other jobs associated with deduplication include garbage collection , integrity scrubbing , and (when disabling deduplication) unoptimization . There are several deployment scenarios considered for data deduplication: - General purpose file servers Users often store multiple copies of the same, or similar, documents and files. Up to 30-50% of this space can be reclaimed using deduplication. - Virtualized Desktop Infrastructre (VDI) deployments Virtual hard disks that are used for remote desktops are essentially identical. Data Deduplication can also amelioriate the drop in storage performance when many users simultaneously log in at the start of the day, called a VDI boot storm . - Backup snapshots are an ideal deployment scenario because of the data is so duplicative. Deduplication is especially useful for disk drive backups, since snapshots typically differ little from each other. File shares Windows Server 2016 supports file shares via two protocols, both of which require the fs-fileserver role service: - SMB , long the standard for Windows networks - NFS , typically used in Linux, requires the installation of fs-nfs-service role service BranchCache enables client computers at remote locations to cache files accessed from shares, so that other computers at the same location can access them. Install the FS-BranchCache feature and enable the File and Printer Sharing and Branchcache - Hosted Cache Server (uses HTTPS) firewall display groups. Media Every track of a hard drive platter is split into disk sectors , traditionally 512 bytes. A block is commonly called an \"allocation unit\" in Windows, but also commonly called a cluster. Storage left over unused in partially unused blocks is known as slack space . A new disk must first be initialized , that is, a partition table style must be chosen: - GPT : 128 partitions per disk, maximum volume size of 18 exabytes (2 60 bytes). Booting from a GPT drive is not possible unless the computer architecture supports EFI-based boot partitions. - MBR : older format that is commonly used for removable media, supporting volumes up to 2 TB with up to 4 primary partitions , although a common workaround is to make one of these partitions an extended partition , which can be be further subdivided into logical drives Mounting a partition as a single filesystem produces a volume , although the distinction can often be lost. The exception would be a case where a volume spans multiple partitions or physical disks, as is possible with software RAID. Virtual hard disks can be created with [Powershell][New-VHD] or in diskmgmt.msc and come in two formats: - VHD - VHDX Only 2 filesystem options are available for modern servers: - NTFS supports volumes up to 16 TB with the default 4 KB allocation unit size (but 256 TB with the 64 KB allocation unit size) and is required by some Windows Server services like AD DS , File Replication Service , Volume Shadow Copy Service , and Distributed File System - ReFS uses the same system of permissions as NTFS and offers error checking and repair capabilities that NTFS does not, but it does not support NTFS features like file compression, Encrypted File System , and disk quotas. ReFS supports a maximum file size of 16 exabytes and volumes up to 1 yobibyte (2 80 bytes) Software RAID can be implemented by creating Spanned , Striped , or RAID-5 volumes in diskmgmt.msc . A more modern and preferred technique is to create storage pools in [Storage Spaces][Storage Spaces]. S2D Although a cluster can normally be created in the GUI Failover Cluster Manager , in order to use Storage Spaces Direct the system must be prevented from automatically creating storage, which necessitates creation in PowerShell with the NoStorage switch parameter, and then S2D must be enabled using Enable-ClusterStorageSpacesDirect . This command scans all cluster nodes for local, unpartitioned disks , which are added to a single storage pool and classified by media type in order to use the fastest disks for caching. The recommended drive configuration for a node in an S2D cluster is a minimum of six drives, with at least 2 SSDs and at least 4 HDDs, with no RAID or other intelligence that cannot be disabled. Caching is configured automatically, depending on the combination of drives present - NVMe + SSD : NVMe drives are configured as a write-only cache for the SSD drives - NVMe + HDD : NVMe drives are read/write cache - NVME + SSD + HDD : NVME are write-only for the SSD drives and read/write for HDD drives - SSD + HDD : SSD drives are read/write cache Microsoft defined two deployment scenarios for Storage Spaces Direct: - Disaggregated which creates two separate clusters, one of which is a Scale-out File Server dedicated to storage, essentially functioning as a SAN. This solution requires the [DCB][DCB] role for traffic management. At least two 10Gbps Ethernet adapters are recommended per node, preferably adapters that use RDMA. - Hyper-converged , where a single cluster node hosts VMs and storage. This solution is much less expensive because it requires less hardware and generates much less network traffic, but storage and compute can't scale independently: adding a node to storage necessarily entails adding one to the Hyper-V hosts, and vice versa. Storage Replica Storage Replica supports one-way replication between standalone servers, between clusters, and between storage devices within an [ asymmetric (stretch) cluster ][asymmetric cluster]. - Synchronous replication is possible when the replicated volumes can mirror data immediately, ensuring no data loss in case of failover - Asynchronous replication is preferable when the replication partner is located over a WAN link Storage Replica improves on DFS Replication, which is exclusively asynchronous and file-based, by using SMBv3 (port 445). Storage Replica requires two virtual disks, one for logs and one for data, which are the same size for each replication partner, and all the physical disks must use the same sector size. WSUS Windows Server Update Services (WSUS) can be configured from the command-line with wsusutil.exe. There are 5 basic WSUS architecture configurations Single WSUS Server downloads updates from Microsoft Update, and all the computers on the network download updates from it. A single server can usupport up to 25,000 clients. Replica WSUS Servers: a central WSUS server downloads from Microsoft Update, and after approval the updates are distributed to downstream servers at remote locations. Autonomous WSUS Servers: a central WSUS server downloads from Microsoft Update, all of which are distributed to remote servers; each remote site's administrators are individually responsible for evaluating and approving updates. Low-bandwidth WSUS Servers at remote sites download only the list of approved updates, which are then retrieved from Microsoft Update over the Internet, minimizing WAN traffic. Disconnected WSUS Servers have updates imported from offline media (DVD-ROMs, portable drives, etc), utilizing no WAN or Internet bandwidth whatsoever. When a computer first communicates with a WSUS server, it is added to the All Computers and and Unassigned Computers group automatically, which is created by default. Windows Server Backup To back up a VM without any downtime, integration services must be installed and enabled, and all disks must be basic disks formatted with NTFS . Windows Server Backup - System state includes boot files, Active Directory files, SYSVOL (when run on a DC), the registry, and other data. - System reserved is a special partition containing Boot Manager and Boot Configuration data. \ud83d\udcd8 Glossary adfind Query the schema version associated with Active Directory [Desmond][Desmond2009]: 53 adfind -schema -s base objectVersion adprep Prepare Active Directory for Windows Server upgrades. Must be run on the Infrastructure Master role owner with the flag /domainprep . [Desmond][Desmond2009]: 29 arp a d s bcdedit Change Windows bootloader to Linux, while dual booting ::Manjaro bcdedit /set {bootmgr} path \\EFI\\manjaro\\grubx64.efi ::Fedora bcdedit /set {bootmgr} path \\EFI\\fedora\\shim.efi Enable or disable Test Signing Mode ref bcdedit /set testsign on bcdedit /set testsign off bootrec Windows Recovery Environment command that repairs a system partition Use when boot sector not found bootrec /fixboot Use when BCD file has been corrupted bootrec /rebuildbcd cmdkey add delete generic list pass smartcard user Add a user name and password for user Mikedan to access computer Server01 with the password Kleo docs.microsoft.com cmdkey /add:server01 /user:mikedan /pass:Kleo dism Add-Driver Add-Package Add-ProvisionedAppxPackage Append-Image Apply-Image Apply-Unattend Capture-Image Cleanup-Image Commit-Image Disable-Feature Enable-Feature Export-Driver Export-Image Get-Driverinfo Get-Drivers Get-Featureinfo Get-Features Get-ImageInfo Get-MountedImageInfo Get-Packageinfo Get-Packages Get-ProvisionedAppxPackages List-Image Remount-Image Remove-Driver Remove-Image Remove-Package Remove-ProvisionedAppxPackage Set-ProvisionedAppxDataFile Unmount-Image Mount an image Zacker : 71 dism /mount-image /imagefile:$FILENAME /index:$N /name:$IMAGENAME /mountdir:$PATH Practice Labs dism /mount-wim /wimfile:c:\\images\\install.wim /index:1 /mountdir:c:\\mount Add a driver to an image file that you have already mounted Zacker : 72 dism /image:$FOLDERNAME /add-driver /driver:$DRIVERNAME /recurse Commit changes and unmount the image Zacker : 75 dism /unmount-image /mountdir:c:\\mount /commit Determine exact name of Windows features that can be enabled and disabled Zacker : 75 dism /image:c:\\mount /get-features Scan an image, checking for corruption dism /Online /Cleanup-Image /ScanHealth Check an image to see whether any corruption has been detected dism /Online /Cleanup-Image /CheckHealth Repair an offline dicsk using a mounted image as a repair source dism /Image:C:\\offline /Cleanup-Image /RestoreHealth /Source:C:\\test\\mount\\windows Zacker: 71-75 dism /mount-image /imagefile:C:\\images\\install.wim /index:1 /mountdir:C:\\mount dism /add-package /image:C:\\mount /packagepath:C:\\updates dism /add-driver /image:C:\\mount /driver:C:\\drivers\\display.driver\\nv_dispi.inf dism /commit-image /image:C:\\mount dism /unmount-image /image:C:\\mount djoin Perform an offline domain join for a Nano Server Zacker: 46 djoin /provision /domain practicelabs /machine PLABNANOSRV01 /savefile .\\odjblob Load the odjblob file created offline on the Nano Server. djoin /requestodj /loadfile c:\\odjblob /windowspath c:\\windows /localos dnscmd Replicate an AD-integrated DNS zone to specific DCs ref dnscmd . /CreateDirectoryPartition FQDN Enable GlobalNames zone support dnscmd <servername> /config /enableglobalnamessupport 1 Observe status of socket pool dnscmd /info /socketpoolsize Configure DNS socket pool size (0 through 10,000) dnscmd /Config /SocketPoolSize <value> dsquery Find the Active Directory Schema version from the command-line ref pwsh dsquery * cn=schema,cn=configuration,dc=domain,dc=com -scope base -attr objectVersion\" JEA Just Enough Administration (JEA) allows special remote sessions that limit which cmdlets and parameters can be used in a remote PowerShell session. These are implemented as restricted endpoints , to which only members of a specific security group can gain access. This offers a way to administer remote servers and move away from the traditional method using RDP. net Map a network location to a drive letter Practice Lab net use x: \\\\192.168.0.35\\c$ Stop/start a service net stop dns net start dns netdom Rename a computer netdom renamecomputer %computername% /newname: newcomputername Join a computer to a domain cf. Add-Computer , Zacker: 21 netdom join %computername% /domain: domainname /userd: username /password:* netsh Enable port forwarding (\" portproxy \") to a WSL2 distribution ( src ) netsh interface portproxy add v4tov4 listenaddress=0.0.0.0 listenport=2222 connectaddress=172.23.129.80 connectport=2222 Configure DNS to be dynamically assigned netsh interface ip set dns \"Wi-Fi\" dhcp Delete Wi-Fi profiles netsh wlan delete profile name=* Turn off Windows firewall netsh advfirewall set allprofiles state off Enable firewall rule group netsh advfirewall firewall set rule group=\u201dFile and Printer Sharing\u201d new enable=yes Show Wi-Fi passwords ( src netsh wlan show profile wifi key=clear Check/reset WinHTTP proxy netsh winhttp show proxy netsh winhttp reset proxy ntdsutil Used to transfer FSMO roles between domain controllers. [ Desmond: 30 ][Desmond2009] regsvr32 Register a DLL dependency in order to enable the Active Directory Schema MMC snap-in on a DC [Desmond][Desmond2009]: 54 regsvr32 schmmgmt.dll route p print add change delete Basic usage route add 192 .168.2.1 mask ( 255 .255.255.0 ) 192 .168.2.4 runas env netonly profile / no profile savecred showtrustlevels smartcard trustlevel user: Settings appsfeatures personalization printers windowsupdate about activation apps-volume appsforwebsites assignedaccess autoplay backup batterysaver bluetooth camera clipboard colors connecteddevices cortana crossdevice datausage dateandtime defaultapps delivery-optimization developers deviceencryption devices-touchpad display easeofaccess-display emailandaccounts findmydevice fonts keyboard lockscreen maps messaging mobile-devices mousetouchpad multitasking network network-wifi nfctransactions nightlight notifications optionalfeatures otherusers pen personalization-background personalization-colors personalization-start personalization-start-places phone powersleep privacy project proximity quiethours quietmomentsgame quietmomentspresentation quietmomentsscheduled recovery regionformatting regionlanguage remotedesktop savelocations screenrotation signinoptions signinoptions-launchfaceenrollment sound speech speech startupapps storagepolicies storagesense surfacehub-accounts surfacehub-calling surfacehub-devicemanagenent surfacehub-sessioncleanup surfacehub-welcome sync tabletmode taskbar themes troubleshoot typing usb videoplayback wheel windowsdefender windowsinsider workplace yourinfo sfc sfc /scannow shutdown Immediate restart shutdown /r /t 0 Log off shutdown /L slmgr ato dli dlv ipk rearm upk xpr sysdm 2 3 4 5 tracert On Windows, this command is aliased to traceroute which is the Linux command. [Lammle][Lammle]: 112 wbadmin enable backup get items get versions start backup start recovery start systemstaterecovery -backupTarget -hyperv -vsscopy | -vssFull Backup the entire drive, excluding some VMs wbadmin enable backup -backupTarget \\\\backups\\hostdr\\temp\\ -include:c: -exclude: C:\\VMs\\VM1.vhdx, C:\\VMs\\VMAR.vhd -vsscopy -quiet Zacker : 325-326 wbadmin get versions wbadmin get items -version: 11/14/2016:05:09 wbadmin start recovery -itemtype:app items:cluster -version:01/01/2008-00:00 Zacker : 422 wbadmin start systemstaterecovery -version:11/27/2016-11:07 wbadmin get versions wdsutil initialize-server remInst wdsutil /initialize-server /remInst:\"D:\\RemoteInstall\" winrm # List all WinRM listeners winrm enumerate winrm/config/listener # Display WinRM configuration winrm get winrm/config # Add an address to Trusted Hosts list winrm set winrm/config/client @ { TrustedHosts = \"192.168.10.41\" } winver wmic bios logicaldisk memorychip os path Recover Windows product key [fossbytes.com][https://fossbytes.com/how-to-find-windows-product-key-lost-cmd-powershell-registry/] wmic path softwarelicensingservice get OA3xOriginalProductKey Display information about installed RAM wmic memorychip list full List all objects of type Win32_LogicalDisk using that class's alias logicaldisk . [Desmond][Desmond2009]: 642 pwsh wmic logicaldisk list brief Recover serial number of a Lenovo laptop [pcsupport.lenovo.com][https://pcsupport.lenovo.com/us/en/solutions/find-product-name] wmic bios get serialnumber Display BIOS version wmic bios get biosversion Display operating system architecture wmic os get osarchitecture Display operating system type (48 is Windows 10) wmic os get operatingsystemsku [wsl][msdocs:wsl.exe] l s t \\ export import set-default-version [wsusutil]][msdocs:wsusutil.exe] Specify a location for downloaded updates Zacker : 393 C:\\Program Files\\Update Services\\Tools\\wsusutil.exe postinstall content_dir=d:\\wsus Specify SQL server, when not using the default WID database C:\\Program Files\\Update Services\\Tools\\wsusutil.exe postinstall sql_instance_name=\"db1\\sqlinstance1\"- content_dir=d:\\wsus wt d p split-pane focus-tab Open the default Windows Terminal profile and also an Ubuntu WSL tab [bleepingcomputer.com][https://www.bleepingcomputer.com/news/microsoft/windows-terminal-09-released-with-command-line-arguments-and-more/] wt; new-tab -p \"Ubuntu-18.04\" Open a split pane of the default profile in the D:\\ folder and the cmd profile in the E:\\ folder [bleepingcomputer.com][https://www.bleepingcomputer.com/news/microsoft/windows-terminal-09-released-with-command-line-arguments-and-more/] wt -d d:\\ ; split-pane -p \"cmd\" -d e: Open the default profile and an Ubuntu WSL profile, but with the first tab focused [bleepingcomputer.com][https://www.bleepingcomputer.com/news/microsoft/windows-terminal-09-released-with-command-line-arguments-and-more/] wt ; new-tab -p \"Ubuntu-18.04\"; focus-tab -t0 xcopy Copy from one directory to another Practice Lab xcopy /s c:\\inetpub\\wwwroot c:\\nlbport","title":"Windows Server"},{"location":"Misc/Windows/WS2016/#windows-server","text":"","title":"Windows Server"},{"location":"Misc/Windows/WS2016/#certification-exams","text":"Number Title 70-740 Installation, storage and Compute with Windows Server 2016 70-741 Networking with Windows Server 2016 70-742 Identity with Windows Server 2016 Find notes on labs here .","title":"Certification exams"},{"location":"Misc/Windows/WS2016/#installation","text":"Windows Server 2016 installations are determined by the most suitable installation method, option, and edition. Installation methods : An upgrade is an installation performed in-place with existing data intact and is opposed to a clean installation . A migration is a clean installation with old data transferred over. Migrations are facilitated by Powershell and [command prompt][SmigDeploy.exe] tools Installation options include Desktop Experience, [Server Core][Server Core], and Nano Server . The most important installation edition is Windows Server 2016 Datacenter edition , which is the only edition to have several important features that figure prominently in the exam. Storage Spaces Direct , Storage Replica Shielded VMs Network controller . Various other installation options exist, including: Windows Server 2016 Standard , Essentials , Multipoint Premium , Storage , and Hyper-V editions. Server installations are also influenced by choice of activation model .","title":"Installation"},{"location":"Misc/Windows/WS2016/#licensing","text":"Servicing channels provide a way of separating users into deployment groups for feature and quality updates. Semi-Annual Channel - previously known as Current Branch for Business (CBB) - features updates twice a year. It is more appropriate for non-infrastructure workloads that can be deployed through automation. Long Term Servicing Channel (LTSC) has a minimum servicing lifetime of 10 years and was designed to be used only for specialized devices such as those that control medical equipment or ATM machines, receiving new feature releases every 2-3 years","title":"Licensing"},{"location":"Misc/Windows/WS2016/#server-core","text":"Installing the Windows Server 2016 Server Core foregoes the possibility of later switching back to Desktop Experience, as had been possible in previous editions. Notably, WDS is incompatible with Server Core installations. Server Core installations can be managed with a GUI with the use of MMC snap-ins . Because MMC is reliant on Distributed Component Object Model (DCOM) technology, firewall rules have to be enabled to allow DCOM traffic (ref. Set-NetFirewallRule ).","title":"Server Core"},{"location":"Misc/Windows/WS2016/#nano-server","text":"Nano Server , a new installation option introduced in Windows Server 2016, provides a much smaller footprint and attack surface than even Server Core, but supports only some roles and features. Installation is done by building a VHD image via PowerShell on another computer. That VHD is then deployed as a VM or used as a boot drive for a physical server. Booting a Nano Server VM produces a text-based interface called the Nano Server Recovery Console , a menu system that allows configuration of static network options (DHCP is enabled by default). The DNS server may not be configured interactively, but must be specified when building the image with the Ipv4Dns parameter. If a Nano Server is domain-joined a remote Powershell session will authenticate via Kerberos. If not, its name or IP address must be added to the Trusted Hosts list. The Windows Server 2016 installation media contains a NanoServer directory, from which the NanoServerImageGenerator Powershell module must be imported. It also contains a Packages subdirectory, with CAB files containing roles and features that correspond to named parameters or packages that are specified as values to the Packages named parameter when building a Nano Server image. Cmdlet Description Edit-NanoServerImage Add a role or feature to an existing Nano Server VHD file New-NanoServerImage Used to create a Nano Server VHD file for Nano Server installation","title":"Nano Server"},{"location":"Misc/Windows/WS2016/#activation","text":"Server installations are influenced by choice of activation model. MAK is suitable for small networks, but large enterprises may opt for KMS . - MAK activations are subdivided into Independent and Proxy , based on whether or not a VAMT is used. - KMS activations, which distribute GVLK s, are valid for a period of time and require the installation of a role and management tools . KMS operates on TCP port 1688. - AVMA simplifies the process of activating Hyper-V VMs running Windows Server 2012 or 2016. Active Directory-based activation is an alternative for enterprises who opt to activate licenses through the existing AD DS domain infrastructure. Any domain-joined computers running a supported OS with a GVLK will be activated automatically and transparently. The domain must be extended to the Windows Server 2012 R2 or higher schema level, and a KMS host key must be added using the VAMT. After Microsoft verifies the KMS host key, client computers are activated by receiving an activation object from the DC. MS Docs","title":"Activation"},{"location":"Misc/Windows/WS2016/#images","text":"Many enterprises have begun virtualizing their server environments to take advantage of the many cost, reliability, and performance benefits that this change creates. Migrations should start with systems that are peripheral to main business interests before moving on to those that are more vital. A carefully documented protocol should be developed to facilitate the conversion of physical hard disks to VHDs for use in Hyper-V guests. Supported guest OSes include Linux and FreeBSD. The Microsoft Assessment and Planning (MAP) Toolkit is a free software tool that intelligently constructs a database of the hardware, software, and performance of computers on a network to plan for an operating system upgrade or virtualization. MAP supports the following discovery methods: Active Directory Domain Services Windows networking protocols System Center Configuration Manager IP address range scanning Computer names entered manually or imported from a file Server Core relies on the command-line for system maintenance, including updates which must be installed directly to the image using dism.exe or equivalent Powershell commands.","title":"Images"},{"location":"Misc/Windows/WS2016/#containers","text":"","title":"Containers"},{"location":"Misc/Windows/WS2016/#dns","text":"","title":"DNS"},{"location":"Misc/Windows/WS2016/#installation_1","text":"DNS server role requiremenets: - Statically assigned IP - Signed-in user must be member of local Administrators group There are several recommended DNS deployment scenarios, all of which involve installing DNS on a Server Core or Nano Server instance. This is because these installation options offer a reduced attack surface, a reduced resource footprint, and reduced patching requirements. - DNS on DC : All DNS features are available and supports AD-integrated, primary, secondary, and stub zones. - DNS on RODC : Passes DNS zone updates to a writeable DC - DNS on standalone member server : Supports file-based primary, secondary, and stub zones but requiring zone replication because there is no integration over AD.","title":"Installation"},{"location":"Misc/Windows/WS2016/#nano-server_1","text":"Installing DNS on a running Nano Server image requires running Install-NanoServerPackage as well as enabling the \"DNS-Server-Full-Role\" optional feature using Enable-WindowsOptionalFeature . As of early 2017, Nano Server only supported a few roles, including DNS , but was only able to do so with some limitations - Nano Server can only support file-based DNS and cannot host AD-integrated zones. - Nano Server only supports the Semi-Annual servicing channel license. - Nano Server is not suitable for primary zones, only caching-only, forwarder, or secondary zone DNS servers","title":"Nano Server"},{"location":"Misc/Windows/WS2016/#zones","text":"Zones can be considered one or more DNS domains or subdomains, associated with zone files , which compose the DNS database itself and contain two types of entries: Parser commands , which provide shorthand ways to enter records: $ORIGIN , $INCLUDE , and $TTL Resource records are whitespace delimited text files with columns for name, time to live, class, type, and data The copies of zone files local to individual DNS servers can be primary (read/write) or secondary (read-only). A primary zone is a writable copy of a DNS zone that exists on a DNS server. A secondary zone is a read-only replica of a primary zone and necessitates the presence of a primary zone for the same zone. Defining a secondary zone via PowerShell requires specifying that zone's MasterServers . In Windows Server, zone files can also be integrated with Active Directory, making what is called an Active Directory Integrated Zone . These allow multi-master zones , meaning any DC can process zone updates and the zone can be replicated to any DC in the domain or forest. An AD-integrated zone can be specified by passing the ReplicationScope parameter to the Add-DnsServerPrimaryZone cmdlet. Stub zones contains only name server (NS) records of another zone, but unlike a forwarder is able to update when name servers in a target zone change. Reverse Lookup zones are used to resolve IP addresses to FQDNs. Reverse lookup zones for public IP address space are often administered by ISPs, and they are useful in spam filtering to double-check the source domain name with the IP address. GlobalNames zones provide \"single label name resolution\" (as opposed to a FQDN) and are intended to replace WINS servers.","title":"Zones"},{"location":"Misc/Windows/WS2016/#query-traffic","text":"The process of resolving a query by querying other DNS servers is called recursion . Recursion can be disabled outright but Windows Server 2016 supports recursion scopes which will allow recursion to be disabled unless certain conditions are met (such as receiving the request on a particular interface). There are two types of query in the context of recursion: - Recursive query sent by the petitioner: that is, the original query which begins recursion. - Iterative query : individual queries sent out to authoritative name servers in order to resolve a recursive query. Root hints are preconfigured root servers that are necessary to begin the recursion process. The DNS Server service stores root hints in %systemroot%\\System32\\dns\\CACHE.DNS . These can be edited through the GUI or by using the PowerShell commands Add- , Import- , Remove- , and Set-DnsServerRootHint . Forwarding of a request occurs when a petitioned DNS server is unable to resolve the query because it is both: - Non-authoritative for the specified zone, and - Does not have the response cached. Two actions are possible when forwarding: - Configure a DNS server only to respond to queries it can satisfy by referencing locally-stored zone information, forwarding all other requests. - Configure forwarding for specific zones through conditional forwarding A secondary zone is not to be confused with delegation , where a DNS server delegates authority over part of its namespace (i.e. a subdomain) to one or more other servers. Windows Server 2016 supports a DNS GlobalNames zone meant to supercede WINS, which served a role similar to DNS for the old NetBIOS naming standard. NetBIOS names use a nonhierarchical structure (i.e. are a single name and not divisible into sub-domains) based on a name up to 16 characters long (although the 16th character defines a particular service running on the host defined by the previous 15). An organization must share a single GlobalNames zone, which must be created in PowerShell manually.","title":"Query traffic"},{"location":"Misc/Windows/WS2016/#resource-records","text":"Zone scavenging allows servers with stale records to remove them. This feature is disabled by default, but can be set at the server or zone level. Type Description A IPv4 address record AAAA IPv6 address record CNAME Hostname or alias for hosts in the domain MX Where mail for the domain should be delivered NS Name servers PTR Reverse lookup SOA Each zone contains a single SOA record SRV Generalized service location record, used for newer protocols instead of protocol-specific records TXT Typically holds machine-readable data","title":"Resource records"},{"location":"Misc/Windows/WS2016/#security","text":"DNSSEC offers security features using public key certificates. A socket pool can be used to configure the DNS server to use a random source port when issuing DNS queries. Response rate limiting can pose a defense against DNS DoS attacks by ignoring potentially malicious, repetitive requests. DNS-based Authentication of Named Entities (DANE) is supported by Windows Server 2016 to reduce man-in-the-middle attacks. DANE works by informing DNS clients requesting records from the domain which Certification Authoority they must expect digital certificates to be issued from.","title":"Security"},{"location":"Misc/Windows/WS2016/#policies","text":"Zone transfer policies can prevent or allow zone transfers to any server, to name servers, or to servers specified by FQDN or IP address. DNS Policy is a new feature in Windows Server 2016 that can control DNS server behavior depending on certain criteria. These criteria include: Client subnet Recursion scope Zone scope","title":"Policies"},{"location":"Misc/Windows/WS2016/#dnssec","text":"DNSSEC is a security setting for DNS that enables all DNS records in a zone to be digitally signed by a trust anchor which validates DNSKEY resource records. Root and top-level domain zones already have trust anchors configured and merely have to have it enabled. To implement trust anchors: - A TrustAnchors zone must be created, which will store public keys associated with specific zones. A trust anchor from the secured zone must be created on every DNS server that hosts the zone. - A Name Resolution Policy Table (NRPT) GPO must be created (Windows Settings\\Name Resolution Policy) This option can require DNSSEC based on computer name prefix or suffix, FQDN, or subnet. - DNSSEC key master is a special DNS server that generates and manages signing keys for DNSSEC protected zones. DANE allows you to publish certificate information within the DNS zone, rather than one of the thousands of trusted CAs. This protects against rogue/compromised CAs issuing illegitimate TLS certificates. Two cryptographic keys: - Zone Signing Key (ZSK) signs zone data including individual resource records other than DNSKEY. It is also used to create the KSK. - Key Signing Key (KSK) is used to sign all DNSKEY records at the zone root. DNSSEC record types: - RRSIG \"resource record signature\" each of which matches and provides a signature for an existing record in a zone - NSEC proves nonexistence of a record - NSEC3 NSEC replacement that prevents zone walking - NSEC3PARAM specifies the NSEC3 records included in response for DNS names that don't exist - DNSKEY stores public key used to verify a signature - DS delegation signer records secure delegations","title":"DNSSEC"},{"location":"Misc/Windows/WS2016/#dsc","text":"IaaS management of servers is possible with Desired State Configuration (DSC) , a feature of Windows PowerShell where script files stored on a central server can apply specific a specific configuration to nodes. These scripts are idempotent , meaning that they can be applied repeatedly without generating errors. The DSC model is composed of phases 1. Authoring Phase, where MOF definitions are created 2. Staging Phase, where declarative MOFs are staged and a Configuration calculated per node 3. \"Make It So\" Phase, where declarative Configurations are implemented through imperative providers Components of DSC scripts include: - Local Configuration Manager : engine running on the client system that received configurations from the DSC server and applies them to the target. - Node block specifies the names of target computers. - Resource block specifies settings or components and the values that the configuration script should assign to them. DSC configurations can be deployed in two different refresh modes Pull architecture : target LCM periodically retrieves configuration from a Pull Server , which consolidates MOF files. Push architecture : configuration is sent to target in response to explicit invocation of Start-DSCConfiguration on the server. LCM has to be configured to accept Configurations of either refresh mode.","title":"DSC"},{"location":"Misc/Windows/WS2016/#tasks","text":"Set LCM to push mode [ DSCLocalConfigurationManager ()] Configuration LCMConfig { Node localhost { Settings { RefreshMode = 'Push' } } } Install Telnet client Configuration InstallTelnetLocal { Import-DscResource -ModuleName 'PSDesiredStateConfiguration' Node localhost { WindowsOptionalFeature InstallTelnet { Name = \"Telnet-Client\" Ensure = \"Present\" } } } Install WSL Configuration InstallWSLLocal { Import-DscResource -ModuleName 'PSDesiredStateConfiguration' Node localhost { WindowsOptionalFeature InstallWSL { Name = \"Microsoft-Windows-Subsystem-Linux\" Ensure = \"Present\" } } }","title":"Tasks"},{"location":"Misc/Windows/WS2016/#failover-clusters","text":"Failover clusters are composed of computers called nodes and can be created using New-Cluster . which typically possess a secondary network adapter, used for cluster communications. Before Windows Server 2016, all cluster nodes had to belong to the same domain, but now this is but one of several possible cluster types called a single-domain cluster. A failover cluster can also be multi-domain , or workgroup , depending on how or if the servers are joined to domains. A cluster can also be detached from AD, even though its nodes are joined. A cluster whose servers are joined to a single domain is typically associated with a cluster name object in Active Directory, which serves as its administrative access point . A workgroup cluster or a detached cluster need to have the cluster's network name registered in DNS as its administrative access point, which can be specified in Powershell with the AdministrativeAccessPoint named parameter. Additionally, on a workgroup cluster the same local administrator account must be created on every node, preferably the builtin Administrator account, although a different account can be configured if a particular Registry key is [created][New-ItemProperty] on each node. Nodes that are domain-joined support CredSSP or Kerberos authentication, but workgroup nodes support NTLM authentication only. Three types of witness resources can help to ensure a quorum takes place in clusters. This is necessary to prevent a split-brain situation, where communication failures between nodes cause separate segments of the clusters to continue operating independently of each other. A witness is created when a cluster has an even number of nodes, and only one can be configured. [pwsh][Set-ClusterQuorum] - Disk witness : dedicated disk in shared storage that contains a copy of the cluster database - File Share witness : SMB file share containing a Witness.log file with information about the cluster - Cloud witness : blob stored in Azure Scale-out File Server (SoFS) is a clustered role providing highly available storage to cluster nodes. SoFS ensures continuous availability in the case of a node failure. Using SoFS, multiple nodes can also access the same block of storage at the same time, and for this reason is is an active/active or dual active system, as opposed to one where only one node provides accessible shares, or an active/passive system. SoFS is specifically recommended for use on Hyper-V and SQL Server clusters and can be installed with Add-ClusterScaleOutFileServer . SoFS shares are created with the [ New-SmbShare ][New-SmbShare] PowerShell cmdlet. SoFS shares are located on Cluster Shared Volumes (CSV) , a shared disk containing an NTFS or ReFS volume that is made accessible for read and write operations by all nodes within a failover cluster. CSVs solved a historical problem with using NTFS volumes with VMs in previous versions of Windows Server. NTFS is designed to be accessed by only one operating system instance at a time. In Windows Server 2008 and earlier, this meant that only one node could access a disk at a time, which had to be mounted and dismounted for every VM. The solution was to create a pseudo-file system called CSVFS , sitting on top of NTFS, that enables multiple drives to modify a disk's content at the same time, but restricting access to the metada to the owner or coordinator . The coordinator node refers to the cluster node where NTFS for the clustered CSV disk is mounted, any other node is called a Data Server (DS) . VM resiliency can be configured by adjusting settings in response to changes in VM state: - Unmonitored : VM owning a role is not being monitored by the Cluster Service - Isolated : Node is not currently an active member of the cluster, but still possess the role - Quarantine : Node has been drained of its roles and removed from the cluster for a specified length of time. Cluster Operating System Rolling Upgrade is a new feature that reduces downtime by making it possible for a cluster to have nodes running both Windows Server 2012 R2 and Window Server 2016. Using this feature, nodes can be brought down for an upgrade. When [Storage Spaces][Storage Spaces] is combined with a failover cluster, the solution is known as Clustered Storage Spaces .","title":"Failover clusters"},{"location":"Misc/Windows/WS2016/#high-availability","text":"Hyper-V Replica allows simple failover to occur between Hyper-V hosts, without the need for a cluster. To configure a simple one-way failover solution using Hyper-V Replica, configure the destination VM as a replica server , either in Hyper-V Manager or PowerShell. [ Set-VMReplicationServer ][Set-VMReplicationServer] lab The destination host must also have firewall ports opened corresponding to the authentication method chosen. The source VM, which is to be replicated, must have its options configured through the Enable Replication wizard . [ Enable-VMReplication ][Enable-VMReplication] To use Hyper-V Replica as a (two-way) failover solution, configure both VMs as replica servers. Migrations can take place one of three methods: - Live Migration moves only the system state and live memory contents, not data files. Live migration requires that the hosts be, if not clustered, at least part of the same (or a trusted) domain. Live Migration requires that VHD files be placed on shared storage and both hosts have appropriate permissions to access said storage. An unpopulated VM is created on the destination with the same resources as the source before transferring memory pages. Once the servers have an identical memory state, the source VM is suspended and the destination takes over. Hyper-V notifies the network switch of the change, diverting network traffic to the destination. Authentication can be made by [CredSSP][CredSSP] or Kerberos. When a Hyper-V cluster is created, the Failover Cluster Manager launches the High Availability Wizard, which configures the VM to support Live Migration. The same thing can be done with the PowerShell cmdlet Add-ClusterVirtualMachineRole . Additionally, using Kerberos authentication for live migration requires constrained delegation , which enables a server to act on behalf of a user for only certain defined services. This must be configured within Active Directory Users and Computers , by opening the Properties of the source Computer object, and changing the setting under the Delegation tab. - An additional, outdated method of migration is quick migration , which was present in Windows Server prior to the introduction of live migration and persists in Windows Server 2016 for backward compatibility. A quick migration involves pausing the VM, saving its state, moving the VM to the new owner, and starting it again. A quick migration always involves a short period of VM downtime. Shared Nothing Live Migration requires that source and destination VMs be members of the same (or trusted) domain, and source and domain servers must be running the same processor family (Intel or AMD) and linked by an Ethernet network running a minimum of 1 Gbps. Additionally, both Hyper-V hosts must be running idential virtual switches that use the same name; otherwise the migration process will be interrupted to prompt the operator to select a switch on the destination server. The process of migrating is almost identical to a Live Migration, except that you select the \"Move the Virtual Machine's Data To a Single Location\" option on the Choose Move Options page of the Move Wizard. Storage Migration works by first creating new VHDs on the destination corresponding to those on the source server. While the source server continues to operate using local files, Hyper-V begins mirroring disk writes to the destination server and begins a single-pass copy of the source disks to the destination begins, skipping blocks that have already been copied. Once the copy has completed, the VM begins working from the destination server and the source files are deleted. For a VM that is shut off, storage migration is equivalent to simply copying files from source to destination. Site-aware clusters have failover affinity . Node fairnes evalutes memory and CPU loads on cluster nodes over time.","title":"High availability"},{"location":"Misc/Windows/WS2016/#cluster-management","text":"VM Monitoring allows specific services to be restarted or failed-over when a problem occurs. To use VM Monitoring: - The guest must be joined to the same domain as the host - The host administrator must be a member of the guest's local Administrators group - And Windows Firewall rules in the Virtual Machine Monitoring group must be enabled. The service can then be monitored using [ Add-ClusterVMMonitoredItem ][Add-ClusterVMMonitoredItem].","title":"Cluster management"},{"location":"Misc/Windows/WS2016/#migration","text":"VMs can be moved from node to node of a cluster using live , storage , or quick migrations. VM network health protection is a feature (enabled by default) that detects whether a VM on a cluster node has a functional connection to a designated network. If not, the cluster live migrates the VM role to another node that does have such a connection. This setting can be controlled in Hyper-V Manager > VM Settings > Advanced Features > Protected network","title":"Migration"},{"location":"Misc/Windows/WS2016/#gpo","text":"Group Policy Objects (GPO) facilitate the uniform administration of large numbers of users and computers. GPOs can be local or domain-based . Local GPOs come in several varieties, applied in the following order (last takes highest precedence): - Local Group Policy applied to computers - Administrators and Non-Administrators Local Group Policy applied to users based on their membership in local Administrators group. - User-specific Local Group Policy : Domain-based GPOs consist of two components a [ container ][Group Policy container] and a [ template ][Group Policy template]. These are stored in different locations and replicated by different means. - Containers define the fundamental attributes of a GPO, each of which is assigned a GUID, and are stored in the AD DS database and replicated to other domain controllers using intrasite or intersite AD DS replication schedule. - Templates, a collection of files and folders that define the actual GPO settings, are stored in the SYSVOL shared folder ( %SystemRoot%\\SYSVOL\\Domain\\Poligicies\\{GUID} ) on all DCs. SYSVOL replication is handled by the DFS Replication Agent since Windows Server 2008. A GPO consists of 2 top-level nodes: - Computer Configuration contains settings that are applied to computer objects to which the GPO is linked - User Configuration containers user-related settings, applied when a user signs in and thereafter and automatically refreshed every 90-120 minutes Beneath each of these nodes are folders that group settings - Software Settings - Windows Settings allows basic configuration for computers or users - Administrative Templates contains Registry settings that control user, computer, and app behavior and settings, grouped logically into folders Although domain controllers store and serve GPOs, the client computer itself must request and apply the GPOs using the Group Policy Client service. Client-side extensions process the GPOs once downloaded Starter GPOs are intended for use in large organizations with a proliferation of GPOs that share settings. Starter GPOs can be imported from, and exported to, a .CAB file. Once a GPO is created it must be linked to a container object in AD DS for it to apply to objects, a process known as scoping . GPOs can be linked to Sites, Domains, and OUs. If multiple GPOs are linked to the same container, the link order must be configured. There are 2 default GPOs in an AD DS domain, which can be reset using arguments to the dcgpofix command. - Default Domain Policy, linked to the domain object - Default Domain Controllers Policy, linked to the Domain Controllers OU Although it is possible to link the same GPO to multiple containers, it is recommended to import (i.e. copy) a GPO from another domain. This process effecitvely restores the settings of another GPO into a newly created GPO, which is then linked to another container.","title":"GPO"},{"location":"Misc/Windows/WS2016/#hyper-v","text":"[Hyper-V][Hyper-V] is a Type I hypervisor and role that allows a Windows Server 2016 host to create VMs, called guests . In Type I virtualization, the hypervisor forms an abstraction layer that interacts directly with the host hardware. In this model, the individual environments created by the hypervisor, including the host operating system and guest VMs, are called [partitions][partition]. Hyper-V Server, a free product available for download is limited to the command-line Server Core interface, however it does include Sconfig to aid configuration. Hyper-V can be managed remotely using the GUI (Hyper-V Manager, hyper-v-tools ), or Powershell ( hyper-v-powershell ). Authentication can be via Kerberos or Credential Security Support Provider (CredSSP) , which must be enabled on both server and client. PowerShell remoting - Explicit remoting involves opening a PowerShell session to a remote session - Implicit remoting involves running a cmdlet specifying the ComputerName parameter. PowerShell Direct allows easy remoting to VMs by using the -VmName Powershell parameter using a PowerShell session. [Nested virtualization][Nested virtualization] is a new capability where a virtual host running Windows Server 2016 on a physical host also running Windows Server 2016 can host nested VMs.","title":"Hyper-V"},{"location":"Misc/Windows/WS2016/#host-configuration","text":"[Hyper-V][Hyper-V] is a Type I hypervisor and role that allows a Windows Server 2016 host to create VMs, called guests . In Type I virtualization, the hypervisor forms an abstraction layer that interacts directly with the host hardware. In this model, the individual environments created by the hypervisor, including the host operating system and guest VMs, are called [partitions][partition]. Hyper-V Server, a free product available for download is limited to the command-line Server Core interface, however it does include Sconfig to aid configuration. Hyper-V can be managed remotely using the GUI (Hyper-V Manager, hyper-v-tools ), or Powershell ( hyper-v-powershell ). Authentication can be via Kerberos or Credential Security Support Provider (CredSSP) , which must be enabled on both server and client. PowerShell remoting - Explicit remoting involves opening a PowerShell session to a remote session - Implicit remoting involves running a cmdlet specifying the ComputerName parameter. PowerShell Direct allows easy remoting to VMs by using the -VmName Powershell parameter using a PowerShell session. [Nested virtualization][Nested virtualization] is a new capability where a virtual host running Windows Server 2016 on a physical host also running Windows Server 2016 can host nested VMs.","title":"Host configuration"},{"location":"Misc/Windows/WS2016/#networking","text":"Virtual switches can be external , internal , or private (in order of decreasing access). Up to 8 network adapters can be [added][Add-VMNetworkAdapter] to a Windows Server 2016 Hyper-V VM. Hyper-V maintains a pool of MAC addresses which are assigned to virtual network adapters as they are created. Hyper-V MAC addresses begin with 00-15-5D , followed by the last two bytes of the IP address assigned to the server's physical network adapter (i.e. last two octets), then a final byte. Generation 1 VMs supported synthetic and legacy virtual network adapters, but in Generation 2 VMs only synthetic adapters are used. Generation 1 VMs can only boot from network (PXE) when using a legacy adapter. Physical hosts running Windows Server 2016 can support teams of up to 32 NICs, but Hyper-V VMs are limited to teams of two. The team must first be configured in the host operating system and appears as a single interface in the Virtual Switch Manager. High-performance embedded teaming , reliant on RDMA , can only be configured with Powershell . - Teaming Mode - Switch Independent : switch is unaware of presence of NIC team and does not load balance to members; Windows is performing the teaming - Switch Dependent : switch determines how to distribute inbound network traffic; only supported by specialty hardware - Static Teaming : switch and host are manually configured (typically supported by server-class switches) - Link Aggregation Control Protocol (LACP) : dynamically identifies links that are connected between the host and the switch - Load Balancing Mode - Address Hash : a hash is created based on address components of the packet, which is used to reasonably balance adapters - Hyper-V Port : NIC teams configured on Hyper-V hosts give VMs independent MAC addresses - Dynamic : outbound loads are distributed based on a hash of the TCP ports and IP addresses Virtual machine queuing will enhance performance if a physical host supports it and it is enabled . Bandwidth management is achieved by setting limits on the virtual network adapter, in the GUI or in [Powershell][Set-VMNetworkAdapter].","title":"Networking"},{"location":"Misc/Windows/WS2016/#storage","text":"The New Virtual Machine Wizard presents different options for Generation 1 vs. Generation 2 VMs. - Generation 1 VMs provide two IDE controllers, which host the hard drive and a DVD drive, and an unpopulated SCSI controller which can host additional disks. - Generation 2 VMs, however, have only a single SCSI controller, which hosts all virtual drives. A new VHD can be created using - Hyper-V Manager through the New Virtual Hard Disk Wizard - Disk Management ( diskmgmt.msc ), however the option to create a differencing disk is not available, nor can specific block or sector size be specified - PowerShell Shared virtual disk files are preferably created as VHD sets . Pass-through disks make exclusive use of a physical disk. pwsh Standard checkpoints (previously known as \"snapshots\" in Windows Server 2012 and before) with the extensions AVHD or AVHDX save the state, data, and hardware configuration of a VM. They are recommended for development and testing but are not a replacement for backup software nor recommended for production environmentsj, because restoring them in a production environment will interrupt running services. Production checkpoints do not save memory state, but use Volume Shadow Copy Service (Windows) or File System Freeze (Linux) inside the guest to create \"point in time\" images of the VM.","title":"Storage"},{"location":"Misc/Windows/WS2016/#shielded-vms","text":"Shielded VMs are a feature exclusive to the Datacenter Edition of Windows Server 2016. As a result of increased virtualization, physical servers that were once secured physically were migrated to Hyper-V hosts that are less secure because they are accessible to fabric administrators . Shielded VMs were introduced to protect tenant workloads from inspection, theft, and tampering as a result of being run on potentially compromised hosts. A security concept closely associated to shielded VMs is the guarded fabric , which is a collection of nodes cooperating to protect shielded Hyper-V guests. The guarded fabric consists of: - Host Guardian Service (HGS) utilizes remote attestation to confirm that a node is trusted; if so, it releases a key enabling the shielded VM to be started. HGS is typically a cluster of 3 nodes. - Guarded hosts : Windows Server 2016 Datacenter edition Hyper-V hosts that can run shielded VMs only if they can prove they are running in a known, trusted state to the Host Guardian Service. - Shielded VMs In a production environment, a fabric manager like Virtual Machine Manager would be used to deploy shielded VMs (which are signified by a shield icon). Shielded VMs must run Windows (8+) or Windows Server (2012+), although Linux shielded VMs are now also supported since version Windows Server version 1709. Shielded VMs are produced by a three-stage process (VHD -> Shielded template -> Shielded VMs) 1. Preparation : Install and configure an OS onto a virtual disk file 2. Templatization : Convert virtual disk file into a shielded template 3. Provisioning : Create one or more shielded VMs from the shielded template Configure HGS in its own new forest YouTube Install-WindowsFeature HostGuardianServiceRole -Restart Install-HgsServer -HgsDomainName 'savtechhgs.net' -SafeModeAdministratorPassword $adminPassword -Restart Shielding Data is created and owned by tenant VM owners and contains secrets needed to create shielded VMs that must be protected from the fabric admin. Resources: Intro to shielded VMs Create a shielded VM using Powershell Linux Shielded VM How To Shielded VM Demonstration and Quick Setup Guarded Fabric Deployment Guide for Windows Server 2016 Deploying Shielded VMs and a Guarded Fabric with Windows Server 2016","title":"Shielded VMs"},{"location":"Misc/Windows/WS2016/#attestation","text":"There are two modes of attestation supported by HGS: Hardware-trusted attestation Hardware-trusted attestation mode requires : Measured boot : TPMv2 to seal software and hardware configuration details measured at boot Code integrity enforcement to strictly define permissible software Platform Identity Verification : Active Directory is not sufficient to identify the host. Rather, an identity key rooted in the host TPM is used for identity. Remote attestation based on asymmetric key pairs Admin-trusted attestation was previously based on guarded host membership in a designated AD DS security group, but is deprecated beginning with Windows Server 2019. Host identity is [verified]](https://youtu.be/B2vFrdXd5jg?t=525) by checking security group permission No Measured Boot or Code Integrity Validation Intended to aid transition to Hardware-trusted attestation mode for hosts produced before TPMv2","title":"Attestation"},{"location":"Misc/Windows/WS2016/#vm-configuration","text":"VMs are associated with a variety of file types: Extension Description .vmc XML-format VM configuration .vhd, .vhdx Virtual hard disks .vsv Saved-state files VMs can be created in Hyper-V, and a machine's RAM can even be changed dynamically. Hyper-V guests can take advantage of a suite of features to enhance performance and functionality. - Virtualization of NUMA architecture - Smart paging for when VMs that use dynamic memory restart and temporarily need more memory than is available on the host, for example at boot - Monitoring resource usage, to minimize cost overruns when guests run in the cloud - Disk and GPU passthrough, and other PCI-x devices, with DDA pwsh - Increased performance of interactive sessions that use [VMConnect][VMConnect.exe] Microsoft supports some Linux distributions, like Ubuntu, with built-in Linux Integration Services , which improve performance by providing custom drivers to interface with Hyper-V. Some distributions like CentOS and Oracle come with integrated LIS packages, but free LIS packages provided by Microsoft for download from the Microsoft Download Center support additional features and come with the additional benefit of being versioned. These packages are provided as tarballs or ISO images, and must be loaded directly into the running guest operating system. FreeBSD has included full support for FreeBSD Integration Services (BIS) since version 10. Secure Boot has to be disabled when loading Hyper-V VMs running Linux distributions, since UEFI does not have certificates for non-Windows operating systems by default. Some distributions supported by Microsoft do have certificates in the UEFI Certificate Authority . Different versions of Hyper-V create VMs associated with that version (Windows Server 2016 uses Hyper-V 8.0). VMs created by older versions of Hyper-V can be [updated][Update-VMVersion], but once updated they may no longer run on a host of a previous version. Importing an exported VM can be done in three ways: - Register : exported files are left as-is and the guest's ID is maintained; - Restore : exported files copied to the host's default locations or ones that are otherwise specified; ID is maintained - Copy : exported files are copied; new ID generated PXE boot is supported in two scenarios: - Generation 1 VMs with a legacy virtual network adapter support PXE boot (but not synthetic ). Generation 1 VMs are limited to 2 TB in size and do not support many of the advanced features that Generation 2 VMs do. But PXE Boot remains one of the primary reasons to continue using a Generation 1 VM. - Generation 2 VMs with a synthetic network adapter also support PXE boot. would also support bandwidth management and VMQ. Generation 2 VMs also do not support 32-bit OSes, including: - Windows Server 2008, R2 - Windows 7 - Older Linux distros - FreeBSD (all) VMs cannot be upgraded from Generation 1 to Generation 2 easily, although a script named Convert-VMGeneration was once provided by Microsoft and can still be found. But the VM's version , referring to the version of Hyper-V used to create it, can be upgraded with Upgrade-VMVersion .","title":"VM configuration"},{"location":"Misc/Windows/WS2016/#monitoring","text":"Performance Monitor is a program that allows realtime monitoring of hundreds of different system performance statistics, called performance counters . Counters can be viewed in several ways, including line graph, histogram bar graph, and report views. Every counter added to a graph is associated with a computer, a performance object (hardware or software component to be monitored), a performance counter (statistic), and an instance. A data collector set captures counter statistics for later review. A single data collector set can gather performance counter data from multiple VMs. Event trace data cannot be combined with performance data in the same data collector set. Expiration dates can be set for data collector sets, but if actively collecting data the expiration date will not stop collection. A performance alert is a type of data collector set that can track system performance and log events in the application event log. Alerts can be triggered when a performance counter value exceeds a certain threshold. Only members of the local groups Administrators and Performance Log Users can create alerts, but the Log on as a batch user right must be granted to members of Performance Log Users. A hard fault occurs when data is swapped between memory and disk.","title":"Monitoring"},{"location":"Misc/Windows/WS2016/#performance-counters","text":"Counter Acceptable values Processor: % Processor Time <85% Processor: Interrupts/Sec cf. baseline System: Processor Queue Length <2 Server Work Queues: Queue Length <4 Memory: Page Faults/Sec <5 Memory: Pages/Sec <20 Memory: Available MBytes >5% of physical memory Memory: Committed Bytes < physical memory Memory: Pool Non-Paged Bytes Stable PhysicalDisk: Disk Bytes/Sec cf. baseline PhysicalDisk: Avg. Disk Bytes/Transfer cf. baseline PhysicalDisk: Current Disk Queue Length <2 per spindle PhysicalDisk: % Disk Time <90% LogicalDisk: % Free Space >20% Network Interface: Bytes Total/Sec cf. baseline Network Interface: Output Queue Length <2 Server: Bytes Total/Sec 50% of total bandwidth","title":"Performance counters"},{"location":"Misc/Windows/WS2016/#network-load-balancing","text":"Cluster VMs can be configured to drain their workloads to other nodes when being shutdown using Suspend-ClusterNode NLB Clusters are made of hosts , while Failover Clusters are made of nodes . NLB port rules control how the cluster functions and are defined by two operational parameters: Affinity : associate client requests to cluster hosts. When no affinity is specified, all network requests are load-balanced across the cluster without regard to their source. Filtering mode : specify how the cluster handles traffic described by port range and protocols; can be single or multiple hosts. When a port rule is not configured, the default host will receive all network traffic. Windows Server NLB Clusters can be upgraded to Windows Server 2016 in two ways: - Rolling upgrade brings only a single host down at a time, upgrading it before adding it and proceeding to the next one - Simultaneous upgrade brings the entire NLB cluster goes down NLB clusters have a Cluster Operation Mode setting specifying what kind of TCP/IP traffic the cluster hosts should use - Unicast : NLB replaces the MAC address on the interface with the cluster's virtual MAC address, causing traffic to go to all hosts. Cluster hosts are prevented from communicating with each other in this mode. In this case, a second network adapter must be installed in order to facilitate normal communication between NLB cluster hosts. - Multicast : NLB adds a multicast MAC address to the network interface on each host that does not replace the original.","title":"Network Load Balancing"},{"location":"Misc/Windows/WS2016/#storage_1","text":"Every track of a hard drive platter is split into disk sectors , traditionally 512 bytes. A block is commonly called an \"allocation unit\" in Windows, but also commonly called a cluster. Storage left over unused in partially unused blocks is known as slack space . A new disk must first be initialized , that is, a partition table style must be chosen: - GPT : 128 partitions per disk, maximum volume size of 18 exabytes (2 60 bytes). Booting from a GPT drive is not possible unless the computer architecture supports EFI-based boot partitions. - MBR : older format that is commonly used for removable media, supporting volumes up to 2 TB with up to 4 primary partitions , although a common workaround is to make one of these partitions an extended partition , which can be be further subdivided into logical drives Mounting a partition as a single filesystem produces a volume , although the distinction can often be lost. The exception would be a case where a volume spans multiple partitions or physical disks, as is possible with software RAID. Virtual hard disks can be created with [Powershell][New-VHD] or in diskmgmt.msc and come in two formats: - VHD - VHDX Only 2 filesystem options are available for modern servers: - NTFS supports volumes up to 16 TB with the default 4 KB allocation unit size (but 256 TB with the 64 KB allocation unit size) and is required by some Windows Server services like AD DS , File Replication Service , Volume Shadow Copy Service , and Distributed File System - ReFS uses the same system of permissions as NTFS and offers error checking and repair capabilities that NTFS does not, but it does not support NTFS features like file compression, Encrypted File System , and disk quotas. ReFS supports a maximum file size of 16 exabytes and volumes up to 1 yobibyte (2 80 bytes) Software RAID can be implemented by creating Spanned , Striped , or RAID-5 volumes in diskmgmt.msc . A more modern and preferred technique is to create storage pools in [Storage Spaces][Storage Spaces].","title":"Storage"},{"location":"Misc/Windows/WS2016/#dedup","text":"Data deduplication (\"dedup\") is a role service that conserves storage space by storing only one copy of redundant chunks of files. Data duplication is appropriate to specific workloads, like backup volumes and file servers. It is not appropriate for database storage or operating system data or boot volumes. Data deduplication had required NTFS , although ReFS is supported since 1709. Data deduplication runs as a low-priority background process when the system is idle, by default; however its behavior can be configured based on its intended usage. Deduplication works by scanning files, and breaking them into unique chunks of various sizes that are collected in a chunk store . The original locations of chunks are replaced by reparse points . When a file is recently written, it is written in the standard, unoptimized form; the accumulation of such files is known as churn . Other jobs associated with deduplication include garbage collection , integrity scrubbing , and (when disabling deduplication) unoptimization . There are several deployment scenarios considered for data deduplication: - General purpose file servers Users often store multiple copies of the same, or similar, documents and files. Up to 30-50% of this space can be reclaimed using deduplication. - Virtualized Desktop Infrastructre (VDI) deployments Virtual hard disks that are used for remote desktops are essentially identical. Data Deduplication can also amelioriate the drop in storage performance when many users simultaneously log in at the start of the day, called a VDI boot storm . - Backup snapshots are an ideal deployment scenario because of the data is so duplicative. Deduplication is especially useful for disk drive backups, since snapshots typically differ little from each other.","title":"Dedup"},{"location":"Misc/Windows/WS2016/#file-shares","text":"Windows Server 2016 supports file shares via two protocols, both of which require the fs-fileserver role service: - SMB , long the standard for Windows networks - NFS , typically used in Linux, requires the installation of fs-nfs-service role service BranchCache enables client computers at remote locations to cache files accessed from shares, so that other computers at the same location can access them. Install the FS-BranchCache feature and enable the File and Printer Sharing and Branchcache - Hosted Cache Server (uses HTTPS) firewall display groups.","title":"File shares"},{"location":"Misc/Windows/WS2016/#media","text":"Every track of a hard drive platter is split into disk sectors , traditionally 512 bytes. A block is commonly called an \"allocation unit\" in Windows, but also commonly called a cluster. Storage left over unused in partially unused blocks is known as slack space . A new disk must first be initialized , that is, a partition table style must be chosen: - GPT : 128 partitions per disk, maximum volume size of 18 exabytes (2 60 bytes). Booting from a GPT drive is not possible unless the computer architecture supports EFI-based boot partitions. - MBR : older format that is commonly used for removable media, supporting volumes up to 2 TB with up to 4 primary partitions , although a common workaround is to make one of these partitions an extended partition , which can be be further subdivided into logical drives Mounting a partition as a single filesystem produces a volume , although the distinction can often be lost. The exception would be a case where a volume spans multiple partitions or physical disks, as is possible with software RAID. Virtual hard disks can be created with [Powershell][New-VHD] or in diskmgmt.msc and come in two formats: - VHD - VHDX Only 2 filesystem options are available for modern servers: - NTFS supports volumes up to 16 TB with the default 4 KB allocation unit size (but 256 TB with the 64 KB allocation unit size) and is required by some Windows Server services like AD DS , File Replication Service , Volume Shadow Copy Service , and Distributed File System - ReFS uses the same system of permissions as NTFS and offers error checking and repair capabilities that NTFS does not, but it does not support NTFS features like file compression, Encrypted File System , and disk quotas. ReFS supports a maximum file size of 16 exabytes and volumes up to 1 yobibyte (2 80 bytes) Software RAID can be implemented by creating Spanned , Striped , or RAID-5 volumes in diskmgmt.msc . A more modern and preferred technique is to create storage pools in [Storage Spaces][Storage Spaces].","title":"Media"},{"location":"Misc/Windows/WS2016/#s2d","text":"Although a cluster can normally be created in the GUI Failover Cluster Manager , in order to use Storage Spaces Direct the system must be prevented from automatically creating storage, which necessitates creation in PowerShell with the NoStorage switch parameter, and then S2D must be enabled using Enable-ClusterStorageSpacesDirect . This command scans all cluster nodes for local, unpartitioned disks , which are added to a single storage pool and classified by media type in order to use the fastest disks for caching. The recommended drive configuration for a node in an S2D cluster is a minimum of six drives, with at least 2 SSDs and at least 4 HDDs, with no RAID or other intelligence that cannot be disabled. Caching is configured automatically, depending on the combination of drives present - NVMe + SSD : NVMe drives are configured as a write-only cache for the SSD drives - NVMe + HDD : NVMe drives are read/write cache - NVME + SSD + HDD : NVME are write-only for the SSD drives and read/write for HDD drives - SSD + HDD : SSD drives are read/write cache Microsoft defined two deployment scenarios for Storage Spaces Direct: - Disaggregated which creates two separate clusters, one of which is a Scale-out File Server dedicated to storage, essentially functioning as a SAN. This solution requires the [DCB][DCB] role for traffic management. At least two 10Gbps Ethernet adapters are recommended per node, preferably adapters that use RDMA. - Hyper-converged , where a single cluster node hosts VMs and storage. This solution is much less expensive because it requires less hardware and generates much less network traffic, but storage and compute can't scale independently: adding a node to storage necessarily entails adding one to the Hyper-V hosts, and vice versa.","title":"S2D"},{"location":"Misc/Windows/WS2016/#storage-replica","text":"Storage Replica supports one-way replication between standalone servers, between clusters, and between storage devices within an [ asymmetric (stretch) cluster ][asymmetric cluster]. - Synchronous replication is possible when the replicated volumes can mirror data immediately, ensuring no data loss in case of failover - Asynchronous replication is preferable when the replication partner is located over a WAN link Storage Replica improves on DFS Replication, which is exclusively asynchronous and file-based, by using SMBv3 (port 445). Storage Replica requires two virtual disks, one for logs and one for data, which are the same size for each replication partner, and all the physical disks must use the same sector size.","title":"Storage Replica"},{"location":"Misc/Windows/WS2016/#wsus","text":"Windows Server Update Services (WSUS) can be configured from the command-line with wsusutil.exe. There are 5 basic WSUS architecture configurations Single WSUS Server downloads updates from Microsoft Update, and all the computers on the network download updates from it. A single server can usupport up to 25,000 clients. Replica WSUS Servers: a central WSUS server downloads from Microsoft Update, and after approval the updates are distributed to downstream servers at remote locations. Autonomous WSUS Servers: a central WSUS server downloads from Microsoft Update, all of which are distributed to remote servers; each remote site's administrators are individually responsible for evaluating and approving updates. Low-bandwidth WSUS Servers at remote sites download only the list of approved updates, which are then retrieved from Microsoft Update over the Internet, minimizing WAN traffic. Disconnected WSUS Servers have updates imported from offline media (DVD-ROMs, portable drives, etc), utilizing no WAN or Internet bandwidth whatsoever. When a computer first communicates with a WSUS server, it is added to the All Computers and and Unassigned Computers group automatically, which is created by default.","title":"WSUS"},{"location":"Misc/Windows/WS2016/#windows-server-backup","text":"To back up a VM without any downtime, integration services must be installed and enabled, and all disks must be basic disks formatted with NTFS . Windows Server Backup - System state includes boot files, Active Directory files, SYSVOL (when run on a DC), the registry, and other data. - System reserved is a special partition containing Boot Manager and Boot Configuration data.","title":"Windows Server Backup"},{"location":"Misc/Windows/WS2016/#glossary","text":"","title":"\ud83d\udcd8 Glossary"},{"location":"Misc/Windows/WS2016/#adfind","text":"Query the schema version associated with Active Directory [Desmond][Desmond2009]: 53 adfind -schema -s base objectVersion","title":"adfind"},{"location":"Misc/Windows/WS2016/#adprep","text":"Prepare Active Directory for Windows Server upgrades. Must be run on the Infrastructure Master role owner with the flag /domainprep . [Desmond][Desmond2009]: 29","title":"adprep"},{"location":"Misc/Windows/WS2016/#arp","text":"a d s","title":"arp"},{"location":"Misc/Windows/WS2016/#bcdedit","text":"Change Windows bootloader to Linux, while dual booting ::Manjaro bcdedit /set {bootmgr} path \\EFI\\manjaro\\grubx64.efi ::Fedora bcdedit /set {bootmgr} path \\EFI\\fedora\\shim.efi Enable or disable Test Signing Mode ref bcdedit /set testsign on bcdedit /set testsign off","title":"bcdedit"},{"location":"Misc/Windows/WS2016/#bootrec","text":"Windows Recovery Environment command that repairs a system partition Use when boot sector not found bootrec /fixboot Use when BCD file has been corrupted bootrec /rebuildbcd","title":"bootrec"},{"location":"Misc/Windows/WS2016/#cmdkey","text":"add delete generic list pass smartcard user Add a user name and password for user Mikedan to access computer Server01 with the password Kleo docs.microsoft.com cmdkey /add:server01 /user:mikedan /pass:Kleo","title":"cmdkey"},{"location":"Misc/Windows/WS2016/#dism","text":"Add-Driver Add-Package Add-ProvisionedAppxPackage Append-Image Apply-Image Apply-Unattend Capture-Image Cleanup-Image Commit-Image Disable-Feature Enable-Feature Export-Driver Export-Image Get-Driverinfo Get-Drivers Get-Featureinfo Get-Features Get-ImageInfo Get-MountedImageInfo Get-Packageinfo Get-Packages Get-ProvisionedAppxPackages List-Image Remount-Image Remove-Driver Remove-Image Remove-Package Remove-ProvisionedAppxPackage Set-ProvisionedAppxDataFile Unmount-Image Mount an image Zacker : 71 dism /mount-image /imagefile:$FILENAME /index:$N /name:$IMAGENAME /mountdir:$PATH Practice Labs dism /mount-wim /wimfile:c:\\images\\install.wim /index:1 /mountdir:c:\\mount Add a driver to an image file that you have already mounted Zacker : 72 dism /image:$FOLDERNAME /add-driver /driver:$DRIVERNAME /recurse Commit changes and unmount the image Zacker : 75 dism /unmount-image /mountdir:c:\\mount /commit Determine exact name of Windows features that can be enabled and disabled Zacker : 75 dism /image:c:\\mount /get-features Scan an image, checking for corruption dism /Online /Cleanup-Image /ScanHealth Check an image to see whether any corruption has been detected dism /Online /Cleanup-Image /CheckHealth Repair an offline dicsk using a mounted image as a repair source dism /Image:C:\\offline /Cleanup-Image /RestoreHealth /Source:C:\\test\\mount\\windows Zacker: 71-75 dism /mount-image /imagefile:C:\\images\\install.wim /index:1 /mountdir:C:\\mount dism /add-package /image:C:\\mount /packagepath:C:\\updates dism /add-driver /image:C:\\mount /driver:C:\\drivers\\display.driver\\nv_dispi.inf dism /commit-image /image:C:\\mount dism /unmount-image /image:C:\\mount","title":"dism"},{"location":"Misc/Windows/WS2016/#djoin","text":"Perform an offline domain join for a Nano Server Zacker: 46 djoin /provision /domain practicelabs /machine PLABNANOSRV01 /savefile .\\odjblob Load the odjblob file created offline on the Nano Server. djoin /requestodj /loadfile c:\\odjblob /windowspath c:\\windows /localos","title":"djoin"},{"location":"Misc/Windows/WS2016/#dnscmd","text":"Replicate an AD-integrated DNS zone to specific DCs ref dnscmd . /CreateDirectoryPartition FQDN Enable GlobalNames zone support dnscmd <servername> /config /enableglobalnamessupport 1 Observe status of socket pool dnscmd /info /socketpoolsize Configure DNS socket pool size (0 through 10,000) dnscmd /Config /SocketPoolSize <value>","title":"dnscmd"},{"location":"Misc/Windows/WS2016/#dsquery","text":"Find the Active Directory Schema version from the command-line ref pwsh dsquery * cn=schema,cn=configuration,dc=domain,dc=com -scope base -attr objectVersion\"","title":"dsquery"},{"location":"Misc/Windows/WS2016/#jea","text":"Just Enough Administration (JEA) allows special remote sessions that limit which cmdlets and parameters can be used in a remote PowerShell session. These are implemented as restricted endpoints , to which only members of a specific security group can gain access. This offers a way to administer remote servers and move away from the traditional method using RDP.","title":"JEA"},{"location":"Misc/Windows/WS2016/#net","text":"Map a network location to a drive letter Practice Lab net use x: \\\\192.168.0.35\\c$ Stop/start a service net stop dns net start dns","title":"net"},{"location":"Misc/Windows/WS2016/#netdom","text":"Rename a computer netdom renamecomputer %computername% /newname: newcomputername Join a computer to a domain cf. Add-Computer , Zacker: 21 netdom join %computername% /domain: domainname /userd: username /password:*","title":"netdom"},{"location":"Misc/Windows/WS2016/#netsh","text":"Enable port forwarding (\" portproxy \") to a WSL2 distribution ( src ) netsh interface portproxy add v4tov4 listenaddress=0.0.0.0 listenport=2222 connectaddress=172.23.129.80 connectport=2222 Configure DNS to be dynamically assigned netsh interface ip set dns \"Wi-Fi\" dhcp Delete Wi-Fi profiles netsh wlan delete profile name=* Turn off Windows firewall netsh advfirewall set allprofiles state off Enable firewall rule group netsh advfirewall firewall set rule group=\u201dFile and Printer Sharing\u201d new enable=yes Show Wi-Fi passwords ( src netsh wlan show profile wifi key=clear Check/reset WinHTTP proxy netsh winhttp show proxy netsh winhttp reset proxy","title":"netsh"},{"location":"Misc/Windows/WS2016/#ntdsutil","text":"Used to transfer FSMO roles between domain controllers. [ Desmond: 30 ][Desmond2009]","title":"ntdsutil"},{"location":"Misc/Windows/WS2016/#regsvr32","text":"Register a DLL dependency in order to enable the Active Directory Schema MMC snap-in on a DC [Desmond][Desmond2009]: 54 regsvr32 schmmgmt.dll","title":"regsvr32"},{"location":"Misc/Windows/WS2016/#route","text":"p print add change delete Basic usage route add 192 .168.2.1 mask ( 255 .255.255.0 ) 192 .168.2.4","title":"route"},{"location":"Misc/Windows/WS2016/#runas","text":"env netonly profile / no profile savecred showtrustlevels smartcard trustlevel user:","title":"runas"},{"location":"Misc/Windows/WS2016/#settings","text":"appsfeatures personalization printers windowsupdate about activation apps-volume appsforwebsites assignedaccess autoplay backup batterysaver bluetooth camera clipboard colors connecteddevices cortana crossdevice datausage dateandtime defaultapps delivery-optimization developers deviceencryption devices-touchpad display easeofaccess-display emailandaccounts findmydevice fonts keyboard lockscreen maps messaging mobile-devices mousetouchpad multitasking network network-wifi nfctransactions nightlight notifications optionalfeatures otherusers pen personalization-background personalization-colors personalization-start personalization-start-places phone powersleep privacy project proximity quiethours quietmomentsgame quietmomentspresentation quietmomentsscheduled recovery regionformatting regionlanguage remotedesktop savelocations screenrotation signinoptions signinoptions-launchfaceenrollment sound speech speech startupapps storagepolicies storagesense surfacehub-accounts surfacehub-calling surfacehub-devicemanagenent surfacehub-sessioncleanup surfacehub-welcome sync tabletmode taskbar themes troubleshoot typing usb videoplayback wheel windowsdefender windowsinsider workplace yourinfo","title":"Settings"},{"location":"Misc/Windows/WS2016/#sfc","text":"sfc /scannow","title":"sfc"},{"location":"Misc/Windows/WS2016/#shutdown","text":"Immediate restart shutdown /r /t 0 Log off shutdown /L","title":"shutdown"},{"location":"Misc/Windows/WS2016/#slmgr","text":"ato dli dlv ipk rearm upk xpr","title":"slmgr"},{"location":"Misc/Windows/WS2016/#sysdm","text":"2 3 4 5","title":"sysdm"},{"location":"Misc/Windows/WS2016/#tracert","text":"On Windows, this command is aliased to traceroute which is the Linux command. [Lammle][Lammle]: 112","title":"tracert"},{"location":"Misc/Windows/WS2016/#wbadmin","text":"enable backup get items get versions start backup start recovery start systemstaterecovery -backupTarget -hyperv -vsscopy | -vssFull Backup the entire drive, excluding some VMs wbadmin enable backup -backupTarget \\\\backups\\hostdr\\temp\\ -include:c: -exclude: C:\\VMs\\VM1.vhdx, C:\\VMs\\VMAR.vhd -vsscopy -quiet Zacker : 325-326 wbadmin get versions wbadmin get items -version: 11/14/2016:05:09 wbadmin start recovery -itemtype:app items:cluster -version:01/01/2008-00:00 Zacker : 422 wbadmin start systemstaterecovery -version:11/27/2016-11:07 wbadmin get versions","title":"wbadmin"},{"location":"Misc/Windows/WS2016/#wdsutil","text":"initialize-server remInst wdsutil /initialize-server /remInst:\"D:\\RemoteInstall\"","title":"wdsutil"},{"location":"Misc/Windows/WS2016/#winrm","text":"# List all WinRM listeners winrm enumerate winrm/config/listener # Display WinRM configuration winrm get winrm/config # Add an address to Trusted Hosts list winrm set winrm/config/client @ { TrustedHosts = \"192.168.10.41\" }","title":"winrm"},{"location":"Misc/Windows/WS2016/#winver","text":"","title":"winver"},{"location":"Misc/Windows/WS2016/#wmic","text":"bios logicaldisk memorychip os path Recover Windows product key [fossbytes.com][https://fossbytes.com/how-to-find-windows-product-key-lost-cmd-powershell-registry/] wmic path softwarelicensingservice get OA3xOriginalProductKey Display information about installed RAM wmic memorychip list full List all objects of type Win32_LogicalDisk using that class's alias logicaldisk . [Desmond][Desmond2009]: 642 pwsh wmic logicaldisk list brief Recover serial number of a Lenovo laptop [pcsupport.lenovo.com][https://pcsupport.lenovo.com/us/en/solutions/find-product-name] wmic bios get serialnumber Display BIOS version wmic bios get biosversion Display operating system architecture wmic os get osarchitecture Display operating system type (48 is Windows 10) wmic os get operatingsystemsku","title":"wmic"},{"location":"Misc/Windows/WS2016/#wslmsdocswslexe","text":"l s t \\ export import set-default-version","title":"[wsl][msdocs:wsl.exe]"},{"location":"Misc/Windows/WS2016/#wsusutilmsdocswsusutilexe","text":"Specify a location for downloaded updates Zacker : 393 C:\\Program Files\\Update Services\\Tools\\wsusutil.exe postinstall content_dir=d:\\wsus Specify SQL server, when not using the default WID database C:\\Program Files\\Update Services\\Tools\\wsusutil.exe postinstall sql_instance_name=\"db1\\sqlinstance1\"- content_dir=d:\\wsus","title":"[wsusutil]][msdocs:wsusutil.exe]"},{"location":"Misc/Windows/WS2016/#wt","text":"d p split-pane focus-tab Open the default Windows Terminal profile and also an Ubuntu WSL tab [bleepingcomputer.com][https://www.bleepingcomputer.com/news/microsoft/windows-terminal-09-released-with-command-line-arguments-and-more/] wt; new-tab -p \"Ubuntu-18.04\" Open a split pane of the default profile in the D:\\ folder and the cmd profile in the E:\\ folder [bleepingcomputer.com][https://www.bleepingcomputer.com/news/microsoft/windows-terminal-09-released-with-command-line-arguments-and-more/] wt -d d:\\ ; split-pane -p \"cmd\" -d e: Open the default profile and an Ubuntu WSL profile, but with the first tab focused [bleepingcomputer.com][https://www.bleepingcomputer.com/news/microsoft/windows-terminal-09-released-with-command-line-arguments-and-more/] wt ; new-tab -p \"Ubuntu-18.04\"; focus-tab -t0","title":"wt"},{"location":"Misc/Windows/WS2016/#xcopy","text":"Copy from one directory to another Practice Lab xcopy /s c:\\inetpub\\wwwroot c:\\nlbport","title":"xcopy"},{"location":"Misc/Windows/diskpart/","text":"diskpart Moffitt disk formatting script SELECT DISK 0 CLEAN CONVERT gpt CREATE PARTITION primary SIZE=1024 FORMAT QUICK FS=NTFS LABEL=\"Recovery\" SET ID=\"de94bba4-06d1-4d40-a16a-bfd50179d6ac\" CREATE PARTITION efi SIZE=750 ASSIGN LETTER=K FORMAT QUICK FS=FAT32 LABEL=\"System\" CREATE PARTITION MSR SIZE=128 CREATE PARTITION PRIMARY ASSIGN LETTER=C FORMAT QUICK FS=NTFS EXIT","title":"Diskpart"},{"location":"Misc/Windows/diskpart/#diskpart","text":"Moffitt disk formatting script SELECT DISK 0 CLEAN CONVERT gpt CREATE PARTITION primary SIZE=1024 FORMAT QUICK FS=NTFS LABEL=\"Recovery\" SET ID=\"de94bba4-06d1-4d40-a16a-bfd50179d6ac\" CREATE PARTITION efi SIZE=750 ASSIGN LETTER=K FORMAT QUICK FS=FAT32 LABEL=\"System\" CREATE PARTITION MSR SIZE=128 CREATE PARTITION PRIMARY ASSIGN LETTER=C FORMAT QUICK FS=NTFS EXIT","title":"diskpart"},{"location":"Python/","text":"\ud83d\udc0d Python Decorators A decorator is any function that accepts a function and returns a function. Decorators are one of the main ways that Python implements functional programming principles. Functions are first-class objects and can be passed as parameters. import logging def hello_wrapper ( name , func ): func ( f 'Hello { name } ' ) hello_wrapper ( \"world\" , func = print ) # Hello world hello_wrapper ( \"logs\" , func = logging . warning ) # WARNING:root:Hello logs with open ( 'hello.txt' , 'w' ) as f : hello_wrapper ( 'everyone!' , func = f . write ) import random def anagram ( t ): l = [ c for c in t ] random . shuffle ( l ) print ( \"\" . join ( l )) hello_wrapper ( 'Japushku' , anagram ) # eHoulhluaskpJ The function has to be passed as a reference, actually calling it will cause the wrapper function to attempt to execute the value returned by the inner function. hello_wrapper ( \"world\" , func = print ()) # Error def outer (): print ( 'Hi from the outer function' ) def inner (): print ( 'Hello from the inner function' ) inner () We can use the __name__ attribute to access a passed function's name. def hello ( func ): print ( f 'Hello { func . __name__ } ' ) hello ( outer ) # Hello outer We can also return functions, which can then be invoked def hello ( func ): print ( f 'Hello { func . __name__ } ' ) return func hello ( outer )() ''' Hi from the outer function Hello from the inner function ''' new_outer = hello ( outer ) new_outer is outer # True def wrapper ( func ): print ( f 'Before { func . __name__ } ' ) func () print ( f 'After { func . __name__ } ' ) wrapper ( outer ) ''' Before outer Hi from the outer function Hello from the inner function After outer ''' The true decorator pattern appears here, where wrapper is called the decorator and outer has been decorated . def wrapper ( func ): def _wrapper (): print ( f 'Before { func . __name__ } ' ) func () print ( f 'After { func . __name__ } ' ) return _wrapper outer = wrapper ( outer ) But the usual syntax since Python 2.4 is to place the decorator on the line above the decorated function, preceded by @ : @wrapper def outer (): print ( 'Hi from the outer function' ) def inner (): print ( 'Hello from the inner function' ) inner () _wrapper here does not accept any positional arguments, so wrapping functions that take arguments will produce a TypeError @wrapper def say_hello ( name ): print ( f 'Hello { name } !' ) # error The solution is to incorporate *args, **kwargs into the definition of the inner function, as well as the invocation of the function passed in. def wrapper ( func ): def _wrapper ( * args , ** kwargs ): print ( f 'Before { func . __name__ } ' ) func ( * args , ** kwargs ) print ( f 'After { func . __name__ } ' ) return _wrapper Returned values are not captured yet: def wrapper ( func ): def _wrapper ( * args , ** kwargs ): print ( f 'Before { func . __name__ } ' ) value = func ( * args , ** kwargs ) print ( f 'After { func . __name__ } ' ) return value return _wrapper Inspecting the decorated function's __name__ attribute reveals that it is still named _wrapper say_hello . __name__ # '_wrapper' This is also true for other attributes, including docstring. functools.wraps is a decorator factory to reassign attributes to the wrapped function. This is considered superior to the functools.update_wrapper function which is also available. def wrapper ( func ): @functools . wraps ( func ) def _wrapper ( * args , ** kwargs ): print ( f 'Before { func . __name__ } ' ) value = func ( * args , ** kwargs ) print ( f 'After { func . __name__ } ' ) return value return _wrapper This forms an ideal starting template for the creation of custom decorators. Classes Properties In the Python documentation , attributes accessed with accessor functions are called managed attributes , which makes the term equivalent to properties in C#. Three methods can be defined using the @property decorator Constructor Getter Setter Deleter def __init__ ( self , price ): self . _price = price @property def price ( self ): return self . _price @price . setter def price ( self , new_price ): if new_price > 0 : self . _price = new_price else : raise ValueError @price . deleter def price ( self ): del self . _price Class methods The @classmethod decorator prevents the interpreter from passing in the instantiated object using self , rather the class itself is passed in as the cls argument. This means that the methods decorated as such must take not self as the first argument but cls @classmethod def classmethod ( cls ): pass The @staticmethod decorator prevents the interpreter from passing any additional arguments whatsoever. The resulting method has no access to the object itself nor the class and functions like a procedurally defined function. Formatting flake8 , black , and yapf are CLI tools used to automatically format Python code. Virtual environments pipenv pipenv --python 3 .6 venv Create a virtual environment named project python -m venv project virtualenv Create a virtual environment named project using a different version of Python virtualenv -p /usr/bin/python2 project Testing Pytest is a popular testing framework preferred to unittest by many Python developers because it follows Pythonic conventions more closely. In contrast to unittest's custom methods, pytest relies on the builtin assert statement. pytest unittest Class under test from phonebook import PhoneBook import pytest @pytest . fixture def phonebook (): phonebook = PhoneBook () yield phonebook phonebook . clear () def test_lookup_by_name ( phonebook ): phonebook . add ( \"Bob\" , \"1234\" ) assert \"1234\" == phonebook . lookup ( \"Bob\" ) def test_phonebook_contains_all_names ( phonebook ): phonebook . add ( \"Bob\" , \"1234\" ) assert \"Bob\" in phonebook . names () def test_missing_name_raises_error ( phonebook ): with pytest . raises ( KeyError ): phonebook . lookup ( \"Bob\" ) python -m pytest import unittest from phonebook import PhoneBook class PhoneBookTest ( unittest . TestCase ): def test_lookup_by_name ( self ): self . phonebook . add ( \"Bob\" , \"12345\" ) number = self . phonebook . lookup ( \"Bob\" ) self . assertEqual ( \"12345\" , number ) def test_missing_name ( self ): with self . assertRaises ( KeyError ): self . phonebook . lookup ( \"missing\" ) def test_empty_phonebook_is_consistent ( self ): self . assertTrue ( self . phonebook . is_consistent ()) def setUp ( self ) -> None : self . phonebook = PhoneBook () def tearDown ( self ) -> None : self . phonebook . clear () python -m unittest import os class PhoneBook : def __init__ ( self , cache_directory = os . getcwd ()): self . numbers = {} self . filename = os . path . join ( cache_directory , \"phonebook.txt\" ) self . cache = open ( self . filename , \"w\" ) def add ( self , name , number ): self . numbers [ name ] = number def lookup ( self , name ): return self . numbers [ name ] def is_consistent ( self ): return True def names ( self ): return set ( self . numbers . keys ()) def clear ( self ): self . cache . close () os . remove ( self . filename ) Doctest A doctest is a docstring containing what looks like interactive Python sessions. Python Docs \"\"\" Return the factorial of n, an exact integer >= 0. >>> [factorial(n) for n in range(6)] [1, 1, 2, 6, 24, 120] >>> factorial(30) 265252859812191058636308480000000 >>> factorial(-1) Traceback (most recent call last): ... ValueError: n must be >= 0 Factorials of floats are OK, but the float must be an exact integer: >>> factorial(30.1) Traceback (most recent call last): ... ValueError: n must be exact integer >>> factorial(30.0) 265252859812191058636308480000000 It must also not be ridiculously large: >>> factorial(1e100) Traceback (most recent call last): ... OverflowError: n too large \"\"\" This can then be run if __name__ == '__main__' : import doctest doctest . testmod () pytest PyTest relies on the built-in assert statement. Fixtures The @pytest.fixture decorator facilitiates the creation of test fixtures. The fixture function's name is used as argument to the test case, and the value returned can be used by the logic within. ( src ) Any clean-up logic can be invoked in this fixture as well by replacing return with yield . Pytest also provides its own tmpdir test fixture for temporary directories. ( src ) Before After tmpdir from phonebook import PhoneBook import pytest def test_lookup_by_name ( phonebook ): phonebook = PhoneBook () phonebook . add ( \"Bob\" , \"1234\" ) assert \"1234\" == phonebook . lookup ( \"Bob\" ) def test_phonebook_contains_all_names ( phonebook ): phonebook = PhoneBook () phonebook . add ( \"Bob\" , \"1234\" ) assert \"Bob\" in phonebook . names () def test_missing_name_raises_error ( phonebook ): phonebook = PhoneBook () with pytest . raises ( KeyError ): phonebook . lookup ( \"Bob\" ) from phonebook import PhoneBook import pytest @pytest . fixture def phonebook (): phonebook = PhoneBook () yield phonebook phonebook . clear () def test_lookup_by_name ( phonebook ): # phonebook = PhoneBook() phonebook . add ( \"Bob\" , \"1234\" ) assert \"1234\" == phonebook . lookup ( \"Bob\" ) def test_phonebook_contains_all_names ( phonebook ): # phonebook = PhoneBook() phonebook . add ( \"Bob\" , \"1234\" ) assert \"Bob\" in phonebook . names () def test_missing_name_raises_error ( phonebook ): # phonebook = PhoneBook() with pytest . raises ( KeyError ): phonebook . lookup ( \"Bob\" ) from phonebook import PhoneBook import pytest @pytest . fixture def phonebook ( tmpdir ): phonebook = PhoneBook ( tmpdir ) return phonebook def test_lookup_by_name ( phonebook ): # phonebook = PhoneBook() phonebook . add ( \"Bob\" , \"1234\" ) assert \"1234\" == phonebook . lookup ( \"Bob\" ) def test_phonebook_contains_all_names ( phonebook ): # phonebook = PhoneBook() phonebook . add ( \"Bob\" , \"1234\" ) assert \"Bob\" in phonebook . names () def test_missing_name_raises_error ( phonebook ): # phonebook = PhoneBook() with pytest . raises ( KeyError ): phonebook . lookup ( \"Bob\" ) unittest unittest is a testing framework built into Python's Standard Library that was based on JUnit. unittest came out in 2001, when JUnit was being ported and adapted to many languages. Collectively, these frameworks were referred to as the xUnit family . unittest's method names do not follow Python conventions because it predates the PEP-8 naming standard. unittest allows you to create test classes that inherit from TestCase . Assertions Assertions are implemented in individual methods of the TestCase subclass through unittest methods like assertEqual and assertRaises , etc. Notably, TestCase subclasses must not have an __init__() constructor method defined. def test_lookup_by_name ( self ): phonebook = PhoneBook () phonebook . add ( \"Bob\" , \"12345\" ) number = phonebook . lookup ( \"Bob\" ) self . assertEqual ( \"12345\" , number ) assertRaises must be placed in a context manager. Here, the test case will run the code within the with block and check to make sure it raises the specified exception: KeyError : ( src ) def test_missing_name ( self ): fleet = Fleet () with self . assertRaises ( KeyError ): fleet . lookup ( \"bla\" ) Fixtures setUp is run before every test method, allowing a test fixture to be created to avoid repetitive code. tearDown is called after every method, which allows these resources to be released, even if the test case raises an exception. However, if it is setUp that raises the exception, then neither the test case nor tearDown will run. ( src , src ) Before After import unittest from phonebook import PhoneBook class PhoneBookTest ( unittest . TestCase ): def test_lookup_by_name ( self ): phonebook = PhoneBook () phonebook . add ( \"Bob\" , \"12345\" ) number = phonebook . lookup ( \"Bob\" ) self . assertEqual ( \"12345\" , number ) def test_missing_name ( self ): phonebook = PhoneBook () with self . assertRaises ( KeyError ): phonebook . lookup ( \"missing\" ) @unittest . skip ( \"WIP\" ) def test_empty_phonebook_is_consistent ( self ): phonebook = PhoneBook () self . assertTrue ( phonebook . is_consistent ()) import unittest from phonebook import PhoneBook class PhoneBookTest ( unittest . TestCase ): def setUp ( self ) -> None : self . phonebook = PhoneBook () def tearDown ( self ) -> None : self . phonebook . clear () def test_lookup_by_name ( self ): # phonebook = PhoneBook() self . phonebook . add ( \"Bob\" , \"12345\" ) number = self . phonebook . lookup ( \"Bob\" ) self . assertEqual ( \"12345\" , number ) def test_missing_name ( self ): # phonebook = PhoneBook() with self . assertRaises ( KeyError ): self . phonebook . lookup ( \"missing\" ) @unittest . skip ( \"WIP\" ) def test_empty_phonebook_is_consistent ( self ): # phonebook = PhoneBook() self . assertTrue ( self . phonebook . is_consistent ()) The @unittest.skip decorator will tell the test runner to skip the decorated test case ( src ) @unittest . skip ( \"WIP\" ) def test_empty_phonebook_is_consistent ( self ): phonebook = PhoneBook () self . assertTrue ( phonebook . is_consistent ()) The command line entry point is made with a call to unittest.main() , which executes the tests. ( src ) import unittest from my_sum import sum class TestSum ( unittest . TestCase ): def test_list_int ( self ): \"\"\" Test that it can sum a list of integers \"\"\" data = [ 1 , 2 , 3 ] result = sum ( data ) self . assertEqual ( result , 6 ) if __name__ == '__main__' : unittest . main () Integration tests By convention, tests are put in their own directory as sibling to the main module ( in order to be able to import it ). Integration and unit tests should be organized separately. . \u251c\u2500\u2500 project \u2502 \u2514\u2500\u2500 __init__.py \u2514\u2500\u2500 tests \u251c\u2500\u2500 integration \u2514\u2500\u2500 unit Run all integration tests within specified directory. python -m unittest discover -s tests/integration Tasks Deserialize YAML JSON import yaml with open ( './starships.yaml' ) as f : starships = yaml . safe_load ( f ) import json with open ( './starships.json' ) as f : data = json . load ( f ) Serialize YAML JSON import yaml with open ( './starships.yaml' , 'w' ) as f : yaml . dump ( starships , f ) import json with open ( '/starships.json' , \"w\" ) as f : json . dump ( data , f ) Modules When learning unfamiliar packages and importing them in a demonstration script, care must be taken that the demonstration script does not have the same name as the package being studied. If so, attempting to import the package while in an interpreter within that directory will cause the interpreter to try importing the incomplete script and not the package. When running a Python interpreter within this directory, the files \"calc\" and \"main\" can be imported as modules by specifying their names with no file extension. . \u251c\u2500\u2500 calc.py \u2514\u2500\u2500 main.py import calc # No errors import main # No errors Specifying the full filename including extension does produce an error import calc.py # Error import main.py # Error Glossary Method resolution order Method resolution order (MRO) is the order of base classes that are searched when using super() . It is accessed with __mro__ , which returns a tuple of base classes in order of precedence, ending in object which is the root class of all classes. Non-interactive debugging Non-interactive debugging is the most basic form of debugging, dependent on print or log statements placed within the body of code. Type slot A type slot is any of a number of fields within each magic method, including __new__() , __init__() , and __prepare__() (which returns a dictionary-like object that's used as the local namespace for all code from the class body)","title":"\ud83d\udc0d Python"},{"location":"Python/#python","text":"","title":"\ud83d\udc0d Python"},{"location":"Python/#decorators","text":"A decorator is any function that accepts a function and returns a function. Decorators are one of the main ways that Python implements functional programming principles. Functions are first-class objects and can be passed as parameters. import logging def hello_wrapper ( name , func ): func ( f 'Hello { name } ' ) hello_wrapper ( \"world\" , func = print ) # Hello world hello_wrapper ( \"logs\" , func = logging . warning ) # WARNING:root:Hello logs with open ( 'hello.txt' , 'w' ) as f : hello_wrapper ( 'everyone!' , func = f . write ) import random def anagram ( t ): l = [ c for c in t ] random . shuffle ( l ) print ( \"\" . join ( l )) hello_wrapper ( 'Japushku' , anagram ) # eHoulhluaskpJ The function has to be passed as a reference, actually calling it will cause the wrapper function to attempt to execute the value returned by the inner function. hello_wrapper ( \"world\" , func = print ()) # Error def outer (): print ( 'Hi from the outer function' ) def inner (): print ( 'Hello from the inner function' ) inner () We can use the __name__ attribute to access a passed function's name. def hello ( func ): print ( f 'Hello { func . __name__ } ' ) hello ( outer ) # Hello outer We can also return functions, which can then be invoked def hello ( func ): print ( f 'Hello { func . __name__ } ' ) return func hello ( outer )() ''' Hi from the outer function Hello from the inner function ''' new_outer = hello ( outer ) new_outer is outer # True def wrapper ( func ): print ( f 'Before { func . __name__ } ' ) func () print ( f 'After { func . __name__ } ' ) wrapper ( outer ) ''' Before outer Hi from the outer function Hello from the inner function After outer ''' The true decorator pattern appears here, where wrapper is called the decorator and outer has been decorated . def wrapper ( func ): def _wrapper (): print ( f 'Before { func . __name__ } ' ) func () print ( f 'After { func . __name__ } ' ) return _wrapper outer = wrapper ( outer ) But the usual syntax since Python 2.4 is to place the decorator on the line above the decorated function, preceded by @ : @wrapper def outer (): print ( 'Hi from the outer function' ) def inner (): print ( 'Hello from the inner function' ) inner () _wrapper here does not accept any positional arguments, so wrapping functions that take arguments will produce a TypeError @wrapper def say_hello ( name ): print ( f 'Hello { name } !' ) # error The solution is to incorporate *args, **kwargs into the definition of the inner function, as well as the invocation of the function passed in. def wrapper ( func ): def _wrapper ( * args , ** kwargs ): print ( f 'Before { func . __name__ } ' ) func ( * args , ** kwargs ) print ( f 'After { func . __name__ } ' ) return _wrapper Returned values are not captured yet: def wrapper ( func ): def _wrapper ( * args , ** kwargs ): print ( f 'Before { func . __name__ } ' ) value = func ( * args , ** kwargs ) print ( f 'After { func . __name__ } ' ) return value return _wrapper Inspecting the decorated function's __name__ attribute reveals that it is still named _wrapper say_hello . __name__ # '_wrapper' This is also true for other attributes, including docstring. functools.wraps is a decorator factory to reassign attributes to the wrapped function. This is considered superior to the functools.update_wrapper function which is also available. def wrapper ( func ): @functools . wraps ( func ) def _wrapper ( * args , ** kwargs ): print ( f 'Before { func . __name__ } ' ) value = func ( * args , ** kwargs ) print ( f 'After { func . __name__ } ' ) return value return _wrapper This forms an ideal starting template for the creation of custom decorators.","title":"Decorators"},{"location":"Python/#classes","text":"","title":"Classes"},{"location":"Python/#properties","text":"In the Python documentation , attributes accessed with accessor functions are called managed attributes , which makes the term equivalent to properties in C#. Three methods can be defined using the @property decorator Constructor Getter Setter Deleter def __init__ ( self , price ): self . _price = price @property def price ( self ): return self . _price @price . setter def price ( self , new_price ): if new_price > 0 : self . _price = new_price else : raise ValueError @price . deleter def price ( self ): del self . _price","title":"Properties"},{"location":"Python/#class-methods","text":"The @classmethod decorator prevents the interpreter from passing in the instantiated object using self , rather the class itself is passed in as the cls argument. This means that the methods decorated as such must take not self as the first argument but cls @classmethod def classmethod ( cls ): pass The @staticmethod decorator prevents the interpreter from passing any additional arguments whatsoever. The resulting method has no access to the object itself nor the class and functions like a procedurally defined function.","title":"Class methods"},{"location":"Python/#formatting","text":"flake8 , black , and yapf are CLI tools used to automatically format Python code.","title":"Formatting"},{"location":"Python/#virtual-environments","text":"","title":"Virtual environments"},{"location":"Python/#pipenv","text":"pipenv --python 3 .6","title":"pipenv"},{"location":"Python/#venv","text":"Create a virtual environment named project python -m venv project","title":"venv"},{"location":"Python/#virtualenv","text":"Create a virtual environment named project using a different version of Python virtualenv -p /usr/bin/python2 project","title":"virtualenv"},{"location":"Python/#testing","text":"Pytest is a popular testing framework preferred to unittest by many Python developers because it follows Pythonic conventions more closely. In contrast to unittest's custom methods, pytest relies on the builtin assert statement. pytest unittest Class under test from phonebook import PhoneBook import pytest @pytest . fixture def phonebook (): phonebook = PhoneBook () yield phonebook phonebook . clear () def test_lookup_by_name ( phonebook ): phonebook . add ( \"Bob\" , \"1234\" ) assert \"1234\" == phonebook . lookup ( \"Bob\" ) def test_phonebook_contains_all_names ( phonebook ): phonebook . add ( \"Bob\" , \"1234\" ) assert \"Bob\" in phonebook . names () def test_missing_name_raises_error ( phonebook ): with pytest . raises ( KeyError ): phonebook . lookup ( \"Bob\" ) python -m pytest import unittest from phonebook import PhoneBook class PhoneBookTest ( unittest . TestCase ): def test_lookup_by_name ( self ): self . phonebook . add ( \"Bob\" , \"12345\" ) number = self . phonebook . lookup ( \"Bob\" ) self . assertEqual ( \"12345\" , number ) def test_missing_name ( self ): with self . assertRaises ( KeyError ): self . phonebook . lookup ( \"missing\" ) def test_empty_phonebook_is_consistent ( self ): self . assertTrue ( self . phonebook . is_consistent ()) def setUp ( self ) -> None : self . phonebook = PhoneBook () def tearDown ( self ) -> None : self . phonebook . clear () python -m unittest import os class PhoneBook : def __init__ ( self , cache_directory = os . getcwd ()): self . numbers = {} self . filename = os . path . join ( cache_directory , \"phonebook.txt\" ) self . cache = open ( self . filename , \"w\" ) def add ( self , name , number ): self . numbers [ name ] = number def lookup ( self , name ): return self . numbers [ name ] def is_consistent ( self ): return True def names ( self ): return set ( self . numbers . keys ()) def clear ( self ): self . cache . close () os . remove ( self . filename )","title":"Testing"},{"location":"Python/#doctest","text":"A doctest is a docstring containing what looks like interactive Python sessions. Python Docs \"\"\" Return the factorial of n, an exact integer >= 0. >>> [factorial(n) for n in range(6)] [1, 1, 2, 6, 24, 120] >>> factorial(30) 265252859812191058636308480000000 >>> factorial(-1) Traceback (most recent call last): ... ValueError: n must be >= 0 Factorials of floats are OK, but the float must be an exact integer: >>> factorial(30.1) Traceback (most recent call last): ... ValueError: n must be exact integer >>> factorial(30.0) 265252859812191058636308480000000 It must also not be ridiculously large: >>> factorial(1e100) Traceback (most recent call last): ... OverflowError: n too large \"\"\" This can then be run if __name__ == '__main__' : import doctest doctest . testmod ()","title":"Doctest"},{"location":"Python/#pytest","text":"PyTest relies on the built-in assert statement.","title":"pytest"},{"location":"Python/#fixtures","text":"The @pytest.fixture decorator facilitiates the creation of test fixtures. The fixture function's name is used as argument to the test case, and the value returned can be used by the logic within. ( src ) Any clean-up logic can be invoked in this fixture as well by replacing return with yield . Pytest also provides its own tmpdir test fixture for temporary directories. ( src ) Before After tmpdir from phonebook import PhoneBook import pytest def test_lookup_by_name ( phonebook ): phonebook = PhoneBook () phonebook . add ( \"Bob\" , \"1234\" ) assert \"1234\" == phonebook . lookup ( \"Bob\" ) def test_phonebook_contains_all_names ( phonebook ): phonebook = PhoneBook () phonebook . add ( \"Bob\" , \"1234\" ) assert \"Bob\" in phonebook . names () def test_missing_name_raises_error ( phonebook ): phonebook = PhoneBook () with pytest . raises ( KeyError ): phonebook . lookup ( \"Bob\" ) from phonebook import PhoneBook import pytest @pytest . fixture def phonebook (): phonebook = PhoneBook () yield phonebook phonebook . clear () def test_lookup_by_name ( phonebook ): # phonebook = PhoneBook() phonebook . add ( \"Bob\" , \"1234\" ) assert \"1234\" == phonebook . lookup ( \"Bob\" ) def test_phonebook_contains_all_names ( phonebook ): # phonebook = PhoneBook() phonebook . add ( \"Bob\" , \"1234\" ) assert \"Bob\" in phonebook . names () def test_missing_name_raises_error ( phonebook ): # phonebook = PhoneBook() with pytest . raises ( KeyError ): phonebook . lookup ( \"Bob\" ) from phonebook import PhoneBook import pytest @pytest . fixture def phonebook ( tmpdir ): phonebook = PhoneBook ( tmpdir ) return phonebook def test_lookup_by_name ( phonebook ): # phonebook = PhoneBook() phonebook . add ( \"Bob\" , \"1234\" ) assert \"1234\" == phonebook . lookup ( \"Bob\" ) def test_phonebook_contains_all_names ( phonebook ): # phonebook = PhoneBook() phonebook . add ( \"Bob\" , \"1234\" ) assert \"Bob\" in phonebook . names () def test_missing_name_raises_error ( phonebook ): # phonebook = PhoneBook() with pytest . raises ( KeyError ): phonebook . lookup ( \"Bob\" )","title":"Fixtures"},{"location":"Python/#unittest","text":"unittest is a testing framework built into Python's Standard Library that was based on JUnit. unittest came out in 2001, when JUnit was being ported and adapted to many languages. Collectively, these frameworks were referred to as the xUnit family . unittest's method names do not follow Python conventions because it predates the PEP-8 naming standard. unittest allows you to create test classes that inherit from TestCase .","title":"unittest"},{"location":"Python/#assertions","text":"Assertions are implemented in individual methods of the TestCase subclass through unittest methods like assertEqual and assertRaises , etc. Notably, TestCase subclasses must not have an __init__() constructor method defined. def test_lookup_by_name ( self ): phonebook = PhoneBook () phonebook . add ( \"Bob\" , \"12345\" ) number = phonebook . lookup ( \"Bob\" ) self . assertEqual ( \"12345\" , number ) assertRaises must be placed in a context manager. Here, the test case will run the code within the with block and check to make sure it raises the specified exception: KeyError : ( src ) def test_missing_name ( self ): fleet = Fleet () with self . assertRaises ( KeyError ): fleet . lookup ( \"bla\" )","title":"Assertions"},{"location":"Python/#fixtures_1","text":"setUp is run before every test method, allowing a test fixture to be created to avoid repetitive code. tearDown is called after every method, which allows these resources to be released, even if the test case raises an exception. However, if it is setUp that raises the exception, then neither the test case nor tearDown will run. ( src , src ) Before After import unittest from phonebook import PhoneBook class PhoneBookTest ( unittest . TestCase ): def test_lookup_by_name ( self ): phonebook = PhoneBook () phonebook . add ( \"Bob\" , \"12345\" ) number = phonebook . lookup ( \"Bob\" ) self . assertEqual ( \"12345\" , number ) def test_missing_name ( self ): phonebook = PhoneBook () with self . assertRaises ( KeyError ): phonebook . lookup ( \"missing\" ) @unittest . skip ( \"WIP\" ) def test_empty_phonebook_is_consistent ( self ): phonebook = PhoneBook () self . assertTrue ( phonebook . is_consistent ()) import unittest from phonebook import PhoneBook class PhoneBookTest ( unittest . TestCase ): def setUp ( self ) -> None : self . phonebook = PhoneBook () def tearDown ( self ) -> None : self . phonebook . clear () def test_lookup_by_name ( self ): # phonebook = PhoneBook() self . phonebook . add ( \"Bob\" , \"12345\" ) number = self . phonebook . lookup ( \"Bob\" ) self . assertEqual ( \"12345\" , number ) def test_missing_name ( self ): # phonebook = PhoneBook() with self . assertRaises ( KeyError ): self . phonebook . lookup ( \"missing\" ) @unittest . skip ( \"WIP\" ) def test_empty_phonebook_is_consistent ( self ): # phonebook = PhoneBook() self . assertTrue ( self . phonebook . is_consistent ()) The @unittest.skip decorator will tell the test runner to skip the decorated test case ( src ) @unittest . skip ( \"WIP\" ) def test_empty_phonebook_is_consistent ( self ): phonebook = PhoneBook () self . assertTrue ( phonebook . is_consistent ()) The command line entry point is made with a call to unittest.main() , which executes the tests. ( src ) import unittest from my_sum import sum class TestSum ( unittest . TestCase ): def test_list_int ( self ): \"\"\" Test that it can sum a list of integers \"\"\" data = [ 1 , 2 , 3 ] result = sum ( data ) self . assertEqual ( result , 6 ) if __name__ == '__main__' : unittest . main ()","title":"Fixtures"},{"location":"Python/#integration-tests","text":"By convention, tests are put in their own directory as sibling to the main module ( in order to be able to import it ). Integration and unit tests should be organized separately. . \u251c\u2500\u2500 project \u2502 \u2514\u2500\u2500 __init__.py \u2514\u2500\u2500 tests \u251c\u2500\u2500 integration \u2514\u2500\u2500 unit Run all integration tests within specified directory. python -m unittest discover -s tests/integration","title":"Integration tests"},{"location":"Python/#tasks","text":"Deserialize YAML JSON import yaml with open ( './starships.yaml' ) as f : starships = yaml . safe_load ( f ) import json with open ( './starships.json' ) as f : data = json . load ( f ) Serialize YAML JSON import yaml with open ( './starships.yaml' , 'w' ) as f : yaml . dump ( starships , f ) import json with open ( '/starships.json' , \"w\" ) as f : json . dump ( data , f )","title":"Tasks"},{"location":"Python/#modules","text":"When learning unfamiliar packages and importing them in a demonstration script, care must be taken that the demonstration script does not have the same name as the package being studied. If so, attempting to import the package while in an interpreter within that directory will cause the interpreter to try importing the incomplete script and not the package. When running a Python interpreter within this directory, the files \"calc\" and \"main\" can be imported as modules by specifying their names with no file extension. . \u251c\u2500\u2500 calc.py \u2514\u2500\u2500 main.py import calc # No errors import main # No errors Specifying the full filename including extension does produce an error import calc.py # Error import main.py # Error","title":"Modules"},{"location":"Python/#glossary","text":"Method resolution order Method resolution order (MRO) is the order of base classes that are searched when using super() . It is accessed with __mro__ , which returns a tuple of base classes in order of precedence, ending in object which is the root class of all classes. Non-interactive debugging Non-interactive debugging is the most basic form of debugging, dependent on print or log statements placed within the body of code. Type slot A type slot is any of a number of fields within each magic method, including __new__() , __init__() , and __prepare__() (which returns a dictionary-like object that's used as the local namespace for all code from the class body)","title":"Glossary"},{"location":"Python/Modules/Argparse/","text":"Argparse The ArgumentParser object exposes an attribute that contains the value passed in from the command-line. This attribute takes its identifier from the dest keyword argument when invoking the add_argument() method. import argparse def get_args (): parser = argparse . ArgumentParser () parser . add_argument ( dest = 'bar' , description = helptext , # (1) ) return parser . parse_args () def main (): args = get_args () . bar The optional value assigned to description will be displayed when running the script with the options -h or --help A help string can be provided as a keyword argument to help . parser . add_argument ( \"foo\" , help = \"bar\" ) A data type can be specified as an argument to type : parser . add_argument ( \"foo\" , type = int ) An alternative name for the dest value on the command-line (but which does not affect the identifier of the attribute on which the value is exposed) can be specified by metavar . parser . add_argument ( \"foo\" , metavar = \"bar\" ) The examples above used positional parameters (i.e. an argument). A named parameter (an option or flag, i.e. -h , --help , etc) requires - at the beginning of the string and values from the command-line to be passed after = or Space . add_argument supports the required keyword argument for named parameters. Note that use of the option on the command-line at all requires an argument to it, even if the option is not required itself. parser . add_argument ( '-r' , '--radius' , type = int , required = True , help = 'radius' ) A flag option can be created by defining an action keyword parameter. ( src ) parser . add_argument ( '-o' , '--on' , help = 'A boolean flag' , action = 'store_true' ) add_mutually_exclusive_group() can be used to add a group of mutually exclusive arguments. In this case, add_argument() is invoked on the new object returned by this method and not directly on the ArgumentParser() object. g = ArgumentParser . add_mutually_exclusive_group () g . add_argument ( \"-v\" , \"--verbose\" , action = \"store_true\" ) g . add_argument ( \"-q\" , \"--quiet\" , \"-s\" , \"--silent\" , action = \"store_true\" , help = 'quiet/silent mode' ) User input can be restricted by providing a value for choices , which will accept any iterable value including lists, ranges, and strings: parser . add_argument ( \"foo\" , choices = [ \"bar\" , \"baz\" ]) parser . add_argument ( \"foo\" , choices = range ( 1 , 10 )) parser . add_argument ( \"foo\" , choices = 'Hello, world!' ) # equivalent to ['H','e', ...] Sources Python documentation","title":"Argparse"},{"location":"Python/Modules/Argparse/#argparse","text":"The ArgumentParser object exposes an attribute that contains the value passed in from the command-line. This attribute takes its identifier from the dest keyword argument when invoking the add_argument() method. import argparse def get_args (): parser = argparse . ArgumentParser () parser . add_argument ( dest = 'bar' , description = helptext , # (1) ) return parser . parse_args () def main (): args = get_args () . bar The optional value assigned to description will be displayed when running the script with the options -h or --help A help string can be provided as a keyword argument to help . parser . add_argument ( \"foo\" , help = \"bar\" ) A data type can be specified as an argument to type : parser . add_argument ( \"foo\" , type = int ) An alternative name for the dest value on the command-line (but which does not affect the identifier of the attribute on which the value is exposed) can be specified by metavar . parser . add_argument ( \"foo\" , metavar = \"bar\" ) The examples above used positional parameters (i.e. an argument). A named parameter (an option or flag, i.e. -h , --help , etc) requires - at the beginning of the string and values from the command-line to be passed after = or Space . add_argument supports the required keyword argument for named parameters. Note that use of the option on the command-line at all requires an argument to it, even if the option is not required itself. parser . add_argument ( '-r' , '--radius' , type = int , required = True , help = 'radius' ) A flag option can be created by defining an action keyword parameter. ( src ) parser . add_argument ( '-o' , '--on' , help = 'A boolean flag' , action = 'store_true' ) add_mutually_exclusive_group() can be used to add a group of mutually exclusive arguments. In this case, add_argument() is invoked on the new object returned by this method and not directly on the ArgumentParser() object. g = ArgumentParser . add_mutually_exclusive_group () g . add_argument ( \"-v\" , \"--verbose\" , action = \"store_true\" ) g . add_argument ( \"-q\" , \"--quiet\" , \"-s\" , \"--silent\" , action = \"store_true\" , help = 'quiet/silent mode' ) User input can be restricted by providing a value for choices , which will accept any iterable value including lists, ranges, and strings: parser . add_argument ( \"foo\" , choices = [ \"bar\" , \"baz\" ]) parser . add_argument ( \"foo\" , choices = range ( 1 , 10 )) parser . add_argument ( \"foo\" , choices = 'Hello, world!' ) # equivalent to ['H','e', ...] Sources Python documentation","title":"Argparse"},{"location":"Python/Modules/Asyncio/","text":"Asyncio The asyncio module offers an implementation of coroutines which allow tasks to control context switching to implement concurrency . The await keyword is a checkpoint that indicates where it is safe for the process to go to another coroutine, allowing total control over context switching . import asyncio import time counter = 0 async def func1 (): global counter while True : counter += 1 counter -= 1 await asyncio . sleep ( 0 ) async def func2 (): global counter while True : counter += 1 counter -= 1 await asyncio . sleep ( 0 ) asyncio . gather ( func1 (), func2 ()) asyncio . get_event_loop () . run_forever () async def get_users (): users = await client . do_query ( 'select * from users' ) return users async def main (): task = asyncio . create_task ( get_users ()) # ... await task asyncio . run ( main ()) Allows the joining of multiple threads. async def get_users (): users = await client . do_query ( 'select * from users' ) return users async def main (): await asyncio . gather ( get_users (), get_users (), ) asyncio . run ( main ()) async def get_users (): users = await client . do_query ( 'select * from users' ) return users asyncio . run ( get_users ()) async def main (): users = await get_users () print ( users ) asyncio . run ( main ()) Sources: Demistifying Python's Async and Await keywords","title":"Asyncio"},{"location":"Python/Modules/Asyncio/#asyncio","text":"The asyncio module offers an implementation of coroutines which allow tasks to control context switching to implement concurrency . The await keyword is a checkpoint that indicates where it is safe for the process to go to another coroutine, allowing total control over context switching . import asyncio import time counter = 0 async def func1 (): global counter while True : counter += 1 counter -= 1 await asyncio . sleep ( 0 ) async def func2 (): global counter while True : counter += 1 counter -= 1 await asyncio . sleep ( 0 ) asyncio . gather ( func1 (), func2 ()) asyncio . get_event_loop () . run_forever () async def get_users (): users = await client . do_query ( 'select * from users' ) return users async def main (): task = asyncio . create_task ( get_users ()) # ... await task asyncio . run ( main ()) Allows the joining of multiple threads. async def get_users (): users = await client . do_query ( 'select * from users' ) return users async def main (): await asyncio . gather ( get_users (), get_users (), ) asyncio . run ( main ()) async def get_users (): users = await client . do_query ( 'select * from users' ) return users asyncio . run ( get_users ()) async def main (): users = await get_users () print ( users ) asyncio . run ( main ()) Sources: Demistifying Python's Async and Await keywords","title":"Asyncio"},{"location":"Python/Modules/Azure.cosmos/","text":"Azure.cosmos import azure.cosmos from azure.cosmos.partition_key import PartitionKey database = cosmos_client . create_database ( 'RetailDemo' ) container = database . create_container ( id = 'WebsiteData' , partition_key = PartitionKey ( path = '/CartID' )) print ( 'Container WebsiteData created' )","title":"Azure.cosmos"},{"location":"Python/Modules/Azure.cosmos/#azurecosmos","text":"import azure.cosmos from azure.cosmos.partition_key import PartitionKey database = cosmos_client . create_database ( 'RetailDemo' ) container = database . create_container ( id = 'WebsiteData' , partition_key = PartitionKey ( path = '/CartID' )) print ( 'Container WebsiteData created' )","title":"Azure.cosmos"},{"location":"Python/Modules/Bullet/","text":"Bullet bullet.Check() implements a checkbox widget: cli = bullet . Check ( prompt = \"Choose from the following items: \" , choices = [ 'pepperoni' , 'sausage' , 'green peppers' ]) bullet.Bullet() implements a radio button widget: cli = bullet . Bullet ( prompt = \"Choose from the following items: \" , choices = [ 'red' , 'white' , 'blue' ]) The resulting object then exposes a launch() method. cli . launch ()","title":"Bullet"},{"location":"Python/Modules/Bullet/#bullet","text":"bullet.Check() implements a checkbox widget: cli = bullet . Check ( prompt = \"Choose from the following items: \" , choices = [ 'pepperoni' , 'sausage' , 'green peppers' ]) bullet.Bullet() implements a radio button widget: cli = bullet . Bullet ( prompt = \"Choose from the following items: \" , choices = [ 'red' , 'white' , 'blue' ]) The resulting object then exposes a launch() method. cli . launch ()","title":"Bullet"},{"location":"Python/Modules/Click/","text":"Click Click modifies functions using decorators whch determine the command-line arguments elements that the decorated function can see. Hello, World import click @click . command () def hello (): click . echo ( 'Hello World!' ) if __name__ = '__main__' : hello () Modified Hello World import click @click . command () @click . option ( '--count' , default = 1 , help = 'number of greetings' ) @click . argument ( 'name' ) def hello ( count , name ): for x in range ( count ): click . echo ( 'Hello %s !' % name ) if __name__ == '__main__' : hello () Developing the pdfcropper tool; passing --examref changes the numbers. import click @click . command () @click . option ( '--examref' , is_flag = True ) def hello ( examref ): top , right , bottom , left = 0 , 0 , 0 , 0 if examref : top , right , bottom , left = 1 , 2 , 3 , 4 click . echo ( f 'Your numbers are: top ( { top } ), right { right } , bottom { bottom } , left { left } ' ) if __name__ == '__main__' : hello () @click.group() decorators allow nested command groups to be created. There are two ways of adding commands to command groups : - Using the group as a decorator, whereby the name of the function decorated by @click.group() is then used to decorate commands: @click . group () def group1 () pass @group1 . command () def command1 (): pass Using the add_command method @click . group () def group1 () pass @click . command () def command1 (): pass group1 . add_command ( command1 ) For example, to imitate the nested commands available in netsh : netsh interface ip @click . group () def interface (): pass @interface . command ( 'ip' ) @click . argument ( 'args' , nargs =- 1 ) # All arguments passed in as tuple \"args\" def interface_ip ( args ): pass Docstrings of groups and commands show up as progressive help messages when they are invoked from the command-line. @click . group () def cli (): pass @click . command () def initdb (): click . echo ( 'Initialized the database' ) @click . command () def dropdb (): click . echo ( 'Dropped the database' ) cli . add_command ( initdb ) cli . add_command ( dropdb ) if __name__ == '__main__' : cli () CommandCollection flattens the structure of grouped commands so that the commands in all the contained groups appear in a single tier. It also becomes the entry-point of the script. Example from GitHub : # Three command groups cli1, cli2, and cli3 declared: @click . group () def cli1 (): pass @click . group () def cli2 (): pass @click . group () def cli3 (): pass # Three commands each belonging to a separate group @cli1 . command () def server (): pass @cli2 . command () def console (): pass @cli3 . command () def routes (): pass # CommandCollection flattens the grouped commands such that all the commands are available at once: cli = click . CommandCollection ( sources = [ cli1 , cli2 , cli3 ]) if __name__ == '__main__' : cli ()","title":"Click"},{"location":"Python/Modules/Click/#click","text":"Click modifies functions using decorators whch determine the command-line arguments elements that the decorated function can see. Hello, World import click @click . command () def hello (): click . echo ( 'Hello World!' ) if __name__ = '__main__' : hello () Modified Hello World import click @click . command () @click . option ( '--count' , default = 1 , help = 'number of greetings' ) @click . argument ( 'name' ) def hello ( count , name ): for x in range ( count ): click . echo ( 'Hello %s !' % name ) if __name__ == '__main__' : hello () Developing the pdfcropper tool; passing --examref changes the numbers. import click @click . command () @click . option ( '--examref' , is_flag = True ) def hello ( examref ): top , right , bottom , left = 0 , 0 , 0 , 0 if examref : top , right , bottom , left = 1 , 2 , 3 , 4 click . echo ( f 'Your numbers are: top ( { top } ), right { right } , bottom { bottom } , left { left } ' ) if __name__ == '__main__' : hello () @click.group() decorators allow nested command groups to be created. There are two ways of adding commands to command groups : - Using the group as a decorator, whereby the name of the function decorated by @click.group() is then used to decorate commands: @click . group () def group1 () pass @group1 . command () def command1 (): pass Using the add_command method @click . group () def group1 () pass @click . command () def command1 (): pass group1 . add_command ( command1 ) For example, to imitate the nested commands available in netsh : netsh interface ip @click . group () def interface (): pass @interface . command ( 'ip' ) @click . argument ( 'args' , nargs =- 1 ) # All arguments passed in as tuple \"args\" def interface_ip ( args ): pass Docstrings of groups and commands show up as progressive help messages when they are invoked from the command-line. @click . group () def cli (): pass @click . command () def initdb (): click . echo ( 'Initialized the database' ) @click . command () def dropdb (): click . echo ( 'Dropped the database' ) cli . add_command ( initdb ) cli . add_command ( dropdb ) if __name__ == '__main__' : cli () CommandCollection flattens the structure of grouped commands so that the commands in all the contained groups appear in a single tier. It also becomes the entry-point of the script. Example from GitHub : # Three command groups cli1, cli2, and cli3 declared: @click . group () def cli1 (): pass @click . group () def cli2 (): pass @click . group () def cli3 (): pass # Three commands each belonging to a separate group @cli1 . command () def server (): pass @cli2 . command () def console (): pass @cli3 . command () def routes (): pass # CommandCollection flattens the grouped commands such that all the commands are available at once: cli = click . CommandCollection ( sources = [ cli1 , cli2 , cli3 ]) if __name__ == '__main__' : cli ()","title":"Click"},{"location":"Python/Modules/Collections/","text":"Collections from collections import namedtuple Card = namedtuple ( 'Card' ,[ 'rank' , 'suit' ]) ` City = namedtuple ( 'City' , 'Name Country Population Coordinates' . split ( ' ' )]","title":"Collections"},{"location":"Python/Modules/Collections/#collections","text":"from collections import namedtuple Card = namedtuple ( 'Card' ,[ 'rank' , 'suit' ]) ` City = namedtuple ( 'City' , 'Name Country Population Coordinates' . split ( ' ' )]","title":"Collections"},{"location":"Python/Modules/Colorama/","text":"Colorama Colorama provides a set of enums that resolve to terminal codes when concatenated with strings. colorama . Fore . GREEN colorama . Style . RESET_ALL","title":"Colorama"},{"location":"Python/Modules/Colorama/#colorama","text":"Colorama provides a set of enums that resolve to terminal codes when concatenated with strings. colorama . Fore . GREEN colorama . Style . RESET_ALL","title":"Colorama"},{"location":"Python/Modules/Csv/","text":"Csv with open ( 'file.csv' , newline = '' ): data = [ row for row in csv . reader ( f )] csv.DictReader with open ( 'greeks.csv' ) as f : reader = csv . DictReader ( f ) for row in reader : print ( row [ 'name' ], row [ 'city' ], row [ 'dob' ])","title":"Csv"},{"location":"Python/Modules/Csv/#csv","text":"with open ( 'file.csv' , newline = '' ): data = [ row for row in csv . reader ( f )] csv.DictReader with open ( 'greeks.csv' ) as f : reader = csv . DictReader ( f ) for row in reader : print ( row [ 'name' ], row [ 'city' ], row [ 'dob' ])","title":"Csv"},{"location":"Python/Modules/Datetime/","text":"Datetime datetime . date ( 2016 , 7 , 24 ) datetime . date . today () Difference between datetime objects is a timedelta Parse strings into datetime objects datetime . strptime ( datestring , formatstring ) # Various metacharacters are defined for `strptime` datetime . datetime . strptime ( '06/30/1992' , '%m/ %d /%Y' )","title":"Datetime"},{"location":"Python/Modules/Datetime/#datetime","text":"datetime . date ( 2016 , 7 , 24 ) datetime . date . today () Difference between datetime objects is a timedelta Parse strings into datetime objects datetime . strptime ( datestring , formatstring ) # Various metacharacters are defined for `strptime` datetime . datetime . strptime ( '06/30/1992' , '%m/ %d /%Y' )","title":"Datetime"},{"location":"Python/Modules/Discord_py/","text":"discord.py pip install discord.py client = discord.Client () Client objects expose a decorator that is used for event handlers, functions named after various events: - on_ready - on_member_join - on_error - on_message @client . event async def on_ready (): print ( f ' { client . user } has connected to Discord!' ) Another decorator is exposed for in-chat commands ( commands.Bot has to be instantiated first.) @bot . command ( name = 'roll_dice' , help = 'Simulates rolling dice.' ) async def roll ( ctx , number_of_dice : int , number_of_sides : int ): dice = [ str ( random . choice ( range ( 1 , number_of_sides + 1 ))) for _ in range ( number_of_dice ) ] await ctx . send ( ', ' . join ( dice )) client . run ( token ) bot = comands . Bot ( command_prefix = '!' )","title":"discord.py"},{"location":"Python/Modules/Discord_py/#discordpy","text":"pip install discord.py client = discord.Client () Client objects expose a decorator that is used for event handlers, functions named after various events: - on_ready - on_member_join - on_error - on_message @client . event async def on_ready (): print ( f ' { client . user } has connected to Discord!' ) Another decorator is exposed for in-chat commands ( commands.Bot has to be instantiated first.) @bot . command ( name = 'roll_dice' , help = 'Simulates rolling dice.' ) async def roll ( ctx , number_of_dice : int , number_of_sides : int ): dice = [ str ( random . choice ( range ( 1 , number_of_sides + 1 ))) for _ in range ( number_of_dice ) ] await ctx . send ( ', ' . join ( dice )) client . run ( token ) bot = comands . Bot ( command_prefix = '!' )","title":"discord.py"},{"location":"Python/Modules/Django/","text":"Django A typical Django project contains multiple apps , which are Python packages containing their own models, views, templates, and urls. A model is the single definitive source of information about your data, and contains the essential fields and behaviors of the data you're storing. Migrations are necessary when Model classes are updated. And for projects sufficiently advanced, migration scripts must be developed for any such changes. Async Server Gateway Interface (ASGI) is the spiritual successor to, and superset of, WSGI . It implements the new Python standard for asynchronous web servers and applications, which resembles that of websockets . From WSGI to ASGI WSGI is coupled tightly with the synchronous request-response model familiar from HTTP 1.1. URL patterns (stored in the urlpatterns list defined in the project-level urls.py file) can be parameterized. Here, the template <int:x> specifies an integer to be assigned to the view parameter x . from app.views import my_view urlpatterns = [ path ( '/example/<int:x>' , my_view ) ] modelform_factory can be used to automatically produce a webform from a Model class. # views.py MeetingForm = modelform_factory ( Meeting , exclude = []) This can be placed into a template using the {{ form }} template tag. Note, a {% csrf_token %} template tag must also be present for a submit button to work. { % block content % } < h1 > Plan a new meeting </ h1 > < form method = \"POST\" > < table > {{ form }} </ table > { % csrf_token % } < button type = \"submit\" > Create </ button > </ form > { % endblock content % } When the modelform_factory class has been defined, it is instantiated within the view function . This object exposes several methods: - is_valid data validation is strongly recommended for any form input - save imports the validated form data into the database def new ( request ): if request . method == 'POST' : form = MeetingForm ( request . POST ) if form . is_valid (): form . save () return redirect ( \"home\" ) else : form = MeetingForm () return render ( request , \"meetings/new.html\" , { \"form\" : form }) Template Django templates are HTML files with additional markup to signify places where data can be dynamically inserted. The data used by the views file is called the template context . Templates must be placed within the /templates folder within the app, and it is considered best practice to place templates within a nested subdirectory within it, e.g. /templates/app . Django template tags are specified beween {% .. %} and allow for interpolation of data. < ul > { % for m in meetings % } { % endfor % } </ ul > URLs can be built by using the url template tag, specifying the name of a URL urlpatterns = [ path ( '' , home , name = 'home' ) ] < a href = \"{ % u rl 'home' %}\" > Home </ a > Models In Django, a Model class is mapped to a database table. Each object is a record in that table. Model objects expose several attributes and methods Get all objects meetings = Meeting . objects . all () Get count of objects in database count = Meeting . objects . count () Query for a specific object meeting = Meeting . objects . get ( pk = id ) get_object_or_404 may be better for most cases meeting = get_object_or_404 ( Meeting , pk = id ) Adding a new app python manage.py startapp app Then add to settings.py in project directory INSTALLED_APPS = [ # ... 'app' , ] There appears to be much flexibility in the arrangement of input controls in a form. So long as the Submit button is child to the form element, tasks are accepted in the To-Do app. Per Bulma documentation, the field class is intended as a container for label.label s, .control s, and optional p.help text. In contrast, control is a block container meant to enhance single form controls and can only contain input , select , button , or icon elements. form .field ( method= \"POST\" , action= \"/\" ) label .label Enter something to do .control | {{form.title}} | {% csrf_token %} button .button.is-primary ( type= \"submit\" ) Submit","title":"Django"},{"location":"Python/Modules/Django/#django","text":"A typical Django project contains multiple apps , which are Python packages containing their own models, views, templates, and urls. A model is the single definitive source of information about your data, and contains the essential fields and behaviors of the data you're storing. Migrations are necessary when Model classes are updated. And for projects sufficiently advanced, migration scripts must be developed for any such changes. Async Server Gateway Interface (ASGI) is the spiritual successor to, and superset of, WSGI . It implements the new Python standard for asynchronous web servers and applications, which resembles that of websockets . From WSGI to ASGI WSGI is coupled tightly with the synchronous request-response model familiar from HTTP 1.1. URL patterns (stored in the urlpatterns list defined in the project-level urls.py file) can be parameterized. Here, the template <int:x> specifies an integer to be assigned to the view parameter x . from app.views import my_view urlpatterns = [ path ( '/example/<int:x>' , my_view ) ] modelform_factory can be used to automatically produce a webform from a Model class. # views.py MeetingForm = modelform_factory ( Meeting , exclude = []) This can be placed into a template using the {{ form }} template tag. Note, a {% csrf_token %} template tag must also be present for a submit button to work. { % block content % } < h1 > Plan a new meeting </ h1 > < form method = \"POST\" > < table > {{ form }} </ table > { % csrf_token % } < button type = \"submit\" > Create </ button > </ form > { % endblock content % } When the modelform_factory class has been defined, it is instantiated within the view function . This object exposes several methods: - is_valid data validation is strongly recommended for any form input - save imports the validated form data into the database def new ( request ): if request . method == 'POST' : form = MeetingForm ( request . POST ) if form . is_valid (): form . save () return redirect ( \"home\" ) else : form = MeetingForm () return render ( request , \"meetings/new.html\" , { \"form\" : form })","title":"Django"},{"location":"Python/Modules/Django/#template","text":"Django templates are HTML files with additional markup to signify places where data can be dynamically inserted. The data used by the views file is called the template context . Templates must be placed within the /templates folder within the app, and it is considered best practice to place templates within a nested subdirectory within it, e.g. /templates/app . Django template tags are specified beween {% .. %} and allow for interpolation of data. < ul > { % for m in meetings % } { % endfor % } </ ul > URLs can be built by using the url template tag, specifying the name of a URL urlpatterns = [ path ( '' , home , name = 'home' ) ] < a href = \"{ % u rl 'home' %}\" > Home </ a >","title":"Template"},{"location":"Python/Modules/Django/#models","text":"In Django, a Model class is mapped to a database table. Each object is a record in that table. Model objects expose several attributes and methods Get all objects meetings = Meeting . objects . all () Get count of objects in database count = Meeting . objects . count () Query for a specific object meeting = Meeting . objects . get ( pk = id ) get_object_or_404 may be better for most cases meeting = get_object_or_404 ( Meeting , pk = id ) Adding a new app python manage.py startapp app Then add to settings.py in project directory INSTALLED_APPS = [ # ... 'app' , ] There appears to be much flexibility in the arrangement of input controls in a form. So long as the Submit button is child to the form element, tasks are accepted in the To-Do app. Per Bulma documentation, the field class is intended as a container for label.label s, .control s, and optional p.help text. In contrast, control is a block container meant to enhance single form controls and can only contain input , select , button , or icon elements. form .field ( method= \"POST\" , action= \"/\" ) label .label Enter something to do .control | {{form.title}} | {% csrf_token %} button .button.is-primary ( type= \"submit\" ) Submit","title":"Models"},{"location":"Python/Modules/Dotenv/","text":"Dotenv pip install -U python-dotenv Load a .env file placed in the current working directory. load_dotenv () value = os . getenv ( 'key' )","title":"Dotenv"},{"location":"Python/Modules/Dotenv/#dotenv","text":"pip install -U python-dotenv Load a .env file placed in the current working directory. load_dotenv () value = os . getenv ( 'key' )","title":"Dotenv"},{"location":"Python/Modules/FastAPI/","text":"FastAPI Variables values can be taken from the route or from query parameters following a question mark. Routes Query parameters from fastapi import FastAPI starships = FastAPI () @starships . get ( \"/starships/ {registry} \" ) def get_starship ( name : str ): return { \"response\" : f \"Hello, { name } \" } from fastapi import FastAPI starships = FastAPI () @starships . get ( \"/\" ) def get_starship ( name : str = \"world\" ): return { \"response\" : f \"Hello, { name } \" } FastAPI is notable for being able to use type hints to construct data models, which are much lighter than the object relational models used by other frameworks. FastAPI Django from pydantic import BaseModel class Starship ( BaseModel ): name : str registry : str crew : int from django.db import models class Starship ( models . Model ): name = models . CharField ( max_length = 50 ) registry = models . CharField ( max_length = 15 ) crew = models . IntegerField () Dogfood data can be incorporated by using the keyword argument unpacking or \"double splat\" operator ( ** ) data = { \"name\" : \"USS Enterprise\" , \"registry\" : \"NCC-1701\" , \"crew\" : 203 } enterprise = Starship ( ** data ) POST method definitions then can use this newly defined class to validate posted data db = [] @app . post ( \"/starships\" ) async def create_starship ( starship : Starship ): db . append ( starship ) FastAPI supports Jinja templates to serve HTML templates import fastapi from fastapi.templating import Jinja2Templates # specifies the directory where templates are to be found templates = Jinja2Templates ( \"templates\" ) api = fastapi . APIRouter () @api . get ( '/' ) def index ( request : starlette . requests . Request ): return templates . TemplateResponse ( \"helloworld.html\" , { \"request\" : request }) By default, FastAPI also exposes web applications at /docs where you can test out all the exposed API methods. FastAPI integrates with ASGI servers like Uvicorn and Hypercorn, which can run a specific web application by name from the command-line or from within the script: Shell Python uvicorn main:starships --port 7000 import uvicorn uvicorn . run ( starships , port = 7000 )","title":"FastAPI"},{"location":"Python/Modules/FastAPI/#fastapi","text":"Variables values can be taken from the route or from query parameters following a question mark. Routes Query parameters from fastapi import FastAPI starships = FastAPI () @starships . get ( \"/starships/ {registry} \" ) def get_starship ( name : str ): return { \"response\" : f \"Hello, { name } \" } from fastapi import FastAPI starships = FastAPI () @starships . get ( \"/\" ) def get_starship ( name : str = \"world\" ): return { \"response\" : f \"Hello, { name } \" } FastAPI is notable for being able to use type hints to construct data models, which are much lighter than the object relational models used by other frameworks. FastAPI Django from pydantic import BaseModel class Starship ( BaseModel ): name : str registry : str crew : int from django.db import models class Starship ( models . Model ): name = models . CharField ( max_length = 50 ) registry = models . CharField ( max_length = 15 ) crew = models . IntegerField () Dogfood data can be incorporated by using the keyword argument unpacking or \"double splat\" operator ( ** ) data = { \"name\" : \"USS Enterprise\" , \"registry\" : \"NCC-1701\" , \"crew\" : 203 } enterprise = Starship ( ** data ) POST method definitions then can use this newly defined class to validate posted data db = [] @app . post ( \"/starships\" ) async def create_starship ( starship : Starship ): db . append ( starship ) FastAPI supports Jinja templates to serve HTML templates import fastapi from fastapi.templating import Jinja2Templates # specifies the directory where templates are to be found templates = Jinja2Templates ( \"templates\" ) api = fastapi . APIRouter () @api . get ( '/' ) def index ( request : starlette . requests . Request ): return templates . TemplateResponse ( \"helloworld.html\" , { \"request\" : request }) By default, FastAPI also exposes web applications at /docs where you can test out all the exposed API methods. FastAPI integrates with ASGI servers like Uvicorn and Hypercorn, which can run a specific web application by name from the command-line or from within the script: Shell Python uvicorn main:starships --port 7000 import uvicorn uvicorn . run ( starships , port = 7000 )","title":"FastAPI"},{"location":"Python/Modules/Functools/","text":"Functools For higher-order functions (functions that act on or return other functions) Apply function of two arguments cumulatively to the items of iterable in order to reduce it to a single value functools . reduce ( function , iterable [, initializer ]) Calculate a cumulative sum functools . reduce ( lambda x , y : x + y , [ 1 , 2 , 3 , 4 , 5 ])","title":"Functools"},{"location":"Python/Modules/Functools/#functools","text":"For higher-order functions (functions that act on or return other functions) Apply function of two arguments cumulatively to the items of iterable in order to reduce it to a single value functools . reduce ( function , iterable [, initializer ]) Calculate a cumulative sum functools . reduce ( lambda x , y : x + y , [ 1 , 2 , 3 , 4 , 5 ])","title":"Functools"},{"location":"Python/Modules/Glob/","text":"Glob Produce a list of strings glob . glob ( '*.py' )","title":"Glob"},{"location":"Python/Modules/Glob/#glob","text":"Produce a list of strings glob . glob ( '*.py' )","title":"Glob"},{"location":"Python/Modules/Heapq/","text":"Heapq Support heaps , data objects where each node is either greater than or equal to its parent ( max-heap ) or less than or equal to its parent ( min-heap ) Create a heap from {iterable} heapq . heapify ( iterable ) Remove and return the smallest element of {heap} heapq . heappop ( heap ) Replace the smallest element of {heap} with {element} heapq . heapreplace ( heap , element )","title":"Heapq"},{"location":"Python/Modules/Heapq/#heapq","text":"Support heaps , data objects where each node is either greater than or equal to its parent ( max-heap ) or less than or equal to its parent ( min-heap ) Create a heap from {iterable} heapq . heapify ( iterable ) Remove and return the smallest element of {heap} heapq . heappop ( heap ) Replace the smallest element of {heap} with {element} heapq . heapreplace ( heap , element )","title":"Heapq"},{"location":"Python/Modules/Http/","text":"http Start an HTTP server for the current directory python http.server","title":"http"},{"location":"Python/Modules/Http/#http","text":"Start an HTTP server for the current directory python http.server","title":"http"},{"location":"Python/Modules/Itertools/","text":"itertools cycle() works like next(), but it restarts from the beginning of the iterable that is passed as argument after the last element has been reached. with open ( 'raven' ) as f : raven = [ l for l in f ] itertools . cycle ( raven )","title":"itertools"},{"location":"Python/Modules/Itertools/#itertools","text":"cycle() works like next(), but it restarts from the beginning of the iterable that is passed as argument after the last element has been reached. with open ( 'raven' ) as f : raven = [ l for l in f ] itertools . cycle ( raven )","title":"itertools"},{"location":"Python/Modules/Json/","text":"Json Deserialize Serialize import json with open ( 'starships.json' ) as f : data = json . load ( f ) import json with open ( 'starships.json' , \"w\" ) as f : json . dump ( data , f )","title":"Json"},{"location":"Python/Modules/Json/#json","text":"Deserialize Serialize import json with open ( 'starships.json' ) as f : data = json . load ( f ) import json with open ( 'starships.json' , \"w\" ) as f : json . dump ( data , f )","title":"Json"},{"location":"Python/Modules/Logging/","text":"Logging import logging def main (): logging . basicConfig ( filename = '/tmp/learn-logging.log' , level = logging . ERROR , format = ' %(asctime)s %(levelname)s : %(message)s ' ) logging . info ( \"Once upon a midnight dreary,\" ) logging . warning ( 'While I pondered weak and weary,' ) logging . error ( 'Over many a quaint and curious volume of forgotten lore,' ) if __name__ == '__main__' : main ()","title":"Logging"},{"location":"Python/Modules/Logging/#logging","text":"import logging def main (): logging . basicConfig ( filename = '/tmp/learn-logging.log' , level = logging . ERROR , format = ' %(asctime)s %(levelname)s : %(message)s ' ) logging . info ( \"Once upon a midnight dreary,\" ) logging . warning ( 'While I pondered weak and weary,' ) logging . error ( 'Over many a quaint and curious volume of forgotten lore,' ) if __name__ == '__main__' : main ()","title":"Logging"},{"location":"Python/Modules/Npyscreen/","text":"Npyscreen Widget library and application framework built on top of ncurses . Documentation ] Three main types of object compose npyscreen applications: - Application objects manage forms and other classes - Form objects form the canvas upon which widgets are arrayed - Form general-purpose - FormMutt - Widget objects are individual controls - TitleText text entry - TitleSelectOne equivalent to radio buttons - TitleDateCombo allows picking of date on a small calendar npyscreen.wrapper_basic is the main entry point import npyscreen def myFunction ( * args ): pass if __name__ == '__main__' : npyscreen . wrapper_basic ( myFunction ) print \"Blink and you missed it!\" npyscreen.Form is equivalent to the Tk() object, which is typically instantiated as win in GUI frameworks. F = npyscreen . Form ( name = 'My Test Application' ) Several important methods are key: - create() The standard constructor calls this method, which does nothing by default and is meant to be overriden in subclasses. Widgets are defined here. npyscreen.FormMutt imitates a UI layout popularized by applications like mutt , irssi , and vim , with a title bar at the top, a command line at the bottom, and a status line directly above the command line. ACTION_CONTROLLER can be defined in the FormMutt subclass as the name of a subclass of ActionControllerSimple . Commands for the application can be defined as callbacks in the create() method. self . add_action ( ident , call_back , True ) Callbacks are called with the following arguments: call_back ( command_line , control_widget_proxy , live = True ) class ActionControllerSearch ( npyscreen . ActionControllerSimple ): def create ( self ): self . add_action ( '^/.*' , self . set_search , True ) def set_search ( self , command_line , widget_proxy , live ): self . parent . value . set_filter ( command_line [ 1 :]) self . parent . wMain . values = self . parent . value . get () self . parent . wMain . display () class FmSearchActive ( npyscreen . FormMuttActiveTraditional ): ACTION_CONTROLLER = ActionControllerSearch npyscreen.NPSAppManaged is the preferred superclass to support object-oriented implementation. class MyApplication ( npyscreen . NPSAppManaged ): pass Calling run() method of application object as main entry point. run() activates the default form, which should be given an id of MAIN if __name__ == '__main__' : TestApp = MyApplication () . run () print \"All objects, baby.\" Using a try / except block to allow for well-mannered exit in case of KeyboardInterrupt (Ctrl+C) GitHub try : App () . run () except KeyboardInterrupt : sys . exit ( 0 ) There are three methods for registering a Form object with a NPSAppManaged instance; - addForm() creates a new form and returns a weakref.proxy to it - addFormClass() register a class of Form rather than an instance - registerForm() It continually displays the Form named by its NEXT_ACTIVE_FORM attribute. Use the afterEditing method to allow exiting. class myEmployeeForm ( npyscreen . Form ): def afterEditing ( self ): self . parentApp . setNextForm ( None )","title":"Npyscreen"},{"location":"Python/Modules/Npyscreen/#npyscreen","text":"Widget library and application framework built on top of ncurses . Documentation ] Three main types of object compose npyscreen applications: - Application objects manage forms and other classes - Form objects form the canvas upon which widgets are arrayed - Form general-purpose - FormMutt - Widget objects are individual controls - TitleText text entry - TitleSelectOne equivalent to radio buttons - TitleDateCombo allows picking of date on a small calendar npyscreen.wrapper_basic is the main entry point import npyscreen def myFunction ( * args ): pass if __name__ == '__main__' : npyscreen . wrapper_basic ( myFunction ) print \"Blink and you missed it!\" npyscreen.Form is equivalent to the Tk() object, which is typically instantiated as win in GUI frameworks. F = npyscreen . Form ( name = 'My Test Application' ) Several important methods are key: - create() The standard constructor calls this method, which does nothing by default and is meant to be overriden in subclasses. Widgets are defined here. npyscreen.FormMutt imitates a UI layout popularized by applications like mutt , irssi , and vim , with a title bar at the top, a command line at the bottom, and a status line directly above the command line. ACTION_CONTROLLER can be defined in the FormMutt subclass as the name of a subclass of ActionControllerSimple . Commands for the application can be defined as callbacks in the create() method. self . add_action ( ident , call_back , True ) Callbacks are called with the following arguments: call_back ( command_line , control_widget_proxy , live = True ) class ActionControllerSearch ( npyscreen . ActionControllerSimple ): def create ( self ): self . add_action ( '^/.*' , self . set_search , True ) def set_search ( self , command_line , widget_proxy , live ): self . parent . value . set_filter ( command_line [ 1 :]) self . parent . wMain . values = self . parent . value . get () self . parent . wMain . display () class FmSearchActive ( npyscreen . FormMuttActiveTraditional ): ACTION_CONTROLLER = ActionControllerSearch npyscreen.NPSAppManaged is the preferred superclass to support object-oriented implementation. class MyApplication ( npyscreen . NPSAppManaged ): pass Calling run() method of application object as main entry point. run() activates the default form, which should be given an id of MAIN if __name__ == '__main__' : TestApp = MyApplication () . run () print \"All objects, baby.\" Using a try / except block to allow for well-mannered exit in case of KeyboardInterrupt (Ctrl+C) GitHub try : App () . run () except KeyboardInterrupt : sys . exit ( 0 ) There are three methods for registering a Form object with a NPSAppManaged instance; - addForm() creates a new form and returns a weakref.proxy to it - addFormClass() register a class of Form rather than an instance - registerForm() It continually displays the Form named by its NEXT_ACTIVE_FORM attribute. Use the afterEditing method to allow exiting. class myEmployeeForm ( npyscreen . Form ): def afterEditing ( self ): self . parentApp . setNextForm ( None )","title":"Npyscreen"},{"location":"Python/Modules/Optparse/","text":"optparse Instantiate the parser object parser = optparse . OptionParser ( usage = __doc__ . strip ()) # add an option parser . add_option ( '--timeout' )","title":"optparse"},{"location":"Python/Modules/Optparse/#optparse","text":"Instantiate the parser object parser = optparse . OptionParser ( usage = __doc__ . strip ()) # add an option parser . add_option ( '--timeout' )","title":"optparse"},{"location":"Python/Modules/Os/","text":"os Execute shell command given by string. The value returned is actually the exit code, not the output of the command to STDOUT. os . system ( 'ls -la' ) Store output in a variable os . popen ( 'ls -la' ) . read () Navigate filesystem os . getcwd () os . chdir ( path ) Test for existence of a file os . path . isfile ( file )","title":"os"},{"location":"Python/Modules/Os/#os","text":"Execute shell command given by string. The value returned is actually the exit code, not the output of the command to STDOUT. os . system ( 'ls -la' ) Store output in a variable os . popen ( 'ls -la' ) . read () Navigate filesystem os . getcwd () os . chdir ( path ) Test for existence of a file os . path . isfile ( file )","title":"os"},{"location":"Python/Modules/Pygobject/","text":"Pygobject Tasks Development environment Red Hat Ubuntu dnf install python3-venv python3-wheel dnf install gcc zlib-devel bzip2 bzip2-devel readline-devel sqlite sqlite-devel openssl-devel tk-devel git python3-cairo-devel cairo-gobject-devel gobject-introspection-devel pip install pygobject apt install python3-gi python3-gi-cairo gir1.2-gtk-3.0 libgirepository1.0-dev pip install pygobject Boilerplate Interface <?xml version=\"1.0\" encoding=\"UTF-8\"?> <interface> <requires lib= \"gtk+\" version= \"3.40\" /> <object class= \"GtkApplicationWindow\" id= \"window\" > <property name= \"title\" > My GTK App </property> <property name= \"default-width\" > 300 </property> <property name= \"default-height\" > 300 </property> </object> </interface> import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk # (1) class ApplicationWindow ( Gtk . ApplicationWindow ): # (2) def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) # (3) self . set_size_request ( 300 , 300 ) self . set_title ( \"My GTK App\" ) self . show_all () self . present () class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.learning-gtk' ) # (4) def do_activate ( self ): self . window = ApplicationWindow ( application = self ) if __name__ == '__main__' : app = Application () # (5) app . run () # (6) Note that the gi module's require_version() function must be called before importing Gtk. The recommended way of using the PyGTK API is to subclass and modify the Application and ApplicationWindow classes. These were introduced in GTK+ versions 3.0 and 3.4 respectively and are meant to be used as base classes. PyGTK also offers an alternative Gtk.Window class, which like ApplicationWindow is a subclass of Gtk.Container, and which still appears in many tutorials. The ApplicationWindow subclass calls the superclass's constructor. The UI is composed by adding widgets to this subclass by calling self.add() . Typically a single Box container is added to the top-level container and controls are added to that container. The Application subclass also calls its superclass's constructor and exposes a do_activate() method that instantiates the ApplicationWindow subclass and assigns that object to self.window before calling self.window.present() . The Application subclass essentially acts as a wrapper around ApplicationWindow. At the script's entrypoint, the Application subclass itself is instantiated and its run method is called. In online tutorials that use Window , typically the Application wrapper class does not appear. The Gtk.main() method must be called somewhere in the script in order for the UI to appear. Dice roller import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk import random from math import floor class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) scale_adj = Gtk . Adjustment . new ( 1 , 0 , 6 , 1 , 2 , 0 ) self . scale = Gtk . Scale . new ( Gtk . Orientation . HORIZONTAL , scale_adj ) self . scale . set_digits ( 0 ) button = Gtk . Button . new_with_label ( \"Throw\" ) button . connect ( \"clicked\" , self . on_button_clicked ) self . label = Gtk . Label . new () box = Gtk . Box . new ( Gtk . Orientation . VERTICAL , 5 ) box . pack_start ( self . scale , False , True , 0 ) box . pack_start ( button , False , True , 0 ) box . pack_start ( self . label , False , True , 0 ) self . add ( box ) self . set_size_request ( 200 , 200 ) def on_button_clicked ( self , button ): dice = floor ( self . scale . get_value ()) results = [ random . randrange ( 6 ) for i in range ( dice )] self . label . set_text ( str ( results )) class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.myapp' ) def do_activate ( self ): self . window = ApplicationWindow ( application = self ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = Application () app . run () MenuBar import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk class AppWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_size_request ( 250 , - 1 ) menubar = Gtk . MenuBar . new () self . add ( menubar ) file = Gtk . MenuItem . new_with_label ( \"File\" ) menubar . append ( file ) filemenu = Gtk . Menu . new () file . set_submenu ( filemenu ) new = Gtk . MenuItem . new_with_label ( \"New\" ) open = Gtk . MenuItem . new_with_label ( \"Open\" ) filemenu . append ( new ) filemenu . append ( open ) edit = Gtk . MenuItem . new_with_label ( \"Edit\" ) menubar . append ( edit ) editmenu = Gtk . Menu . new () edit . set_submenu ( editmenu ) cut = Gtk . MenuItem . new_with_label ( \"Cut\" ) copy = Gtk . MenuItem . new_with_label ( \"Copy\" ) paste = Gtk . MenuItem . new_with_label ( \"Paste\" ) editmenu . append ( cut ) editmenu . append ( copy ) editmenu . append ( paste ) help = Gtk . MenuItem . new_with_label ( \"Help\" ) menubar . append ( help ) helpmenu = Gtk . Menu . new () help . set_submenu ( helpmenu ) contents = Gtk . MenuItem . new_with_label ( \"Help\" ) about = Gtk . MenuItem . new_with_label ( \"About\" ) helpmenu . append ( contents ) helpmenu . append ( about ) class Application ( Gtk . Application ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , application_id = \"org.example.myapp\" , ** kwargs ) self . window = None def do_activate ( self ): if not self . window : self . window = AppWindow ( application = self , title = \"Menu Bars\" ) self . window . show_all () self . window . present () if __name__ == \"__main__\" : app = Application () app . run () Login import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_size_request ( 300 , 300 ) grid = Gtk . Grid . new () image = Gtk . Image . new_from_icon_name ( \"dialog-password\" , Gtk . IconSize . DIALOG ) grid . attach ( image , 0 , 0 , 1 , 1 ) grid . attach ( Gtk . Label ( label = \"Enter your credentials.\" ), 0 , 1 , 2 , 1 ) grid . attach ( Gtk . Label ( label = \"User name:\" ), 0 , 2 , 1 , 1 ) grid . attach ( Gtk . Entry (), 1 , 2 , 1 , 1 ) grid . attach ( Gtk . Label ( label = \"Password:\" ), 0 , 3 , 1 , 1 ) grid . attach ( Gtk . Entry ( visibility = False ), 1 , 3 , 1 , 1 ) class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = \"org.example.myapp\" ) def do_activate ( self ): self . window = ApplicationWindow ( application = self , title = \"Hello, World!\" ) self . window . show_all () self . window . present () if __name__ == \"__main__\" : app = Application () app . run () Hello, World! Window title Label Button reveal Interactive HeaderBar import sys import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , name , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_default_size ( 300 , 300 ) self . set_title ( f \"Hello, { name } !\" ) self . show_all () class Application ( Gtk . Application ): def __init__ ( self , name ): super () . __init__ ( application_id = \"com.example.learning-gtk\" ) self . name = name def do_activate ( self ): self . window = ApplicationWindow ( application = self , name = self . name ) if __name__ == '__main__' : if len ( sys . argv ) > 1 : app = Application ( sys . argv [ - 1 ]) else : app = Application ( \"World\" ) app . run () import sys import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , name , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_size_request ( 300 , 300 ) self . add ( Gtk . Label ( label = f \"Hello, { name } !\" )) class Application ( Gtk . Application ): def __init__ ( self , name = \"World\" , * args , ** kwargs ): self . name = name super () . __init__ ( * args , application_id = \"org.example.myapp\" , ** kwargs ) def do_activate ( self ): self . window = ApplicationWindow ( application = self , name = self . name , title = \"Hello, World!\" ) self . window . show_all () self . window . present () if __name__ == '__main__' : if len ( sys . argv ) > 1 : app = Application ( sys . argv [ - 1 ]) else : app = Application ( \"World\" ) app . run () Interface <?xml version=\"1.0\" encoding=\"UTF-8\"?> <interface> <requires lib= \"gtk+\" version= \"3.40\" > <object class= \"GtkApplicationWindow\" id= \"window\" > <property name= \"title\" > My GTK App </property> <property name= \"default-width\" > 300 </property> <property name= \"default-height\" > 300 </property> <child> <object class= \"GtkButton\" id= \"button\" > <property name= \"label\" > Press me! </property> <property name= \"margin-top\" > 12 </property> <property name= \"margin-bottom\" > 12 </property> <property name= \"margin-start\" > 12 </property> <property name= \"margin-end\" > 12 </property> </object> </child> </object> </interface> import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.myapp' ) def do_activate ( self ): builder = Gtk . Builder . new_from_file ( 'hw-button.ui' ) self . window = builder . get_object ( 'window' ) self . button = builder . get_object ( 'button' ) self . button . connect ( 'clicked' , self . on_button_clicked ) self . window . connect ( 'destroy' , Gtk . main_quit ) self . window . show_all () self . window . present () def on_button_clicked ( self , button ): self . button . set_label ( 'Hello, World!' ) def run ( self ): super () . run () Gtk . main () if __name__ == '__main__' : app = Application () app . run () import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) box = Gtk . Box ( orientation = Gtk . Orientation . VERTICAL , spacing = 10 ) self . add ( box ) question = Gtk . Label . new ( \"What is your name?\" ) box . add ( question ) self . entry = Gtk . Entry ( text = \"World\" ) box . add ( self . entry ) button = Gtk . Button . new_with_mnemonic ( \"Greet\" ) button . connect ( \"clicked\" , self . on_button_clicked , self ) box . add ( button ) def on_button_clicked ( self , button , parent ): dialog = Gtk . MessageDialog ( message_type = Gtk . MessageType . INFO , text = f \"Hello, { parent . entry . get_text () } \" , parent = parent , ) dialog . add_button ( \"OK\" , Gtk . ResponseType . OK ) dialog . run () dialog . destroy () class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.myapp' ) def do_activate ( self ): self . window = ApplicationWindow ( application = self ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = Application () app . run () import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk import sys class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_default_size ( - 1 , - 1 ) # headerbar = Gtk.HeaderBar(title=f\"Hello, {name}!\", subtitle=\"HeaderBar example\", show_close_button=True) headerbar = Gtk . HeaderBar () headerbar . set_title ( f \"Hello, World!\" ) headerbar . set_subtitle ( \"HeaderBar example\" ) headerbar . set_show_close_button ( True ) self . set_titlebar ( headerbar ) button = Gtk . Button ( label = \"Greet\" ) button . connect ( \"clicked\" , self . on_button_clicked , self ) headerbar . add ( button ) self . entry = Gtk . Entry ( text = \"World\" , name = \"entry\" ) headerbar . add ( self . entry ) def on_button_clicked ( self , button , parent ): dialog = Gtk . MessageDialog ( message_type = Gtk . MessageType . INFO , text = f \"Hello, { parent . entry . get_text () } !\" , parent = parent , ) dialog . add_button ( \"O_K\" , Gtk . ResponseType . OK ) dialog . run () dialog . destroy () class HeaderBar ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = \"org.example.headerbar\" ) def do_activate ( self ): self . window = ApplicationWindow ( application = self ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = HeaderBar () app . run () Starships Hardcoded data CSV import Sortable columns Event handler MessageDialog import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) ships = [ 'USS Enterprise' , 'USS Defiant' , 'USS Voyager' ] self . treeview = Gtk . TreeView ( model = Gtk . ListStore . new (( str ,))) column = Gtk . TreeViewColumn ( \"Ship\" , Gtk . CellRendererText (), text = 0 ) self . treeview . append_column ( column ) for s in ships : self . treeview . get_model () . append (( s ,)) self . add ( self . treeview ) self . set_size_request ( - 1 , - 1 ) class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.myapp' ) def do_activate ( self ): self . window = ApplicationWindow ( application = self ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = Application () app . run () import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk import csv class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) with open ( '/home/jasper/dogfood/csv/starships.csv' , mode = 'r' ) as f : reader = csv . reader ( f ) self . headers = [ h . title () for h in next ( reader )] self . data = [ r for r in reader ] self . treeview = Gtk . TreeView ( model = Gtk . ListStore . new (( str , str , str , str ))) for h in self . headers : column = Gtk . TreeViewColumn ( h , Gtk . CellRendererText (), text = self . headers . index ( h )) self . treeview . append_column ( column ) for r in self . data : self . treeview . get_model () . append ( r ) self . treeview . connect ( 'row-activated' , self . on_row_activated ) self . add ( self . treeview ) self . set_size_request ( - 1 , - 1 ) class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.myapp' ) def do_activate ( self ): self . window = ApplicationWindow ( application = self ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = Application () app . run () import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk import csv class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) with open ( '/home/jasper/dogfood/csv/starships.csv' , mode = 'r' ) as f : reader = csv . reader ( f ) self . headers = [ h . title () for h in next ( reader )] self . data = [ r for r in reader ] self . treeview = Gtk . TreeView ( model = Gtk . ListStore . new (( str , str , str , str ))) for h in self . headers : column = Gtk . TreeViewColumn ( h , Gtk . CellRendererText (), text = self . headers . index ( h )) column . set_sort_column_id ( self . headers . index ( h )) self . treeview . append_column ( column ) for r in self . data : self . treeview . get_model () . append ( r ) self . treeview . connect ( 'row-activated' , self . on_row_activated ) self . add ( self . treeview ) self . set_size_request ( - 1 , - 1 ) class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.myapp' ) def do_activate ( self ): self . window = ApplicationWindow ( application = self ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = Application () app . run () import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk import csv class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) with open ( '/home/jasper/dogfood/csv/starships.csv' , mode = 'r' ) as f : reader = csv . reader ( f ) self . headers = [ h . title () for h in next ( reader )] self . data = [ r for r in reader ] self . treeview = Gtk . TreeView ( model = Gtk . ListStore . new (( str , str , str , str ))) for h in self . headers : column = Gtk . TreeViewColumn ( h , Gtk . CellRendererText (), text = self . headers . index ( h )) column . set_sort_column_id ( self . headers . index ( h )) self . treeview . append_column ( column ) for r in self . data : self . treeview . get_model () . append ( r ) self . treeview . connect ( 'row-activated' , self . on_row_activated ) self . add ( self . treeview ) self . set_size_request ( - 1 , - 1 ) def on_row_activated ( self , treeview , path , col ): model = treeview . get_model () print ( f 'Using path object as index to model: { model [ path ][:] } ' ) class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.myapp' ) def do_activate ( self ): self . window = ApplicationWindow ( application = self ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = Application () app . run () import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk import csv class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) with open ( '/home/jasper/dogfood/csv/starships.csv' , mode = 'r' ) as f : reader = csv . reader ( f ) self . headers = [ h . title () for h in next ( reader )] self . data = [ r for r in reader ] self . treeview = Gtk . TreeView ( model = Gtk . ListStore . new (( str , str , str , str ))) for h in self . headers : column = Gtk . TreeViewColumn ( h , Gtk . CellRendererText (), text = self . headers . index ( h )) column . set_sort_column_id ( self . headers . index ( h )) self . treeview . append_column ( column ) for r in self . data : self . treeview . get_model () . append ( r ) self . treeview . connect ( 'row-activated' , self . on_row_activated ) self . add ( self . treeview ) self . set_size_request ( - 1 , - 1 ) def on_row_activated ( self , treeview , path , col ): model = treeview . get_model () dialog = Gtk . MessageDialog ( message_type = Gtk . MessageType . INFO , text = model [ path ][:], parent = self ) dialog . add_button ( \"OK\" , Gtk . ResponseType . OK ) dialog . run () dialog . destroy () class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.myapp' ) def do_activate ( self ): self . window = ApplicationWindow ( application = self ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = Application () app . run ()","title":"Pygobject"},{"location":"Python/Modules/Pygobject/#pygobject","text":"","title":"Pygobject"},{"location":"Python/Modules/Pygobject/#tasks","text":"","title":"Tasks"},{"location":"Python/Modules/Pygobject/#development-environment","text":"Red Hat Ubuntu dnf install python3-venv python3-wheel dnf install gcc zlib-devel bzip2 bzip2-devel readline-devel sqlite sqlite-devel openssl-devel tk-devel git python3-cairo-devel cairo-gobject-devel gobject-introspection-devel pip install pygobject apt install python3-gi python3-gi-cairo gir1.2-gtk-3.0 libgirepository1.0-dev pip install pygobject","title":"Development environment"},{"location":"Python/Modules/Pygobject/#boilerplate","text":"Interface <?xml version=\"1.0\" encoding=\"UTF-8\"?> <interface> <requires lib= \"gtk+\" version= \"3.40\" /> <object class= \"GtkApplicationWindow\" id= \"window\" > <property name= \"title\" > My GTK App </property> <property name= \"default-width\" > 300 </property> <property name= \"default-height\" > 300 </property> </object> </interface> import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk # (1) class ApplicationWindow ( Gtk . ApplicationWindow ): # (2) def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) # (3) self . set_size_request ( 300 , 300 ) self . set_title ( \"My GTK App\" ) self . show_all () self . present () class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.learning-gtk' ) # (4) def do_activate ( self ): self . window = ApplicationWindow ( application = self ) if __name__ == '__main__' : app = Application () # (5) app . run () # (6) Note that the gi module's require_version() function must be called before importing Gtk. The recommended way of using the PyGTK API is to subclass and modify the Application and ApplicationWindow classes. These were introduced in GTK+ versions 3.0 and 3.4 respectively and are meant to be used as base classes. PyGTK also offers an alternative Gtk.Window class, which like ApplicationWindow is a subclass of Gtk.Container, and which still appears in many tutorials. The ApplicationWindow subclass calls the superclass's constructor. The UI is composed by adding widgets to this subclass by calling self.add() . Typically a single Box container is added to the top-level container and controls are added to that container. The Application subclass also calls its superclass's constructor and exposes a do_activate() method that instantiates the ApplicationWindow subclass and assigns that object to self.window before calling self.window.present() . The Application subclass essentially acts as a wrapper around ApplicationWindow. At the script's entrypoint, the Application subclass itself is instantiated and its run method is called. In online tutorials that use Window , typically the Application wrapper class does not appear. The Gtk.main() method must be called somewhere in the script in order for the UI to appear.","title":"Boilerplate"},{"location":"Python/Modules/Pygobject/#dice-roller","text":"import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk import random from math import floor class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) scale_adj = Gtk . Adjustment . new ( 1 , 0 , 6 , 1 , 2 , 0 ) self . scale = Gtk . Scale . new ( Gtk . Orientation . HORIZONTAL , scale_adj ) self . scale . set_digits ( 0 ) button = Gtk . Button . new_with_label ( \"Throw\" ) button . connect ( \"clicked\" , self . on_button_clicked ) self . label = Gtk . Label . new () box = Gtk . Box . new ( Gtk . Orientation . VERTICAL , 5 ) box . pack_start ( self . scale , False , True , 0 ) box . pack_start ( button , False , True , 0 ) box . pack_start ( self . label , False , True , 0 ) self . add ( box ) self . set_size_request ( 200 , 200 ) def on_button_clicked ( self , button ): dice = floor ( self . scale . get_value ()) results = [ random . randrange ( 6 ) for i in range ( dice )] self . label . set_text ( str ( results )) class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.myapp' ) def do_activate ( self ): self . window = ApplicationWindow ( application = self ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = Application () app . run ()","title":"Dice roller"},{"location":"Python/Modules/Pygobject/#menubar","text":"import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk class AppWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_size_request ( 250 , - 1 ) menubar = Gtk . MenuBar . new () self . add ( menubar ) file = Gtk . MenuItem . new_with_label ( \"File\" ) menubar . append ( file ) filemenu = Gtk . Menu . new () file . set_submenu ( filemenu ) new = Gtk . MenuItem . new_with_label ( \"New\" ) open = Gtk . MenuItem . new_with_label ( \"Open\" ) filemenu . append ( new ) filemenu . append ( open ) edit = Gtk . MenuItem . new_with_label ( \"Edit\" ) menubar . append ( edit ) editmenu = Gtk . Menu . new () edit . set_submenu ( editmenu ) cut = Gtk . MenuItem . new_with_label ( \"Cut\" ) copy = Gtk . MenuItem . new_with_label ( \"Copy\" ) paste = Gtk . MenuItem . new_with_label ( \"Paste\" ) editmenu . append ( cut ) editmenu . append ( copy ) editmenu . append ( paste ) help = Gtk . MenuItem . new_with_label ( \"Help\" ) menubar . append ( help ) helpmenu = Gtk . Menu . new () help . set_submenu ( helpmenu ) contents = Gtk . MenuItem . new_with_label ( \"Help\" ) about = Gtk . MenuItem . new_with_label ( \"About\" ) helpmenu . append ( contents ) helpmenu . append ( about ) class Application ( Gtk . Application ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , application_id = \"org.example.myapp\" , ** kwargs ) self . window = None def do_activate ( self ): if not self . window : self . window = AppWindow ( application = self , title = \"Menu Bars\" ) self . window . show_all () self . window . present () if __name__ == \"__main__\" : app = Application () app . run ()","title":"MenuBar"},{"location":"Python/Modules/Pygobject/#login","text":"import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_size_request ( 300 , 300 ) grid = Gtk . Grid . new () image = Gtk . Image . new_from_icon_name ( \"dialog-password\" , Gtk . IconSize . DIALOG ) grid . attach ( image , 0 , 0 , 1 , 1 ) grid . attach ( Gtk . Label ( label = \"Enter your credentials.\" ), 0 , 1 , 2 , 1 ) grid . attach ( Gtk . Label ( label = \"User name:\" ), 0 , 2 , 1 , 1 ) grid . attach ( Gtk . Entry (), 1 , 2 , 1 , 1 ) grid . attach ( Gtk . Label ( label = \"Password:\" ), 0 , 3 , 1 , 1 ) grid . attach ( Gtk . Entry ( visibility = False ), 1 , 3 , 1 , 1 ) class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = \"org.example.myapp\" ) def do_activate ( self ): self . window = ApplicationWindow ( application = self , title = \"Hello, World!\" ) self . window . show_all () self . window . present () if __name__ == \"__main__\" : app = Application () app . run ()","title":"Login"},{"location":"Python/Modules/Pygobject/#hello-world","text":"Window title Label Button reveal Interactive HeaderBar import sys import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , name , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_default_size ( 300 , 300 ) self . set_title ( f \"Hello, { name } !\" ) self . show_all () class Application ( Gtk . Application ): def __init__ ( self , name ): super () . __init__ ( application_id = \"com.example.learning-gtk\" ) self . name = name def do_activate ( self ): self . window = ApplicationWindow ( application = self , name = self . name ) if __name__ == '__main__' : if len ( sys . argv ) > 1 : app = Application ( sys . argv [ - 1 ]) else : app = Application ( \"World\" ) app . run () import sys import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , name , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_size_request ( 300 , 300 ) self . add ( Gtk . Label ( label = f \"Hello, { name } !\" )) class Application ( Gtk . Application ): def __init__ ( self , name = \"World\" , * args , ** kwargs ): self . name = name super () . __init__ ( * args , application_id = \"org.example.myapp\" , ** kwargs ) def do_activate ( self ): self . window = ApplicationWindow ( application = self , name = self . name , title = \"Hello, World!\" ) self . window . show_all () self . window . present () if __name__ == '__main__' : if len ( sys . argv ) > 1 : app = Application ( sys . argv [ - 1 ]) else : app = Application ( \"World\" ) app . run () Interface <?xml version=\"1.0\" encoding=\"UTF-8\"?> <interface> <requires lib= \"gtk+\" version= \"3.40\" > <object class= \"GtkApplicationWindow\" id= \"window\" > <property name= \"title\" > My GTK App </property> <property name= \"default-width\" > 300 </property> <property name= \"default-height\" > 300 </property> <child> <object class= \"GtkButton\" id= \"button\" > <property name= \"label\" > Press me! </property> <property name= \"margin-top\" > 12 </property> <property name= \"margin-bottom\" > 12 </property> <property name= \"margin-start\" > 12 </property> <property name= \"margin-end\" > 12 </property> </object> </child> </object> </interface> import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.myapp' ) def do_activate ( self ): builder = Gtk . Builder . new_from_file ( 'hw-button.ui' ) self . window = builder . get_object ( 'window' ) self . button = builder . get_object ( 'button' ) self . button . connect ( 'clicked' , self . on_button_clicked ) self . window . connect ( 'destroy' , Gtk . main_quit ) self . window . show_all () self . window . present () def on_button_clicked ( self , button ): self . button . set_label ( 'Hello, World!' ) def run ( self ): super () . run () Gtk . main () if __name__ == '__main__' : app = Application () app . run () import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) box = Gtk . Box ( orientation = Gtk . Orientation . VERTICAL , spacing = 10 ) self . add ( box ) question = Gtk . Label . new ( \"What is your name?\" ) box . add ( question ) self . entry = Gtk . Entry ( text = \"World\" ) box . add ( self . entry ) button = Gtk . Button . new_with_mnemonic ( \"Greet\" ) button . connect ( \"clicked\" , self . on_button_clicked , self ) box . add ( button ) def on_button_clicked ( self , button , parent ): dialog = Gtk . MessageDialog ( message_type = Gtk . MessageType . INFO , text = f \"Hello, { parent . entry . get_text () } \" , parent = parent , ) dialog . add_button ( \"OK\" , Gtk . ResponseType . OK ) dialog . run () dialog . destroy () class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.myapp' ) def do_activate ( self ): self . window = ApplicationWindow ( application = self ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = Application () app . run () import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk import sys class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_default_size ( - 1 , - 1 ) # headerbar = Gtk.HeaderBar(title=f\"Hello, {name}!\", subtitle=\"HeaderBar example\", show_close_button=True) headerbar = Gtk . HeaderBar () headerbar . set_title ( f \"Hello, World!\" ) headerbar . set_subtitle ( \"HeaderBar example\" ) headerbar . set_show_close_button ( True ) self . set_titlebar ( headerbar ) button = Gtk . Button ( label = \"Greet\" ) button . connect ( \"clicked\" , self . on_button_clicked , self ) headerbar . add ( button ) self . entry = Gtk . Entry ( text = \"World\" , name = \"entry\" ) headerbar . add ( self . entry ) def on_button_clicked ( self , button , parent ): dialog = Gtk . MessageDialog ( message_type = Gtk . MessageType . INFO , text = f \"Hello, { parent . entry . get_text () } !\" , parent = parent , ) dialog . add_button ( \"O_K\" , Gtk . ResponseType . OK ) dialog . run () dialog . destroy () class HeaderBar ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = \"org.example.headerbar\" ) def do_activate ( self ): self . window = ApplicationWindow ( application = self ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = HeaderBar () app . run ()","title":"Hello, World!"},{"location":"Python/Modules/Pygobject/#starships","text":"Hardcoded data CSV import Sortable columns Event handler MessageDialog import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) ships = [ 'USS Enterprise' , 'USS Defiant' , 'USS Voyager' ] self . treeview = Gtk . TreeView ( model = Gtk . ListStore . new (( str ,))) column = Gtk . TreeViewColumn ( \"Ship\" , Gtk . CellRendererText (), text = 0 ) self . treeview . append_column ( column ) for s in ships : self . treeview . get_model () . append (( s ,)) self . add ( self . treeview ) self . set_size_request ( - 1 , - 1 ) class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.myapp' ) def do_activate ( self ): self . window = ApplicationWindow ( application = self ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = Application () app . run () import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk import csv class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) with open ( '/home/jasper/dogfood/csv/starships.csv' , mode = 'r' ) as f : reader = csv . reader ( f ) self . headers = [ h . title () for h in next ( reader )] self . data = [ r for r in reader ] self . treeview = Gtk . TreeView ( model = Gtk . ListStore . new (( str , str , str , str ))) for h in self . headers : column = Gtk . TreeViewColumn ( h , Gtk . CellRendererText (), text = self . headers . index ( h )) self . treeview . append_column ( column ) for r in self . data : self . treeview . get_model () . append ( r ) self . treeview . connect ( 'row-activated' , self . on_row_activated ) self . add ( self . treeview ) self . set_size_request ( - 1 , - 1 ) class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.myapp' ) def do_activate ( self ): self . window = ApplicationWindow ( application = self ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = Application () app . run () import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk import csv class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) with open ( '/home/jasper/dogfood/csv/starships.csv' , mode = 'r' ) as f : reader = csv . reader ( f ) self . headers = [ h . title () for h in next ( reader )] self . data = [ r for r in reader ] self . treeview = Gtk . TreeView ( model = Gtk . ListStore . new (( str , str , str , str ))) for h in self . headers : column = Gtk . TreeViewColumn ( h , Gtk . CellRendererText (), text = self . headers . index ( h )) column . set_sort_column_id ( self . headers . index ( h )) self . treeview . append_column ( column ) for r in self . data : self . treeview . get_model () . append ( r ) self . treeview . connect ( 'row-activated' , self . on_row_activated ) self . add ( self . treeview ) self . set_size_request ( - 1 , - 1 ) class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.myapp' ) def do_activate ( self ): self . window = ApplicationWindow ( application = self ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = Application () app . run () import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk import csv class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) with open ( '/home/jasper/dogfood/csv/starships.csv' , mode = 'r' ) as f : reader = csv . reader ( f ) self . headers = [ h . title () for h in next ( reader )] self . data = [ r for r in reader ] self . treeview = Gtk . TreeView ( model = Gtk . ListStore . new (( str , str , str , str ))) for h in self . headers : column = Gtk . TreeViewColumn ( h , Gtk . CellRendererText (), text = self . headers . index ( h )) column . set_sort_column_id ( self . headers . index ( h )) self . treeview . append_column ( column ) for r in self . data : self . treeview . get_model () . append ( r ) self . treeview . connect ( 'row-activated' , self . on_row_activated ) self . add ( self . treeview ) self . set_size_request ( - 1 , - 1 ) def on_row_activated ( self , treeview , path , col ): model = treeview . get_model () print ( f 'Using path object as index to model: { model [ path ][:] } ' ) class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.myapp' ) def do_activate ( self ): self . window = ApplicationWindow ( application = self ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = Application () app . run () import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk import csv class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) with open ( '/home/jasper/dogfood/csv/starships.csv' , mode = 'r' ) as f : reader = csv . reader ( f ) self . headers = [ h . title () for h in next ( reader )] self . data = [ r for r in reader ] self . treeview = Gtk . TreeView ( model = Gtk . ListStore . new (( str , str , str , str ))) for h in self . headers : column = Gtk . TreeViewColumn ( h , Gtk . CellRendererText (), text = self . headers . index ( h )) column . set_sort_column_id ( self . headers . index ( h )) self . treeview . append_column ( column ) for r in self . data : self . treeview . get_model () . append ( r ) self . treeview . connect ( 'row-activated' , self . on_row_activated ) self . add ( self . treeview ) self . set_size_request ( - 1 , - 1 ) def on_row_activated ( self , treeview , path , col ): model = treeview . get_model () dialog = Gtk . MessageDialog ( message_type = Gtk . MessageType . INFO , text = model [ path ][:], parent = self ) dialog . add_button ( \"OK\" , Gtk . ResponseType . OK ) dialog . run () dialog . destroy () class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.myapp' ) def do_activate ( self ): self . window = ApplicationWindow ( application = self ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = Application () app . run ()","title":"Starships"},{"location":"Python/Modules/Pyinstaller/","text":"Pyinstaller Source: RealPython tutorial Installing PyInstaller , even in a virtual environment, will install the pyinstaller executable to $HOME/.local/bin. On Windows, it is installed to another directory within LOCALAPPDATA . pip install pyinstaller PyInstaller creates primarily 3 items: - .spec file, named after the CLI script - build/ folder, which can be ignored - dist/ folder, containing the final artifact at dist/cli/cli or dist/cli/cli.exe Several options are available hidden-import name onefile pyinstaller script.py --onefile On Windows, if PyInstaller is run from a virtual environment without necessary modules installed, they may not be available for compilation into the artifact. This does not appear to be an issue with Linux. This problem appears to be specific to certain modules, like emoji .","title":"Pyinstaller"},{"location":"Python/Modules/Pyinstaller/#pyinstaller","text":"Source: RealPython tutorial Installing PyInstaller , even in a virtual environment, will install the pyinstaller executable to $HOME/.local/bin. On Windows, it is installed to another directory within LOCALAPPDATA . pip install pyinstaller PyInstaller creates primarily 3 items: - .spec file, named after the CLI script - build/ folder, which can be ignored - dist/ folder, containing the final artifact at dist/cli/cli or dist/cli/cli.exe Several options are available hidden-import name onefile pyinstaller script.py --onefile On Windows, if PyInstaller is run from a virtual environment without necessary modules installed, they may not be available for compilation into the artifact. This does not appear to be an issue with Linux. This problem appears to be specific to certain modules, like emoji .","title":"Pyinstaller"},{"location":"Python/Modules/Pythonnet/","text":"pythonnet Docs: ? ! Developers recommend Mono version 5.20.1 Issues 939 On Ubuntu, the eoan universe repository has to be added deb https://archive.ubuntu.com/ubuntu/ eoan universe deb https://archive.ubuntu.com/ubuntu/ eoan-updates universe But I can't figure out how to add the older version, because the recommended syntax produces the error \"Unable to correct problems, you have held broken packages\" sudo apt install mono-devel = 5 .18.0.240+dfsg-3 Maybe try the tarballs on Mono's website... Or maybe there's another repo I don't know about.. apt install clang libglib2.0-dev python3-dev pip install pycparser pythonnet pip install -U setuptools","title":"pythonnet"},{"location":"Python/Modules/Pythonnet/#pythonnet","text":"Docs: ? ! Developers recommend Mono version 5.20.1 Issues 939 On Ubuntu, the eoan universe repository has to be added deb https://archive.ubuntu.com/ubuntu/ eoan universe deb https://archive.ubuntu.com/ubuntu/ eoan-updates universe But I can't figure out how to add the older version, because the recommended syntax produces the error \"Unable to correct problems, you have held broken packages\" sudo apt install mono-devel = 5 .18.0.240+dfsg-3 Maybe try the tarballs on Mono's website... Or maybe there's another repo I don't know about.. apt install clang libglib2.0-dev python3-dev pip install pycparser pythonnet pip install -U setuptools","title":"pythonnet"},{"location":"Python/Modules/Random/","text":"random Random choice with replacement random . choice ( iterable ) Shuffle elements of an iterable in-place [FP:42] random . shuffle ( iterable )","title":"random"},{"location":"Python/Modules/Random/#random","text":"Random choice with replacement random . choice ( iterable ) Shuffle elements of an iterable in-place [FP:42] random . shuffle ( iterable )","title":"random"},{"location":"Python/Modules/Scrapy/","text":"scrapy Best used to obtain one \"stream\" of data at a time, without trying to obtain data from different pages scrapy runspider spider.py -o file.json Display HTML source of the scraped page print ( response . txt ) Get {URL} fetch ( 'url' ) Select a CSS selector # Returns a `SelectorList` response . css ( 'p' ) # Retrieve full HTML elements response . css ( 'p' ) . extract () Retrieve only the text within the element response . css ( 'p::text' ) . extract () response . css ( 'p::text' ) . extract_first () response . css ( 'p::text' ) . extract ()[ 0 ] Get the href attribute value for an anchor tag response . css ( 'a' ) . attrib [ 'href' ] Launch Scrapy shell and scrape $URL scrapy shell $URL Make a default spider named {quotes} that will be restricted to {domain} scrapy genspider quotes domain scrapy runspider scrapy1 . py Run a spider, saving scraped data to a JSON file scrapy runspider spider . py - o items . json Method which contains most of the logic of the spider, especially after the yield keyword. For multiple items, a structural basis for iteration must be found and for each iteration, data is yielded Extract URL from link using standard CSS selection techniques Add the domain name to a relative link response . urljoin () Recursively call the parse method again on the next page yield scrapy . Request ( url = next_page_url , callback = self . parse ) Scrape detail pages - parse_details would be a spider method sibling to the main parse method - if a detail page has more information than the main, then the yield keyword should be in parse_details yield scrapy . Request ( url = { url }, callback = self . parse_details )","title":"scrapy"},{"location":"Python/Modules/Scrapy/#scrapy","text":"Best used to obtain one \"stream\" of data at a time, without trying to obtain data from different pages scrapy runspider spider.py -o file.json Display HTML source of the scraped page print ( response . txt ) Get {URL} fetch ( 'url' ) Select a CSS selector # Returns a `SelectorList` response . css ( 'p' ) # Retrieve full HTML elements response . css ( 'p' ) . extract () Retrieve only the text within the element response . css ( 'p::text' ) . extract () response . css ( 'p::text' ) . extract_first () response . css ( 'p::text' ) . extract ()[ 0 ] Get the href attribute value for an anchor tag response . css ( 'a' ) . attrib [ 'href' ] Launch Scrapy shell and scrape $URL scrapy shell $URL Make a default spider named {quotes} that will be restricted to {domain} scrapy genspider quotes domain scrapy runspider scrapy1 . py Run a spider, saving scraped data to a JSON file scrapy runspider spider . py - o items . json Method which contains most of the logic of the spider, especially after the yield keyword. For multiple items, a structural basis for iteration must be found and for each iteration, data is yielded Extract URL from link using standard CSS selection techniques Add the domain name to a relative link response . urljoin () Recursively call the parse method again on the next page yield scrapy . Request ( url = next_page_url , callback = self . parse ) Scrape detail pages - parse_details would be a spider method sibling to the main parse method - if a detail page has more information than the main, then the yield keyword should be in parse_details yield scrapy . Request ( url = { url }, callback = self . parse_details )","title":"scrapy"},{"location":"Python/Modules/Setuptools/","text":"setuptools Setuptools is for uploading to PyPi. To create self-contained executable files, use pyinstaller . PROJECT \u251c\u2500\u2500 PROJECT # (1) \u2502 \u2514\u2500\u2500 init.py \u2514\u2500\u2500 setup.py # (2) 1 directory, 2 files Additional code files will be placed in here Containing a call to setuptools.setup() setup.py import setuptools setuptools . setup ( name = 'funniest' , version = '0.1' , description = 'The funniest joke in the world' , url = 'http://github.com/storborg/funniest' , author = 'Flying Circus' , author_email = 'flyingcircus@example.com' , license = 'MIT' , packages = [ 'funniest' ], zip_safe = False ) If the package has dependencies, they can be added by appending an install_requires keyword argument passing an array of the module names setup ( install_requires = [ 'markdown' , ], ) Reserve the name, upload package metadata, and create the pypi.python.org webpage python setup . py register Create a source distribution, producing a tarball inside the top-level directory python setup . py sdist Upload the source distribution python setup . py sdist upload Do all the above in a single step python setup . py register sdist upload","title":"setuptools"},{"location":"Python/Modules/Setuptools/#setuptools","text":"Setuptools is for uploading to PyPi. To create self-contained executable files, use pyinstaller . PROJECT \u251c\u2500\u2500 PROJECT # (1) \u2502 \u2514\u2500\u2500 init.py \u2514\u2500\u2500 setup.py # (2) 1 directory, 2 files Additional code files will be placed in here Containing a call to setuptools.setup() setup.py import setuptools setuptools . setup ( name = 'funniest' , version = '0.1' , description = 'The funniest joke in the world' , url = 'http://github.com/storborg/funniest' , author = 'Flying Circus' , author_email = 'flyingcircus@example.com' , license = 'MIT' , packages = [ 'funniest' ], zip_safe = False ) If the package has dependencies, they can be added by appending an install_requires keyword argument passing an array of the module names setup ( install_requires = [ 'markdown' , ], ) Reserve the name, upload package metadata, and create the pypi.python.org webpage python setup . py register Create a source distribution, producing a tarball inside the top-level directory python setup . py sdist Upload the source distribution python setup . py sdist upload Do all the above in a single step python setup . py register sdist upload","title":"setuptools"},{"location":"Python/Modules/Socket/","text":"socket The socket module is Python's standard interface for the transport layer. Sockets can be classified by family AF_INET Internet AF_UNIX for UNIX sockets and type : - SOCK_STREAM TCP - SOCK_DGRAM UDP These enum values are required upon initialization of a socket object: [Ortega][Ortega]: 25 client_socket = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) Sources: Sockets tutorial TCP server TCP Client UDP server UDP client import socket with socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) as s : s . bind (( HOST , PORT )) import socket with socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) as s : s . connect (( HOST , PORT )) import socket with socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) as s : s . bind (( HOST , PORT )) import socket msg = \"Hello, world!\" with socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) as s : s . sendto ( msg . encode (), ( HOST , PORT )) Define port on which to listen for connections. serversocket . bind (( 'localhost' , 80 )) Connect to a remote socket in one direction client_socket . connect (( 'www.packtpub.com' , 80 )) Convert a domain name into IPv4 address socket . gethostbyname ( 'packtpub.com' ) # '83.166.169.231' Defaults to localhost with no arguments s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) s . bind (( socket . gethostname (), 1234 )) Get protocol name from port number socket . getservbyport ( 80 ) # 'http' Listen to a maximum of 10 connections serversocket . listen ( 10 ) Receive bytestream from server msg = s . recv ( 1024 ) print ( msg . decode ( 'utf-8' ))","title":"socket"},{"location":"Python/Modules/Socket/#socket","text":"The socket module is Python's standard interface for the transport layer. Sockets can be classified by family AF_INET Internet AF_UNIX for UNIX sockets and type : - SOCK_STREAM TCP - SOCK_DGRAM UDP These enum values are required upon initialization of a socket object: [Ortega][Ortega]: 25 client_socket = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) Sources: Sockets tutorial TCP server TCP Client UDP server UDP client import socket with socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) as s : s . bind (( HOST , PORT )) import socket with socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) as s : s . connect (( HOST , PORT )) import socket with socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) as s : s . bind (( HOST , PORT )) import socket msg = \"Hello, world!\" with socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) as s : s . sendto ( msg . encode (), ( HOST , PORT )) Define port on which to listen for connections. serversocket . bind (( 'localhost' , 80 )) Connect to a remote socket in one direction client_socket . connect (( 'www.packtpub.com' , 80 )) Convert a domain name into IPv4 address socket . gethostbyname ( 'packtpub.com' ) # '83.166.169.231' Defaults to localhost with no arguments s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) s . bind (( socket . gethostname (), 1234 )) Get protocol name from port number socket . getservbyport ( 80 ) # 'http' Listen to a maximum of 10 connections serversocket . listen ( 10 ) Receive bytestream from server msg = s . recv ( 1024 ) print ( msg . decode ( 'utf-8' ))","title":"socket"},{"location":"Python/Modules/Sqlite3/","text":"sqlite3 Create a Connect connection object and employee.db (binary) if it doesn't exist conn = sqlite . connect ( 'employee.db' ) Create a Connect.Cursor object c = conn . cursor () Perform SQL commands with Connect.Cursor.execute() . Create tablename with fields field of type type ( null , integer , real , text , blob ); never use Python's native string operations (f-strings, etc) to form commands, because this method is vulnerable to SQL injection. YouTube c . execute ( '''CREATE TABLE {tablename} ( {field} {type} , {field} {type} ...)) Save changes conn . commit () Close connection conn . close ()","title":"sqlite3"},{"location":"Python/Modules/Sqlite3/#sqlite3","text":"Create a Connect connection object and employee.db (binary) if it doesn't exist conn = sqlite . connect ( 'employee.db' ) Create a Connect.Cursor object c = conn . cursor () Perform SQL commands with Connect.Cursor.execute() . Create tablename with fields field of type type ( null , integer , real , text , blob ); never use Python's native string operations (f-strings, etc) to form commands, because this method is vulnerable to SQL injection. YouTube c . execute ( '''CREATE TABLE {tablename} ( {field} {type} , {field} {type} ...)) Save changes conn . commit () Close connection conn . close ()","title":"sqlite3"},{"location":"Python/Modules/Subprocess/","text":"subprocess subprocess modules allows you to spawn new processes, interact with file descriptors, and obtain exit codes. The recommended approach is to use the run() function as default, which runs a CLI command with options as a list of strings and returns a CompletedProcess instance.\\ Execute shell command Unlike os.system , subprocess.run() takes a list of arguments. subprocess . run ([ 'ls' , '-l,' . '], 0) Set capture_output to True to save output, stored as property stdout of the returned object. data = subprocess . run ([ 'ls,' - l ',' . '], 0, capture_output=True) The data is stored as a bytestring , which can be decoded to a normal string. data . stdout . decode ( 'utf-8' ) This return a CompletedProcess instance with the command's output stored under the stdout property subprocess . run ([ 'ls' , '-l' , '/dev/null' ], capture_output = True ) This will raise a CalledProcessError exception because of the non-zero exit code subprocess . run ( 'exit 1' , shell = True , check = True )","title":"subprocess"},{"location":"Python/Modules/Subprocess/#subprocess","text":"subprocess modules allows you to spawn new processes, interact with file descriptors, and obtain exit codes. The recommended approach is to use the run() function as default, which runs a CLI command with options as a list of strings and returns a CompletedProcess instance.\\ Execute shell command Unlike os.system , subprocess.run() takes a list of arguments. subprocess . run ([ 'ls' , '-l,' . '], 0) Set capture_output to True to save output, stored as property stdout of the returned object. data = subprocess . run ([ 'ls,' - l ',' . '], 0, capture_output=True) The data is stored as a bytestring , which can be decoded to a normal string. data . stdout . decode ( 'utf-8' ) This return a CompletedProcess instance with the command's output stored under the stdout property subprocess . run ([ 'ls' , '-l' , '/dev/null' ], capture_output = True ) This will raise a CalledProcessError exception because of the non-zero exit code subprocess . run ( 'exit 1' , shell = True , check = True )","title":"subprocess"},{"location":"Python/Modules/Sys/","text":"sys Return site-specific directory where Python files are installed sys . prefix # (1) /usr/local/ by default","title":"sys"},{"location":"Python/Modules/Sys/#sys","text":"Return site-specific directory where Python files are installed sys . prefix # (1) /usr/local/ by default","title":"sys"},{"location":"Python/Modules/Tabulate/","text":"","title":"Tabulate"},{"location":"Python/Modules/Termcolor/","text":"termcolor Print text in a color code termcolor . cprint ( text , color )","title":"termcolor"},{"location":"Python/Modules/Termcolor/#termcolor","text":"Print text in a color code termcolor . cprint ( text , color )","title":"termcolor"},{"location":"Python/Modules/Threading/","text":"threading Docs counter = 0 lock = threading . RLock () def func1 (): global counter while True : with lock : counter += 1 counter -= 1 def func2 (): global counter while True : with lock : counter += 1 counter -= 1 threading . Thrad ( target = func1 ) . start () threading . Thrad ( target = func2 ) . start () counter = 0 def func1 (): global counter while True : counter += 1 counter -= 1 def func2 (): global counter while True : counter += 1 counter -= 1 threading . Thrad ( target = func1 ) . start () threading . Thrad ( target = func2 ) . start ()","title":"threading"},{"location":"Python/Modules/Threading/#threading","text":"Docs counter = 0 lock = threading . RLock () def func1 (): global counter while True : with lock : counter += 1 counter -= 1 def func2 (): global counter while True : with lock : counter += 1 counter -= 1 threading . Thrad ( target = func1 ) . start () threading . Thrad ( target = func2 ) . start () counter = 0 def func1 (): global counter while True : counter += 1 counter -= 1 def func2 (): global counter while True : counter += 1 counter -= 1 threading . Thrad ( target = func1 ) . start () threading . Thrad ( target = func2 ) . start ()","title":"threading"},{"location":"Python/Modules/Weakref/","text":"Weakref Weak references are references to objects which return exceptions when that object has been garbage collected Create a weak reference to {object} # A weak reference created using `ref` must be dereferenced r = weakref . ref ( obj ) r () . method () r . method () # will not work # A weak reference created using `proxy` does not need to be dereferenced: weakref . proxy ( obj )","title":"Weakref"},{"location":"Python/Modules/Weakref/#weakref","text":"Weak references are references to objects which return exceptions when that object has been garbage collected Create a weak reference to {object} # A weak reference created using `ref` must be dereferenced r = weakref . ref ( obj ) r () . method () r . method () # will not work # A weak reference created using `proxy` does not need to be dereferenced: weakref . proxy ( obj )","title":"Weakref"},{"location":"Python/Modules/Winrm/","text":"Winrm Winrm allows you to connect Linux and Windows hosts over WinRM. adamtheautomator.com Begin a WinRM session. If no errors are thrown, the session has been successfully established session = winrm . Session ( ipaddress , auth = ( username , password ) )","title":"Winrm"},{"location":"Python/Modules/Winrm/#winrm","text":"Winrm allows you to connect Linux and Windows hosts over WinRM. adamtheautomator.com Begin a WinRM session. If no errors are thrown, the session has been successfully established session = winrm . Session ( ipaddress , auth = ( username , password ) )","title":"Winrm"},{"location":"Python/Modules/Xml/","text":"xml books.xml <?xml version=\"1.0\"?> <catalog> <book id= \"bk101\" > <author> Gambardella, Matthew </author> <title> XML Developer's Guide </title> <genre> Computer </genre> <price> 44.95 </price> <publish_date> 2000-10-01 </publish_date> <description> An in-depth look at creating applications with XML. </description> </book> <book id= \"bk102\" > <author> Ralls, Kim </author> <title> Midnight Rain </title> <genre> Fantasy </genre> <price> 5.95 </price> <publish_date> 2000-12-16 </publish_date> <description> A former architect battles corporate zombies, an evil sorceress, and her own childhood to become queen of the world. </description> </book> <book id= \"bk103\" > <author> Corets, Eva </author> <title> Maeve Ascendant </title> <genre> Fantasy </genre> <price> 5.95 </price> <publish_date> 2000-11-17 </publish_date> <description> After the collapse of a nanotechnology society in England, the young survivors lay the foundation for a new society. </description> </book> <book id= \"bk104\" > <author> Corets, Eva </author> <title> Oberon's Legacy </title> <genre> Fantasy </genre> <price> 5.95 </price> <publish_date> 2001-03-10 </publish_date> <description> In post-apocalypse England, the mysterious agent known only as Oberon helps to create a new life for the inhabitants of London. Sequel to Maeve Ascendant. </description> </book> <book id= \"bk105\" > <author> Corets, Eva </author> <title> The Sundered Grail </title> <genre> Fantasy </genre> <price> 5.95 </price> <publish_date> 2001-09-10 </publish_date> <description> The two daughters of Maeve, half-sisters, battle one another for control of England. Sequel to Oberon's Legacy. </description> </book> <book id= \"bk106\" > <author> Randall, Cynthia </author> <title> Lover Birds </title> <genre> Romance </genre> <price> 4.95 </price> <publish_date> 2000-09-02 </publish_date> <description> When Carla meets Paul at an ornithology conference, tempers fly as feathers get ruffled. </description> </book> <book id= \"bk107\" > <author> Thurman, Paula </author> <title> Splish Splash </title> <genre> Romance </genre> <price> 4.95 </price> <publish_date> 2000-11-02 </publish_date> <description> A deep sea diver finds true love twenty thousand leagues beneath the sea. </description> </book> <book id= \"bk108\" > <author> Knorr, Stefan </author> <title> Creepy Crawlies </title> <genre> Horror </genre> <price> 4.95 </price> <publish_date> 2000-12-06 </publish_date> <description> An anthology of horror stories about roaches, centipedes, scorpions and other insects. </description> </book> <book id= \"bk109\" > <author> Kress, Peter </author> <title> Paradox Lost </title> <genre> Science Fiction </genre> <price> 6.95 </price> <publish_date> 2000-11-02 </publish_date> <description> After an inadvertant trip through a Heisenberg Uncertainty Device, James Salway discovers the problems of being quantum. </description> </book> <book id= \"bk110\" > <author> O'Brien, Tim </author> <title> Microsoft .NET: The Programming Bible </title> <genre> Computer </genre> <price> 36.95 </price> <publish_date> 2000-12-09 </publish_date> <description> Microsoft's .NET initiative is explored in detail in this deep programmer's reference. </description> </book> <book id= \"bk111\" > <author> O'Brien, Tim </author> <title> MSXML3: A Comprehensive Guide </title> <genre> Computer </genre> <price> 36.95 </price> <publish_date> 2000-12-01 </publish_date> <description> The Microsoft MSXML3 parser is covered in detail, with attention to XML DOM interfaces, XSLT processing, SAX and more. </description> </book> <book id= \"bk112\" > <author> Galos, Mike </author> <title> Visual Studio 7: A Comprehensive Guide </title> <genre> Computer </genre> <price> 49.95 </price> <publish_date> 2001-04-16 </publish_date> <description> Microsoft Visual Studio 7 is explored in depth, looking at how Visual Basic, Visual C++, C#, and ASP+ are integrated into a comprehensive development environment. </description> </book> </catalog> The etree submodule contains the ElementTree object which can open a string filename to deserialize XML data using parse() , which returns an ElementTree object, representing an XML document. A Python string can also be parsed with fromstring() , which actually returns an Element object. File String tree = xml . etree . ElementTree . parse ( 'books.xml' ) tree = xml . etree . ElementTree . fromstring ( books ) The getroot() method returns an Element object of the XML document's root node. root = tree . getroot () The parsed data can be displayed using the tostring() static method, providing an Element as argument. ElementTree . tostring ( root ) Children of an element can be filtered using findall() . This returns a list of Elements. books = root . findall ( 'book' ) Any Element object exposes an attrib property which returns a dictionary of attributes. [b.attrib for b in books] Attributes can be written to an Element using the set() method. root . set ( 'foo' , 'bar' ) Attributes can also be manipulated on the attrib property with normal Python dictionary operations. Setting Deleting root . attrib [ 'foo' ] = 'bar' del ( root . attrib [ 'hello' ]) Commit changes to disk. The argument can be a string representing the filename or a file object (in which case the file must be opened as a binary). Encoding can be specified (default is UTF-8 ) and a XML declaration can also be automatically generated. String File object tree . write ( 'books.xml' , encoding = 'UTF-16' , xml_declaration = True ) with open ( 'books.xml' , 'wb' ) as f : tree . write ( f ) Find elements by element name tree . findall ( 'book' )","title":"xml"},{"location":"Python/Modules/Xml/#xml","text":"books.xml <?xml version=\"1.0\"?> <catalog> <book id= \"bk101\" > <author> Gambardella, Matthew </author> <title> XML Developer's Guide </title> <genre> Computer </genre> <price> 44.95 </price> <publish_date> 2000-10-01 </publish_date> <description> An in-depth look at creating applications with XML. </description> </book> <book id= \"bk102\" > <author> Ralls, Kim </author> <title> Midnight Rain </title> <genre> Fantasy </genre> <price> 5.95 </price> <publish_date> 2000-12-16 </publish_date> <description> A former architect battles corporate zombies, an evil sorceress, and her own childhood to become queen of the world. </description> </book> <book id= \"bk103\" > <author> Corets, Eva </author> <title> Maeve Ascendant </title> <genre> Fantasy </genre> <price> 5.95 </price> <publish_date> 2000-11-17 </publish_date> <description> After the collapse of a nanotechnology society in England, the young survivors lay the foundation for a new society. </description> </book> <book id= \"bk104\" > <author> Corets, Eva </author> <title> Oberon's Legacy </title> <genre> Fantasy </genre> <price> 5.95 </price> <publish_date> 2001-03-10 </publish_date> <description> In post-apocalypse England, the mysterious agent known only as Oberon helps to create a new life for the inhabitants of London. Sequel to Maeve Ascendant. </description> </book> <book id= \"bk105\" > <author> Corets, Eva </author> <title> The Sundered Grail </title> <genre> Fantasy </genre> <price> 5.95 </price> <publish_date> 2001-09-10 </publish_date> <description> The two daughters of Maeve, half-sisters, battle one another for control of England. Sequel to Oberon's Legacy. </description> </book> <book id= \"bk106\" > <author> Randall, Cynthia </author> <title> Lover Birds </title> <genre> Romance </genre> <price> 4.95 </price> <publish_date> 2000-09-02 </publish_date> <description> When Carla meets Paul at an ornithology conference, tempers fly as feathers get ruffled. </description> </book> <book id= \"bk107\" > <author> Thurman, Paula </author> <title> Splish Splash </title> <genre> Romance </genre> <price> 4.95 </price> <publish_date> 2000-11-02 </publish_date> <description> A deep sea diver finds true love twenty thousand leagues beneath the sea. </description> </book> <book id= \"bk108\" > <author> Knorr, Stefan </author> <title> Creepy Crawlies </title> <genre> Horror </genre> <price> 4.95 </price> <publish_date> 2000-12-06 </publish_date> <description> An anthology of horror stories about roaches, centipedes, scorpions and other insects. </description> </book> <book id= \"bk109\" > <author> Kress, Peter </author> <title> Paradox Lost </title> <genre> Science Fiction </genre> <price> 6.95 </price> <publish_date> 2000-11-02 </publish_date> <description> After an inadvertant trip through a Heisenberg Uncertainty Device, James Salway discovers the problems of being quantum. </description> </book> <book id= \"bk110\" > <author> O'Brien, Tim </author> <title> Microsoft .NET: The Programming Bible </title> <genre> Computer </genre> <price> 36.95 </price> <publish_date> 2000-12-09 </publish_date> <description> Microsoft's .NET initiative is explored in detail in this deep programmer's reference. </description> </book> <book id= \"bk111\" > <author> O'Brien, Tim </author> <title> MSXML3: A Comprehensive Guide </title> <genre> Computer </genre> <price> 36.95 </price> <publish_date> 2000-12-01 </publish_date> <description> The Microsoft MSXML3 parser is covered in detail, with attention to XML DOM interfaces, XSLT processing, SAX and more. </description> </book> <book id= \"bk112\" > <author> Galos, Mike </author> <title> Visual Studio 7: A Comprehensive Guide </title> <genre> Computer </genre> <price> 49.95 </price> <publish_date> 2001-04-16 </publish_date> <description> Microsoft Visual Studio 7 is explored in depth, looking at how Visual Basic, Visual C++, C#, and ASP+ are integrated into a comprehensive development environment. </description> </book> </catalog> The etree submodule contains the ElementTree object which can open a string filename to deserialize XML data using parse() , which returns an ElementTree object, representing an XML document. A Python string can also be parsed with fromstring() , which actually returns an Element object. File String tree = xml . etree . ElementTree . parse ( 'books.xml' ) tree = xml . etree . ElementTree . fromstring ( books ) The getroot() method returns an Element object of the XML document's root node. root = tree . getroot () The parsed data can be displayed using the tostring() static method, providing an Element as argument. ElementTree . tostring ( root ) Children of an element can be filtered using findall() . This returns a list of Elements. books = root . findall ( 'book' ) Any Element object exposes an attrib property which returns a dictionary of attributes. [b.attrib for b in books] Attributes can be written to an Element using the set() method. root . set ( 'foo' , 'bar' ) Attributes can also be manipulated on the attrib property with normal Python dictionary operations. Setting Deleting root . attrib [ 'foo' ] = 'bar' del ( root . attrib [ 'hello' ]) Commit changes to disk. The argument can be a string representing the filename or a file object (in which case the file must be opened as a binary). Encoding can be specified (default is UTF-8 ) and a XML declaration can also be automatically generated. String File object tree . write ( 'books.xml' , encoding = 'UTF-16' , xml_declaration = True ) with open ( 'books.xml' , 'wb' ) as f : tree . write ( f ) Find elements by element name tree . findall ( 'book' )","title":"xml"},{"location":"Python/Modules/Yaml/","text":"Yaml pip install pyyaml Deserialize Serialize import yaml with open ( './starships.yaml' ) as f : starships = yaml . safe_load ( f ) There is a load method but it requires specifying one of four possible values for the Loader kwarg. import yaml with open ( './starships.yaml' , 'w' ) as f : yaml . dump ( starships , f ) Resources Introduction to YAML","title":"Yaml"},{"location":"Python/Modules/Yaml/#yaml","text":"pip install pyyaml Deserialize Serialize import yaml with open ( './starships.yaml' ) as f : starships = yaml . safe_load ( f ) There is a load method but it requires specifying one of four possible values for the Loader kwarg. import yaml with open ( './starships.yaml' , 'w' ) as f : yaml . dump ( starships , f ) Resources Introduction to YAML","title":"Yaml"},{"location":"Rust/","text":"Overview Rust's distinguishing feature as a programming language is its ability to prevent invalid data access at compile time. \u2014Tim McNamara Rust offers zero-cost abstractions , where using the abstraction imposes no additional runtime overhead. Documentation Rust uses C-style line comments using // and block comments using /* , */ Doc comments support markdown and are used to generate documentation with the use of cargo doc . Markdown code blocks containing test cases are known as doc tests and can be run with cargo test , for library crates only. Note that markdown code blocks in Rust doc comments don't need a language annotation. Outer doc comments are preceded by /// and are written immediately preceding the code blocks they document Inner doc comments are preceded by //! and are written within code blocks, similar to docstrings in Python Data Variable declarations are called bindings in Rust, and by convention variable names are in snake_case (lower-case letters with words delimited by _ ). They are globally scoped by default unless they are declared in a code block. Variables are immutable by default, so if their values are to change they must be marked with mut . However, immutable variables are distinct from constants declared using const , which cannot be made mutable at all. const identifiers are conventionally written in capitalized snake_case. let language = \"&nbsp;\" ; // Immutable let mut language = \"&nbsp;\" ; // Mutable const language = \"&nbsp;\" ; // Constant Data type is explicitly specified on initialization after colon, and this same syntax is used to type function parameters and return types: let language : String = \"&nbsp;\" ; A special ! type indicates that the function never returns // RIA p. 78 fn read ( f : & mut File , save_to : & mut Vec < u8 > ) -> ! { unimplemented! (); } Numbers can be typed by appending the data type to the value itself. Digits can be separated with _ let x = 255 i8 ; let y = 1_024_ i16 ; Because implicit integer conversions are a well-known source of bugs and security holes, conversion must be explicit using the as keyword: do_something ( x as i32 ); Common mathematical calculations are implemented as methods, which can be called directly on variables or literals or as associated functions of the type: let x = ( 4.5_ f64 ). floor () let y = 4.0_ f64 ; let z = y . sqrt (); let a = f64 :: sqrt ( 4.0 ); Common constants can be found in each type's consts module, i.e. std::f32::consts . Other values like MIN , MAX , INFINITY , NEG_INFINITY , and NAN (not-a-number) are also implemented as consts. Shadowing and masking are terms that refer to using a locally scoped variable with the same identifier as a global variable. When in the local variable's block, the local variable is said to be masking the global one, which conversely is shadowing the local. Once the local scope is exited, the global variable's variable is accessible again and the local variable is destroyed. Shadowing is used in the Guessing Game coding task to parse the input string as an integer. Integers can be fixed-length or variable-length. Fixed-size integers can be signed ( i ) or unsigned ( u ) and 8, 16, 32, or 64 bits: i.e. i8 , u64 etc. Variable-size integers can be pointer-sized signed isize or pointer-sized unsigned usize , the size of both of which depend on the architecture of the host system. Arrays and Tuples are considered primitive data types, albeit Compound ones. Integer and Float are considered Numeric Scalars , while Boolean and Chars are considered Non-Numeric Scalars . Floating point numbers can be single-precision f32 or double-precision f64 . Booleans are true or false (lower-case). Arrays are homogeneous sequences of elements and must be of a fixed length, declared at initialization, although the type can be determined implicitly. Type can be inferred by the compiler or explicitly annotated after a colon: Implicit Explicit let var = 0 ; let arr = [ 0 ; 4 ]; let person = ( \"John\" , 35 , \"Doe\" ); let var : u8 = 0 ; let arr : [ i32 ; 4 ] = [ 0 , 0 , 0 , 0 ]; let person : ( & str , i32 , & str ) = ( \"John\" , 35 , \"Doe\" ); Array length can be given by the len() method. Array slicing let arr : [ i32 ; 4 ] = [ 1 , 2 , 3 , 4 ]; let slice_array2 : & [ i32 ] = & arr [ 0 .. 2 ]; Tuples, like arrays, are fixed-length. But unlike arrays they are heterogeneous sequences of elements. Tuples can be destructured (i.e. unpacked) let person = ( \"John\" , 35 , \"Doe\" ); let ( first_name , age , last_name ) = person ; Tuples can be made mutable with the mut keyword. Rust's standard library includes a number of collections A vector stores a variable number of values of a single type in a sequence A String is a collection of characters A hash map is a key-value store Ownership One of the key and unique features of Rust is the concept of ownership , which achieves memory safety without the use of a garbage collector. Stack versus heap The stack and heap are locations in memory that are available to the application at runtime. The stack is a LIFO that is used for sizes that are known at compile-time. It is comparable to a stack of plates of uniform size from which plates can be removed only from the top. Values are said to be \" pushed onto \" or \" popped off \" the stack. The heap is less organized and less efficient, and is used for sizes that are known only at runtime. The operating system must search for an appropriate memory location based on the application's request at runtime, returning a pointer, and this process makes the heap less efficient than the stack. Memory locations are said to be \" allocated on \" the heap. Some complex data types like String are composed of a pointer, stored in the stack because its size is known at compile-time, that points to a location on the heap that holds the String's contents, which are known only at runtime. In other languages these statements would cause s2 to become a shallow copy of s1, pointing to the same location in memory where the String contents are stored. However, ownership rules in Rust cause s1 to be invalidated because s2 becomes the owner of the data contents on the heap, and this is called a move . let s1 = String :: from ( \"Hello, world!\" ); let s2 = s1 ; println ( \"{}\" , s1 ); // compiler error This allows Rust to avoid the double free error caused by attempting to free the same memory location twice, which can cause corruption and security issues. When the variable goes out of scope, its backing memory is freed. A deep copy is still possible with the common clone method: let s2 = s1 . clone (); This behavior is only for data that is stored on the heap, not the stack. The size of integers is known at compile-time, so they are stored entirely on the stack, and therefore copies of the values are efficiently made. let x = 5 ; let y = x ; println! ( \"{}\" , x ); // no error More specifically, certain types have a special annotation called the Copy trait which enable this behavior. For types that \"are Copy \" - i.e. have the Copy trait - an older variable is still usable after assignment. Copy types include integers, booleans, chars, floats, and tuples containing only other Copy types. Function calls also exhibit move behavior; after a variable is passed as argument to a function, the function owns it and it may not be used again in its original context unless ownership is returned. If the value is not returned, the argument's contents are consumed and it may not be used again in the calling context. This is why most function calls in Rust use references , prefixing the variable identifier with & , a process called borrowing . Passing a value to a function while transferring ownership is called \"passing by value\" or a move , whereas using a reference is called \"passing by reference\". Passing by value Returning value By reference 1 2 3 4 5 6 7 8 9 10 fn main () { let s = String :: from ( \"Dgiapusccu\" ); hello_world ( s ); println! ( \"Hello again, {}!\" , s ); // compiler error } fn hello_world ( s : String ) { println! ( \"Hello, {}!\" , s ) } 1 2 3 4 5 6 7 8 9 10 fn main () { let s = String :: from ( \"Dgiapusccu\" ); let s = hello_world ( s ); println! ( \"Hello again, {}!\" , s ); } fn hello_world ( s : String ) -> String { println! ( \"Hello, {}!\" , s ); s } 1 2 3 4 5 6 7 8 9 10 fn main () { let s = String :: from ( \"Dgiapusccu\" ); hello_world ( & s ); println! ( \"Hello again, {}!\" , s ); } fn hello_world ( s : & String ) { println! ( \"Hello, {}!\" , s ) } References Pointer types like Box<T> and those internal to String and Vec are owning pointers : when the owner is dropped the referent is deallocated. Nonowning pointer types are called references and have no effect on their referents' lifetimes. There are two types of reference: Shared references let you read but not modify the referent Multiple shared references to the same value can be created. Mutable references allow both reading and modifying of the referent To prevent data races only one mutable reference to a location in a scope can exist. A mutable reference and an immutable one cannot coexist in the same scope. Moves after a borrow are also forbidden, for this same reason. Here, the call to push() causes the vector to be reallocated on the heap after an immutable borrow was made. let mut data = vec! [ 1 , 2 , 3 ]; let x = & data [ 0 ]; data . push ( 4 ); println! ( \"{}\" , x ); Copy and Clone Rust provides two traits that relate to the copying of data: Copy and Clone . Copy allows values stored on the stack only to be duplicated. Any type whose parts all implement Copy can also derive Copy. However, this trait is rarely required since primitives stored on the stack already have optimizations available. In the background this relies on the memcpy syscall. Clone is for explicitly creating a deep copy of values, especially those allocated on the heap. Types that implement Copy also trivially implement Clone. Collections Accessing tuple elements is done with the . operator let coord : ( i8 , i8 ) = ( 10 , 20 ); println! ( \"{}, {}\" , coord . 0 , coord . 1 ); Paradigms OOP Strictly speaking, OOP is not actually implemented in Rust because there is no inheritance. However, objects that combine data with logic can be created struct s and impl s. A group of methods that are shared by multiple types can have their signatures defined by a trait . Types then implement the trait, and functions can be defined that accept any type that does so by specifying the trait instead of a single concrete type. pub fn notify ( item : impl Summary ) { println! ( \"Breaking news! {}\" , item . summarize ()); } TDD Tests are functions annotated with the #[test] attribute . Tests fail when the test function panics. #[test] fn math_works () { assert_eq! ( 2 + 2 , 4 ); } #[test] fn fails () { panic! ( \"This test will fail\" ); } Tests can also be incorporated in documentation as markdown code blocks. Concurrency std::sync::atomic provides thread-safe types modeled on the atomics of C++20. This model introduces the concept of atomic accesses which tell the hardware and compiler what ordering it has with relation to other accesses. This largely boils down to preventing reordering of instructions and determining how writes are propagated to other threads. std::sync::RwLock is a reader-writer lock, which allows a number of readers or at most one writer at any point in time. A Mutex, by comparison, does not distinguish between readers or writers and blocks any threads waiting for the lock to become available. let lock = std :: sync :: RwLock :: new ( 1 ); let mut n = lock . write (). unwrap (); // (1) * n = 2 ; assert! ( lock . try_read (). is_err ()); // (2) write attempts to acquire the rwlock with exclusive write access and returns a LockResult , which is really just a type alias for a Result. try_read attempts to acquire the rwlock with shared read access.","title":"Overview"},{"location":"Rust/#overview","text":"Rust's distinguishing feature as a programming language is its ability to prevent invalid data access at compile time. \u2014Tim McNamara Rust offers zero-cost abstractions , where using the abstraction imposes no additional runtime overhead.","title":"Overview"},{"location":"Rust/#documentation","text":"Rust uses C-style line comments using // and block comments using /* , */ Doc comments support markdown and are used to generate documentation with the use of cargo doc . Markdown code blocks containing test cases are known as doc tests and can be run with cargo test , for library crates only. Note that markdown code blocks in Rust doc comments don't need a language annotation. Outer doc comments are preceded by /// and are written immediately preceding the code blocks they document Inner doc comments are preceded by //! and are written within code blocks, similar to docstrings in Python","title":"Documentation"},{"location":"Rust/#data","text":"Variable declarations are called bindings in Rust, and by convention variable names are in snake_case (lower-case letters with words delimited by _ ). They are globally scoped by default unless they are declared in a code block. Variables are immutable by default, so if their values are to change they must be marked with mut . However, immutable variables are distinct from constants declared using const , which cannot be made mutable at all. const identifiers are conventionally written in capitalized snake_case. let language = \"&nbsp;\" ; // Immutable let mut language = \"&nbsp;\" ; // Mutable const language = \"&nbsp;\" ; // Constant Data type is explicitly specified on initialization after colon, and this same syntax is used to type function parameters and return types: let language : String = \"&nbsp;\" ; A special ! type indicates that the function never returns // RIA p. 78 fn read ( f : & mut File , save_to : & mut Vec < u8 > ) -> ! { unimplemented! (); } Numbers can be typed by appending the data type to the value itself. Digits can be separated with _ let x = 255 i8 ; let y = 1_024_ i16 ; Because implicit integer conversions are a well-known source of bugs and security holes, conversion must be explicit using the as keyword: do_something ( x as i32 ); Common mathematical calculations are implemented as methods, which can be called directly on variables or literals or as associated functions of the type: let x = ( 4.5_ f64 ). floor () let y = 4.0_ f64 ; let z = y . sqrt (); let a = f64 :: sqrt ( 4.0 ); Common constants can be found in each type's consts module, i.e. std::f32::consts . Other values like MIN , MAX , INFINITY , NEG_INFINITY , and NAN (not-a-number) are also implemented as consts. Shadowing and masking are terms that refer to using a locally scoped variable with the same identifier as a global variable. When in the local variable's block, the local variable is said to be masking the global one, which conversely is shadowing the local. Once the local scope is exited, the global variable's variable is accessible again and the local variable is destroyed. Shadowing is used in the Guessing Game coding task to parse the input string as an integer. Integers can be fixed-length or variable-length. Fixed-size integers can be signed ( i ) or unsigned ( u ) and 8, 16, 32, or 64 bits: i.e. i8 , u64 etc. Variable-size integers can be pointer-sized signed isize or pointer-sized unsigned usize , the size of both of which depend on the architecture of the host system. Arrays and Tuples are considered primitive data types, albeit Compound ones. Integer and Float are considered Numeric Scalars , while Boolean and Chars are considered Non-Numeric Scalars . Floating point numbers can be single-precision f32 or double-precision f64 . Booleans are true or false (lower-case). Arrays are homogeneous sequences of elements and must be of a fixed length, declared at initialization, although the type can be determined implicitly. Type can be inferred by the compiler or explicitly annotated after a colon: Implicit Explicit let var = 0 ; let arr = [ 0 ; 4 ]; let person = ( \"John\" , 35 , \"Doe\" ); let var : u8 = 0 ; let arr : [ i32 ; 4 ] = [ 0 , 0 , 0 , 0 ]; let person : ( & str , i32 , & str ) = ( \"John\" , 35 , \"Doe\" ); Array length can be given by the len() method. Array slicing let arr : [ i32 ; 4 ] = [ 1 , 2 , 3 , 4 ]; let slice_array2 : & [ i32 ] = & arr [ 0 .. 2 ]; Tuples, like arrays, are fixed-length. But unlike arrays they are heterogeneous sequences of elements. Tuples can be destructured (i.e. unpacked) let person = ( \"John\" , 35 , \"Doe\" ); let ( first_name , age , last_name ) = person ; Tuples can be made mutable with the mut keyword. Rust's standard library includes a number of collections A vector stores a variable number of values of a single type in a sequence A String is a collection of characters A hash map is a key-value store","title":"Data"},{"location":"Rust/#ownership","text":"One of the key and unique features of Rust is the concept of ownership , which achieves memory safety without the use of a garbage collector. Stack versus heap The stack and heap are locations in memory that are available to the application at runtime. The stack is a LIFO that is used for sizes that are known at compile-time. It is comparable to a stack of plates of uniform size from which plates can be removed only from the top. Values are said to be \" pushed onto \" or \" popped off \" the stack. The heap is less organized and less efficient, and is used for sizes that are known only at runtime. The operating system must search for an appropriate memory location based on the application's request at runtime, returning a pointer, and this process makes the heap less efficient than the stack. Memory locations are said to be \" allocated on \" the heap. Some complex data types like String are composed of a pointer, stored in the stack because its size is known at compile-time, that points to a location on the heap that holds the String's contents, which are known only at runtime. In other languages these statements would cause s2 to become a shallow copy of s1, pointing to the same location in memory where the String contents are stored. However, ownership rules in Rust cause s1 to be invalidated because s2 becomes the owner of the data contents on the heap, and this is called a move . let s1 = String :: from ( \"Hello, world!\" ); let s2 = s1 ; println ( \"{}\" , s1 ); // compiler error This allows Rust to avoid the double free error caused by attempting to free the same memory location twice, which can cause corruption and security issues. When the variable goes out of scope, its backing memory is freed. A deep copy is still possible with the common clone method: let s2 = s1 . clone (); This behavior is only for data that is stored on the heap, not the stack. The size of integers is known at compile-time, so they are stored entirely on the stack, and therefore copies of the values are efficiently made. let x = 5 ; let y = x ; println! ( \"{}\" , x ); // no error More specifically, certain types have a special annotation called the Copy trait which enable this behavior. For types that \"are Copy \" - i.e. have the Copy trait - an older variable is still usable after assignment. Copy types include integers, booleans, chars, floats, and tuples containing only other Copy types. Function calls also exhibit move behavior; after a variable is passed as argument to a function, the function owns it and it may not be used again in its original context unless ownership is returned. If the value is not returned, the argument's contents are consumed and it may not be used again in the calling context. This is why most function calls in Rust use references , prefixing the variable identifier with & , a process called borrowing . Passing a value to a function while transferring ownership is called \"passing by value\" or a move , whereas using a reference is called \"passing by reference\". Passing by value Returning value By reference 1 2 3 4 5 6 7 8 9 10 fn main () { let s = String :: from ( \"Dgiapusccu\" ); hello_world ( s ); println! ( \"Hello again, {}!\" , s ); // compiler error } fn hello_world ( s : String ) { println! ( \"Hello, {}!\" , s ) } 1 2 3 4 5 6 7 8 9 10 fn main () { let s = String :: from ( \"Dgiapusccu\" ); let s = hello_world ( s ); println! ( \"Hello again, {}!\" , s ); } fn hello_world ( s : String ) -> String { println! ( \"Hello, {}!\" , s ); s } 1 2 3 4 5 6 7 8 9 10 fn main () { let s = String :: from ( \"Dgiapusccu\" ); hello_world ( & s ); println! ( \"Hello again, {}!\" , s ); } fn hello_world ( s : & String ) { println! ( \"Hello, {}!\" , s ) }","title":"Ownership"},{"location":"Rust/#references","text":"Pointer types like Box<T> and those internal to String and Vec are owning pointers : when the owner is dropped the referent is deallocated. Nonowning pointer types are called references and have no effect on their referents' lifetimes. There are two types of reference: Shared references let you read but not modify the referent Multiple shared references to the same value can be created. Mutable references allow both reading and modifying of the referent To prevent data races only one mutable reference to a location in a scope can exist. A mutable reference and an immutable one cannot coexist in the same scope. Moves after a borrow are also forbidden, for this same reason. Here, the call to push() causes the vector to be reallocated on the heap after an immutable borrow was made. let mut data = vec! [ 1 , 2 , 3 ]; let x = & data [ 0 ]; data . push ( 4 ); println! ( \"{}\" , x );","title":"References"},{"location":"Rust/#copy-and-clone","text":"Rust provides two traits that relate to the copying of data: Copy and Clone . Copy allows values stored on the stack only to be duplicated. Any type whose parts all implement Copy can also derive Copy. However, this trait is rarely required since primitives stored on the stack already have optimizations available. In the background this relies on the memcpy syscall. Clone is for explicitly creating a deep copy of values, especially those allocated on the heap. Types that implement Copy also trivially implement Clone.","title":"Copy and Clone"},{"location":"Rust/#collections","text":"Accessing tuple elements is done with the . operator let coord : ( i8 , i8 ) = ( 10 , 20 ); println! ( \"{}, {}\" , coord . 0 , coord . 1 );","title":"Collections"},{"location":"Rust/#paradigms","text":"","title":"Paradigms"},{"location":"Rust/#oop","text":"Strictly speaking, OOP is not actually implemented in Rust because there is no inheritance. However, objects that combine data with logic can be created struct s and impl s. A group of methods that are shared by multiple types can have their signatures defined by a trait . Types then implement the trait, and functions can be defined that accept any type that does so by specifying the trait instead of a single concrete type. pub fn notify ( item : impl Summary ) { println! ( \"Breaking news! {}\" , item . summarize ()); }","title":"OOP"},{"location":"Rust/#tdd","text":"Tests are functions annotated with the #[test] attribute . Tests fail when the test function panics. #[test] fn math_works () { assert_eq! ( 2 + 2 , 4 ); } #[test] fn fails () { panic! ( \"This test will fail\" ); } Tests can also be incorporated in documentation as markdown code blocks.","title":"TDD"},{"location":"Rust/#concurrency","text":"std::sync::atomic provides thread-safe types modeled on the atomics of C++20. This model introduces the concept of atomic accesses which tell the hardware and compiler what ordering it has with relation to other accesses. This largely boils down to preventing reordering of instructions and determining how writes are propagated to other threads. std::sync::RwLock is a reader-writer lock, which allows a number of readers or at most one writer at any point in time. A Mutex, by comparison, does not distinguish between readers or writers and blocks any threads waiting for the lock to become available. let lock = std :: sync :: RwLock :: new ( 1 ); let mut n = lock . write (). unwrap (); // (1) * n = 2 ; assert! ( lock . try_read (). is_err ()); // (2) write attempts to acquire the rwlock with exclusive write access and returns a LockResult , which is really just a type alias for a Result. try_read attempts to acquire the rwlock with shared read access.","title":"Concurrency"},{"location":"Rust/Cargo/","text":"Cargo Cargo is Rust's package manager. New crates can be created with the cargo command-line utility. cargo new hello_cargo cargo new hello_cargo --vcs none # Prevent creation of a Git repo Extensions to Cargo are available, which add subcommands like add and watch cargo install cargo-edit cargo install cargo-watch During development, a crate can be compiled and run cargo run If behind a corporate firewall, where SSL certificates are substituted, a special flag must be set in ~/.cargo/config.toml to allow package downloads. [http] check-revoke = false Generate documentation from doc comments using a built-in static site generator, and then open it. cargo doc --open Other commands: cargo fmt format code cargo test run doctests Features Every crate has features that can be enabled in the Cargo.toml. Crate features are declared in the Cargo.toml rocket = { version = \"0.5.0-rc.1\" , default-features = false , features = [ \"json\" ]} These can also be added from the command-line cargo add rocket --features json --no-default-features These are distinct from the \"feature flag\" inner attributes referring to features of unstable Rust .","title":"Cargo"},{"location":"Rust/Cargo/#cargo","text":"Cargo is Rust's package manager. New crates can be created with the cargo command-line utility. cargo new hello_cargo cargo new hello_cargo --vcs none # Prevent creation of a Git repo Extensions to Cargo are available, which add subcommands like add and watch cargo install cargo-edit cargo install cargo-watch During development, a crate can be compiled and run cargo run If behind a corporate firewall, where SSL certificates are substituted, a special flag must be set in ~/.cargo/config.toml to allow package downloads. [http] check-revoke = false Generate documentation from doc comments using a built-in static site generator, and then open it. cargo doc --open Other commands: cargo fmt format code cargo test run doctests","title":"Cargo"},{"location":"Rust/Cargo/#features","text":"Every crate has features that can be enabled in the Cargo.toml. Crate features are declared in the Cargo.toml rocket = { version = \"0.5.0-rc.1\" , default-features = false , features = [ \"json\" ]} These can also be added from the command-line cargo add rocket --features json --no-default-features These are distinct from the \"feature flag\" inner attributes referring to features of unstable Rust .","title":"Features"},{"location":"Rust/Combinators/","text":"","title":"Combinators"},{"location":"Rust/Crates/","text":"Modules Projects and code dependencies in Rust are called crates , equivalent to modules in Python. The term modules in Rust allow code to be organized for readability and controlled for privacy. A package contains one or more crates and contains a Cargo.toml file. mod declares a module, which can be nested crate is the root of the module tree, equivalent to cd / super moves up the module tree one node, similar to cd .. use .. as is similar to creating a shortcut or symlink. Rust convention is to bring a function's parent into scope in order to mark function calls as unmistakeably belonging to external code. However, for structs and other data structures the full path is specified.sdfsdf pub use statements are used to construct a convenient API by allowing the namespace to be flattened Rust 2018 The module system was simplified in Rust 2018 . One of the main changes was the elimination of the extern crate keywords which used to be necessary while importing a crate into a project. The only thing that is necessary now is to add the crate to Cargo.toml","title":"Modules"},{"location":"Rust/Crates/#modules","text":"Projects and code dependencies in Rust are called crates , equivalent to modules in Python. The term modules in Rust allow code to be organized for readability and controlled for privacy. A package contains one or more crates and contains a Cargo.toml file. mod declares a module, which can be nested crate is the root of the module tree, equivalent to cd / super moves up the module tree one node, similar to cd .. use .. as is similar to creating a shortcut or symlink. Rust convention is to bring a function's parent into scope in order to mark function calls as unmistakeably belonging to external code. However, for structs and other data structures the full path is specified.sdfsdf pub use statements are used to construct a convenient API by allowing the namespace to be flattened","title":"Modules"},{"location":"Rust/Crates/#rust-2018","text":"The module system was simplified in Rust 2018 . One of the main changes was the elimination of the extern crate keywords which used to be necessary while importing a crate into a project. The only thing that is necessary now is to add the crate to Cargo.toml","title":"Rust 2018"},{"location":"Rust/Error/","text":"Error handling Error handling in Rust is closely tied to a specific type, the Result enum: enum Result < T , E > { Ok ( T ), Err ( E ), } Plainly, Results encapsulate the success or failure of an operation, and the idiomatic \"Rustic\" way to return a value from a function is to wrap it in Ok (or an error wrapped in Err ). Results expose many methods to handle errors. Some like unwrap and expect are familiar to Rust learners, and they are commonly used at the beginning stages of development as crude forms of error handling. Other methods include the family of combinators which allow operations to be conducted on wrapped values, including errors, and for these operations to be chained together in a way that is fluent and intuitive. Rust distinguishes between recoverable and unrecoverable errors. Recoverable errors can be handled by program logic, whereas unrecoverable errors result in a crash (ref. panic ). Question mark operator The question mark operator ( ? ) is used only within the body of functions that return Results and only at the end of function calls that also return results. It is similar to using a match expression in that the value wrapped by Result is returned to the calling code. fn main () { match outer () { Err ( e ) => eprintln! ( \"inner() returned an error!\" ), _ => println! ( \"No error!\" ) }; } fn outer () -> Result < T , E > { let value = inner () ? ; // (1) } fn inner () -> Result < T , E > { Err () } If inner() returns an Error (which it does), it is immediately returned as the return value of outer() If the error types are mismatched then the compiler will automatically attempt to convert them using from , so the From trait for the inner error type should be implemented for the outer error type. use std :: io ; use std :: io :: Read ; use std :: fs :: File ; fn read_username_from_file () -> Result < String , io :: Error > { let mut s = String :: new (); File :: open ( \"hello.txt\" ) ? // (1) . read_to_string ( & mut s ) ? ; // (2) Ok ( s ) } Equivalent to: let mut f = match File :: open ( \"hello.txt\" ) { Ok ( file ) => file , Err ( e ) => return Err ( e ), }; Equivalent to: let mut s = String :: new (); match f . read_to_string ( & mut s ) { Ok ( _ ) => Ok ( s ), Err ( e ) => Err ( e ), } Combinators Combinators are a functional programming approach to error handling that use a variety of Result methods to run operations on wrapped types, especially errors. and_then The and_then combinator can be used to take a value wrapped in Ok and pass it to another function. An error short circuits this method and is returned immediately. Here, it is used trivially to display a greeting after receiving a string interactively. fn main () { get_name (). and_then ( display_name ); } fn get_name () -> Result < String , std :: io :: Error > { let mut name = String :: new (); std :: io :: stdin (). read_line ( & mut name ) ? ; Ok ( name . trim (). to_string ()) } fn display_name ( name : String ) -> Result < (), std :: io :: Error > { println! ( \"Hello, {}!\" , name ); Ok (()) } and_then also figures prominently in Ken Youens-Clark's Command-Line Rust , where he uses it to pass a Config object containing values parsed from the command-line using clap. Standard entrypoint in KYC's Rust CLI programs fn main () { if let Err ( e ) = catr :: get_args (). and_then ( catr :: run ) { eprintln! ( \"{}\" , e ); std :: process :: exit ( 1 ); } } map The map combinator runs a function on the wrapped value, changing its type, or passes along the Error. Naive map() struct Person { name : String , age : i32 , } fn display ( person : Option < Person > ) { match person { Some ( person ) => println! ( \"{:?}\" , person ), None => println! ( \"No person\" ) } } fn main () { let person = Some ( Person { name : \"Amita\" . to_string (), age : 16 }); display ( person ); } struct Person { name : String , age : i32 , } fn display ( person : Option < Person > ) { let result : Option < String > = person . map ( | p | p . name ) . filter ( | x | x . contains ( \"Ami\" )) . take (); println! ( \"{:?}\" , result . unwrap ()); } fn main () { let person : Option < Person > = Some ( Person { name : \"Amita\" . to_string (), age : 16 , }); display ( person ); }","title":"Error handling"},{"location":"Rust/Error/#error-handling","text":"Error handling in Rust is closely tied to a specific type, the Result enum: enum Result < T , E > { Ok ( T ), Err ( E ), } Plainly, Results encapsulate the success or failure of an operation, and the idiomatic \"Rustic\" way to return a value from a function is to wrap it in Ok (or an error wrapped in Err ). Results expose many methods to handle errors. Some like unwrap and expect are familiar to Rust learners, and they are commonly used at the beginning stages of development as crude forms of error handling. Other methods include the family of combinators which allow operations to be conducted on wrapped values, including errors, and for these operations to be chained together in a way that is fluent and intuitive. Rust distinguishes between recoverable and unrecoverable errors. Recoverable errors can be handled by program logic, whereas unrecoverable errors result in a crash (ref. panic ).","title":"Error handling"},{"location":"Rust/Error/#question-mark-operator","text":"The question mark operator ( ? ) is used only within the body of functions that return Results and only at the end of function calls that also return results. It is similar to using a match expression in that the value wrapped by Result is returned to the calling code. fn main () { match outer () { Err ( e ) => eprintln! ( \"inner() returned an error!\" ), _ => println! ( \"No error!\" ) }; } fn outer () -> Result < T , E > { let value = inner () ? ; // (1) } fn inner () -> Result < T , E > { Err () } If inner() returns an Error (which it does), it is immediately returned as the return value of outer() If the error types are mismatched then the compiler will automatically attempt to convert them using from , so the From trait for the inner error type should be implemented for the outer error type. use std :: io ; use std :: io :: Read ; use std :: fs :: File ; fn read_username_from_file () -> Result < String , io :: Error > { let mut s = String :: new (); File :: open ( \"hello.txt\" ) ? // (1) . read_to_string ( & mut s ) ? ; // (2) Ok ( s ) } Equivalent to: let mut f = match File :: open ( \"hello.txt\" ) { Ok ( file ) => file , Err ( e ) => return Err ( e ), }; Equivalent to: let mut s = String :: new (); match f . read_to_string ( & mut s ) { Ok ( _ ) => Ok ( s ), Err ( e ) => Err ( e ), }","title":"Question mark operator"},{"location":"Rust/Error/#combinators","text":"Combinators are a functional programming approach to error handling that use a variety of Result methods to run operations on wrapped types, especially errors.","title":"Combinators"},{"location":"Rust/Error/#and_then","text":"The and_then combinator can be used to take a value wrapped in Ok and pass it to another function. An error short circuits this method and is returned immediately. Here, it is used trivially to display a greeting after receiving a string interactively. fn main () { get_name (). and_then ( display_name ); } fn get_name () -> Result < String , std :: io :: Error > { let mut name = String :: new (); std :: io :: stdin (). read_line ( & mut name ) ? ; Ok ( name . trim (). to_string ()) } fn display_name ( name : String ) -> Result < (), std :: io :: Error > { println! ( \"Hello, {}!\" , name ); Ok (()) } and_then also figures prominently in Ken Youens-Clark's Command-Line Rust , where he uses it to pass a Config object containing values parsed from the command-line using clap. Standard entrypoint in KYC's Rust CLI programs fn main () { if let Err ( e ) = catr :: get_args (). and_then ( catr :: run ) { eprintln! ( \"{}\" , e ); std :: process :: exit ( 1 ); } }","title":"and_then"},{"location":"Rust/Error/#map","text":"The map combinator runs a function on the wrapped value, changing its type, or passes along the Error. Naive map() struct Person { name : String , age : i32 , } fn display ( person : Option < Person > ) { match person { Some ( person ) => println! ( \"{:?}\" , person ), None => println! ( \"No person\" ) } } fn main () { let person = Some ( Person { name : \"Amita\" . to_string (), age : 16 }); display ( person ); } struct Person { name : String , age : i32 , } fn display ( person : Option < Person > ) { let result : Option < String > = person . map ( | p | p . name ) . filter ( | x | x . contains ( \"Ami\" )) . take (); println! ( \"{:?}\" , result . unwrap ()); } fn main () { let person : Option < Person > = Some ( Person { name : \"Amita\" . to_string (), age : 16 , }); display ( person ); }","title":"map"},{"location":"Rust/Glossary/","text":"Glossary Associated type Associated types connect a type placeholder with a trait such that the trait method definitions use these placeholder types in their signatures. With generics, we must annotate the types in each implementation, but using an associated type forces a single implementation. Associated type Generic pub trait Iterator { type Item ; fn next ( & mut self ) -> Option < Self :: Item > ; } pub trait Iterator < T > { fn next ( & mut self ) -> Option < T > ; } Attribute There are several types of attribute: Inner attributes apply to the item that the attribute is declared in and begin with a shebang Feature flags #![feature(proc_macro_hygiene, decl_macro)] Suppress compiler warnings about unused variables #![allow(unused_variables)] Outer attributes apply to the item that directly follows the attribute Allow pretty printing of a struct #[derive(Debug)] Suppress warnings about unused functions #[allow(dead_code) Box The most straightforward and commonly used smart pointer , allowing data to be stored on the heap rather than the stack. Boxes can be dereferenced just like references. let x = 5 ; let y = Box :: new ( x ); // Equivalent to: let y = &x; assert_eq! ( 5 , * y ); Cell Cell<T> is a smart pointer that allows mutation inside an immutable struct (\"interior mutability\"). To access the referenced value, unlike other pointers it does not take the dereferencing operator * but exposes various getter and setter methods such as get() , set() , etc. use std :: cell :: Cell ; fn main () { let num : i8 = 20 ; let cell : Cell < i8 > = Cell :: new ( num ); println! ( \"Value of number: {}\" , num ); println! ( \"Value of cell: {}\" , cell . get ()); } Immutable structs can be made partially mutable using Cells. In this example, the crew value is changed despite the fact that the object is immutable. Cell is used less often than the similar RefCell Cell RefCell use std :: cell :: Cell ; #[derive(Debug)] struct Starship { name : String , registry : String , crew : Cell < i16 > } fn main () { let enterprise = Starship { name : \"USS Enterprise\" . to_string (), registry : \"NCC-1701\" . to_string (), crew : Cell :: new ( 400 ) }; dbg! ( & enterprise ); enterprise . crew . set ( 405 ); dbg! ( & enterprise ); } use std :: cell :: RefCell ; #[derive(Debug)] struct Starship { name : String , registry : String , crew : RefCell < i16 > } fn main () { let enterprise = Starship { name : \"USS Enterprise\" . to_string (), registry : \"NCC-1701\" . to_string (), crew : RefCell :: new ( 400 ) }; dbg! ( & enterprise ); * enterprise . crew . borrow_mut () = 405 ; dbg! ( & enterprise ); } Clap Clap is a CLI framework. Also see structopt . Closure Closures are anonymous functions that can be saved in a variable or passed as arguments to functions. Closure definitions in Rust use pipe characters | to enclose the parameter list, followed by a code block. Because this code block is placed on the right side of a variable assignment statement, the closing curly bracket is followed by a semicolon. Type annotations are optional with closures because the compiler is typically able to infer type information from the context. Closure Function let do_stuff = | arg | { // ... }; fn do_stuff ( arg : u8 ) -> u32 { // ... } This variable is then called like a function. do_stuff ( \"bla\" ); Compute-expensive closures can be memoized by placing them in a struct that caches the resulting value: struct Cacher < T > where T : Fn ( u32 ) -> u32 { calculation : T , value : Option < u32 > , } impl < T > Cacher < T > where T : Fn ( u32 ) -> u32 { fn new ( calculation : T ) -> Cacher < T > { Cacher { calculation , value : None , } } fn value ( & mut self , arg : u32 ) -> u32 { match self . value { Some ( v ) => v , None => { let v = ( self . calculation )( arg ); self . value = Some ( v ); v } } } } Closures can access variables from their environment , or enclosing scope, something which functions are forbidden to do. Here, the compiler will raise an error when using the function and suggest the closure form in the error message. Function Closure fn main () { let x = 4 ; fn equal_to_x ( z : i8 ) -> bool { z == x } let y = 4 ; assert! ( equal_to_x ( y )); } fn main () { let x = 4 ; let equal_to_x ( z : i8 ) = | z | z == x ; let y = 4 ; assert! ( equal_to_x ( y )); } The move keyword makes a closure take ownership of all captured variables Here, using move produces a compile-time error because println!() attempts to borrow x after it is moved in the closure definition. Commenting this line removes the error. move Without move fn main () { let x = vec! [ 1 , 2 , 3 ]; let equal_to_x = move | z | z == x ; println! ( \"can't use x here: {:?}\" , x ); let y = vec! [ 1 , 2 , 3 ]; assert! ( equal_to_x ( y )); } fn main () { let x = vec! [ 1 , 2 , 3 ]; let equal_to_x = | z | z == x ; let y = vec! [ 1 , 2 , 3 ]; assert! ( equal_to_x ( y )); } Combinator Copy Copy is a trait that is implemented on simple types that are allocated on the stack alone. These types include integer and floating-point number types, char, bool, and fixed-size arrays and tuples. It can be implemented on other types very simply, with the use of derive . Crate A crate is a component of a package which produces a library or executable. There are two types of crate: Library crates , of which there may at most one in a package Binary crates , of which there may be many in a package The crate root is the source file that the compiler uses to create the root module of the crate. Custom derive One of the three types of macro in Rust that specifies code added with the derive attribute . Feature where the default implementation of a trait is generated by annotating a struct with an attribute . Here, #[derive(Debug)] supports the use of the {:?} placeholder for pretty-printing. #[derive(Debug)] struct Starship { name : String , registry : String , crew : i16 , captain : Officer , class : StarshipClass , } cfg The cfg macro is used for conditional compilation and evaluates configuration at compile-time. Debug-only code not to be used in release builds if cfg! debug_assertions ) { eprintln! ( \"debug: {:?} -> {:?}\" , record , fields ); } let my_directory = if cfg! ( windows ) { \"windows-specific-directory\" } else { \"unix-directory\" }; dbg Not to be confused with the Debug trait , which is used with the normal println!() macro ! dbg! allows evaluation and printing of expressions during debugging or running with cargo run . Code Output fn main () { let mut a : i32 = 0 ; while a < 100 { a += 1 ; dbg! ( a ); } println! ( \"Done\" ); } [src/main.rs:5] a = 1 [src/main.rs:5] a = 2 [src/main.rs:5] a = 3 [src/main.rs:5] a = 4 ... Enum The variants of an enum can have different types and associated data. enum IpAddr { V4 ( u8 , u8 , u8 , u8 ), V6 ( String ) } extern extern facilitates the creation and use of FFI . Here, the \"C\" ABI, which specifies how to call the function at the assembly level, is specified: extern \"C\" { fn abs ( input : i32 ) -> i32 ; } fn main () { unsafe { println! ( \"Absolute value of -3 according to C: {}\" , abs ( - 3 )); } } extern crate specifies a dependency on an external crate. This is no longer needed in Rust since 2018. futures Rust's main mechanism for asynchronous programming, implemented in the Tokio crate HashMap use std :: collections :: HashMap ; let mut scores = HashMap :: new (); scores . insert ( String :: from ( \"Blue\" ), 10 ); scores . insert ( String :: from ( \"Yellow\" ), 20 ); Hash maps are homogeneous: all keys must be of one type and all values of another. A hash map can be zipped from two vectors: use std :: collections :: HashMap ; let teams = vec! [ String :: from ( \"Blue\" ), String :: from ( \"Yellow\" )]; let initial_scores = vec! [ 10 , 50 ]; let scores : HashMap < _ , _ > = teams . iter (). zip ( initial_scores . iter ()). collect (); For non-Copy types, the hash map becomes the new owner of data values on assignment. if let if let is syntactic sugar for a pattern that matches one pattern while ignoring the rest. Notably, the syntax takes the pattern before the expression similar to a match arm. if let if let Some ( 3 ) = some_u8_value { println! ( \"three\" ); } match let some_u8_value = Some ( 0 u8 ); match some_u8_value { Some ( 3 ) => println! ( \"three\" ),, _ => (), } It is used to provide a default value for a string in the following example. fn main () { let mut name = String :: new (); if let Some ( s ) = std :: env :: args (). nth ( 1 ) { name = s ; } else { name = String :: from ( \"World\" ); } println! ( \"Hello, {}!\" , name ); } Iterator The iterator pattern is one that allows logic to be performed on a sequence of items in turn. An iterator in Rust is anything that implements the Iterator trait . This trait only requires implementation of a single method: next() , which returns one item of the iterator at a time wrapped in Some and None when the iterator is consumed. Many types return an iterator by calling the iter() method, which can then be iterated over using a for .. in loop. Alternatively, the next() method can be called directly. Loop next() let v1 = vec! [ 1 , 2 , 3 ]; for i in v1 . iter () { println! ( \"{}\" , i ); } let v1 = vec! [ 1 , 2 , 3 ]; let v1_iter = v1 . iter (); assert_eq! ( v1_iter . next (), Some ( & 1 )); assert_eq! ( v1_iter . next (), Some ( & 2 )); assert_eq! ( v1_iter . next (), Some ( & 3 )); assert_eq! ( v1_iter . next (), None ); Other methods are defined on the Iterator trait. Consuming iterators are those that call next() : sum collect transforms an iterator into a collection Iterator adapters change iterators into different kinds of iterators: map filter sum map fn iterator_sum () { let v1 = vec! [ 1 , 2 , 3 ]; let total : i32 = v1 . iter (). sum (); assert_eq! ( total , 6 ); } fn main () { let v1 = vec! [ 1 , 2 , 3 ]; let v2 : Vec < _ > = v1 . iter (). map ( | x | x + 1 ). collect (); let total : i32 = v2 . iter (). sum (); println! ( \"{}\" , total ); } Notably, a for loop consumes an iterator because it is implicitly converted to an iterator with into_iter() . Error Correct fn main () { let v = vec! [ 'H' , 'e' , 'l' , 'l' , 'o' ]; for i in v { print! ( \"{}\" , i ); } println! (); println! ( \"{:?}\" , v ); // (1) } Because each value of the vector is moved, it is consumed. The compiler will produce error E0382 at this line. fn main () { let v = vec! [ 'H' , 'e' , 'l' , 'l' , 'o' ]; for i in & v { print! ( \"{}\" , i ); } println! (); println! ( \"{:?}\" , v ); } Lifetimes Every reference must have a lifetime , which helps the compiler avoid dangling references , a known source of bugs and vulnerabilities. The Rust compiler has a borrow checker that compares scopes to ensure that all scopes are valid. But when a function has references to code from outside the function, it is impossible for Rust to determine the lifetimes of parameters or return values on its own. Reference parameters must be annotated with lifetime annotations with an unusual syntax using a single single-quotation character ' followed by a very short identifier, conventionally the letter a : & i32 // immutable reference &' a i32 // immutable reference with explicit lifetime &' a mut i32 // mutable reference with explicit lifetime However, lifetime annotations are only understood in a function signature where more than one is used. This example tells the compiler that the function takes two parameters and returns a value that all live at least as long as lifetime 'a . When concrete references are passed to this function, the smaller of the two concrete lifetimes passed in the arguments is substituted for the generic value: fn do_something <' a > ( x : & ' a str , y : & ' a str ) -> & ' a str { // --snip-- } The lifetime for the returned value must match that of one of the parameters. If not, the value would necessarily refer to a value created within the function, which would go out of scope at the end of the function and creating a dangling reference. Structs that hold references must also hold lifetime annotations and cannot outlive the referenced values. Function definitions also take the lifetime annotation on the impl keyword, i.e. impl<'a> struct method struct Starship <' a > { name : & ' a str } fn main () { let name = \"USS Enterprise\" ; let enterprise = Starship { name : & name }; println! ( \"{}\" , enterprise . name ); // => \"USS Enterprise\" } struct Starship <' a > { name : & ' a str , registry : & ' a str , } impl <' a > std :: fmt :: Display for Starship <' a > { fn fmt ( & self , f : & mut std :: fmt :: Formatter <' _ > ) -> std :: fmt :: Result { write! ( f , \"{} {}\" , self . name , self . registry ) } } fn main () { let name = \"USS Enterprise\" ; let registry = \"NCC-1701\" ; let enterprise = Starship { name : & name , registry : & registry }; println! ( \"{}\" , enterprise ); // => \"USS Enterprise NCC-1701\" } macro Referring to a family of features in Rust: Declarative macros with macro_rules! procedural macros : Custom #[derive] macros that specify code added with the derive attribute used on structs and enums Attribute -like macros that define custom attributes usable on any item Function-like macros that look like function calls but operate on the tokens specified as their argument [ assert!() ]](https://doc.rust-lang.org/std/macro.assert.html) invoke panic!() if the provided expression cannot be evaluated to true at runtime cfg!() compiles code based on compile-time evaluation of configuration dbg!() eprintln!() print to STDERR format!() concatenate strings panic!() terminates the program with code 101 and should be used when the program reaches an unrecoverable state println!() print to STDOUT [ unimplemented!() ]](https://doc.rust-lang.org/core/macro.unimplemented.html) try!() for which the ? operator is syntactic sugar match match is a powerful control flow operator that resembles a switch statement. match works on integers, ranges of integers, bools, enums, tuples, arrays, and structs. match allows a value to be compared against a series of patterns, which can be literal values as well as numerous other things. Each pattern is within a match arm , which is composed of the pattern and some code, here an expression. Recursive Fibonacci sequence implementation: if/else match 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 const FIB_ZERO : u64 = 1 ; const FIB_ONE : u64 = 1 ; fn fib ( n : u64 ) -> u64 { if n == 0 { FIB_ZERO } else if n == 1 { FIB_ONE } else { fib ( n - 1 ) + fib ( n - 2 ) } } fn main () { let n : u64 = std :: env :: args (). nth ( 1 ). unwrap (). parse (). unwrap (); for i in 1 .. n { println! ( \"{}: {}\" , i , fib ( i )); } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 const FIB_ZERO : u64 = 1 ; const FIB_ONE : u64 = 1 ; fn fib ( n : u64 ) -> u64 { match n { 0 => FIB_ZERO , 1 => FIB_ONE , _ => fib ( n - 1 ) + fib ( n - 2 ) } } fn main () { let n : u64 = std :: env :: args (). nth ( 1 ). unwrap (). parse (). unwrap (); for i in 1 .. n { println! ( \"{}: {}\" , i , fib ( i )); } } Patterns can also bind to parts of the values that match the pattern. Simple enum Nested enum 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 enum Coin { Penny , Nickel , Dime , Quarter } fn value_in_cents ( coin : Coin ) -> u8 { match coin { Coin :: Penny => 1 , Coin :: Nickel => 5 , Coin :: Dime => 10 , Coin :: Quarter => 25 , } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 enum Coin { Penny , Nickel , Dime , Quarter ( UsState ) } enum UsState { Alabama , Alaska , // --snip-- } fn value_in_cents ( coin : Coin ) -> u8 { match coin { Coin :: Penny => 1 , Coin :: Nickel => 5 , Coin :: Dime => 10 , Coin :: Quarter => { println! ( \"State quarter from {:?}!\" , state ); return 25 ; } } } In this example, the match statement only announces when a Coin::Quarter(state) is encountered, but all other cases are handled by the placeholder _ . The if let syntax is equivalent: match if let let mut count = 0 ; match coin { Coin :: Quarter ( state ) => println! ( \"State quarter from {:?}!\" , state ), _ => count += 1 , } let mut count = 0 ; if let Coin :: Quarter ( state ) = coin { println! ( \"State quarter from {:?}!\" , state ); } else { count += 1 ; } Option enum Option < T > { Some ( T ), None } Option serves as a wrapper around a value. Rust doesn't have the same implementation of null values that other languages do because handling null values is complicated and when unexpected they cause bugs. Rather than null values, Rust implements null as a variant None of the enum Option<T> The reason for this is because Rust conventionally handles enums in a match statement, which requires exhaustive enumeration of all possible cases. The compiler itself will raise an error if you compose a match statement which leaves some potential cases unhandled. Rc Rc<T> is a reference-counting pointer that will keep track of how many owners it has. This number is exposed using Rc::strong_count() , passing a reference to the Rc pointer being interrogated. This smart pointer allows multiple owners of the same value. However, the dereferenced value may not be modified because the DerefMut trait is not implemented for Rc. use std :: rc :: Rc ; fn main () { println! ( \"Hello, world!\" ); let num : i8 = 7 ; let pointer1 : Rc < i8 > = Rc :: new ( num ); let pointer2 : Rc < i8 > = Rc :: clone ( & pointer1 ); let pointer3 : Rc < i8 > = Rc :: clone ( & pointer2 ); * pointer3 += 1 ; // Error! println! ( \"Number of references: {}\" , Rc :: strong_count ( & pointer1 )); // 3 println! ( \"Value of num: {}\" , * pointer1 ); } To achieve interior mutability, Rc is paired with RefCell or Cell . RefCell RefCell is a mutable memory location with dynamically checked borrow rules (i.e. at runtime). Unlike the getter and setter methods of its cousin Cell , RefCell can be treated like any pointer by using the dereferencing operator * . RefCell does expose methods that determine how the wrapped value is returned. The borrow_mut() method is used to change the value of a RefCell which must be dereferenced because it returns a memory location. RefCell Cell use std :: cell :: RefCell ; #[derive(Debug)] struct Starship { name : String , registry : String , crew : RefCell < i16 > } fn main () { let enterprise = Starship { name : \"USS Enterprise\" . to_string (), registry : \"NCC-1701\" . to_string (), crew : RefCell :: new ( 400 ) }; dbg! ( & enterprise ); * enterprise . crew . borrow_mut () = 405 ; dbg! ( & enterprise ); } use std :: cell :: Cell ; #[derive(Debug)] struct Starship { name : String , registry : String , crew : Cell < i16 > } fn main () { let enterprise = Starship { name : \"USS Enterprise\" . to_string (), registry : \"NCC-1701\" . to_string (), crew : Cell :: new ( 400 ) }; dbg! ( & enterprise ); enterprise . crew . set ( 405 ); dbg! ( & enterprise ); } Here a mutable struct is owned by two Rc pointers, and a change applied through one pointer can be investigated using the other one. Note that there is no dereferencing operator because the Rc itself points to the RefCell which returns a memory location. use std :: rc :: Rc ; use std :: cell :: RefCell ; #[derive(Debug)] struct Starship { name : String , registry : String , crew : i16 } fn main () { let enterprise = Starship { name : \"USS Enterprise\" . to_string (), registry : \"NCC-1701\" . to_string (), crew : 400 }; let enterprise_ptr1 = Rc :: new ( RefCell :: new ( enterprise )); let enterprise_ptr2 = Rc :: clone ( & enterprise_ptr1 ); enterprise_ptr1 . borrow_mut (). crew = 405 ; dbg! ( & enterprise_ptr2 ); } I'm not sure what to make of this... Vector RefCell use std :: cell :: RefCell ; use std :: rc :: Rc ; fn main () { let v = vec! [ 'H' , 'e' , 'l' , 'l' , 'o' ]; for i in v { print! ( \"{}\" , i ); } println! ( \"\" ); } use std :: cell :: RefCell ; use std :: rc :: Rc ; fn main () { let ptr = Rc :: new ( RefCell :: new ( vec! [ 'H' , 'e' , 'l' , 'l' , 'o' ])); for i in &* ptr . borrow () { // (1) print! ( \"{}\" , i ); } println! ( \"\" ); } A for loop in Rust consumes the iterable's elements because it is really a syntactic sugar for a call to IntoIterator, which Vector implements. In other words the values are moved. Result See Error handling Smart pointer Smart pointers are data structures that act like references (which is but one of and the simplest pointer) but provide additional functionality and metadata. The smart pointer pattern is used frequently in Rust. In Rust, smart pointers are structs that implement two traits : Deref (which allows an instance of the smart pointer struct to behave like a reference) and Drop (which allows you to run custom logic when the pointer goes out of scope) Here a custom pointer is implemented. struct MyBox < T > ( T ) { data : String , } impl < T > MyBox < T > { fn new ( x : T ) -> MyBox < T > { MyBox ( x ) } } impl < T > std :: ops :: Deref for MyBox < T > { // Define an **associated type** for the Deref trait to use type Target = T ; fn deref ( & self ) -> & T { & self . 0 } } impl < T > Drop for MyBox < T > { fn drop ( & mut self ) { println! ( \"Dropping MyBox\" ); } } Now the smart pointer supports the dereferencing operator * and a message is displayed when it goes out of scope. fn main () { let x = 5 ; let y = MyBox :: new ( x ); assert_eq! ( 5 , * y ); // (1) } Behind the scenes, the compiler is really dereferencing the value returned by deref() : * ( y . deref ()) The drop() method cannot be called explicitly in order to avoid the double free error that would occur when the variable eventually goes out of scope. Alternatively, std::mem::drop() (already in the prelude) can be called explicitly. drop ( y ); Other smart pointers in the standard library include: Box Rc<T> which enables multiple owners of the same data and is used to count references, but only in single-threaded contexts RefCell<T> which enforces borrowing rules but only at runtime String The String type provided by Rust's standard library is implemented as a series of bytes and is distinct from string slices ( &str ) which are implemented in the core language. Strings can be initialized with the new() method just like vectors . String slices expose a to_string() method for conversion to a String. Alternatively, you can use String::from() to convert a string slice to a string. to_string String::from() let s = \"initial contents\" . to_string (); let s = String :: from ( \"initial contents\" ); Mutable strings can be concatenated with the push_str() method or with the + operator, which results in a move of the left operand and requires a reference for the right operand. The format! macro, which returns a String, is also available for more complicated concatenations. push_str + operator let mut s = String :: from ( \"Hello, \" ); s . push_str ( \"world!\" ); let s1 = String :: from ( \"Hello, \" ); let s2 = String :: from ( \"world!\" ); let s = s1 + & s2 ; Strings do not support indexing because they do not have the std::ops::Index trait . However, the chars() method returns an iterator that can be looped: for c in \"Hello, world!\" . chars () { println! ( \"{}\" , c ); } String slices can be generated from Strings using slice operator .. , which is equivalent to the : operator in languages like Python. let s = String :: from ( \"Hello, world!\" ); let w1 = s [ .. 5 ]; // equivalent to s[0..5] let w2 = s [ 7 .. ]; // equivalent to s[7..len] Because the compiler implicitly converts String to &str , as a practical matter functions that accept strings should be refactored to accept string slices. Strings must be initialized with the constructor. In the following example, for some reason, the compiler produces a \"borrow after move\" error if the constructor is not called. This might be because without the constructor, the Copy trait is not implemented in the initialized object. Compiler error No error 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 fn main () { let mut v : Vec < String > = Vec :: new (); loop { let mut input : String ; println! ( \"Enter to-do list item ('q' to quit): \" ); std :: io :: stdin (). read_line ( & mut input ). unwrap (); match input . trim () { \"q\" => break , s => v . push ( s . trim (). to_string ()), } } for i in v { println! ( \"{}\" , i ); } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 fn main () { let mut v : Vec < String > = Vec :: new (); loop { let mut input : String = String :: new (); println! ( \"Enter to-do list item ('q' to quit): \" ); std :: io :: stdin (). read_line ( & mut input ). unwrap (); match input . trim () { \"q\" => break , s => v . push ( s . trim (). to_string ()), } } for i in v { println! ( \"{}\" , i ); } } String methods: push_str append a string slice lines returns an iterator of string slices replace replace a pattern let s = String :: from ( \"Hello, World!\" ); let s = & s . replace ( \"World\" , \"Planet\" ); Struct ... structopt structopt is a CLI framework. Also see clap . Trait A trait defines functionality that can be shared with many types in a way that recalls dunder methods in Python. For example, default output to the terminal using println!() is implemented in the Display trait (analogous to the __str__ method in Python). Display trait struct Starship <' a > { // --snip-- } impl <' a > std :: fmt :: Display for Starship <' a > { fn fmt ( & self , f : & mut std :: fmt :: Formatter ) -> std :: fmt :: Result { write! ( f , \"{} {}\" , self . name , self . registry ) } } fn main () { let enterprise = Starship :: new (); println! ( enterprise ); } A trait defines the signature of a method intended to be implemented by many types, similar to virtual methods or interfaces. Traits are implemented in impl blocks that specify the type. trait Summary { fn summary ( & self ) -> String ; // (1) } impl Summary for NewsArticle { fn summary ( & self ) -> & str { format! ( \"{}, by {} ({})\" , self . headline , self . author , self . location ); } } A trait definition looks similar to the signature of a function with no code block. Default implementations of a trait can be provided in the trait definition. pub trait Summary { fn summarize ( & self ) -> String { String :: from ( \"(Read more...)\" ); } } Common traits include: Copy , Deref , Drop , Display , Fn , and Iterator Implementing a trait: #[derive(Debug)] struct Wrapper { wrapped : String } impl std :: convert :: From < String > for Wrapper { fn from ( item : String ) -> Self { Wrapper { wrapped : item } } } impl std :: convert :: From < Wrapper > for String { fn from ( item : Wrapper ) -> String { item . wrapped } } fn main () { let bar = String :: from ( \"Bar\" ); println! ( \"{:?}\" , Foo :: from ( bar )); let wrapper = Wrapper { wrapped : String :: from ( \"Hello, World!\" ) }; println! ( \"{}\" , String :: from ( wrapper )); } trait bound A trait bound allows functions to accept any type that implements a trait. Multiple traits by delimiting trait names with + . A simpler syntax accommodates simple cases by specifying the impl keyword followed by the trait name, rather than a concrete type. Trait bound Syntactic sugar pub fn notify < T : Summary > ( item : T ) { // ... pub fn notify < T : Summary + Display > ( item : T ) { // ... pub fn notify ( item : impl Summary ) { // ... pub fn notify ( item : impl Summary + Display ) { // ... Multiple trait bounds can clutter the function signature, reducing legibility. In these cases, a where clause can be used: fn some_function < T , U > ( t : T , u : U ) -> i32 where T : Display + Clone , U : Clone + Debug { // ... The impl Trait syntax is also available for return values: fn returns_summarizable () -> impl Summary { // ... Tuple struct Tuple structs are constructed using the struct keyword and function similar to named tuples in other languages. Their fields are not named but rather numbered. enum Cargo { Stuff , Things , Crap } impl From < Cargo > for String { fn from ( item : Cargo ) -> String { match item { Cargo :: Stuff => String :: from ( \"stuff\" ), Cargo :: Things => String :: from ( \"things\" ), Cargo :: Crap => String :: from ( \"crap\" ), } } } struct Ship ( Cargo ); struct Train ( Cargo ); struct Truck ( Cargo ); fn main () { let freight = Truck ( Cargo :: Stuff ); println! ( \"We got a truckload of {}\" , String :: from ( freight . 0 )); } Vector A vector is most often built using the vec! macro or by instantiating it and adding elements with push() method. let v = vec! [ 1 , 2 , 3 ]; let mut v = Vec :: new (); v . push ( 1 ); v . push ( 2 ); v . push ( 3 ); Referencing elements can be done with the index operator, which panics on an invalid index, or the get() method, which returns None without panicking. let invalid = & v [ 100 ]; let invalid = v . get ( 100 ); The for .. in loop works well with a vector. If vector elements are going to be changed, the reference must be made mutable and the dereference operator must be used. Immutable Mutable let v = vec! [ 100 , 32 , 57 ]; for i in & v { println! ( \"{}\" , i ); } let v = vec! [ 100 , 32 , 57 ]; for i in & mut v { * i += 50 ; } Although a vector's elements must be of the same type, because enum variants can be associated with a type and value they can be combined with match() to create collections with many types. enum SpreadsheetCell { Int ( i32 ), Float ( f64 ), Text ( String ) } let row = vec! [ SpreadsheetCell :: Int ( 3 ), SpreadsheetCell :: Text ( String :: from ( \"blue\" )), SpreadsheetCell :: Float ( 10.12 ), ] Methods: reserve reserves space in memory for more elements, greater than or equal to self.len() + argument ( usize )","title":"Glossary"},{"location":"Rust/Glossary/#glossary","text":"Associated type Associated types connect a type placeholder with a trait such that the trait method definitions use these placeholder types in their signatures. With generics, we must annotate the types in each implementation, but using an associated type forces a single implementation. Associated type Generic pub trait Iterator { type Item ; fn next ( & mut self ) -> Option < Self :: Item > ; } pub trait Iterator < T > { fn next ( & mut self ) -> Option < T > ; }","title":"Glossary"},{"location":"Rust/Glossary/#attribute","text":"There are several types of attribute: Inner attributes apply to the item that the attribute is declared in and begin with a shebang Feature flags #![feature(proc_macro_hygiene, decl_macro)] Suppress compiler warnings about unused variables #![allow(unused_variables)] Outer attributes apply to the item that directly follows the attribute Allow pretty printing of a struct #[derive(Debug)] Suppress warnings about unused functions #[allow(dead_code)","title":"Attribute"},{"location":"Rust/Glossary/#box","text":"The most straightforward and commonly used smart pointer , allowing data to be stored on the heap rather than the stack. Boxes can be dereferenced just like references. let x = 5 ; let y = Box :: new ( x ); // Equivalent to: let y = &x; assert_eq! ( 5 , * y );","title":"Box"},{"location":"Rust/Glossary/#cell","text":"Cell<T> is a smart pointer that allows mutation inside an immutable struct (\"interior mutability\"). To access the referenced value, unlike other pointers it does not take the dereferencing operator * but exposes various getter and setter methods such as get() , set() , etc. use std :: cell :: Cell ; fn main () { let num : i8 = 20 ; let cell : Cell < i8 > = Cell :: new ( num ); println! ( \"Value of number: {}\" , num ); println! ( \"Value of cell: {}\" , cell . get ()); } Immutable structs can be made partially mutable using Cells. In this example, the crew value is changed despite the fact that the object is immutable. Cell is used less often than the similar RefCell Cell RefCell use std :: cell :: Cell ; #[derive(Debug)] struct Starship { name : String , registry : String , crew : Cell < i16 > } fn main () { let enterprise = Starship { name : \"USS Enterprise\" . to_string (), registry : \"NCC-1701\" . to_string (), crew : Cell :: new ( 400 ) }; dbg! ( & enterprise ); enterprise . crew . set ( 405 ); dbg! ( & enterprise ); } use std :: cell :: RefCell ; #[derive(Debug)] struct Starship { name : String , registry : String , crew : RefCell < i16 > } fn main () { let enterprise = Starship { name : \"USS Enterprise\" . to_string (), registry : \"NCC-1701\" . to_string (), crew : RefCell :: new ( 400 ) }; dbg! ( & enterprise ); * enterprise . crew . borrow_mut () = 405 ; dbg! ( & enterprise ); }","title":"Cell"},{"location":"Rust/Glossary/#clap","text":"Clap is a CLI framework. Also see structopt .","title":"Clap"},{"location":"Rust/Glossary/#closure","text":"Closures are anonymous functions that can be saved in a variable or passed as arguments to functions. Closure definitions in Rust use pipe characters | to enclose the parameter list, followed by a code block. Because this code block is placed on the right side of a variable assignment statement, the closing curly bracket is followed by a semicolon. Type annotations are optional with closures because the compiler is typically able to infer type information from the context. Closure Function let do_stuff = | arg | { // ... }; fn do_stuff ( arg : u8 ) -> u32 { // ... } This variable is then called like a function. do_stuff ( \"bla\" ); Compute-expensive closures can be memoized by placing them in a struct that caches the resulting value: struct Cacher < T > where T : Fn ( u32 ) -> u32 { calculation : T , value : Option < u32 > , } impl < T > Cacher < T > where T : Fn ( u32 ) -> u32 { fn new ( calculation : T ) -> Cacher < T > { Cacher { calculation , value : None , } } fn value ( & mut self , arg : u32 ) -> u32 { match self . value { Some ( v ) => v , None => { let v = ( self . calculation )( arg ); self . value = Some ( v ); v } } } } Closures can access variables from their environment , or enclosing scope, something which functions are forbidden to do. Here, the compiler will raise an error when using the function and suggest the closure form in the error message. Function Closure fn main () { let x = 4 ; fn equal_to_x ( z : i8 ) -> bool { z == x } let y = 4 ; assert! ( equal_to_x ( y )); } fn main () { let x = 4 ; let equal_to_x ( z : i8 ) = | z | z == x ; let y = 4 ; assert! ( equal_to_x ( y )); } The move keyword makes a closure take ownership of all captured variables Here, using move produces a compile-time error because println!() attempts to borrow x after it is moved in the closure definition. Commenting this line removes the error. move Without move fn main () { let x = vec! [ 1 , 2 , 3 ]; let equal_to_x = move | z | z == x ; println! ( \"can't use x here: {:?}\" , x ); let y = vec! [ 1 , 2 , 3 ]; assert! ( equal_to_x ( y )); } fn main () { let x = vec! [ 1 , 2 , 3 ]; let equal_to_x = | z | z == x ; let y = vec! [ 1 , 2 , 3 ]; assert! ( equal_to_x ( y )); }","title":"Closure"},{"location":"Rust/Glossary/#combinator","text":"","title":"Combinator"},{"location":"Rust/Glossary/#copy","text":"Copy is a trait that is implemented on simple types that are allocated on the stack alone. These types include integer and floating-point number types, char, bool, and fixed-size arrays and tuples. It can be implemented on other types very simply, with the use of derive . Crate A crate is a component of a package which produces a library or executable. There are two types of crate: Library crates , of which there may at most one in a package Binary crates , of which there may be many in a package The crate root is the source file that the compiler uses to create the root module of the crate. Custom derive One of the three types of macro in Rust that specifies code added with the derive attribute . Feature where the default implementation of a trait is generated by annotating a struct with an attribute . Here, #[derive(Debug)] supports the use of the {:?} placeholder for pretty-printing. #[derive(Debug)] struct Starship { name : String , registry : String , crew : i16 , captain : Officer , class : StarshipClass , } cfg The cfg macro is used for conditional compilation and evaluates configuration at compile-time. Debug-only code not to be used in release builds if cfg! debug_assertions ) { eprintln! ( \"debug: {:?} -> {:?}\" , record , fields ); } let my_directory = if cfg! ( windows ) { \"windows-specific-directory\" } else { \"unix-directory\" };","title":"Copy"},{"location":"Rust/Glossary/#dbg","text":"Not to be confused with the Debug trait , which is used with the normal println!() macro ! dbg! allows evaluation and printing of expressions during debugging or running with cargo run . Code Output fn main () { let mut a : i32 = 0 ; while a < 100 { a += 1 ; dbg! ( a ); } println! ( \"Done\" ); } [src/main.rs:5] a = 1 [src/main.rs:5] a = 2 [src/main.rs:5] a = 3 [src/main.rs:5] a = 4 ... Enum The variants of an enum can have different types and associated data. enum IpAddr { V4 ( u8 , u8 , u8 , u8 ), V6 ( String ) }","title":"dbg"},{"location":"Rust/Glossary/#extern","text":"extern facilitates the creation and use of FFI . Here, the \"C\" ABI, which specifies how to call the function at the assembly level, is specified: extern \"C\" { fn abs ( input : i32 ) -> i32 ; } fn main () { unsafe { println! ( \"Absolute value of -3 according to C: {}\" , abs ( - 3 )); } } extern crate specifies a dependency on an external crate. This is no longer needed in Rust since 2018. futures Rust's main mechanism for asynchronous programming, implemented in the Tokio crate","title":"extern"},{"location":"Rust/Glossary/#hashmap","text":"use std :: collections :: HashMap ; let mut scores = HashMap :: new (); scores . insert ( String :: from ( \"Blue\" ), 10 ); scores . insert ( String :: from ( \"Yellow\" ), 20 ); Hash maps are homogeneous: all keys must be of one type and all values of another. A hash map can be zipped from two vectors: use std :: collections :: HashMap ; let teams = vec! [ String :: from ( \"Blue\" ), String :: from ( \"Yellow\" )]; let initial_scores = vec! [ 10 , 50 ]; let scores : HashMap < _ , _ > = teams . iter (). zip ( initial_scores . iter ()). collect (); For non-Copy types, the hash map becomes the new owner of data values on assignment.","title":"HashMap"},{"location":"Rust/Glossary/#if-let","text":"if let is syntactic sugar for a pattern that matches one pattern while ignoring the rest. Notably, the syntax takes the pattern before the expression similar to a match arm. if let if let Some ( 3 ) = some_u8_value { println! ( \"three\" ); } match let some_u8_value = Some ( 0 u8 ); match some_u8_value { Some ( 3 ) => println! ( \"three\" ),, _ => (), } It is used to provide a default value for a string in the following example. fn main () { let mut name = String :: new (); if let Some ( s ) = std :: env :: args (). nth ( 1 ) { name = s ; } else { name = String :: from ( \"World\" ); } println! ( \"Hello, {}!\" , name ); }","title":"if let"},{"location":"Rust/Glossary/#iterator","text":"The iterator pattern is one that allows logic to be performed on a sequence of items in turn. An iterator in Rust is anything that implements the Iterator trait . This trait only requires implementation of a single method: next() , which returns one item of the iterator at a time wrapped in Some and None when the iterator is consumed. Many types return an iterator by calling the iter() method, which can then be iterated over using a for .. in loop. Alternatively, the next() method can be called directly. Loop next() let v1 = vec! [ 1 , 2 , 3 ]; for i in v1 . iter () { println! ( \"{}\" , i ); } let v1 = vec! [ 1 , 2 , 3 ]; let v1_iter = v1 . iter (); assert_eq! ( v1_iter . next (), Some ( & 1 )); assert_eq! ( v1_iter . next (), Some ( & 2 )); assert_eq! ( v1_iter . next (), Some ( & 3 )); assert_eq! ( v1_iter . next (), None ); Other methods are defined on the Iterator trait. Consuming iterators are those that call next() : sum collect transforms an iterator into a collection Iterator adapters change iterators into different kinds of iterators: map filter sum map fn iterator_sum () { let v1 = vec! [ 1 , 2 , 3 ]; let total : i32 = v1 . iter (). sum (); assert_eq! ( total , 6 ); } fn main () { let v1 = vec! [ 1 , 2 , 3 ]; let v2 : Vec < _ > = v1 . iter (). map ( | x | x + 1 ). collect (); let total : i32 = v2 . iter (). sum (); println! ( \"{}\" , total ); } Notably, a for loop consumes an iterator because it is implicitly converted to an iterator with into_iter() . Error Correct fn main () { let v = vec! [ 'H' , 'e' , 'l' , 'l' , 'o' ]; for i in v { print! ( \"{}\" , i ); } println! (); println! ( \"{:?}\" , v ); // (1) } Because each value of the vector is moved, it is consumed. The compiler will produce error E0382 at this line. fn main () { let v = vec! [ 'H' , 'e' , 'l' , 'l' , 'o' ]; for i in & v { print! ( \"{}\" , i ); } println! (); println! ( \"{:?}\" , v ); }","title":"Iterator"},{"location":"Rust/Glossary/#lifetimes","text":"Every reference must have a lifetime , which helps the compiler avoid dangling references , a known source of bugs and vulnerabilities. The Rust compiler has a borrow checker that compares scopes to ensure that all scopes are valid. But when a function has references to code from outside the function, it is impossible for Rust to determine the lifetimes of parameters or return values on its own. Reference parameters must be annotated with lifetime annotations with an unusual syntax using a single single-quotation character ' followed by a very short identifier, conventionally the letter a : & i32 // immutable reference &' a i32 // immutable reference with explicit lifetime &' a mut i32 // mutable reference with explicit lifetime However, lifetime annotations are only understood in a function signature where more than one is used. This example tells the compiler that the function takes two parameters and returns a value that all live at least as long as lifetime 'a . When concrete references are passed to this function, the smaller of the two concrete lifetimes passed in the arguments is substituted for the generic value: fn do_something <' a > ( x : & ' a str , y : & ' a str ) -> & ' a str { // --snip-- } The lifetime for the returned value must match that of one of the parameters. If not, the value would necessarily refer to a value created within the function, which would go out of scope at the end of the function and creating a dangling reference. Structs that hold references must also hold lifetime annotations and cannot outlive the referenced values. Function definitions also take the lifetime annotation on the impl keyword, i.e. impl<'a> struct method struct Starship <' a > { name : & ' a str } fn main () { let name = \"USS Enterprise\" ; let enterprise = Starship { name : & name }; println! ( \"{}\" , enterprise . name ); // => \"USS Enterprise\" } struct Starship <' a > { name : & ' a str , registry : & ' a str , } impl <' a > std :: fmt :: Display for Starship <' a > { fn fmt ( & self , f : & mut std :: fmt :: Formatter <' _ > ) -> std :: fmt :: Result { write! ( f , \"{} {}\" , self . name , self . registry ) } } fn main () { let name = \"USS Enterprise\" ; let registry = \"NCC-1701\" ; let enterprise = Starship { name : & name , registry : & registry }; println! ( \"{}\" , enterprise ); // => \"USS Enterprise NCC-1701\" }","title":"Lifetimes"},{"location":"Rust/Glossary/#macro","text":"Referring to a family of features in Rust: Declarative macros with macro_rules! procedural macros : Custom #[derive] macros that specify code added with the derive attribute used on structs and enums Attribute -like macros that define custom attributes usable on any item Function-like macros that look like function calls but operate on the tokens specified as their argument [ assert!() ]](https://doc.rust-lang.org/std/macro.assert.html) invoke panic!() if the provided expression cannot be evaluated to true at runtime cfg!() compiles code based on compile-time evaluation of configuration dbg!() eprintln!() print to STDERR format!() concatenate strings panic!() terminates the program with code 101 and should be used when the program reaches an unrecoverable state println!() print to STDOUT [ unimplemented!() ]](https://doc.rust-lang.org/core/macro.unimplemented.html) try!() for which the ? operator is syntactic sugar","title":"macro"},{"location":"Rust/Glossary/#match","text":"match is a powerful control flow operator that resembles a switch statement. match works on integers, ranges of integers, bools, enums, tuples, arrays, and structs. match allows a value to be compared against a series of patterns, which can be literal values as well as numerous other things. Each pattern is within a match arm , which is composed of the pattern and some code, here an expression. Recursive Fibonacci sequence implementation: if/else match 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 const FIB_ZERO : u64 = 1 ; const FIB_ONE : u64 = 1 ; fn fib ( n : u64 ) -> u64 { if n == 0 { FIB_ZERO } else if n == 1 { FIB_ONE } else { fib ( n - 1 ) + fib ( n - 2 ) } } fn main () { let n : u64 = std :: env :: args (). nth ( 1 ). unwrap (). parse (). unwrap (); for i in 1 .. n { println! ( \"{}: {}\" , i , fib ( i )); } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 const FIB_ZERO : u64 = 1 ; const FIB_ONE : u64 = 1 ; fn fib ( n : u64 ) -> u64 { match n { 0 => FIB_ZERO , 1 => FIB_ONE , _ => fib ( n - 1 ) + fib ( n - 2 ) } } fn main () { let n : u64 = std :: env :: args (). nth ( 1 ). unwrap (). parse (). unwrap (); for i in 1 .. n { println! ( \"{}: {}\" , i , fib ( i )); } } Patterns can also bind to parts of the values that match the pattern. Simple enum Nested enum 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 enum Coin { Penny , Nickel , Dime , Quarter } fn value_in_cents ( coin : Coin ) -> u8 { match coin { Coin :: Penny => 1 , Coin :: Nickel => 5 , Coin :: Dime => 10 , Coin :: Quarter => 25 , } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 enum Coin { Penny , Nickel , Dime , Quarter ( UsState ) } enum UsState { Alabama , Alaska , // --snip-- } fn value_in_cents ( coin : Coin ) -> u8 { match coin { Coin :: Penny => 1 , Coin :: Nickel => 5 , Coin :: Dime => 10 , Coin :: Quarter => { println! ( \"State quarter from {:?}!\" , state ); return 25 ; } } } In this example, the match statement only announces when a Coin::Quarter(state) is encountered, but all other cases are handled by the placeholder _ . The if let syntax is equivalent: match if let let mut count = 0 ; match coin { Coin :: Quarter ( state ) => println! ( \"State quarter from {:?}!\" , state ), _ => count += 1 , } let mut count = 0 ; if let Coin :: Quarter ( state ) = coin { println! ( \"State quarter from {:?}!\" , state ); } else { count += 1 ; }","title":"match"},{"location":"Rust/Glossary/#option","text":"enum Option < T > { Some ( T ), None } Option serves as a wrapper around a value. Rust doesn't have the same implementation of null values that other languages do because handling null values is complicated and when unexpected they cause bugs. Rather than null values, Rust implements null as a variant None of the enum Option<T> The reason for this is because Rust conventionally handles enums in a match statement, which requires exhaustive enumeration of all possible cases. The compiler itself will raise an error if you compose a match statement which leaves some potential cases unhandled.","title":"Option"},{"location":"Rust/Glossary/#rc","text":"Rc<T> is a reference-counting pointer that will keep track of how many owners it has. This number is exposed using Rc::strong_count() , passing a reference to the Rc pointer being interrogated. This smart pointer allows multiple owners of the same value. However, the dereferenced value may not be modified because the DerefMut trait is not implemented for Rc. use std :: rc :: Rc ; fn main () { println! ( \"Hello, world!\" ); let num : i8 = 7 ; let pointer1 : Rc < i8 > = Rc :: new ( num ); let pointer2 : Rc < i8 > = Rc :: clone ( & pointer1 ); let pointer3 : Rc < i8 > = Rc :: clone ( & pointer2 ); * pointer3 += 1 ; // Error! println! ( \"Number of references: {}\" , Rc :: strong_count ( & pointer1 )); // 3 println! ( \"Value of num: {}\" , * pointer1 ); } To achieve interior mutability, Rc is paired with RefCell or Cell .","title":"Rc"},{"location":"Rust/Glossary/#refcell","text":"RefCell is a mutable memory location with dynamically checked borrow rules (i.e. at runtime). Unlike the getter and setter methods of its cousin Cell , RefCell can be treated like any pointer by using the dereferencing operator * . RefCell does expose methods that determine how the wrapped value is returned. The borrow_mut() method is used to change the value of a RefCell which must be dereferenced because it returns a memory location. RefCell Cell use std :: cell :: RefCell ; #[derive(Debug)] struct Starship { name : String , registry : String , crew : RefCell < i16 > } fn main () { let enterprise = Starship { name : \"USS Enterprise\" . to_string (), registry : \"NCC-1701\" . to_string (), crew : RefCell :: new ( 400 ) }; dbg! ( & enterprise ); * enterprise . crew . borrow_mut () = 405 ; dbg! ( & enterprise ); } use std :: cell :: Cell ; #[derive(Debug)] struct Starship { name : String , registry : String , crew : Cell < i16 > } fn main () { let enterprise = Starship { name : \"USS Enterprise\" . to_string (), registry : \"NCC-1701\" . to_string (), crew : Cell :: new ( 400 ) }; dbg! ( & enterprise ); enterprise . crew . set ( 405 ); dbg! ( & enterprise ); } Here a mutable struct is owned by two Rc pointers, and a change applied through one pointer can be investigated using the other one. Note that there is no dereferencing operator because the Rc itself points to the RefCell which returns a memory location. use std :: rc :: Rc ; use std :: cell :: RefCell ; #[derive(Debug)] struct Starship { name : String , registry : String , crew : i16 } fn main () { let enterprise = Starship { name : \"USS Enterprise\" . to_string (), registry : \"NCC-1701\" . to_string (), crew : 400 }; let enterprise_ptr1 = Rc :: new ( RefCell :: new ( enterprise )); let enterprise_ptr2 = Rc :: clone ( & enterprise_ptr1 ); enterprise_ptr1 . borrow_mut (). crew = 405 ; dbg! ( & enterprise_ptr2 ); } I'm not sure what to make of this... Vector RefCell use std :: cell :: RefCell ; use std :: rc :: Rc ; fn main () { let v = vec! [ 'H' , 'e' , 'l' , 'l' , 'o' ]; for i in v { print! ( \"{}\" , i ); } println! ( \"\" ); } use std :: cell :: RefCell ; use std :: rc :: Rc ; fn main () { let ptr = Rc :: new ( RefCell :: new ( vec! [ 'H' , 'e' , 'l' , 'l' , 'o' ])); for i in &* ptr . borrow () { // (1) print! ( \"{}\" , i ); } println! ( \"\" ); } A for loop in Rust consumes the iterable's elements because it is really a syntactic sugar for a call to IntoIterator, which Vector implements. In other words the values are moved.","title":"RefCell"},{"location":"Rust/Glossary/#result","text":"See Error handling","title":"Result"},{"location":"Rust/Glossary/#smart-pointer","text":"Smart pointers are data structures that act like references (which is but one of and the simplest pointer) but provide additional functionality and metadata. The smart pointer pattern is used frequently in Rust. In Rust, smart pointers are structs that implement two traits : Deref (which allows an instance of the smart pointer struct to behave like a reference) and Drop (which allows you to run custom logic when the pointer goes out of scope) Here a custom pointer is implemented. struct MyBox < T > ( T ) { data : String , } impl < T > MyBox < T > { fn new ( x : T ) -> MyBox < T > { MyBox ( x ) } } impl < T > std :: ops :: Deref for MyBox < T > { // Define an **associated type** for the Deref trait to use type Target = T ; fn deref ( & self ) -> & T { & self . 0 } } impl < T > Drop for MyBox < T > { fn drop ( & mut self ) { println! ( \"Dropping MyBox\" ); } } Now the smart pointer supports the dereferencing operator * and a message is displayed when it goes out of scope. fn main () { let x = 5 ; let y = MyBox :: new ( x ); assert_eq! ( 5 , * y ); // (1) } Behind the scenes, the compiler is really dereferencing the value returned by deref() : * ( y . deref ()) The drop() method cannot be called explicitly in order to avoid the double free error that would occur when the variable eventually goes out of scope. Alternatively, std::mem::drop() (already in the prelude) can be called explicitly. drop ( y ); Other smart pointers in the standard library include: Box Rc<T> which enables multiple owners of the same data and is used to count references, but only in single-threaded contexts RefCell<T> which enforces borrowing rules but only at runtime","title":"Smart pointer"},{"location":"Rust/Glossary/#string","text":"The String type provided by Rust's standard library is implemented as a series of bytes and is distinct from string slices ( &str ) which are implemented in the core language. Strings can be initialized with the new() method just like vectors . String slices expose a to_string() method for conversion to a String. Alternatively, you can use String::from() to convert a string slice to a string. to_string String::from() let s = \"initial contents\" . to_string (); let s = String :: from ( \"initial contents\" ); Mutable strings can be concatenated with the push_str() method or with the + operator, which results in a move of the left operand and requires a reference for the right operand. The format! macro, which returns a String, is also available for more complicated concatenations. push_str + operator let mut s = String :: from ( \"Hello, \" ); s . push_str ( \"world!\" ); let s1 = String :: from ( \"Hello, \" ); let s2 = String :: from ( \"world!\" ); let s = s1 + & s2 ; Strings do not support indexing because they do not have the std::ops::Index trait . However, the chars() method returns an iterator that can be looped: for c in \"Hello, world!\" . chars () { println! ( \"{}\" , c ); } String slices can be generated from Strings using slice operator .. , which is equivalent to the : operator in languages like Python. let s = String :: from ( \"Hello, world!\" ); let w1 = s [ .. 5 ]; // equivalent to s[0..5] let w2 = s [ 7 .. ]; // equivalent to s[7..len] Because the compiler implicitly converts String to &str , as a practical matter functions that accept strings should be refactored to accept string slices. Strings must be initialized with the constructor. In the following example, for some reason, the compiler produces a \"borrow after move\" error if the constructor is not called. This might be because without the constructor, the Copy trait is not implemented in the initialized object. Compiler error No error 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 fn main () { let mut v : Vec < String > = Vec :: new (); loop { let mut input : String ; println! ( \"Enter to-do list item ('q' to quit): \" ); std :: io :: stdin (). read_line ( & mut input ). unwrap (); match input . trim () { \"q\" => break , s => v . push ( s . trim (). to_string ()), } } for i in v { println! ( \"{}\" , i ); } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 fn main () { let mut v : Vec < String > = Vec :: new (); loop { let mut input : String = String :: new (); println! ( \"Enter to-do list item ('q' to quit): \" ); std :: io :: stdin (). read_line ( & mut input ). unwrap (); match input . trim () { \"q\" => break , s => v . push ( s . trim (). to_string ()), } } for i in v { println! ( \"{}\" , i ); } } String methods: push_str append a string slice lines returns an iterator of string slices replace replace a pattern let s = String :: from ( \"Hello, World!\" ); let s = & s . replace ( \"World\" , \"Planet\" );","title":"String"},{"location":"Rust/Glossary/#struct","text":"...","title":"Struct"},{"location":"Rust/Glossary/#structopt","text":"structopt is a CLI framework. Also see clap .","title":"structopt"},{"location":"Rust/Glossary/#trait","text":"A trait defines functionality that can be shared with many types in a way that recalls dunder methods in Python. For example, default output to the terminal using println!() is implemented in the Display trait (analogous to the __str__ method in Python). Display trait struct Starship <' a > { // --snip-- } impl <' a > std :: fmt :: Display for Starship <' a > { fn fmt ( & self , f : & mut std :: fmt :: Formatter ) -> std :: fmt :: Result { write! ( f , \"{} {}\" , self . name , self . registry ) } } fn main () { let enterprise = Starship :: new (); println! ( enterprise ); } A trait defines the signature of a method intended to be implemented by many types, similar to virtual methods or interfaces. Traits are implemented in impl blocks that specify the type. trait Summary { fn summary ( & self ) -> String ; // (1) } impl Summary for NewsArticle { fn summary ( & self ) -> & str { format! ( \"{}, by {} ({})\" , self . headline , self . author , self . location ); } } A trait definition looks similar to the signature of a function with no code block. Default implementations of a trait can be provided in the trait definition. pub trait Summary { fn summarize ( & self ) -> String { String :: from ( \"(Read more...)\" ); } } Common traits include: Copy , Deref , Drop , Display , Fn , and Iterator Implementing a trait: #[derive(Debug)] struct Wrapper { wrapped : String } impl std :: convert :: From < String > for Wrapper { fn from ( item : String ) -> Self { Wrapper { wrapped : item } } } impl std :: convert :: From < Wrapper > for String { fn from ( item : Wrapper ) -> String { item . wrapped } } fn main () { let bar = String :: from ( \"Bar\" ); println! ( \"{:?}\" , Foo :: from ( bar )); let wrapper = Wrapper { wrapped : String :: from ( \"Hello, World!\" ) }; println! ( \"{}\" , String :: from ( wrapper )); } trait bound A trait bound allows functions to accept any type that implements a trait. Multiple traits by delimiting trait names with + . A simpler syntax accommodates simple cases by specifying the impl keyword followed by the trait name, rather than a concrete type. Trait bound Syntactic sugar pub fn notify < T : Summary > ( item : T ) { // ... pub fn notify < T : Summary + Display > ( item : T ) { // ... pub fn notify ( item : impl Summary ) { // ... pub fn notify ( item : impl Summary + Display ) { // ... Multiple trait bounds can clutter the function signature, reducing legibility. In these cases, a where clause can be used: fn some_function < T , U > ( t : T , u : U ) -> i32 where T : Display + Clone , U : Clone + Debug { // ... The impl Trait syntax is also available for return values: fn returns_summarizable () -> impl Summary { // ...","title":"Trait"},{"location":"Rust/Glossary/#tuple-struct","text":"Tuple structs are constructed using the struct keyword and function similar to named tuples in other languages. Their fields are not named but rather numbered. enum Cargo { Stuff , Things , Crap } impl From < Cargo > for String { fn from ( item : Cargo ) -> String { match item { Cargo :: Stuff => String :: from ( \"stuff\" ), Cargo :: Things => String :: from ( \"things\" ), Cargo :: Crap => String :: from ( \"crap\" ), } } } struct Ship ( Cargo ); struct Train ( Cargo ); struct Truck ( Cargo ); fn main () { let freight = Truck ( Cargo :: Stuff ); println! ( \"We got a truckload of {}\" , String :: from ( freight . 0 )); }","title":"Tuple struct"},{"location":"Rust/Glossary/#vector","text":"A vector is most often built using the vec! macro or by instantiating it and adding elements with push() method. let v = vec! [ 1 , 2 , 3 ]; let mut v = Vec :: new (); v . push ( 1 ); v . push ( 2 ); v . push ( 3 ); Referencing elements can be done with the index operator, which panics on an invalid index, or the get() method, which returns None without panicking. let invalid = & v [ 100 ]; let invalid = v . get ( 100 ); The for .. in loop works well with a vector. If vector elements are going to be changed, the reference must be made mutable and the dereference operator must be used. Immutable Mutable let v = vec! [ 100 , 32 , 57 ]; for i in & v { println! ( \"{}\" , i ); } let v = vec! [ 100 , 32 , 57 ]; for i in & mut v { * i += 50 ; } Although a vector's elements must be of the same type, because enum variants can be associated with a type and value they can be combined with match() to create collections with many types. enum SpreadsheetCell { Int ( i32 ), Float ( f64 ), Text ( String ) } let row = vec! [ SpreadsheetCell :: Int ( 3 ), SpreadsheetCell :: Text ( String :: from ( \"blue\" )), SpreadsheetCell :: Float ( 10.12 ), ] Methods: reserve reserves space in memory for more elements, greater than or equal to self.len() + argument ( usize )","title":"Vector"},{"location":"Rust/Scratchpad/","text":"Scratchpad Errors fn main () { do_error (). expect ( \"Error!\" ); } fn do_error () -> Result < String , String > { Err ( \"This error is on purpose!\" . to_string ()) }","title":"Scratchpad"},{"location":"Rust/Scratchpad/#scratchpad","text":"","title":"Scratchpad"},{"location":"Rust/Scratchpad/#errors","text":"fn main () { do_error (). expect ( \"Error!\" ); } fn do_error () -> Result < String , String > { Err ( \"This error is on purpose!\" . to_string ()) }","title":"Errors"},{"location":"Rust/Crates/Actix/","text":"actix Tasks Hello, World! use actix_web :: { App , HttpServer }; use actix_web :: { web , HttpResponse , Responder }; #[actix_web::main] async fn main () -> std :: io :: Result < () > { HttpServer :: new ( || { App :: new () . route ( \"/\" , web :: get (). to ( hello )) }) . bind (( \"127.0.0.1\" , 8000 )) ? // (1) . run () . await } async fn hello () -> impl Responder { HttpResponse :: Ok (). body ( \"Hello, World!\" ) } Note the address and port can be defined as a tuple or as a string . bind ( \"127.0.0.1:8000\" ) ? Static file server use actix_web :: { App , HttpServer }; use actix_files :: Files ; #[actix_web::main] async fn main () -> std :: io :: Result < () > { HttpServer :: new ( || { App :: new () . service ( Files :: new ( \"/\" , \"./static/\" ). index_file ( \"index.html\" )) . wrap ( actix_web :: middleware :: Logger :: default ()) }) . bind (( \"127.0.0.1\" , 8000 )) ? . run () . await }","title":"actix"},{"location":"Rust/Crates/Actix/#actix","text":"","title":"actix"},{"location":"Rust/Crates/Actix/#tasks","text":"","title":"Tasks"},{"location":"Rust/Crates/Actix/#hello-world","text":"use actix_web :: { App , HttpServer }; use actix_web :: { web , HttpResponse , Responder }; #[actix_web::main] async fn main () -> std :: io :: Result < () > { HttpServer :: new ( || { App :: new () . route ( \"/\" , web :: get (). to ( hello )) }) . bind (( \"127.0.0.1\" , 8000 )) ? // (1) . run () . await } async fn hello () -> impl Responder { HttpResponse :: Ok (). body ( \"Hello, World!\" ) } Note the address and port can be defined as a tuple or as a string . bind ( \"127.0.0.1:8000\" ) ?","title":"Hello, World!"},{"location":"Rust/Crates/Actix/#static-file-server","text":"use actix_web :: { App , HttpServer }; use actix_files :: Files ; #[actix_web::main] async fn main () -> std :: io :: Result < () > { HttpServer :: new ( || { App :: new () . service ( Files :: new ( \"/\" , \"./static/\" ). index_file ( \"index.html\" )) . wrap ( actix_web :: middleware :: Logger :: default ()) }) . bind (( \"127.0.0.1\" , 8000 )) ? . run () . await }","title":"Static file server"},{"location":"Rust/Crates/Clap/","text":"clap Clap is a command-line parser with both declarative (preferred, using Parser ) and procedural APIs. Declarative Procedural use clap :: Parser ; // (1) #[derive(Parser)] #[clap(name = \"Hello, World!\" , version, author)] // (2) struct Cli { #[clap(short, long, default_value_t = String::from( \"Hello\" ))] greeting : String , #[clap(default_value_t = String::from( \"World\" ))] name : String , } fn main () { let cli = Cli :: parse (); println! ( \"{}, {}!\" , cli . greeting , cli . name ); } Parser requires the derive and std features: Cargo.toml clap = { version = \"3.0.12\" , features = [ \"std\" , \"derive\" ], default-features = false } Without providing arguments to version or author, the relevant values will be pulled from the crate itself, similar to the crate_authors and crate_version macros for the procedural API. use clap :: { App , Arg , crate_version , crate_authors }; fn main () { let args = App :: new ( \"Say hello\" ) . version ( crate_version ! ()) // (1) . author ( crate_authors ! ()) . arg ( Arg :: new ( \"name\" ) // (2) . default_value ( \"World\" ) . arg ( Arg :: new ( \"greeting\" ) . default_value ( \"Hello\" ) . short ( 'g' ) . long ( \"greeting\" ) . get_matches (); println! ( \"{}, {}!\" , args . value_of ( \"greeting\" ). unwrap (), args . value_of ( \"name\" ). unwrap ()); } The crate_authors and crate_version macros allow you to pull information from the Cargo.toml at compile time. Since clap 3.0.0, new() takes the place of with_name() , which is now deprecated. API Parser Using the Parser derive allows command-line arguments and options to be defined on a struct with an attribute. Any Command , Arg , or PossibleValue method can be used as an attribute. Option Command use clap :: Parser ; #[derive(Parser)] #[clap(name = \"Hello, World!\" )] struct Args { #[clap(short, long, default_value_t = String::from( \"Hello\" ))] greeting : String , #[clap(default_value_t = String::from( \"World\" ))] name : String , } fn main () { let args = Args :: parse (); println! ( \"{}, {}!\" , args . greeting , args . name ); } use clap :: { Parser , Subcommand }; #[derive(Parser)] #[clap(name = \"Hello, World!\" )] struct Args { #[clap(subcommand)] greeting : Greeting , #[clap(default_value_t = String::from( \"World\" ))] name : String } #[derive(Subcommand)] enum Greeting { Hello , Greetings } fn main () { let args = Args :: parse (); println! ( \"{}, {}!\" , args . greeting , args . name ); // (1) } This requires the Display trait to be implemented impl std :: fmt :: Display for Greeting { fn fmt ( & self , _ : & mut std :: fmt :: Formatter ) -> std :: fmt :: Result { match self { Greeting :: Hello => { print! ( \"Hello\" ); Ok (()) }, _ => { print! ( \"Greetings\" ); Ok (()) } } } } clap_app The (now deprecated) clap_app macro was used to create simple applications. Boolean values can be set to true with + and false with ! : + required // Arg::required(true) ! required // Arg::required(false) API Parser ArgEnum and Subcommand can be used in very similar ways. ArgEnum variants do show up in the help output but inline with the Subcommand. ArgEnum Subcommand use clap :: { ArgEnum , Parser }; #[derive(Parser)] struct Cli { #[clap(arg_enum)] command : Actions , } #[derive(Copy, Clone, ArgEnum)] enum Actions { Eat , Drink , } fn main () { let cli = Cli :: parse (); match cli . command { Actions :: Eat => println! ( \"Eating\" ), Actions :: Drink => println! ( \"Drinking\" ), } } use clap :: { Subcommand , Parser }; #[derive(Parser)] struct Cli { #[clap(subcommand)] command : Actions , } #[derive(Subcommand)] enum Actions { Eat , Drink , } fn main () { let cli = Cli :: parse (); match cli . command { Actions :: Eat => println! ( \"Eating\" ), Actions :: Drink => println! ( \"Drinking\" ), } } Apparently they may not be used together, enums with a Subcommand derive attribute require variants that contain Args derived values: use clap :: { Args , Parser , Subcommand }; #[derive(Parser)] struct Cli { #[clap(subcommand)] command : Actions , } #[derive(Subcommand)] enum Actions { #[clap(arg_enum)] Eat ( Foods ), #[clap(arg_enum)] Drink ( Drinks ), } #[derive(Args)] struct Foods { name : String , } #[derive(Args)] struct Drinks { name : String } fn main () { let cli = Cli :: parse (); match cli . command { Actions :: Eat ( f ) => { println! ( \"Eating {}\" , f . name ); } Actions :: Drink ( d ) => { println! ( \"Drinking {}\" , d . name ); } } } use std :: string :: ParseError ; use clap :: { ArgEnum , Args , Parser , Subcommand , }; #[derive(Parser)] struct Cli { #[clap(subcommand)] command : Actions , } #[derive(Subcommand)] enum Actions { #[clap(arg_enum)] Eat ( Food ), #[clap(arg_enum)] Drink ( Drink ), } #[derive(Args)] struct Food { name : String , } #[derive(Args)] struct Drink { drink : Drinks } #[derive(ArgEnum, Clone)] enum Drinks { Coke , Pepsi , Other } impl std :: str :: FromStr for Drinks { type Err = ParseError ; fn from_str ( s : & str ) -> Result < Self , Self :: Err > { match s { \"coke\" => Ok ( Self :: Coke ), \"pepsi\" => Ok ( Self :: Pepsi ), _ => Ok ( Self :: Other ), } } } impl From < Drinks > for String { fn from ( item : Drinks ) -> String { match item { Drinks :: Coke => String :: from ( \"Coke\" ), Drinks :: Pepsi => String :: from ( \"Pepsi\" ), _ => String :: from ( \"Other\" ) } } } fn main () { let cli = Cli :: parse (); match cli . command { Actions :: Eat ( f ) => { println! ( \"Eating {}\" , f . name ); } Actions :: Drink ( d ) => { println! ( \"Drinking {}\" , String :: from ( d . drink )); } } } Starships #[macro_use] extern crate diesel ; use diesel :: prelude :: * ; use diesel :: result :: QueryResult ; use diesel :: sqlite :: SqliteConnection ; mod models ; // (1) use models :: Starship ; mod schema ; // (2) use schema :: starships :: dsl :: * ; use clap :: { Parser , Subcommand }; #[derive(Parser)] struct Cli { #[clap(subcommand)] command : Commands , } #[derive(Subcommand)] enum Commands { Add ( Starship ), Update , Remove , List , } fn main () { let app = Cli :: parse (); match app . command { Commands :: Add ( s ) => add_ship ( & s ), Commands :: List => list_ships (), Commands :: Remove => println! ( \"Removing\" ), Commands :: Update => println! ( \"Updating\" ), } } fn add_ship ( s : & Starship ) { let conn = get_connection (). unwrap (); println! ( \"Adding {:?}\" , s ); s . insert_into ( starships ) . execute ( & conn ) . unwrap (); } fn list_ships () { println! ( \"{:?}\" , get_ships (). unwrap ()); } fn get_connection () -> ConnectionResult < SqliteConnection > { dotenv :: dotenv (). expect ( \"Couldn't load .env file\" ); let url = & std :: env :: var ( \"DATABASE_URL\" ). unwrap (); SqliteConnection :: establish ( url ) } fn get_ships () -> QueryResult < Vec < Starship >> { let conn = get_connection (). unwrap (); starships . load :: < Starship > ( & conn ) } Not that the order of the fields matters (for both postgres as well as sqlite connections), and the primary key should be the first field defined. use crate :: schema :: starships ; use clap :: Args ; #[derive(Args,Debug, Queryable, Insertable, Identifiable, Clone)] #[primary_key(registry)] pub struct Starship { #[clap(long, short)] pub registry : String , #[clap(long, short)] pub name : String , #[clap(long, short)] pub crew : i32 , } 2. table ! { starships ( registry ) { registry -> Text , name -> Text , crew -> Integer , } }","title":"clap"},{"location":"Rust/Crates/Clap/#clap","text":"Clap is a command-line parser with both declarative (preferred, using Parser ) and procedural APIs. Declarative Procedural use clap :: Parser ; // (1) #[derive(Parser)] #[clap(name = \"Hello, World!\" , version, author)] // (2) struct Cli { #[clap(short, long, default_value_t = String::from( \"Hello\" ))] greeting : String , #[clap(default_value_t = String::from( \"World\" ))] name : String , } fn main () { let cli = Cli :: parse (); println! ( \"{}, {}!\" , cli . greeting , cli . name ); } Parser requires the derive and std features: Cargo.toml clap = { version = \"3.0.12\" , features = [ \"std\" , \"derive\" ], default-features = false } Without providing arguments to version or author, the relevant values will be pulled from the crate itself, similar to the crate_authors and crate_version macros for the procedural API. use clap :: { App , Arg , crate_version , crate_authors }; fn main () { let args = App :: new ( \"Say hello\" ) . version ( crate_version ! ()) // (1) . author ( crate_authors ! ()) . arg ( Arg :: new ( \"name\" ) // (2) . default_value ( \"World\" ) . arg ( Arg :: new ( \"greeting\" ) . default_value ( \"Hello\" ) . short ( 'g' ) . long ( \"greeting\" ) . get_matches (); println! ( \"{}, {}!\" , args . value_of ( \"greeting\" ). unwrap (), args . value_of ( \"name\" ). unwrap ()); } The crate_authors and crate_version macros allow you to pull information from the Cargo.toml at compile time. Since clap 3.0.0, new() takes the place of with_name() , which is now deprecated.","title":"clap"},{"location":"Rust/Crates/Clap/#api","text":"","title":"API"},{"location":"Rust/Crates/Clap/#parser","text":"Using the Parser derive allows command-line arguments and options to be defined on a struct with an attribute. Any Command , Arg , or PossibleValue method can be used as an attribute. Option Command use clap :: Parser ; #[derive(Parser)] #[clap(name = \"Hello, World!\" )] struct Args { #[clap(short, long, default_value_t = String::from( \"Hello\" ))] greeting : String , #[clap(default_value_t = String::from( \"World\" ))] name : String , } fn main () { let args = Args :: parse (); println! ( \"{}, {}!\" , args . greeting , args . name ); } use clap :: { Parser , Subcommand }; #[derive(Parser)] #[clap(name = \"Hello, World!\" )] struct Args { #[clap(subcommand)] greeting : Greeting , #[clap(default_value_t = String::from( \"World\" ))] name : String } #[derive(Subcommand)] enum Greeting { Hello , Greetings } fn main () { let args = Args :: parse (); println! ( \"{}, {}!\" , args . greeting , args . name ); // (1) } This requires the Display trait to be implemented impl std :: fmt :: Display for Greeting { fn fmt ( & self , _ : & mut std :: fmt :: Formatter ) -> std :: fmt :: Result { match self { Greeting :: Hello => { print! ( \"Hello\" ); Ok (()) }, _ => { print! ( \"Greetings\" ); Ok (()) } } } }","title":"Parser"},{"location":"Rust/Crates/Clap/#clap_app","text":"The (now deprecated) clap_app macro was used to create simple applications. Boolean values can be set to true with + and false with ! : + required // Arg::required(true) ! required // Arg::required(false)","title":"clap_app"},{"location":"Rust/Crates/Clap/#api_1","text":"","title":"API"},{"location":"Rust/Crates/Clap/#parser_1","text":"ArgEnum and Subcommand can be used in very similar ways. ArgEnum variants do show up in the help output but inline with the Subcommand. ArgEnum Subcommand use clap :: { ArgEnum , Parser }; #[derive(Parser)] struct Cli { #[clap(arg_enum)] command : Actions , } #[derive(Copy, Clone, ArgEnum)] enum Actions { Eat , Drink , } fn main () { let cli = Cli :: parse (); match cli . command { Actions :: Eat => println! ( \"Eating\" ), Actions :: Drink => println! ( \"Drinking\" ), } } use clap :: { Subcommand , Parser }; #[derive(Parser)] struct Cli { #[clap(subcommand)] command : Actions , } #[derive(Subcommand)] enum Actions { Eat , Drink , } fn main () { let cli = Cli :: parse (); match cli . command { Actions :: Eat => println! ( \"Eating\" ), Actions :: Drink => println! ( \"Drinking\" ), } } Apparently they may not be used together, enums with a Subcommand derive attribute require variants that contain Args derived values: use clap :: { Args , Parser , Subcommand }; #[derive(Parser)] struct Cli { #[clap(subcommand)] command : Actions , } #[derive(Subcommand)] enum Actions { #[clap(arg_enum)] Eat ( Foods ), #[clap(arg_enum)] Drink ( Drinks ), } #[derive(Args)] struct Foods { name : String , } #[derive(Args)] struct Drinks { name : String } fn main () { let cli = Cli :: parse (); match cli . command { Actions :: Eat ( f ) => { println! ( \"Eating {}\" , f . name ); } Actions :: Drink ( d ) => { println! ( \"Drinking {}\" , d . name ); } } } use std :: string :: ParseError ; use clap :: { ArgEnum , Args , Parser , Subcommand , }; #[derive(Parser)] struct Cli { #[clap(subcommand)] command : Actions , } #[derive(Subcommand)] enum Actions { #[clap(arg_enum)] Eat ( Food ), #[clap(arg_enum)] Drink ( Drink ), } #[derive(Args)] struct Food { name : String , } #[derive(Args)] struct Drink { drink : Drinks } #[derive(ArgEnum, Clone)] enum Drinks { Coke , Pepsi , Other } impl std :: str :: FromStr for Drinks { type Err = ParseError ; fn from_str ( s : & str ) -> Result < Self , Self :: Err > { match s { \"coke\" => Ok ( Self :: Coke ), \"pepsi\" => Ok ( Self :: Pepsi ), _ => Ok ( Self :: Other ), } } } impl From < Drinks > for String { fn from ( item : Drinks ) -> String { match item { Drinks :: Coke => String :: from ( \"Coke\" ), Drinks :: Pepsi => String :: from ( \"Pepsi\" ), _ => String :: from ( \"Other\" ) } } } fn main () { let cli = Cli :: parse (); match cli . command { Actions :: Eat ( f ) => { println! ( \"Eating {}\" , f . name ); } Actions :: Drink ( d ) => { println! ( \"Drinking {}\" , String :: from ( d . drink )); } } }","title":"Parser"},{"location":"Rust/Crates/Clap/#starships","text":"#[macro_use] extern crate diesel ; use diesel :: prelude :: * ; use diesel :: result :: QueryResult ; use diesel :: sqlite :: SqliteConnection ; mod models ; // (1) use models :: Starship ; mod schema ; // (2) use schema :: starships :: dsl :: * ; use clap :: { Parser , Subcommand }; #[derive(Parser)] struct Cli { #[clap(subcommand)] command : Commands , } #[derive(Subcommand)] enum Commands { Add ( Starship ), Update , Remove , List , } fn main () { let app = Cli :: parse (); match app . command { Commands :: Add ( s ) => add_ship ( & s ), Commands :: List => list_ships (), Commands :: Remove => println! ( \"Removing\" ), Commands :: Update => println! ( \"Updating\" ), } } fn add_ship ( s : & Starship ) { let conn = get_connection (). unwrap (); println! ( \"Adding {:?}\" , s ); s . insert_into ( starships ) . execute ( & conn ) . unwrap (); } fn list_ships () { println! ( \"{:?}\" , get_ships (). unwrap ()); } fn get_connection () -> ConnectionResult < SqliteConnection > { dotenv :: dotenv (). expect ( \"Couldn't load .env file\" ); let url = & std :: env :: var ( \"DATABASE_URL\" ). unwrap (); SqliteConnection :: establish ( url ) } fn get_ships () -> QueryResult < Vec < Starship >> { let conn = get_connection (). unwrap (); starships . load :: < Starship > ( & conn ) } Not that the order of the fields matters (for both postgres as well as sqlite connections), and the primary key should be the first field defined. use crate :: schema :: starships ; use clap :: Args ; #[derive(Args,Debug, Queryable, Insertable, Identifiable, Clone)] #[primary_key(registry)] pub struct Starship { #[clap(long, short)] pub registry : String , #[clap(long, short)] pub name : String , #[clap(long, short)] pub crew : i32 , } 2. table ! { starships ( registry ) { registry -> Text , name -> Text , crew -> Integer , } }","title":"Starships"},{"location":"Rust/Crates/Failure/","text":"failure The failure crate is an error-handling library API Fail The Fail trait extends errors with a variety of traits and methods. use failure_derive :: Fail ; #[derive(Fail)] enum CustomErr { #[fail(display= \"Foo error\" )] CustomErrorFoo ( String ), #[fail(display= \"Bar error\" )] CustomErrorBar ( String ), }","title":"failure"},{"location":"Rust/Crates/Failure/#failure","text":"The failure crate is an error-handling library","title":"failure"},{"location":"Rust/Crates/Failure/#api","text":"","title":"API"},{"location":"Rust/Crates/Failure/#fail","text":"The Fail trait extends errors with a variety of traits and methods. use failure_derive :: Fail ; #[derive(Fail)] enum CustomErr { #[fail(display= \"Foo error\" )] CustomErrorFoo ( String ), #[fail(display= \"Bar error\" )] CustomErrorBar ( String ), }","title":"Fail"},{"location":"Rust/Crates/Gtk-rs/","text":"gtk-rs Tasks Development environment Red Hat dnf install gtk4-devel gcc Ubuntu apt install libgtk-4-dev build-essential Boilerplate Interface <?xml version=\"1.0\" encoding=\"UTF-8\"?> <interface> <requires lib= \"gtk+\" version= \"3.40\" /> <object class= \"GtkApplicationWindow\" id= \"window\" > <property name= \"title\" > My GTK App </property> <property name= \"default-width\" > 300 </property> <property name= \"default-height\" > 300 </property> </object> </interface> use gtk4 :: { Application , ApplicationWindow }; fn main () { let app = Application :: builder () . application_id ( \"com.example.learning-gtk\" ) . build (); app . connect_activate ( build_ui ); app . run (); } fn build_ui ( app : & Application ) { let window = ApplicationWindow :: builder () . application ( app ) . default_width ( 300 ) . default_height ( 300 ) . title ( \"My GTK App\" ) . build (); window . present (); } Clicker This appears to be a common demonstration of data binding in various GUI frameworks, the code below is taken from here . use gtk4 :: prelude :: * ; use gtk4 :: { glib , Application , ApplicationWindow , Box , Button , Orientation }; use std :: { cell :: Cell , rc :: Rc }; fn main () { // Create a new application let app = Application :: builder () . application_id ( \"org.gtk-rs.example\" ) . build (); // Connect to \"activate\" signal of `app` app . connect_activate ( build_ui ); // Run the application app . run (); } fn build_ui ( app : & Application ) { // Create two buttons let button_increase = Button :: builder () . label ( \"Increase\" ) . margin_top ( 12 ) . margin_bottom ( 12 ) . margin_start ( 12 ) . margin_end ( 12 ) . build (); let button_decrease = Button :: builder () . label ( \"Decrease\" ) . margin_top ( 12 ) . margin_bottom ( 12 ) . margin_start ( 12 ) . margin_end ( 12 ) . build (); // Reference-counted object with inner mutability let number = Rc :: new ( Cell :: new ( 0 )); // (1) // Connect callbacks // When a button is clicked, `number` and label of the other button will be changed button_increase . connect_clicked ( glib :: clone ! ( @ weak number , @ weak button_decrease => // (2) move | _ | { number . set ( number . get () + 1 ); button_decrease . set_label ( & number . get (). to_string ()); })); button_decrease . connect_clicked ( glib :: clone ! ( @ weak button_increase => // (3) move | _ | { number . set ( number . get () - 1 ); button_increase . set_label ( & number . get (). to_string ()); })); // Add buttons to `gtk_box` let gtk_box = Box :: builder (). orientation ( Orientation :: Vertical ). build (); gtk_box . append ( & button_increase ); gtk_box . append ( & button_decrease ); // Create a window let window = ApplicationWindow :: builder () . application ( app ) . title ( \"My GTK App\" ) . child ( & gtk_box ) . build (); // Present the window window . present (); } It appears number must be an Rc<Cell>. Changing it to a raw i8 causes compilation errors within glib::clone . Removing the weak reference to number causes a compilation error. Adding a weak reference to number here also causes a compilation error. Hello, World! Window frame TODO At the moment, this example is broken because I don't know how to pass the string into the Application struct for string interpolation. use gtk4 :: prelude :: * ; use gtk4 :: { Application , ApplicationWindow }; fn main () { let name = String :: new (); if let Some ( s ) = 42 std :: env :: args (). nth ( 1 ) { name = s ; } else { name = String :: from ( \"World\" ); }; let app = Application :: builder () . application_id ( \"com.example.learning-gtk\" ) . build (); app . connect_activate ( build_ui ); println! ( \"{}\" , app . name ); app . run (); } fn build_ui ( app : & Application ) { let window = ApplicationWindow :: builder () . application ( app ) . title ( \"Hello, World!\" ) . default_height ( 300 ) . default_width ( 300 ) . build (); window . present (); } Button reveal Interface <?xml version=\"1.0\" encoding=\"UTF-8\"?> <interface> <requires lib= \"gtk+\" version= \"3.40\" > <object class= \"GtkApplicationWindow\" id= \"window\" > <property name= \"title\" > My GTK App </property> <property name= \"default-width\" > 300 </property> <property name= \"default-height\" > 300 </property> <child> <object class= \"GtkButton\" id= \"button\" > <property name= \"label\" > Press me! </property> <property name= \"margin-top\" > 12 </property> <property name= \"margin-bottom\" > 12 </property> <property name= \"margin-start\" > 12 </property> <property name= \"margin-end\" > 12 </property> </object> </child> </object> </interface> use gtk :: prelude :: * ; use gtk :: { Application , ApplicationWindow , Button }; fn main () { let app = Application :: builder () . application_id ( \"org.gtk-rs.example\" ) . build (); app . connect_activate ( build_ui ); app . run (); } fn build_ui ( app : & Application ) { let builder = gtk :: Builder :: from_string ( include_str! ( \"window.ui\" )); let window : ApplicationWindow = builder . object ( \"window\" ) . expect ( \"Could not get object `window` from builder.\" ); let button : Button = builder . object ( \"button\" ) . expect ( \"Could not get object `button` from builder.\" ); window . set_application ( Some ( app )); button . connect_clicked ( move | button | { // (1) button . set_label ( \"Hello World!\" ); }); window . set_child ( Some ( & button )); window . show_all (); window . present (); } This move keyword appears to be unnecessary. API glib::clone glib::clone! is used to create closures using strong or weak references. Strong Weak use glib ; use glib_macros :: clone ; use std :: rc :: Rc ; let v = Rc :: new ( 1 ); let closure = clone ! ( @ strong v => move | x | { println! ( \"v: {}, x: {}\" , v , x ); }); closure ( 2 ); use glib ; use glib_macros :: clone ; use std :: rc :: Rc ; let u = Rc :: new ( 2 ); let closure = clone ! ( @ weak u => move | x | { println! ( \"u: {}, x: {}\" , u , x ); }); closure ( 3 ); glib::wrapper glib::wrapper is used to wrap GObjects and figures prominently in the To-Do App . Parent classes must be provided after @extends and any interfaces implemented must be provided after @implements wrapper ! { pub struct $name ( $kind < $foreign > ); // (2) match fn { $fn_name => /* (1) */ , .. . } } Closure-like expressions in the match fn block allow copying, freeing, referencing, and dereferencing the value that is being wrapped. There are three possible values for $kind : Boxed (heap allocated types), Shared (records with reference-counted, shared ownership), or Object (classes) wrapper ! { pub struct Button ( Object < ffi :: GtkButton > ) @ extends Bin , Container , Widget , @ implements Buildable , Actionable ; match fn { type_ => || ffi :: gtk_button_get_type (), // (1) } } Action Gio.Action is a way to expose any single task an application or widget does by a name. Classes like Gio.MenuItem and Gtk.ModelButton support properties to set an action name. These actions can be collected into a Gio.ActionGroup . Gio.ActionMap are interfaces implemented by Gtk.ApplicationWindow ActionGroup Adjustment PyGobject gtk-rs gtk Gtk.Adjustment is not a widget per se but is used in many widgets, including spin buttons, view ports, and children of Gtk.Range . page increment and page size refer to actions taken when the user presses PgUp or PgDn Gtk . Adjustment . new ( initial_value , lower_range , upper_range , step_increment , page_increment , page_size ) Alignment PyGobject gtk-rs gtk Gtk.Alignment controls the alignment and size of its child widget. Application PyGobject gtk-rs gtk Gtk.Application gtk4::Application GtkApplication Subclasses of Gtk.Application encapsulate application behavior, including application startup and CLI processing. In practice it is simply a wrapper for the ApplicationWindow class which is instantiated in the do_activate() hook. Notably, the Application subclass provides the value for the application_id kwarg passed to the Gtk.Application constructor. This value is validated, and any simple string is not silently accepted. Application must expose several important methods: do_activate() def do_activate ( self ): self . window = ApplicationWindow ( application = self , title = \"Hello, World!\" ) self . window . show_all () self . window . present () do_startup() ApplicationWindow PyGobject gtk-rs gtk Gtk.ApplicationWindow gtk4::ApplicationWindow GtkApplicationWindow The Gtk.ApplicationWindow class is the main visible window for the application, and the only window for \"single-instance\" applications (which is the default). The ApplicationWindow class was introduced in GTK 3.4. When an action has the prefix win. it specifies that the ApplicationWindow subclass will process the signal. Assistant PyGobject gtk-rs gtk Gtk.Assistant gtk4::Assistant GtkAssistant Gtk.Assistant widgets are used to implement the wizard pattern. Box PyGobject gtk-rs gtk Gtk.Box gtk4::Box GtkBox Builder PyGobject gtk-rs gtk Gtk.Builder gtk4::Builder GtkBuilder Gtk.Builder allows the use of interfaces to define widget layouts. Individual UI elements can be bound if they have an id attribute assigned. Rust Python fn main () { let app = gtk :: Application :: builder () . application_id ( \"org.example.gtk-app\" ) . build (); app . connect_activate ( build_ui ); app . run (); } fn build_ui ( app : & Application ) : { let builder = gtk :: Builder :: from_string ( include_str! ( \"window.ui\" )); let window : ApplicationWindow = builder . object ( \"window\" ) . expect ( \"Error loading ApplicationWindow!\" ); window . set_application ( Some ( app )); window . show_all (); window . present (); } class Application ( Gtk . Application ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( application_id = \"org.example.gtk-app\" ) def do_activate ( self ): builder = Gtk . Builder . new_from_file ( \"window.ui\" ) self . window = builder . get_object ( \"window\" ) self . window . show_all () self . window . present () def run ( self ): super () . run () Gtk . main () CheckButton PyGobject gtk-rs gtk Gtk.CheckButton gtk4::CheckButton GtkCheckButton Gtk.CheckButton s include checkboxes and (when placed into groups) radio buttons. Container PyGobject gtk-rs gtk Gtk.Container gtk::Container GtkContainer (3.0) Both Gtk.ApplicationWindow and Gtk.Window classes indirectly derive from the abstract class Gtk.Container . The main purpose of a container subclass is to allow a parent widget to contain one or more child widgets, and there are two types: Dialog PyGobject gtk-rs gtk Gtk.Dialog provides a convenient way to prompt the user for a small amount of input. It is a widget that can be instantiated and customized in its own right as well as a parent to various subclasses. dialog = Gtk . Dialog ( title = \"Hello, World!\" , parent = parent ) Dialogs are split into two parts: Content area containing interactive widgets Action area containing buttons These areas are both combined in a vertical Box that is assigned to the vbox field. The action area is packed to the end of this vbox, so the pack_start() method is used to add widgets to the content area. Dialog boxes can be modal , meaning they prevent interaction with the main window while open, or nonmodal . Modal Nonmodal dialog = Gtk . Dialog ( title = \"Hello, World!\" , parent = parent , modal = True ) dialog = Gtk . Dialog ( title = \"Hello, World!\" , parent = parent , modal = False ) Gtk.MessageDialog is a subtype of Dialog meant to simplify the process of creating simple dialogs. Buttons are added procedurally using add_button() , passing a display string (with support for mnemonics using _ ) and a ResponseType enum (they once could be added on instantiation by passing a tuple to the buttons keyword argument). dialog . add_button ( \"_OK\" , Gtk . ResponseType . OK ) Methods: add_button() Entry PyGobject gtk-rs gtk Unlike other widgets, Gtk.Entry can be instantiated without using a specific constructor. entry = Gtk . Entry () Default text can be provided by passing a string to the text keyword argument or with the set_text() setter method: kwarg setter entry = Gtk . Entry ( text = \"Hello, World!\" ) entry . set_text ( \"Hello, World!\" ) A password field can be made by concealing text by passing False to visibility or with the set_visibility() setter: kwarg setter password = Gtk . Entry ( visibility = False ) password . set_visibility ( False ) get_text() retrieve contents (string) set_visibility(bool) conceal text EventBox PyGobject gtk-rs gtk Gtk.EventBox is a container widget that allows event handling for widgets like Gtk.Label that do not have an associated GDK window. The event box can be positioned above or below the windows of its child with set_above_child() (False by default.) An EventBox must also have a Gtk.EventMask enum set to specify the type of events the widget may receive. This enum is passed as a value to set_events() . In the following example, an event handler is connected to the EventBox to handle button_press_event . This event handler changes the text of the Label after a double-click. import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk , Gdk class AppWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_border_width ( 10 ) self . set_size_request ( 200 , 50 ) eventbox = Gtk . EventBox . new () label = Gtk . Label . new ( \"Double-Click Me!\" ) eventbox . set_above_child ( False ) eventbox . connect ( \"button_press_event\" , self . on_button_pressed , label ) eventbox . add ( label ) self . add ( eventbox ) eventbox . set_events ( Gdk . EventMask . BUTTON_PRESS_MASK ) eventbox . realize () def on_button_pressed ( self , eventbox , event , label ): if event . type == Gdk . EventType . _2BUTTON_PRESS : text = label . get_text () if text [ 0 ] == 'D' : label . set_text ( \"I Was Double-Clicked!\" ) else : label . set_text ( \"Double-Click Me Again!\" ) return False class Application ( Gtk . Application ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , application_id = \"org.example.myapp\" , ** kwargs ) self . window = None def do_activate ( self ): if not self . window : self . window = AppWindow ( application = self , title = \"Hello World!\" ) self . window . show_all () self . window . present () if __name__ == \"__main__\" : app = Application () app . run () FileChooserDialog PyGobject gtk-rs gtk Gtk.FileChooserDialog is one of the important subtypes of Gtk.Dialog . Like other dialogs, it is provided a title and parent window on instantiation. Additionally a FileChooserAction enum must be specified. FileChooserAction s include: Gtk.FileChooserAction.SAVE Gtk.FileChooserAction.OPEN Gtk.FileChooserAction.SELECT_FOLDER Gtk.FileChooserAction.CREATE_FOLDER dialog = Gtk . FileChooserDialog ( title = \"Save file as ...\" , parent = parent , action = FileChooserAction . SAVE ) Selected files are then retrieved using dialog . get_filenames () | Setter | Property | Description | | --------------------------------------------------------------------------------------------------------------------------------- | ----------------- | | set_current_folder | | Specify directory in filesystem where FileChooser will start | | set_current_name | | For FileChooserAction.SAVE, suggest a filename | | set_select_multiple | select_multiple | For FileChooserAction.OPEN or SELECT_FOLDER, allow multiple file or folder selections | Grid PyGobject gtk-rs gtk Gtk.Grid allows children to be packed in a two-dimensional grid. Grids are instantiated with new() and widgets are laid out by calling attach() (see Login for an example). attach() lay out a widget providing column and row numbers followed by column and row spans grid . attach ( label , 0 , 0 , 1 , 1 ) HeaderBar PyGobject gtk-rs gtk Gtk.HeaderBar allows the titlebar to be customized. Like other widgets, it can be configured on instantiation by providing values to keyword arguments or by using setters. Adding to a window HeaderBars are added with set_titlebar() . This is unlike other widgets which are assigned to an ApplicationWindow or Window using pack_start() , pack_end() , or add() , kwarg setter class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) headerbar = Gtk . HeaderBar ( title = f \"Hello, World!\" , subtitle = \"HeaderBar example\" , show_close_button = True ) self . set_titlebar ( headerbar ) class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) headerbar = Gtk . HeaderBar () headerbar . set_title ( f \"Hello, World!\" ) headerbar . set_subtitle ( \"HeaderBar example\" ) headerbar . set_show_close_button ( True ) self . set_titlebar ( headerbar ) Label PyGobject gtk-rs gtk Note that Gtk.Label sets its text with \"label\" and not \"text\" as you may expect from the corresponding setter. kwarg setter label = Gtk . Label ( label = \"Hello, World!\" ) label . set_text ( \"Hello, World!\" ) ListBox PyGobject gtk-rs gtk Gtk.ListBox is a vertical container of Gtk.ListBoxRow children used as an alternative to TreeView when the children need to be interactive, as in a list of settings. ListBox ListStore PyGobject gtk-rs gtk Gtk.ListStore is one of the two major classes that serves as combination schema and database backing Gtk.TreeView , the other being Gtk.TreeStore . It is instantiated with a sequence of data types, similar to a database schema. These can be standard Python types or GObjects (which are mapped to the Python types anyway): Python types GObject types import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk liststore = Gtk . ListStore (( str , int , str )) import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk , GObject liststore = Gtk . ListStore (( GObject . TYPE_STRING , GOBject ) . TYPE_INT , GObject . TYPE_STRING )) This object then exposes an append method which is used to add records: liststore . append ([ \"Socrates\" , 350 , \"Athens\" ]) The store is then associated with the treeview with set_model treeview . set_model ( liststore ) MenuBar PyGobject gtk-rs gtk Gtk.MenuBar is populated with Gtk.MenuItems , corresponding to the expandable menu items (i.e. \"File\", \"Edit\", and \"Help\"). Gtk.Menu is actually used for the submenu, which like MenuBar is also populared with MenuItems. A Menu is attached to the MenuItem of a MenuBar by using the set_submenu() method on the Menu object. This setter does not have a corresponding kwarg, so all menus have to be constructed procedurally. import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk class AppWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_size_request ( 250 , - 1 ) menubar = Gtk . MenuBar . new () self . add ( menubar ) file = Gtk . MenuItem . new_with_label ( \"File\" ) menubar . append ( file ) filemenu = Gtk . Menu . new () file . set_submenu ( filemenu ) new = Gtk . MenuItem . new_with_label ( \"New\" ) open = Gtk . MenuItem . new_with_label ( \"Open\" ) filemenu . append ( new ) filemenu . append ( open ) edit = Gtk . MenuItem . new_with_label ( \"Edit\" ) menubar . append ( edit ) editmenu = Gtk . Menu . new () edit . set_submenu ( editmenu ) cut = Gtk . MenuItem . new_with_label ( \"Cut\" ) copy = Gtk . MenuItem . new_with_label ( \"Copy\" ) paste = Gtk . MenuItem . new_with_label ( \"Paste\" ) editmenu . append ( cut ) editmenu . append ( copy ) editmenu . append ( paste ) help = Gtk . MenuItem . new_with_label ( \"Help\" ) menubar . append ( help ) helpmenu = Gtk . Menu . new () help . set_submenu ( helpmenu ) contents = Gtk . MenuItem . new_with_label ( \"Help\" ) about = Gtk . MenuItem . new_with_label ( \"About\" ) helpmenu . append ( contents ) helpmenu . append ( about ) class Application ( Gtk . Application ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , application_id = \"org.example.myapp\" , ** kwargs ) self . window = None def do_activate ( self ): if not self . window : self . window = AppWindow ( application = self , title = \"Menu Bars\" ) self . window . show_all () self . window . present () if __name__ == \"__main__\" : app = Application () app . run () Notebook PyGobject gtk-rs gtk Gtk.Notebook is a layout container that organizes content into tabbed pages. It is instantiated with the new() method and pages are appended with the append_page() method, passing content and label widgets as arguments. The tab bar can be placed using set_tab_pos() , passing a Gtk.PositionType enum Top Right Bottom Left notebook = Gtk . Notebook . new () # notebook.set_tab_pos(Gtk.PositionType.TOP) notebook = Gtk . Notebook . new () notebook . set_tab_pos ( Gtk . PositionType . RIGHT ) notebook = Gtk . Notebook . new () notebook . set_tab_pos ( Gtk . PositionType . BOTTOM ) notebook = Gtk . Notebook . new () notebook . set_tab_pos ( Gtk . PositionType . LEFT ) notebook = Gtk . Notebook . new () label = Gtk . Label . new ( \"Tab title\" ) child = Gtk . Label . new ( \"Tab content\" ) notebook . append_page ( child , label ) The label widget is commonly Gtk.Label but can also be a Gtk.Box . The tab bar can be made scrollable using set_scrollable() , passing a bool. Scale PyGobject gtk-rs gtk Gtk.Scale widgets are sliders, and they can be instantiated in one of two ways: new() passing an Adjustment object new_with_range(min, max, step) passing values for minimum, maximum, and step Scale values are stored as doubles, so integers have to be simulated by reducing the number of digits to 0 using set_digits() . By default, the number of digits is set to that of the step value. ScrolledWindow PyGobject gtk-rs gtk Gtk.ScrolledWindow is a decorator container that accepts a single child widget. Widgets that implement the Gtk.Scrollable interface have native scrolling suppport, like Gtk.TreeView , Gtk.TextView, and Gtk.Layout. Other widgets have to use Gtk.Viewport as an adaptor, and must be added to a Viewport which is then added to the ScrolledWindow. It is instantiated with the new() method, optionally passing two Adjustment objects that affect horizontal and vertical scrolling behavior when stepping or paging. scrolled_win = Gtk . ScrolledWindow . new ( None , None ) Statusbar PyGobject gtk-rs gtk Gtk.Statusbar gtk4::Statusbar GtkStatusbar (4.0) Gtk.Statusbar (note the lowercase b ) stores a stack of messages, the topmost of which is displayed. Before adding messages, a context identifier , a unique unsigned integer associated with a context description string, must be retrieved from the newly created Statusbar by passing a string value to get_context_id() . This allows messages to be categorized and pushed to separate stacks. statusbar . push ( context_id , message ) Switch PyGobject gtk-rs gtk Gtk.Switch gtk4::Switch GtkSwitch (4.0) Gtk.Switch allows a user to toggle a boolean value. Switch exposes getters and setters for both state (which is represented by the trough color) and active (switch position) properties. State is the backend to activ, and they are kept in sync. import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_border_width ( 10 ) box_outer = Gtk . Box ( orientation = Gtk . Orientation . VERTICAL , spacing = 6 ) listbox = Gtk . ListBox ( selection_mode = Gtk . SelectionMode . NONE ) row = Gtk . ListBoxRow () hbox = Gtk . Box ( orientation = Gtk . Orientation . HORIZONTAL , spacing = 50 ) label1 = Gtk . Label ( label = \"Automatic Date & Time\" , xalign = 0 ) hbox . add ( label1 ) self . switch = Gtk . Switch ( valign = Gtk . Align . CENTER , state = False ) hbox . add ( self . switch ) row . add ( hbox ) listbox . add ( row ) box_outer . add ( listbox ) self . add ( box_outer ) button = Gtk . Button ( label = \"Click\" ) button . connect ( \"clicked\" , self . on_button_clicked ) box_outer . add ( button ) def on_button_clicked ( self , button ): print ( f \"Value of get_active(): { self . switch . get_active () } \" ) print ( f \"Value of get_state(): { self . switch . get_state () } \" ) class Application ( Gtk . Application ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . window = None def do_activate ( self ): if not self . window : self . window = ApplicationWindow ( application = self ) self . window . show_all () self . window . present () if __name__ == \"__main__\" : app = Application () app . run () TreeView PyGobject gtk-rs gtk Gtk.TreeView gtk4::TreeView GtkTreeView (4.0) GtkTreeView (3.0) In order to create a tree or list in GTK, the Gtk.TreeView widget is paired with a Gtk.TreeModel interface, the most typical implementation of which is Gtk.ListStore or Gtk.TreeStore . TreeView is a complicated widget that must be constructed procedurally: Gtk.TreeView is instantiated. A ListStore is specified as data model and passed in as the value of the model kwarg. The ListStore specifies the schema of the data as a collection of types. treeview = Gtk . TreeView ( model = Gtk . ListStore . new (( str ))) Alternatively, the ListStore can be specified after instantiation. treeview = Gtk . TreeView . new () treeview . set_model ( Gtk . ListStore . new ([ str ])) A Gtk.TreeViewColumn is created for every column in the model. These require a Gtk.CellRenderer to be defined. The TreeViewColumn is added to the treeview by calling the append_column() method on the treeview. The text kwarg appears to refer to the column of the data store to use for the column's values. treeview . append_column ( Gtk . TreeViewColumn ( \"Greeks\" , Gtk . CellRendererText . new (), text = 0 )) Items are added to the ListStore procedurally using the append() method. Note that the method takes only a single argument, so collections like lists or tuples must be used. liststore . append (( \"Socrates\" ,)) liststore . append (( \"Plato\" ,)) liststore . append (( \"Aristotle\" ,)) Changing the number of columns affects the types used to define the ListStore, the appended records, as well as the number of columns added to the TreeView itself. 1 column 2 columns import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . gen_treeview () scrolled_win = Gtk . ScrolledWindow . new ( None , None ) scrolled_win . set_policy ( Gtk . PolicyType . AUTOMATIC , Gtk . PolicyType . AUTOMATIC ) scrolled_win . add ( self . treeview ) self . add ( scrolled_win ) self . set_size_request ( 200 , 200 ) def get_liststore ( self ): store = Gtk . ListStore . new (( str ,)) store . append (( \"Socrates\" ,)) store . append (( \"Plato\" ,)) store . append (( \"Aristotle\" ,)) return store def gen_treeview ( self ): self . treeview = Gtk . TreeView . new () self . treeview . set_model ( self . get_liststore ()) self . treeview . append_column ( Gtk . TreeViewColumn ( \"Greeks\" , Gtk . CellRendererText . new (), text = 0 )) class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.myapp' ) def do_activate ( self ): self . window = ApplicationWindow ( application = self , title = \"Greeks\" ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = Application () app . run () import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . gen_treeview () scrolled_win = Gtk . ScrolledWindow . new ( None , None ) scrolled_win . set_policy ( Gtk . PolicyType . AUTOMATIC , Gtk . PolicyType . AUTOMATIC ) scrolled_win . add ( self . treeview ) self . add ( scrolled_win ) self . set_size_request ( 200 , 200 ) def get_liststore ( self ): store = Gtk . ListStore . new (( str , str )) store . append ([ \"Socrates\" , \"Athens\" ]) store . append ([ \"Plato\" , \"Athens\" ]) store . append ([ \"Aristotle\" , \"Athens\" ]) return store def gen_treeview ( self ): self . treeview = Gtk . TreeView . new () self . treeview . set_model ( self . get_liststore ()) self . treeview . append_column ( Gtk . TreeViewColumn ( \"Greeks\" , Gtk . CellRendererText . new (), text = 0 )) self . treeview . append_column ( Gtk . TreeViewColumn ( \"Place of birth\" , Gtk . CellRendererText . new (), text = 1 )) class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.myapp' ) def do_activate ( self ): self . window = ApplicationWindow ( application = self , title = \"Greeks\" ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = Application () app . run () The model backing a TreeView (usually a ListStore ), can be retrieved with the get_model() method. treeview . get_model () . append (( 'foo' , 'bar' )) TreeView emits several signals: row_activated when a row is double-clicked, with the following implicit argument widget refering to the emitting TreeView widget itself path is a TreePath . column is of type TreeViewcolumn treeview . connect ( \"row_activated\" , self . on_row_activated , widget , path , column ) def on_row_activated ( self , widget , path , column ): row = path . get_indices ()[ 0 ] print ( f \"row= { path . get_indices ()[ 0 ] } ,col= { column . props . title } \" ) print ( widget . get_model ()[ row ][:]) TreePath PyGobject gtk-rs gtk Gtk.TreePath is a type used to implement the rows of a TreeView . Although it prints to an integer with the print statement, it cannot be treated as one. A path object can be passed as the index to a TreeModel like ListStore , as can an integer. The row number of a TreePath from a normal list-style TreeView can be retrieved with the get_indices() method. row = path . get_indices ()[ 0 ] # Using TreePath object as index to model model [ path ][:] # Using row integer as index to model model [ row ][:] Another method on TreePath, get_depth() always returns 1 for list-style TreeViews, but may be more useful for tree-style TreeViews. TreeSelection PyGobject gtk-rs gtk Gtk.TreeSelection objects represent selection information for each tree view. TreeViewColumn PyGobject gtk-rs gtk Gtk.TreekViewColumn gtk4::TreeViewColumn GtkTreeViewColumn (4.0) GtkTreeViewColumn (3.0) Gtk.TreeViewColumn represents a visible column in a Treeview . Its props property exposes many associated values, including title. print ( column . props . title ) A column is made sortable by calling set_sort_column_id() , passing the column of the model to sort by. column . set_sort_column_id ( 0 ) Window PyGobject gtk-rs gtk Gtk.Window gtk4::Window GtkWindow (4.0) GtkWindow (3.0)","title":"gtk-rs"},{"location":"Rust/Crates/Gtk-rs/#gtk-rs","text":"","title":"gtk-rs"},{"location":"Rust/Crates/Gtk-rs/#tasks","text":"","title":"Tasks"},{"location":"Rust/Crates/Gtk-rs/#development-environment","text":"Red Hat dnf install gtk4-devel gcc Ubuntu apt install libgtk-4-dev build-essential","title":"Development environment"},{"location":"Rust/Crates/Gtk-rs/#boilerplate","text":"Interface <?xml version=\"1.0\" encoding=\"UTF-8\"?> <interface> <requires lib= \"gtk+\" version= \"3.40\" /> <object class= \"GtkApplicationWindow\" id= \"window\" > <property name= \"title\" > My GTK App </property> <property name= \"default-width\" > 300 </property> <property name= \"default-height\" > 300 </property> </object> </interface> use gtk4 :: { Application , ApplicationWindow }; fn main () { let app = Application :: builder () . application_id ( \"com.example.learning-gtk\" ) . build (); app . connect_activate ( build_ui ); app . run (); } fn build_ui ( app : & Application ) { let window = ApplicationWindow :: builder () . application ( app ) . default_width ( 300 ) . default_height ( 300 ) . title ( \"My GTK App\" ) . build (); window . present (); }","title":"Boilerplate"},{"location":"Rust/Crates/Gtk-rs/#clicker","text":"This appears to be a common demonstration of data binding in various GUI frameworks, the code below is taken from here . use gtk4 :: prelude :: * ; use gtk4 :: { glib , Application , ApplicationWindow , Box , Button , Orientation }; use std :: { cell :: Cell , rc :: Rc }; fn main () { // Create a new application let app = Application :: builder () . application_id ( \"org.gtk-rs.example\" ) . build (); // Connect to \"activate\" signal of `app` app . connect_activate ( build_ui ); // Run the application app . run (); } fn build_ui ( app : & Application ) { // Create two buttons let button_increase = Button :: builder () . label ( \"Increase\" ) . margin_top ( 12 ) . margin_bottom ( 12 ) . margin_start ( 12 ) . margin_end ( 12 ) . build (); let button_decrease = Button :: builder () . label ( \"Decrease\" ) . margin_top ( 12 ) . margin_bottom ( 12 ) . margin_start ( 12 ) . margin_end ( 12 ) . build (); // Reference-counted object with inner mutability let number = Rc :: new ( Cell :: new ( 0 )); // (1) // Connect callbacks // When a button is clicked, `number` and label of the other button will be changed button_increase . connect_clicked ( glib :: clone ! ( @ weak number , @ weak button_decrease => // (2) move | _ | { number . set ( number . get () + 1 ); button_decrease . set_label ( & number . get (). to_string ()); })); button_decrease . connect_clicked ( glib :: clone ! ( @ weak button_increase => // (3) move | _ | { number . set ( number . get () - 1 ); button_increase . set_label ( & number . get (). to_string ()); })); // Add buttons to `gtk_box` let gtk_box = Box :: builder (). orientation ( Orientation :: Vertical ). build (); gtk_box . append ( & button_increase ); gtk_box . append ( & button_decrease ); // Create a window let window = ApplicationWindow :: builder () . application ( app ) . title ( \"My GTK App\" ) . child ( & gtk_box ) . build (); // Present the window window . present (); } It appears number must be an Rc<Cell>. Changing it to a raw i8 causes compilation errors within glib::clone . Removing the weak reference to number causes a compilation error. Adding a weak reference to number here also causes a compilation error.","title":"Clicker"},{"location":"Rust/Crates/Gtk-rs/#hello-world","text":"","title":"Hello, World!"},{"location":"Rust/Crates/Gtk-rs/#window-frame","text":"TODO At the moment, this example is broken because I don't know how to pass the string into the Application struct for string interpolation. use gtk4 :: prelude :: * ; use gtk4 :: { Application , ApplicationWindow }; fn main () { let name = String :: new (); if let Some ( s ) = 42 std :: env :: args (). nth ( 1 ) { name = s ; } else { name = String :: from ( \"World\" ); }; let app = Application :: builder () . application_id ( \"com.example.learning-gtk\" ) . build (); app . connect_activate ( build_ui ); println! ( \"{}\" , app . name ); app . run (); } fn build_ui ( app : & Application ) { let window = ApplicationWindow :: builder () . application ( app ) . title ( \"Hello, World!\" ) . default_height ( 300 ) . default_width ( 300 ) . build (); window . present (); }","title":"Window frame"},{"location":"Rust/Crates/Gtk-rs/#button-reveal","text":"Interface <?xml version=\"1.0\" encoding=\"UTF-8\"?> <interface> <requires lib= \"gtk+\" version= \"3.40\" > <object class= \"GtkApplicationWindow\" id= \"window\" > <property name= \"title\" > My GTK App </property> <property name= \"default-width\" > 300 </property> <property name= \"default-height\" > 300 </property> <child> <object class= \"GtkButton\" id= \"button\" > <property name= \"label\" > Press me! </property> <property name= \"margin-top\" > 12 </property> <property name= \"margin-bottom\" > 12 </property> <property name= \"margin-start\" > 12 </property> <property name= \"margin-end\" > 12 </property> </object> </child> </object> </interface> use gtk :: prelude :: * ; use gtk :: { Application , ApplicationWindow , Button }; fn main () { let app = Application :: builder () . application_id ( \"org.gtk-rs.example\" ) . build (); app . connect_activate ( build_ui ); app . run (); } fn build_ui ( app : & Application ) { let builder = gtk :: Builder :: from_string ( include_str! ( \"window.ui\" )); let window : ApplicationWindow = builder . object ( \"window\" ) . expect ( \"Could not get object `window` from builder.\" ); let button : Button = builder . object ( \"button\" ) . expect ( \"Could not get object `button` from builder.\" ); window . set_application ( Some ( app )); button . connect_clicked ( move | button | { // (1) button . set_label ( \"Hello World!\" ); }); window . set_child ( Some ( & button )); window . show_all (); window . present (); } This move keyword appears to be unnecessary.","title":"Button reveal"},{"location":"Rust/Crates/Gtk-rs/#api","text":"","title":"API"},{"location":"Rust/Crates/Gtk-rs/#glibclone","text":"glib::clone! is used to create closures using strong or weak references. Strong Weak use glib ; use glib_macros :: clone ; use std :: rc :: Rc ; let v = Rc :: new ( 1 ); let closure = clone ! ( @ strong v => move | x | { println! ( \"v: {}, x: {}\" , v , x ); }); closure ( 2 ); use glib ; use glib_macros :: clone ; use std :: rc :: Rc ; let u = Rc :: new ( 2 ); let closure = clone ! ( @ weak u => move | x | { println! ( \"u: {}, x: {}\" , u , x ); }); closure ( 3 );","title":"glib::clone"},{"location":"Rust/Crates/Gtk-rs/#glibwrapper","text":"glib::wrapper is used to wrap GObjects and figures prominently in the To-Do App . Parent classes must be provided after @extends and any interfaces implemented must be provided after @implements wrapper ! { pub struct $name ( $kind < $foreign > ); // (2) match fn { $fn_name => /* (1) */ , .. . } } Closure-like expressions in the match fn block allow copying, freeing, referencing, and dereferencing the value that is being wrapped. There are three possible values for $kind : Boxed (heap allocated types), Shared (records with reference-counted, shared ownership), or Object (classes) wrapper ! { pub struct Button ( Object < ffi :: GtkButton > ) @ extends Bin , Container , Widget , @ implements Buildable , Actionable ; match fn { type_ => || ffi :: gtk_button_get_type (), // (1) } }","title":"glib::wrapper"},{"location":"Rust/Crates/Gtk-rs/#action","text":"Gio.Action is a way to expose any single task an application or widget does by a name. Classes like Gio.MenuItem and Gtk.ModelButton support properties to set an action name. These actions can be collected into a Gio.ActionGroup . Gio.ActionMap are interfaces implemented by Gtk.ApplicationWindow","title":"Action"},{"location":"Rust/Crates/Gtk-rs/#actiongroup_1","text":"","title":"ActionGroup"},{"location":"Rust/Crates/Gtk-rs/#adjustment","text":"PyGobject gtk-rs gtk Gtk.Adjustment is not a widget per se but is used in many widgets, including spin buttons, view ports, and children of Gtk.Range . page increment and page size refer to actions taken when the user presses PgUp or PgDn Gtk . Adjustment . new ( initial_value , lower_range , upper_range , step_increment , page_increment , page_size )","title":"Adjustment"},{"location":"Rust/Crates/Gtk-rs/#alignment","text":"PyGobject gtk-rs gtk Gtk.Alignment controls the alignment and size of its child widget.","title":"Alignment"},{"location":"Rust/Crates/Gtk-rs/#application","text":"PyGobject gtk-rs gtk Gtk.Application gtk4::Application GtkApplication Subclasses of Gtk.Application encapsulate application behavior, including application startup and CLI processing. In practice it is simply a wrapper for the ApplicationWindow class which is instantiated in the do_activate() hook. Notably, the Application subclass provides the value for the application_id kwarg passed to the Gtk.Application constructor. This value is validated, and any simple string is not silently accepted. Application must expose several important methods: do_activate() def do_activate ( self ): self . window = ApplicationWindow ( application = self , title = \"Hello, World!\" ) self . window . show_all () self . window . present () do_startup()","title":"Application"},{"location":"Rust/Crates/Gtk-rs/#applicationwindow","text":"PyGobject gtk-rs gtk Gtk.ApplicationWindow gtk4::ApplicationWindow GtkApplicationWindow The Gtk.ApplicationWindow class is the main visible window for the application, and the only window for \"single-instance\" applications (which is the default). The ApplicationWindow class was introduced in GTK 3.4. When an action has the prefix win. it specifies that the ApplicationWindow subclass will process the signal.","title":"ApplicationWindow"},{"location":"Rust/Crates/Gtk-rs/#assistant","text":"PyGobject gtk-rs gtk Gtk.Assistant gtk4::Assistant GtkAssistant Gtk.Assistant widgets are used to implement the wizard pattern.","title":"Assistant"},{"location":"Rust/Crates/Gtk-rs/#box","text":"PyGobject gtk-rs gtk Gtk.Box gtk4::Box GtkBox","title":"Box"},{"location":"Rust/Crates/Gtk-rs/#builder","text":"PyGobject gtk-rs gtk Gtk.Builder gtk4::Builder GtkBuilder Gtk.Builder allows the use of interfaces to define widget layouts. Individual UI elements can be bound if they have an id attribute assigned. Rust Python fn main () { let app = gtk :: Application :: builder () . application_id ( \"org.example.gtk-app\" ) . build (); app . connect_activate ( build_ui ); app . run (); } fn build_ui ( app : & Application ) : { let builder = gtk :: Builder :: from_string ( include_str! ( \"window.ui\" )); let window : ApplicationWindow = builder . object ( \"window\" ) . expect ( \"Error loading ApplicationWindow!\" ); window . set_application ( Some ( app )); window . show_all (); window . present (); } class Application ( Gtk . Application ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( application_id = \"org.example.gtk-app\" ) def do_activate ( self ): builder = Gtk . Builder . new_from_file ( \"window.ui\" ) self . window = builder . get_object ( \"window\" ) self . window . show_all () self . window . present () def run ( self ): super () . run () Gtk . main ()","title":"Builder"},{"location":"Rust/Crates/Gtk-rs/#checkbutton","text":"PyGobject gtk-rs gtk Gtk.CheckButton gtk4::CheckButton GtkCheckButton Gtk.CheckButton s include checkboxes and (when placed into groups) radio buttons.","title":"CheckButton"},{"location":"Rust/Crates/Gtk-rs/#container","text":"PyGobject gtk-rs gtk Gtk.Container gtk::Container GtkContainer (3.0) Both Gtk.ApplicationWindow and Gtk.Window classes indirectly derive from the abstract class Gtk.Container . The main purpose of a container subclass is to allow a parent widget to contain one or more child widgets, and there are two types:","title":"Container"},{"location":"Rust/Crates/Gtk-rs/#dialog","text":"PyGobject gtk-rs gtk Gtk.Dialog provides a convenient way to prompt the user for a small amount of input. It is a widget that can be instantiated and customized in its own right as well as a parent to various subclasses. dialog = Gtk . Dialog ( title = \"Hello, World!\" , parent = parent ) Dialogs are split into two parts: Content area containing interactive widgets Action area containing buttons These areas are both combined in a vertical Box that is assigned to the vbox field. The action area is packed to the end of this vbox, so the pack_start() method is used to add widgets to the content area. Dialog boxes can be modal , meaning they prevent interaction with the main window while open, or nonmodal . Modal Nonmodal dialog = Gtk . Dialog ( title = \"Hello, World!\" , parent = parent , modal = True ) dialog = Gtk . Dialog ( title = \"Hello, World!\" , parent = parent , modal = False ) Gtk.MessageDialog is a subtype of Dialog meant to simplify the process of creating simple dialogs. Buttons are added procedurally using add_button() , passing a display string (with support for mnemonics using _ ) and a ResponseType enum (they once could be added on instantiation by passing a tuple to the buttons keyword argument). dialog . add_button ( \"_OK\" , Gtk . ResponseType . OK ) Methods: add_button()","title":"Dialog"},{"location":"Rust/Crates/Gtk-rs/#entry","text":"PyGobject gtk-rs gtk Unlike other widgets, Gtk.Entry can be instantiated without using a specific constructor. entry = Gtk . Entry () Default text can be provided by passing a string to the text keyword argument or with the set_text() setter method: kwarg setter entry = Gtk . Entry ( text = \"Hello, World!\" ) entry . set_text ( \"Hello, World!\" ) A password field can be made by concealing text by passing False to visibility or with the set_visibility() setter: kwarg setter password = Gtk . Entry ( visibility = False ) password . set_visibility ( False ) get_text() retrieve contents (string) set_visibility(bool) conceal text","title":"Entry"},{"location":"Rust/Crates/Gtk-rs/#eventbox","text":"PyGobject gtk-rs gtk Gtk.EventBox is a container widget that allows event handling for widgets like Gtk.Label that do not have an associated GDK window. The event box can be positioned above or below the windows of its child with set_above_child() (False by default.) An EventBox must also have a Gtk.EventMask enum set to specify the type of events the widget may receive. This enum is passed as a value to set_events() . In the following example, an event handler is connected to the EventBox to handle button_press_event . This event handler changes the text of the Label after a double-click. import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk , Gdk class AppWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_border_width ( 10 ) self . set_size_request ( 200 , 50 ) eventbox = Gtk . EventBox . new () label = Gtk . Label . new ( \"Double-Click Me!\" ) eventbox . set_above_child ( False ) eventbox . connect ( \"button_press_event\" , self . on_button_pressed , label ) eventbox . add ( label ) self . add ( eventbox ) eventbox . set_events ( Gdk . EventMask . BUTTON_PRESS_MASK ) eventbox . realize () def on_button_pressed ( self , eventbox , event , label ): if event . type == Gdk . EventType . _2BUTTON_PRESS : text = label . get_text () if text [ 0 ] == 'D' : label . set_text ( \"I Was Double-Clicked!\" ) else : label . set_text ( \"Double-Click Me Again!\" ) return False class Application ( Gtk . Application ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , application_id = \"org.example.myapp\" , ** kwargs ) self . window = None def do_activate ( self ): if not self . window : self . window = AppWindow ( application = self , title = \"Hello World!\" ) self . window . show_all () self . window . present () if __name__ == \"__main__\" : app = Application () app . run ()","title":"EventBox"},{"location":"Rust/Crates/Gtk-rs/#filechooserdialog","text":"PyGobject gtk-rs gtk Gtk.FileChooserDialog is one of the important subtypes of Gtk.Dialog . Like other dialogs, it is provided a title and parent window on instantiation. Additionally a FileChooserAction enum must be specified. FileChooserAction s include: Gtk.FileChooserAction.SAVE Gtk.FileChooserAction.OPEN Gtk.FileChooserAction.SELECT_FOLDER Gtk.FileChooserAction.CREATE_FOLDER dialog = Gtk . FileChooserDialog ( title = \"Save file as ...\" , parent = parent , action = FileChooserAction . SAVE ) Selected files are then retrieved using dialog . get_filenames () | Setter | Property | Description | | --------------------------------------------------------------------------------------------------------------------------------- | ----------------- | | set_current_folder | | Specify directory in filesystem where FileChooser will start | | set_current_name | | For FileChooserAction.SAVE, suggest a filename | | set_select_multiple | select_multiple | For FileChooserAction.OPEN or SELECT_FOLDER, allow multiple file or folder selections |","title":"FileChooserDialog"},{"location":"Rust/Crates/Gtk-rs/#grid","text":"PyGobject gtk-rs gtk Gtk.Grid allows children to be packed in a two-dimensional grid. Grids are instantiated with new() and widgets are laid out by calling attach() (see Login for an example). attach() lay out a widget providing column and row numbers followed by column and row spans grid . attach ( label , 0 , 0 , 1 , 1 )","title":"Grid"},{"location":"Rust/Crates/Gtk-rs/#headerbar","text":"PyGobject gtk-rs gtk Gtk.HeaderBar allows the titlebar to be customized. Like other widgets, it can be configured on instantiation by providing values to keyword arguments or by using setters. Adding to a window HeaderBars are added with set_titlebar() . This is unlike other widgets which are assigned to an ApplicationWindow or Window using pack_start() , pack_end() , or add() , kwarg setter class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) headerbar = Gtk . HeaderBar ( title = f \"Hello, World!\" , subtitle = \"HeaderBar example\" , show_close_button = True ) self . set_titlebar ( headerbar ) class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) headerbar = Gtk . HeaderBar () headerbar . set_title ( f \"Hello, World!\" ) headerbar . set_subtitle ( \"HeaderBar example\" ) headerbar . set_show_close_button ( True ) self . set_titlebar ( headerbar )","title":"HeaderBar"},{"location":"Rust/Crates/Gtk-rs/#label","text":"PyGobject gtk-rs gtk Note that Gtk.Label sets its text with \"label\" and not \"text\" as you may expect from the corresponding setter. kwarg setter label = Gtk . Label ( label = \"Hello, World!\" ) label . set_text ( \"Hello, World!\" )","title":"Label"},{"location":"Rust/Crates/Gtk-rs/#listbox","text":"PyGobject gtk-rs gtk Gtk.ListBox is a vertical container of Gtk.ListBoxRow children used as an alternative to TreeView when the children need to be interactive, as in a list of settings. ListBox","title":"ListBox"},{"location":"Rust/Crates/Gtk-rs/#liststore","text":"PyGobject gtk-rs gtk Gtk.ListStore is one of the two major classes that serves as combination schema and database backing Gtk.TreeView , the other being Gtk.TreeStore . It is instantiated with a sequence of data types, similar to a database schema. These can be standard Python types or GObjects (which are mapped to the Python types anyway): Python types GObject types import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk liststore = Gtk . ListStore (( str , int , str )) import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk , GObject liststore = Gtk . ListStore (( GObject . TYPE_STRING , GOBject ) . TYPE_INT , GObject . TYPE_STRING )) This object then exposes an append method which is used to add records: liststore . append ([ \"Socrates\" , 350 , \"Athens\" ]) The store is then associated with the treeview with set_model treeview . set_model ( liststore )","title":"ListStore"},{"location":"Rust/Crates/Gtk-rs/#menubar","text":"PyGobject gtk-rs gtk Gtk.MenuBar is populated with Gtk.MenuItems , corresponding to the expandable menu items (i.e. \"File\", \"Edit\", and \"Help\"). Gtk.Menu is actually used for the submenu, which like MenuBar is also populared with MenuItems. A Menu is attached to the MenuItem of a MenuBar by using the set_submenu() method on the Menu object. This setter does not have a corresponding kwarg, so all menus have to be constructed procedurally. import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk class AppWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_size_request ( 250 , - 1 ) menubar = Gtk . MenuBar . new () self . add ( menubar ) file = Gtk . MenuItem . new_with_label ( \"File\" ) menubar . append ( file ) filemenu = Gtk . Menu . new () file . set_submenu ( filemenu ) new = Gtk . MenuItem . new_with_label ( \"New\" ) open = Gtk . MenuItem . new_with_label ( \"Open\" ) filemenu . append ( new ) filemenu . append ( open ) edit = Gtk . MenuItem . new_with_label ( \"Edit\" ) menubar . append ( edit ) editmenu = Gtk . Menu . new () edit . set_submenu ( editmenu ) cut = Gtk . MenuItem . new_with_label ( \"Cut\" ) copy = Gtk . MenuItem . new_with_label ( \"Copy\" ) paste = Gtk . MenuItem . new_with_label ( \"Paste\" ) editmenu . append ( cut ) editmenu . append ( copy ) editmenu . append ( paste ) help = Gtk . MenuItem . new_with_label ( \"Help\" ) menubar . append ( help ) helpmenu = Gtk . Menu . new () help . set_submenu ( helpmenu ) contents = Gtk . MenuItem . new_with_label ( \"Help\" ) about = Gtk . MenuItem . new_with_label ( \"About\" ) helpmenu . append ( contents ) helpmenu . append ( about ) class Application ( Gtk . Application ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , application_id = \"org.example.myapp\" , ** kwargs ) self . window = None def do_activate ( self ): if not self . window : self . window = AppWindow ( application = self , title = \"Menu Bars\" ) self . window . show_all () self . window . present () if __name__ == \"__main__\" : app = Application () app . run ()","title":"MenuBar"},{"location":"Rust/Crates/Gtk-rs/#notebook","text":"PyGobject gtk-rs gtk Gtk.Notebook is a layout container that organizes content into tabbed pages. It is instantiated with the new() method and pages are appended with the append_page() method, passing content and label widgets as arguments. The tab bar can be placed using set_tab_pos() , passing a Gtk.PositionType enum Top Right Bottom Left notebook = Gtk . Notebook . new () # notebook.set_tab_pos(Gtk.PositionType.TOP) notebook = Gtk . Notebook . new () notebook . set_tab_pos ( Gtk . PositionType . RIGHT ) notebook = Gtk . Notebook . new () notebook . set_tab_pos ( Gtk . PositionType . BOTTOM ) notebook = Gtk . Notebook . new () notebook . set_tab_pos ( Gtk . PositionType . LEFT ) notebook = Gtk . Notebook . new () label = Gtk . Label . new ( \"Tab title\" ) child = Gtk . Label . new ( \"Tab content\" ) notebook . append_page ( child , label ) The label widget is commonly Gtk.Label but can also be a Gtk.Box . The tab bar can be made scrollable using set_scrollable() , passing a bool.","title":"Notebook"},{"location":"Rust/Crates/Gtk-rs/#scale","text":"PyGobject gtk-rs gtk Gtk.Scale widgets are sliders, and they can be instantiated in one of two ways: new() passing an Adjustment object new_with_range(min, max, step) passing values for minimum, maximum, and step Scale values are stored as doubles, so integers have to be simulated by reducing the number of digits to 0 using set_digits() . By default, the number of digits is set to that of the step value.","title":"Scale"},{"location":"Rust/Crates/Gtk-rs/#scrolledwindow","text":"PyGobject gtk-rs gtk Gtk.ScrolledWindow is a decorator container that accepts a single child widget. Widgets that implement the Gtk.Scrollable interface have native scrolling suppport, like Gtk.TreeView , Gtk.TextView, and Gtk.Layout. Other widgets have to use Gtk.Viewport as an adaptor, and must be added to a Viewport which is then added to the ScrolledWindow. It is instantiated with the new() method, optionally passing two Adjustment objects that affect horizontal and vertical scrolling behavior when stepping or paging. scrolled_win = Gtk . ScrolledWindow . new ( None , None )","title":"ScrolledWindow"},{"location":"Rust/Crates/Gtk-rs/#statusbar","text":"PyGobject gtk-rs gtk Gtk.Statusbar gtk4::Statusbar GtkStatusbar (4.0) Gtk.Statusbar (note the lowercase b ) stores a stack of messages, the topmost of which is displayed. Before adding messages, a context identifier , a unique unsigned integer associated with a context description string, must be retrieved from the newly created Statusbar by passing a string value to get_context_id() . This allows messages to be categorized and pushed to separate stacks. statusbar . push ( context_id , message )","title":"Statusbar"},{"location":"Rust/Crates/Gtk-rs/#switch","text":"PyGobject gtk-rs gtk Gtk.Switch gtk4::Switch GtkSwitch (4.0) Gtk.Switch allows a user to toggle a boolean value. Switch exposes getters and setters for both state (which is represented by the trough color) and active (switch position) properties. State is the backend to activ, and they are kept in sync. import gi gi . require_version ( \"Gtk\" , \"3.0\" ) from gi.repository import Gtk class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_border_width ( 10 ) box_outer = Gtk . Box ( orientation = Gtk . Orientation . VERTICAL , spacing = 6 ) listbox = Gtk . ListBox ( selection_mode = Gtk . SelectionMode . NONE ) row = Gtk . ListBoxRow () hbox = Gtk . Box ( orientation = Gtk . Orientation . HORIZONTAL , spacing = 50 ) label1 = Gtk . Label ( label = \"Automatic Date & Time\" , xalign = 0 ) hbox . add ( label1 ) self . switch = Gtk . Switch ( valign = Gtk . Align . CENTER , state = False ) hbox . add ( self . switch ) row . add ( hbox ) listbox . add ( row ) box_outer . add ( listbox ) self . add ( box_outer ) button = Gtk . Button ( label = \"Click\" ) button . connect ( \"clicked\" , self . on_button_clicked ) box_outer . add ( button ) def on_button_clicked ( self , button ): print ( f \"Value of get_active(): { self . switch . get_active () } \" ) print ( f \"Value of get_state(): { self . switch . get_state () } \" ) class Application ( Gtk . Application ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . window = None def do_activate ( self ): if not self . window : self . window = ApplicationWindow ( application = self ) self . window . show_all () self . window . present () if __name__ == \"__main__\" : app = Application () app . run ()","title":"Switch"},{"location":"Rust/Crates/Gtk-rs/#treeview","text":"PyGobject gtk-rs gtk Gtk.TreeView gtk4::TreeView GtkTreeView (4.0) GtkTreeView (3.0) In order to create a tree or list in GTK, the Gtk.TreeView widget is paired with a Gtk.TreeModel interface, the most typical implementation of which is Gtk.ListStore or Gtk.TreeStore . TreeView is a complicated widget that must be constructed procedurally: Gtk.TreeView is instantiated. A ListStore is specified as data model and passed in as the value of the model kwarg. The ListStore specifies the schema of the data as a collection of types. treeview = Gtk . TreeView ( model = Gtk . ListStore . new (( str ))) Alternatively, the ListStore can be specified after instantiation. treeview = Gtk . TreeView . new () treeview . set_model ( Gtk . ListStore . new ([ str ])) A Gtk.TreeViewColumn is created for every column in the model. These require a Gtk.CellRenderer to be defined. The TreeViewColumn is added to the treeview by calling the append_column() method on the treeview. The text kwarg appears to refer to the column of the data store to use for the column's values. treeview . append_column ( Gtk . TreeViewColumn ( \"Greeks\" , Gtk . CellRendererText . new (), text = 0 )) Items are added to the ListStore procedurally using the append() method. Note that the method takes only a single argument, so collections like lists or tuples must be used. liststore . append (( \"Socrates\" ,)) liststore . append (( \"Plato\" ,)) liststore . append (( \"Aristotle\" ,)) Changing the number of columns affects the types used to define the ListStore, the appended records, as well as the number of columns added to the TreeView itself. 1 column 2 columns import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . gen_treeview () scrolled_win = Gtk . ScrolledWindow . new ( None , None ) scrolled_win . set_policy ( Gtk . PolicyType . AUTOMATIC , Gtk . PolicyType . AUTOMATIC ) scrolled_win . add ( self . treeview ) self . add ( scrolled_win ) self . set_size_request ( 200 , 200 ) def get_liststore ( self ): store = Gtk . ListStore . new (( str ,)) store . append (( \"Socrates\" ,)) store . append (( \"Plato\" ,)) store . append (( \"Aristotle\" ,)) return store def gen_treeview ( self ): self . treeview = Gtk . TreeView . new () self . treeview . set_model ( self . get_liststore ()) self . treeview . append_column ( Gtk . TreeViewColumn ( \"Greeks\" , Gtk . CellRendererText . new (), text = 0 )) class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.myapp' ) def do_activate ( self ): self . window = ApplicationWindow ( application = self , title = \"Greeks\" ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = Application () app . run () import gi gi . require_version ( 'Gtk' , '3.0' ) from gi.repository import Gtk class ApplicationWindow ( Gtk . ApplicationWindow ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . gen_treeview () scrolled_win = Gtk . ScrolledWindow . new ( None , None ) scrolled_win . set_policy ( Gtk . PolicyType . AUTOMATIC , Gtk . PolicyType . AUTOMATIC ) scrolled_win . add ( self . treeview ) self . add ( scrolled_win ) self . set_size_request ( 200 , 200 ) def get_liststore ( self ): store = Gtk . ListStore . new (( str , str )) store . append ([ \"Socrates\" , \"Athens\" ]) store . append ([ \"Plato\" , \"Athens\" ]) store . append ([ \"Aristotle\" , \"Athens\" ]) return store def gen_treeview ( self ): self . treeview = Gtk . TreeView . new () self . treeview . set_model ( self . get_liststore ()) self . treeview . append_column ( Gtk . TreeViewColumn ( \"Greeks\" , Gtk . CellRendererText . new (), text = 0 )) self . treeview . append_column ( Gtk . TreeViewColumn ( \"Place of birth\" , Gtk . CellRendererText . new (), text = 1 )) class Application ( Gtk . Application ): def __init__ ( self ): super () . __init__ ( application_id = 'org.example.myapp' ) def do_activate ( self ): self . window = ApplicationWindow ( application = self , title = \"Greeks\" ) self . window . show_all () self . window . present () if __name__ == '__main__' : app = Application () app . run () The model backing a TreeView (usually a ListStore ), can be retrieved with the get_model() method. treeview . get_model () . append (( 'foo' , 'bar' )) TreeView emits several signals: row_activated when a row is double-clicked, with the following implicit argument widget refering to the emitting TreeView widget itself path is a TreePath . column is of type TreeViewcolumn treeview . connect ( \"row_activated\" , self . on_row_activated , widget , path , column ) def on_row_activated ( self , widget , path , column ): row = path . get_indices ()[ 0 ] print ( f \"row= { path . get_indices ()[ 0 ] } ,col= { column . props . title } \" ) print ( widget . get_model ()[ row ][:])","title":"TreeView"},{"location":"Rust/Crates/Gtk-rs/#treepath","text":"PyGobject gtk-rs gtk Gtk.TreePath is a type used to implement the rows of a TreeView . Although it prints to an integer with the print statement, it cannot be treated as one. A path object can be passed as the index to a TreeModel like ListStore , as can an integer. The row number of a TreePath from a normal list-style TreeView can be retrieved with the get_indices() method. row = path . get_indices ()[ 0 ] # Using TreePath object as index to model model [ path ][:] # Using row integer as index to model model [ row ][:] Another method on TreePath, get_depth() always returns 1 for list-style TreeViews, but may be more useful for tree-style TreeViews.","title":"TreePath"},{"location":"Rust/Crates/Gtk-rs/#treeselection","text":"PyGobject gtk-rs gtk Gtk.TreeSelection objects represent selection information for each tree view.","title":"TreeSelection"},{"location":"Rust/Crates/Gtk-rs/#treeviewcolumn_1","text":"PyGobject gtk-rs gtk Gtk.TreekViewColumn gtk4::TreeViewColumn GtkTreeViewColumn (4.0) GtkTreeViewColumn (3.0) Gtk.TreeViewColumn represents a visible column in a Treeview . Its props property exposes many associated values, including title. print ( column . props . title ) A column is made sortable by calling set_sort_column_id() , passing the column of the model to sort by. column . set_sort_column_id ( 0 )","title":"TreeViewColumn"},{"location":"Rust/Crates/Gtk-rs/#window","text":"PyGobject gtk-rs gtk Gtk.Window gtk4::Window GtkWindow (4.0) GtkWindow (3.0)","title":"Window"},{"location":"Rust/Crates/Rnd/","text":"rnd gen_ratio() return true based on the probability defined by the fraction defined by the arguments (i.e. gen_ratio(1,5) returns true 20% of the time)","title":"rnd"},{"location":"Rust/Crates/Rnd/#rnd","text":"gen_ratio() return true based on the probability defined by the fraction defined by the arguments (i.e. gen_ratio(1,5) returns true 20% of the time)","title":"rnd"},{"location":"Rust/Crates/Rocket/","text":"Rocket API changes Major API changes are in store in the transition from 0.4.10 (current stable version as of the time of this writing) and 0.5.0-rc.1. The guide is written for 0.4 and there is no 0.5 version published yet. These changes include a reorganization of some traits, such as FromForm from request to the new form module. Rocket is a web framework for Rust, along the lines of Flask for Python. The lifecyle of a Rocket request is as follows: Routing -> Validation -> Processing -> Response The process of building a Rocket application has several stages: Mounting routes Managing state Attach fairings Tasks Configuration Rocket can be configured using a Rocket.toml file placed at the crate root to specify host address, port, etc. Some of these settings appear to be necessary for certain REST clients. Rocket.toml [development] address = \"127.0.0.1\" port = 8000 Hello, World! Simple #[macro_use] extern crate rocket ; #[launch] // (1) fn rocket () -> _ { rocket :: build () . mount ( \"/\" , routes ! [ index ]) } #[get( \"/\" )] fn index () -> & ' static str { \"Hello, World!\" } The #[launch] attribute actually generates an async runtime. #[macro_use] extern crate rocket ; #[rocket::main] async fn main () { rocket :: build () . mount ( \"/\" , routes ! [ index ]) . launch (). await ; } #[get( \"/\" )] async fn index () -> & ' static str { \"Hello, World!\" } Parameterized Parameterization is enabled by dynamic routes : #[macro_use] extern crate rocket ; #[launch] fn rocket () -> _ { rocket :: build () . mount ( \"/\" , routes ! [ index ]) } #[get( \"/<name>\" )] // (1) fn index ( name : String ) -> String { format! [ \"Hello, {}!\" , name ] } Alternatively, using a query segment: #[get( \"/?<name>)\" ] In which case the handler would only to the path \" /?name=... \", i.e. curl localhost:8000/?name = Jasper State use rocket :: { get , routes , launch , State }; struct MyConfig { msg : String } #[launch] fn rocket () -> _ { rocket :: build () . mount ( \"/\" , routes ! [ index ,]) . manage ( MyConfig { msg : \"Hello, World!\" . to_string () }) } #[get( \"/\" )] fn index ( state : & State < MyConfig > ) -> String { // (1) String :: from ( & state . msg ) } State generally appears only in the parameter list of route handlers as a reference which must be initialized in the rocket crate itself. File server use rocket :: launch ; // (2) use rocket :: fs :: FileServer ; #[launch] fn rocket () -> _ { rocket :: build () . mount ( \"/\" , FileServer :: from ( \"site\" )) } Template use rocket :: { get , routes , launch , }; use rocket_dyn_templates :: Template ; // (3) #[derive(serde::Serialize)] // (1) struct Message { msg : String } #[launch] fn rocket () -> _ { rocket :: build () . attach ( Template :: fairing ()) // (2) . mount ( \"/\" , routes ! [ index ,]) } #[get( \"/\" )] fn index () -> Template { let context = Message { msg : String :: from ( \"Hello, World!\" ) }; Template :: render ( \"index\" , & context ) // (4) } The context struct used to insert information into the template must have the Serialize derive. Template::fairing() must be attached to the running Rocket instance. A fairing in Rocket parlance refers to structured middleware which expose hooks that allow callbacks to be placed into the request lifecycle to rewrite incoming requests and outgoing responses. Prior to 0.5, the Template struct was in the rocket_contrib crate. Since 0.5, the rocket_dyn_templates crate requires at least one of two features to be enabled to use Template. Cargo.toml rocket_dyn_templates = { version = \"0.1.0-rc.1\" , features = [ \"handlebars\" , \"tera\" ]} Template names passed to Template::render() must correspond to files placed in the path set by the template_dir configuration parameter. The process of Rocket finding these templates is called discovery . index.html.hbs <!doctype html> < html > < head > < title > {{msg}} </ title > </ head > < body > < p > {{msg}} </ p > </ body > </ html > Styled template use rocket :: { get , routes , launch , }; use rocket_dyn_templates :: Template ; // (2) use rocket :: fs :: FileServer ; #[derive(serde::Serialize)] struct Message { msg : String } #[launch] fn rocket () -> _ { rocket :: build () . attach ( Template :: fairing ()) . mount ( \"/\" , FileServer :: from ( \"static\" )) . mount ( \"/\" , routes ! [ index ,]) } #[get( \"/\" )] fn index () -> Template { let context = Message { msg : String :: from ( \"Hello, World!\" ) }; Template :: render ( \"index\" , & context ) // (1) } This template is themed using the Bulma CSS framework, which is served as a static file above. index head head link rel=\"stylesheet\" href=\"bulma.css\" title {{msg}} body section .hero.is-primary .hero-body .container h1 .title {{msg}} Cargo.toml rocket_dyn_templates = { version = \"0.1.0-rc.1\" , features = [ \"handlebars\" ], default-features = false } Starships A naive list-details application can be implemented using the lazy_static module to create a trivial in-memory database. #[macro_use] extern crate rocket ; use lazy_static :: lazy_static ; use std :: collections :: HashMap ; lazy_static ! { static ref STARSHIPS : HashMap <&' static str , Starship > = { let mut map = HashMap :: new (); map . insert ( \"NCC-1701\" , Starship { name : String :: from ( \"USS Enterprise\" ), registry : String :: from ( \"NCC-1701\" ), crew : 203 , }, ); map . insert ( \"NX-74205\" , Starship { name : String :: from ( \"USS Defiant\" ), registry : String :: from ( \"NX-74205\" ), crew : 50 , } ); map }; } #[derive(Debug)] struct Starship { name : String , registry : String , crew : usize , } #[launch] fn rocket () -> _ { rocket :: build (). mount ( \"/\" , routes ! [ ship ,]) } #[get( \"/<registry>\" )] fn ship ( registry : & str ) -> String { let starship = STARSHIPS . get ( registry ); // (1) match starship { Some ( s ) => format! [ \"Found starship: {:?}\" , s ], None => String :: from ( \"No starship found!\" ), } } TODO: This is a good opportunity to use simple string manipulation to make the query case-insensitive, but I can't seem to get it to work. I have to figure out a way to incorporate case-insensitivity here. Also potentially a place to implement regex.. Benchmarking Web applications can be benchmarked using the benchrs tool cargo install benchrs benchrs -c 30 -n 3000 -k http://127.0.0.1:8000/ Glossary Catcher A route handler returning an Option will trigger the 404 error handler or catcher when the None variant is returned. #[catch(404)] fn not_found ( req : & Request ) -> String { format! ( \"{} not found\" , req . uri ()) } Analogous to the mount method and routes! macro for routes, catchers are associated with a Rocket application using the register method and catchers! macro. #[launch] fn rocket () -> _ { rocket :: build () . mount ( \"/\" , routes ! [ index ,]) . register ( \"/\" , catchers ! [ not_found ,]) } Fairing Fairings are Rocket's approach to structured middleware which hook into the request lifecycle and expose callbacks for events such as incoming requests and outgoing responses. The default builtin fairing is Shield , which injects HTTP security and privacy headers to all responses by default. Fairings (callbacks) are attached (registered) to the application's Rocket instance with the attach() method. Some structs like Template expose a fairing() method. #[launch] fn rocket () -> _ { rocket :: build () . attach ( Template :: fairing ()) } Fairings can be created from a function or closure using the AdHoc struct . Forms Forms refers to data submitted by users, referring to the information provided by users on e.g. a subscription page. In rocket these are processed into structs which are decorated with the FreeForm derivable trait, making them form guards . FreeForm is used for collections, that is when more than one form field is available for parsing. Types with a single form field should implement FromFormField instead. Guards Request guards are used to arbitrarily validate requests, particularly API keys. Types that implement the FromRequest trait (and specifically the from_request method, returning an Outcome enum) are request guards. Builtin request guards include CookieJar . They appear as additional parameters in the signature of a route handler. #[get( \"/<param>\" )] fn index ( param : isize , a : A , b : B , c : C ) { /* .. */ } // (1) Here, a , b , and c are request guards. The term data guard refers to types that implement FromData (i.e. rocket::serde::json::Json which is also a form guard). use rocket :: serde :: json :: Json ; use serde :: Deserialize ; #[derive(Deserialize)] struct User { /* ... */ } #[post( \"/user\" , format = \"json\" , data = \"<user>\" )] fn new_user ( user : Json < User > ) { /* ... */ } Databases Integration with database libraries is done through feature flags on the rocket_sync_db_pools crate. Diesel Postgres database pool rocket_sync_db_pools = { version = \"0.1.0-rc.1\" , default-features = false , features = [ \"diesel_postgres_pool\" ,]} Launch A Rocket instance represents a web server and its state, and occupies one of three phases during its lifecycle, each of which is identifiable with a trait: Build enables setting configuration, mounting and registering routes, managing state, and attaching fairings . Ignite represents finalized configuration. Orbit represents a running server. The boilerplate for a Rocket instance in fact returns the application in the Build phase #[macro_use] extern crate rocket ; use rocket :: { Rocket , Build }; #[launch] fn rocket () -> Rocket < Build > { rocket :: build () } Rocket instance in either Build or Ignite phases can be launched by running Rocket::launch() Response Route Routes are associated with handler functions and are typically composed of an HTTP method (GET, POST, etc) and a URI which is further composed of a path and a query . #[get( \"/\" )] fn index () -> & ' static str { \"Hello, World!\" } Both paths and queries can be decomposed into segments, delimited by slashes in the path and ampersands in the query. Any segment can be static or dynamic . Dynamic segments correspond with an eponymous variable that is passed to the route handler. The data type must implement FromParam . Many common primitives, including numbers and Strings, already implement this trait by default. Path segment Query segment #[get( \"/<name>\" )] fn index ( name : String ) -> String { format! [ \"Hello, {}!\" , name ] } #[get( \"/?<name>\" )] fn index ( name : String ) -> String { format! [ \"Hello, {}!\" , name ] } Another dynamic form exists with trailing .. called multiple segments , i.e. #[get(\"/<name..>)\"] . Such types must implement FromSegments . The existing FromSegments implementation for PathBuf already prevents insecure traversal paths using .. . Finally the ignored segment <_> or <_..> is a special case which will not appear in the argument list. #[get( \"/<_>\" )] Multiple handlers can also be defined for the same route, in which case each must have a rank . Routes can also define a format , which is useful in POST, PUT, and DELETE requests where there is a payload. These formats are IANA media types (lowercase), as well as some aliases, such as \"html\" for \"text/html; charset=utf-8\" etc. State A Rocket instance can manage any type that implements Send and Sync with the manage method. A managed state is typically used to handle a persistent database connection. use std :: sync :: atomic :: AtomicU64 ; struct VisitorCounter { visitor : AtomicU64 , } fn rocket () -> _ { let counter = VisitorCounter { visitor : AtomicU64 :: new ( 0 ), }; rocket :: build () . manage ( counter ) . mount ( \"/\" , <!-- .. . - -> ) } This state is exposed as the State request guard: #[get( \"/\" )] fn route ( counter : & State < VisitorCounter > ,) { counter . visitor . fetch_add ( 1 , Ordering :: Relaxed ); println! ( \"The number of visitors is: {}\" , counter . visitor . load ( Ordering :: Relaxed )); }","title":"Rocket"},{"location":"Rust/Crates/Rocket/#rocket","text":"API changes Major API changes are in store in the transition from 0.4.10 (current stable version as of the time of this writing) and 0.5.0-rc.1. The guide is written for 0.4 and there is no 0.5 version published yet. These changes include a reorganization of some traits, such as FromForm from request to the new form module. Rocket is a web framework for Rust, along the lines of Flask for Python. The lifecyle of a Rocket request is as follows: Routing -> Validation -> Processing -> Response The process of building a Rocket application has several stages: Mounting routes Managing state Attach fairings","title":"Rocket"},{"location":"Rust/Crates/Rocket/#tasks","text":"","title":"Tasks"},{"location":"Rust/Crates/Rocket/#configuration","text":"Rocket can be configured using a Rocket.toml file placed at the crate root to specify host address, port, etc. Some of these settings appear to be necessary for certain REST clients. Rocket.toml [development] address = \"127.0.0.1\" port = 8000","title":"Configuration"},{"location":"Rust/Crates/Rocket/#hello-world","text":"","title":"Hello, World!"},{"location":"Rust/Crates/Rocket/#simple","text":"#[macro_use] extern crate rocket ; #[launch] // (1) fn rocket () -> _ { rocket :: build () . mount ( \"/\" , routes ! [ index ]) } #[get( \"/\" )] fn index () -> & ' static str { \"Hello, World!\" } The #[launch] attribute actually generates an async runtime. #[macro_use] extern crate rocket ; #[rocket::main] async fn main () { rocket :: build () . mount ( \"/\" , routes ! [ index ]) . launch (). await ; } #[get( \"/\" )] async fn index () -> & ' static str { \"Hello, World!\" }","title":"Simple"},{"location":"Rust/Crates/Rocket/#parameterized","text":"Parameterization is enabled by dynamic routes : #[macro_use] extern crate rocket ; #[launch] fn rocket () -> _ { rocket :: build () . mount ( \"/\" , routes ! [ index ]) } #[get( \"/<name>\" )] // (1) fn index ( name : String ) -> String { format! [ \"Hello, {}!\" , name ] } Alternatively, using a query segment: #[get( \"/?<name>)\" ] In which case the handler would only to the path \" /?name=... \", i.e. curl localhost:8000/?name = Jasper State use rocket :: { get , routes , launch , State }; struct MyConfig { msg : String } #[launch] fn rocket () -> _ { rocket :: build () . mount ( \"/\" , routes ! [ index ,]) . manage ( MyConfig { msg : \"Hello, World!\" . to_string () }) } #[get( \"/\" )] fn index ( state : & State < MyConfig > ) -> String { // (1) String :: from ( & state . msg ) } State generally appears only in the parameter list of route handlers as a reference which must be initialized in the rocket crate itself. File server use rocket :: launch ; // (2) use rocket :: fs :: FileServer ; #[launch] fn rocket () -> _ { rocket :: build () . mount ( \"/\" , FileServer :: from ( \"site\" )) } Template use rocket :: { get , routes , launch , }; use rocket_dyn_templates :: Template ; // (3) #[derive(serde::Serialize)] // (1) struct Message { msg : String } #[launch] fn rocket () -> _ { rocket :: build () . attach ( Template :: fairing ()) // (2) . mount ( \"/\" , routes ! [ index ,]) } #[get( \"/\" )] fn index () -> Template { let context = Message { msg : String :: from ( \"Hello, World!\" ) }; Template :: render ( \"index\" , & context ) // (4) } The context struct used to insert information into the template must have the Serialize derive. Template::fairing() must be attached to the running Rocket instance. A fairing in Rocket parlance refers to structured middleware which expose hooks that allow callbacks to be placed into the request lifecycle to rewrite incoming requests and outgoing responses. Prior to 0.5, the Template struct was in the rocket_contrib crate. Since 0.5, the rocket_dyn_templates crate requires at least one of two features to be enabled to use Template. Cargo.toml rocket_dyn_templates = { version = \"0.1.0-rc.1\" , features = [ \"handlebars\" , \"tera\" ]} Template names passed to Template::render() must correspond to files placed in the path set by the template_dir configuration parameter. The process of Rocket finding these templates is called discovery . index.html.hbs <!doctype html> < html > < head > < title > {{msg}} </ title > </ head > < body > < p > {{msg}} </ p > </ body > </ html > Styled template use rocket :: { get , routes , launch , }; use rocket_dyn_templates :: Template ; // (2) use rocket :: fs :: FileServer ; #[derive(serde::Serialize)] struct Message { msg : String } #[launch] fn rocket () -> _ { rocket :: build () . attach ( Template :: fairing ()) . mount ( \"/\" , FileServer :: from ( \"static\" )) . mount ( \"/\" , routes ! [ index ,]) } #[get( \"/\" )] fn index () -> Template { let context = Message { msg : String :: from ( \"Hello, World!\" ) }; Template :: render ( \"index\" , & context ) // (1) } This template is themed using the Bulma CSS framework, which is served as a static file above. index head head link rel=\"stylesheet\" href=\"bulma.css\" title {{msg}} body section .hero.is-primary .hero-body .container h1 .title {{msg}} Cargo.toml rocket_dyn_templates = { version = \"0.1.0-rc.1\" , features = [ \"handlebars\" ], default-features = false }","title":"Parameterized"},{"location":"Rust/Crates/Rocket/#starships","text":"A naive list-details application can be implemented using the lazy_static module to create a trivial in-memory database. #[macro_use] extern crate rocket ; use lazy_static :: lazy_static ; use std :: collections :: HashMap ; lazy_static ! { static ref STARSHIPS : HashMap <&' static str , Starship > = { let mut map = HashMap :: new (); map . insert ( \"NCC-1701\" , Starship { name : String :: from ( \"USS Enterprise\" ), registry : String :: from ( \"NCC-1701\" ), crew : 203 , }, ); map . insert ( \"NX-74205\" , Starship { name : String :: from ( \"USS Defiant\" ), registry : String :: from ( \"NX-74205\" ), crew : 50 , } ); map }; } #[derive(Debug)] struct Starship { name : String , registry : String , crew : usize , } #[launch] fn rocket () -> _ { rocket :: build (). mount ( \"/\" , routes ! [ ship ,]) } #[get( \"/<registry>\" )] fn ship ( registry : & str ) -> String { let starship = STARSHIPS . get ( registry ); // (1) match starship { Some ( s ) => format! [ \"Found starship: {:?}\" , s ], None => String :: from ( \"No starship found!\" ), } } TODO: This is a good opportunity to use simple string manipulation to make the query case-insensitive, but I can't seem to get it to work. I have to figure out a way to incorporate case-insensitivity here. Also potentially a place to implement regex..","title":"Starships"},{"location":"Rust/Crates/Rocket/#benchmarking","text":"Web applications can be benchmarked using the benchrs tool cargo install benchrs benchrs -c 30 -n 3000 -k http://127.0.0.1:8000/","title":"Benchmarking"},{"location":"Rust/Crates/Rocket/#glossary","text":"","title":"Glossary"},{"location":"Rust/Crates/Rocket/#catcher","text":"A route handler returning an Option will trigger the 404 error handler or catcher when the None variant is returned. #[catch(404)] fn not_found ( req : & Request ) -> String { format! ( \"{} not found\" , req . uri ()) } Analogous to the mount method and routes! macro for routes, catchers are associated with a Rocket application using the register method and catchers! macro. #[launch] fn rocket () -> _ { rocket :: build () . mount ( \"/\" , routes ! [ index ,]) . register ( \"/\" , catchers ! [ not_found ,]) }","title":"Catcher"},{"location":"Rust/Crates/Rocket/#fairing","text":"Fairings are Rocket's approach to structured middleware which hook into the request lifecycle and expose callbacks for events such as incoming requests and outgoing responses. The default builtin fairing is Shield , which injects HTTP security and privacy headers to all responses by default. Fairings (callbacks) are attached (registered) to the application's Rocket instance with the attach() method. Some structs like Template expose a fairing() method. #[launch] fn rocket () -> _ { rocket :: build () . attach ( Template :: fairing ()) } Fairings can be created from a function or closure using the AdHoc struct .","title":"Fairing"},{"location":"Rust/Crates/Rocket/#forms","text":"Forms refers to data submitted by users, referring to the information provided by users on e.g. a subscription page. In rocket these are processed into structs which are decorated with the FreeForm derivable trait, making them form guards . FreeForm is used for collections, that is when more than one form field is available for parsing. Types with a single form field should implement FromFormField instead.","title":"Forms"},{"location":"Rust/Crates/Rocket/#guards","text":"Request guards are used to arbitrarily validate requests, particularly API keys. Types that implement the FromRequest trait (and specifically the from_request method, returning an Outcome enum) are request guards. Builtin request guards include CookieJar . They appear as additional parameters in the signature of a route handler. #[get( \"/<param>\" )] fn index ( param : isize , a : A , b : B , c : C ) { /* .. */ } // (1) Here, a , b , and c are request guards. The term data guard refers to types that implement FromData (i.e. rocket::serde::json::Json which is also a form guard). use rocket :: serde :: json :: Json ; use serde :: Deserialize ; #[derive(Deserialize)] struct User { /* ... */ } #[post( \"/user\" , format = \"json\" , data = \"<user>\" )] fn new_user ( user : Json < User > ) { /* ... */ }","title":"Guards"},{"location":"Rust/Crates/Rocket/#databases","text":"Integration with database libraries is done through feature flags on the rocket_sync_db_pools crate. Diesel Postgres database pool rocket_sync_db_pools = { version = \"0.1.0-rc.1\" , default-features = false , features = [ \"diesel_postgres_pool\" ,]}","title":"Databases"},{"location":"Rust/Crates/Rocket/#launch","text":"A Rocket instance represents a web server and its state, and occupies one of three phases during its lifecycle, each of which is identifiable with a trait: Build enables setting configuration, mounting and registering routes, managing state, and attaching fairings . Ignite represents finalized configuration. Orbit represents a running server. The boilerplate for a Rocket instance in fact returns the application in the Build phase #[macro_use] extern crate rocket ; use rocket :: { Rocket , Build }; #[launch] fn rocket () -> Rocket < Build > { rocket :: build () } Rocket instance in either Build or Ignite phases can be launched by running Rocket::launch()","title":"Launch"},{"location":"Rust/Crates/Rocket/#response","text":"","title":"Response"},{"location":"Rust/Crates/Rocket/#route","text":"Routes are associated with handler functions and are typically composed of an HTTP method (GET, POST, etc) and a URI which is further composed of a path and a query . #[get( \"/\" )] fn index () -> & ' static str { \"Hello, World!\" } Both paths and queries can be decomposed into segments, delimited by slashes in the path and ampersands in the query. Any segment can be static or dynamic . Dynamic segments correspond with an eponymous variable that is passed to the route handler. The data type must implement FromParam . Many common primitives, including numbers and Strings, already implement this trait by default. Path segment Query segment #[get( \"/<name>\" )] fn index ( name : String ) -> String { format! [ \"Hello, {}!\" , name ] } #[get( \"/?<name>\" )] fn index ( name : String ) -> String { format! [ \"Hello, {}!\" , name ] } Another dynamic form exists with trailing .. called multiple segments , i.e. #[get(\"/<name..>)\"] . Such types must implement FromSegments . The existing FromSegments implementation for PathBuf already prevents insecure traversal paths using .. . Finally the ignored segment <_> or <_..> is a special case which will not appear in the argument list. #[get( \"/<_>\" )] Multiple handlers can also be defined for the same route, in which case each must have a rank . Routes can also define a format , which is useful in POST, PUT, and DELETE requests where there is a payload. These formats are IANA media types (lowercase), as well as some aliases, such as \"html\" for \"text/html; charset=utf-8\" etc.","title":"Route"},{"location":"Rust/Crates/Rocket/#state","text":"A Rocket instance can manage any type that implements Send and Sync with the manage method. A managed state is typically used to handle a persistent database connection. use std :: sync :: atomic :: AtomicU64 ; struct VisitorCounter { visitor : AtomicU64 , } fn rocket () -> _ { let counter = VisitorCounter { visitor : AtomicU64 :: new ( 0 ), }; rocket :: build () . manage ( counter ) . mount ( \"/\" , <!-- .. . - -> ) } This state is exposed as the State request guard: #[get( \"/\" )] fn route ( counter : & State < VisitorCounter > ,) { counter . visitor . fetch_add ( 1 , Ordering :: Relaxed ); println! ( \"The number of visitors is: {}\" , counter . visitor . load ( Ordering :: Relaxed )); }","title":"State"},{"location":"Rust/Crates/Rusqlite/","text":"rusqlite rusqlite is a wrapper for using SQLite from Rust. Similar to the sqlite3 module in the Python standard library, procedural SQL commands are passed as strings to the execute method of a Connection object. Tasks Boilerplate fn main () -> Result < () > { let conn = rusqlite :: Connection :: open ( \"database.db\" ) ? ; conn . execute ( \"SELECT * FROM TABLE\" , []) ? ; Ok (()) } Create table use rusqlite :: { Connection , Result }; fn main () -> Result < () > { let conn = Connection :: open ( \"starships.db\" ) ? ; // (1) conn . execute ( \"CREATE TABLE IF NOT EXISTS starships ( registry TEXT PRIMARY KEY, name TEXT NOT NULL, crew INTEGER )\" ,[] ) ? ; Ok (()) } This will create the database file if it does not exist. Populate table use rusqlite :: { Connection , Result }; struct Starship { name : String , registry : String , crew : u32 } fn main () -> Result < () > { let conn = Connection :: open ( \"starships.db\" ) ? ; let enterprise = Starship { name : \"USS Enterprise\" . to_string (), registry : \"NCC-1701\" . to_string (), crew : 401 }; conn . execute ( \"INSERT INTO starships (name, registry, crew) values (?1, ?2, ?3)\" , // (1) [ enterprise . name , enterprise . registry , enterprise . crew . to_string ()] // (2) ) ? ; Ok (()) } Note the unusual template syntax. All passed values must be passed as String structs.","title":"rusqlite"},{"location":"Rust/Crates/Rusqlite/#rusqlite","text":"rusqlite is a wrapper for using SQLite from Rust. Similar to the sqlite3 module in the Python standard library, procedural SQL commands are passed as strings to the execute method of a Connection object.","title":"rusqlite"},{"location":"Rust/Crates/Rusqlite/#tasks","text":"","title":"Tasks"},{"location":"Rust/Crates/Rusqlite/#boilerplate","text":"fn main () -> Result < () > { let conn = rusqlite :: Connection :: open ( \"database.db\" ) ? ; conn . execute ( \"SELECT * FROM TABLE\" , []) ? ; Ok (()) }","title":"Boilerplate"},{"location":"Rust/Crates/Rusqlite/#create-table","text":"use rusqlite :: { Connection , Result }; fn main () -> Result < () > { let conn = Connection :: open ( \"starships.db\" ) ? ; // (1) conn . execute ( \"CREATE TABLE IF NOT EXISTS starships ( registry TEXT PRIMARY KEY, name TEXT NOT NULL, crew INTEGER )\" ,[] ) ? ; Ok (()) } This will create the database file if it does not exist.","title":"Create table"},{"location":"Rust/Crates/Rusqlite/#populate-table","text":"use rusqlite :: { Connection , Result }; struct Starship { name : String , registry : String , crew : u32 } fn main () -> Result < () > { let conn = Connection :: open ( \"starships.db\" ) ? ; let enterprise = Starship { name : \"USS Enterprise\" . to_string (), registry : \"NCC-1701\" . to_string (), crew : 401 }; conn . execute ( \"INSERT INTO starships (name, registry, crew) values (?1, ?2, ?3)\" , // (1) [ enterprise . name , enterprise . registry , enterprise . crew . to_string ()] // (2) ) ? ; Ok (()) } Note the unusual template syntax. All passed values must be passed as String structs.","title":"Populate table"},{"location":"Rust/Crates/Serde/","text":"serde The serde_json crate provides a rich API for interacting with JSON files. The json macro can be used to serialize any struct decorated with Serialize. use serde_json :: { json , Value }; use serde :: { Deserialize , Serialize }; // (1) #[derive(Debug, Serialize, Deserialize)] struct Starship { name : String , registry : String , crew : u64 } fn main () { let starship : Starship = Starship { name : \"USS Enterprise\" . to_string (), registry : \"NCC-1701\" . to_string (), crew : 400 }; println! ( \"{}\" , json ! ( \"Enterprise\" : { starship })); } The derive attributes Serialize and Deserialize are included from serde directly but only after enabling the derive feature (and not the serde_derive crate). [dependencies] serde = { version = \"^1.0\" , features = [ \"derive\" ]} Although a simpler and more naive implementation is possible, the recommended use of the API is to define a struct that reflects the model of the JSON document and to type the destination variable accordingly. ? There appears to be some bizarre error in the example using &str below. String &str #[derive(Debug, serde_derive::Deserialize)] enum Series { TOS , DS9 , VOY , } #[derive(Debug, serde_derive::Deserialize)] struct Starship { name : String , registry : String , series : Series , } fn main () { let ships : Vec < Starship > = gen_ships ( \"starships.json\" ); println! ( \"{:?}\" , ships ); } fn gen_ships ( fname : & str ) -> Vec < Starship > { let file = std :: fs :: read_to_string ( & fname ). unwrap (); let ships : Vec < Starship > = serde_json :: from_str ( & file ). unwrap (); ships } #[derive(Debug, serde_derive::Deserialize)] enum Series { TOS , DS9 , VOY , } #[derive(Debug, serde_derive::Deserialize)] struct Starship <' a > { name : & ' a str , registry : & ' a str , series : Series , } fn main () { let ships : Vec < Starship > = gen_ships ( \"starships.json\" ); println! ( \"{:?}\" , ships ); } fn gen_ships ( fname : & str ) -> Vec < Starship > { let file = std :: fs :: read_to_string ( & fname ). unwrap (); let ships : Vec < Starship > = serde_json :: from_str ( & file ). unwrap (); // (1) ships } For some reason, the compiler produces an error here, saying that &file is borrowed. Furthermore the compiler will not allow ships to be returned from the function because it \"returns a value referencing data owned by the current function\".","title":"serde"},{"location":"Rust/Crates/Serde/#serde","text":"The serde_json crate provides a rich API for interacting with JSON files. The json macro can be used to serialize any struct decorated with Serialize. use serde_json :: { json , Value }; use serde :: { Deserialize , Serialize }; // (1) #[derive(Debug, Serialize, Deserialize)] struct Starship { name : String , registry : String , crew : u64 } fn main () { let starship : Starship = Starship { name : \"USS Enterprise\" . to_string (), registry : \"NCC-1701\" . to_string (), crew : 400 }; println! ( \"{}\" , json ! ( \"Enterprise\" : { starship })); } The derive attributes Serialize and Deserialize are included from serde directly but only after enabling the derive feature (and not the serde_derive crate). [dependencies] serde = { version = \"^1.0\" , features = [ \"derive\" ]} Although a simpler and more naive implementation is possible, the recommended use of the API is to define a struct that reflects the model of the JSON document and to type the destination variable accordingly. ? There appears to be some bizarre error in the example using &str below. String &str #[derive(Debug, serde_derive::Deserialize)] enum Series { TOS , DS9 , VOY , } #[derive(Debug, serde_derive::Deserialize)] struct Starship { name : String , registry : String , series : Series , } fn main () { let ships : Vec < Starship > = gen_ships ( \"starships.json\" ); println! ( \"{:?}\" , ships ); } fn gen_ships ( fname : & str ) -> Vec < Starship > { let file = std :: fs :: read_to_string ( & fname ). unwrap (); let ships : Vec < Starship > = serde_json :: from_str ( & file ). unwrap (); ships } #[derive(Debug, serde_derive::Deserialize)] enum Series { TOS , DS9 , VOY , } #[derive(Debug, serde_derive::Deserialize)] struct Starship <' a > { name : & ' a str , registry : & ' a str , series : Series , } fn main () { let ships : Vec < Starship > = gen_ships ( \"starships.json\" ); println! ( \"{:?}\" , ships ); } fn gen_ships ( fname : & str ) -> Vec < Starship > { let file = std :: fs :: read_to_string ( & fname ). unwrap (); let ships : Vec < Starship > = serde_json :: from_str ( & file ). unwrap (); // (1) ships } For some reason, the compiler produces an error here, saying that &file is borrowed. Furthermore the compiler will not allow ships to be returned from the function because it \"returns a value referencing data owned by the current function\".","title":"serde"},{"location":"Rust/Crates/Structopt/","text":"structopt Tasks Hello, World! extern crate structopt ; use structopt :: StructOpt ; #[derive(StructOpt)] struct Options { #[structopt(default_value = \"World\" )] /// Name to greet name : String } fn main () { let options = Options :: from_args (); let name = options . name ; println! ( \"Hello, {}!\" , name ); }","title":"structopt"},{"location":"Rust/Crates/Structopt/#structopt","text":"","title":"structopt"},{"location":"Rust/Crates/Structopt/#tasks","text":"","title":"Tasks"},{"location":"Rust/Crates/Structopt/#hello-world","text":"extern crate structopt ; use structopt :: StructOpt ; #[derive(StructOpt)] struct Options { #[structopt(default_value = \"World\" )] /// Name to greet name : String } fn main () { let options = Options :: from_args (); let name = options . name ; println! ( \"Hello, {}!\" , name ); }","title":"Hello, World!"},{"location":"Rust/Crates/lazy_static/","text":"lazy_static use lazy_static :: lazy_static ; use std :: collections :: HashMap ; lazy_static ! { static ref STARSHIPS : HashMap <&' static str , Starship > = { let mut map = HashMap :: new (); map . insert ( \"NCC-1701\" , Starship { name : String :: from ( \"USS Enterprise\" ), registry : String :: from ( \"NCC-1701\" ), crew : 203 , }, ); map . insert ( \"NX-74205\" , Starship { name : String :: from ( \"USS Defiant\" ), registry : String :: from ( \"NX-74205\" ), crew : 50 , }, ); map }; }","title":"lazy\\_static"},{"location":"Rust/Crates/lazy_static/#lazy_static","text":"use lazy_static :: lazy_static ; use std :: collections :: HashMap ; lazy_static ! { static ref STARSHIPS : HashMap <&' static str , Starship > = { let mut map = HashMap :: new (); map . insert ( \"NCC-1701\" , Starship { name : String :: from ( \"USS Enterprise\" ), registry : String :: from ( \"NCC-1701\" ), crew : 203 , }, ); map . insert ( \"NX-74205\" , Starship { name : String :: from ( \"USS Defiant\" ), registry : String :: from ( \"NX-74205\" ), crew : 50 , }, ); map }; }","title":"lazy_static"},{"location":"Rust/Crates/Cursive/","text":"Overview Cursive is a TUI framework. Cursive widgets are called Views (i.e. TextView). Each screen's view tree has a StackView as root. Children are layers that can be pushed and popped. Callbacks are typically closures . Key presses are represented by Event enums. Alphanumeric keypresses are represented by the char itself. Single and multiple modifier key presses are represented by Ctrl(Key) , Alt(Key) , CtrlAlt(Key) , etc. Global keybindings are applied by calling Cursive::add_global_callback() on the application object. Keybindings that are effective only on particular Views are created by wrapping the View in OnEventView .","title":"Overview"},{"location":"Rust/Crates/Cursive/#overview","text":"Cursive is a TUI framework. Cursive widgets are called Views (i.e. TextView). Each screen's view tree has a StackView as root. Children are layers that can be pushed and popped. Callbacks are typically closures . Key presses are represented by Event enums. Alphanumeric keypresses are represented by the char itself. Single and multiple modifier key presses are represented by Ctrl(Key) , Alt(Key) , CtrlAlt(Key) , etc. Global keybindings are applied by calling Cursive::add_global_callback() on the application object. Keybindings that are effective only on particular Views are created by wrapping the View in OnEventView .","title":"Overview"},{"location":"Rust/Crates/Cursive/API/","text":"API Dialog Dialog can be constructed with new() or with the (barely) more concise around() helper method. around() new() use cursive :: { Cursive , CursiveExt }; use cursive :: views :: { TextView , Dialog }; fn main () { let mut siv = Cursive :: new (); siv . add_global_callback ( 'q' , | s | s . quit ()); siv . add_layer ( Dialog :: around ( TextView :: new ( \"Hello, world!\" )) . button ( \"Ok\" , | s | s . quit ()) ); siv . run (); } use cursive :: { Cursive , CursiveExt }; use cursive :: views :: { TextView , Dialog }; fn main () { let mut siv = Cursive :: new (); siv . add_global_callback ( 'q' , | s | s . quit ()); siv . add_layer ( Dialog :: new () . content ( TextView :: new ( \"Hello, world!\" )) . button ( \"Ok\" , | s | s . quit ()) ); siv . run (); } DummyView A DummyView is used as a spacer. use cursive :: views :: { DummyView , TextView , Dialog , LinearLayout }; use cursive :: { Cursive , CursiveExt }; fn main () { let mut siv = Cursive :: new (); siv . add_global_callback ( 'q' , | s | s . quit ()); siv . add_layer ( Dialog :: new () . title ( \"Hello, world!\" ) . content ( LinearLayout :: vertical () . child ( TextView :: new ( \"Hello, World!\" )) . child ( DummyView ) . child ( TextView :: new ( \"Hello again!\" )) ) ); siv . run (); } HexView Hexview is available from the cursive_hexview crate. use cursive :: view :: { Scrollable , Resizable }; use cursive_hexview :: { DisplayState , HexView , HexViewConfig }; use std :: env ; use std :: fs :: File ; use std :: io :: { self , Read }; use std :: path :: Path ; fn read_file ( path : & Path ) -> Result < Vec < u8 > , io :: Error > { let mut file = File :: open ( path ) ? ; let mut buf = Vec :: new (); file . read_to_end ( & mut buf ) ? ; Ok ( buf ) } fn main () { let arg = env :: args () . nth ( 1 ) . expect ( \"Provide a valid filename\" ); let path = Path :: new ( & arg ); let mut cur = cursive :: default (); let view = HexView :: new_from_iter ( read_file ( path ). expect ( \"Cannot read file\" )). display_state ( DisplayState :: Enabled ) . config ( HexViewConfig { bytes_per_line : 48 , bytes_per_group : 8 , .. Default :: default () }) . scrollable (). full_screen (); cur . add_layer ( view ); cur . run (); } LinearLayout A LinearLayout supports horizontal or vertical layout similar to a StackPanel in WinUI. use cursive :: views :: { DummyView , TextView , Dialog , LinearLayout }; use cursive :: { Cursive , CursiveExt }; fn main () { let mut siv = Cursive :: new (); siv . add_global_callback ( 'q' , | s | s . quit ()); siv . add_layer ( Dialog :: new () . title ( \"Hello, world!\" ) . content ( LinearLayout :: vertical () . child ( TextView :: new ( \"Hello, World!\" )) . child ( DummyView ) . child ( TextView :: new ( \"Hello again!\" )) ) ); siv . run (); } OnEventView OnEventView allows keybindings to take effect on wrapped Views. use cursive :: views :: { OnEventView , TextView }; use cursive :: event :: { Key , Event , EventTrigger }; use cursive :: Cursive ; fn main () { let mut siv = cursive :: default (); siv . add_layer ( OnEventView :: new ( TextView :: new ( \"Hello, World! \\n (Q to quit)\" )) . on_event ( 'q' , Cursive :: quit ) // (1) . on_event ( Event :: Key ( Key :: Enter ), | s | { // (2) s . pop_layer (); s . add_layer ( TextView :: new ( \"Enter key pressed!\" )); }) . on_event ( Event :: CtrlChar ( 'a' ), | s | { // (3) s . pop_layer (); s . add_layer ( TextView :: new ( \"Ctrl+A pressed!\" )) }) . on_event ( EventTrigger :: mouse (), | s | { // (4) s . pop_layer (); s . add_layer ( TextView :: new ( \"Mouse clicked!\" )); }) ); siv . run (); } Alphanumeric keypresses are represented by the char itself. Non-alphanumeric keypresses are represented by Event::Key wrapping a Key variant ( Enter , Esc , etc.). Single and multiple modifier key presses are represented by particular Event variants that wrap Key variants for nonalphanumeric keypresses, i.e. Event::Ctrl(Key) , Event::Alt(Key) , Event::CtrlAlt(Key) . Separate Event variants are used for alphanumeric characters (i.e. Event::CtrlChar(char) , Event::Alt(char) , Event::CtrlAltChar(char) , etc) which wrap the char value of the keypress. EventTrigger::mouse() will respond to any mouse click. More specific mouse events may be specified by a MouseEvent variant containing a MouseButton variant (TODO). In fact there are a variety of methods available that affect how Events are routed. on_event() and on_event_inner() register callbacks that are ignored by the wrapped View (?). on_pre_event() and on_pre_event_inner() register callbacks that need preprocessing and control whether the wrapped View should be given the Event. In this example, it appears not to matter which of the methods is used to register callbacks, possibly because the wrapped View ultimately receives the Event in either case. on_event_inner() on_pre_event_inner() use cursive :: traits :: { Scrollable , Resizable }; use cursive :: views :: { Dialog , SelectView , TextView , OnEventView }; use cursive :: event :: EventResult ; fn main () { let mut siv = cursive :: default (); list_ships ( & mut siv ); siv . run (); } fn list_ships ( siv : & mut cursive :: Cursive ) { siv . pop_layer (); let mut select = SelectView :: new (); select . add_all_str ( vec! [ \"USS Enterprise\" , \"USS Voyager\" , \"USS Reliant\" ]); select . set_on_submit ( show_ship ); let select = OnEventView :: new ( select ) . on_event_inner ( 'k' , | s , _ | { let cb = s . select_up ( 1 ); Some ( EventResult :: Consumed ( Some ( cb ))) }) . on_event_inner ( 'j' , | s , _ | { let cb = s . select_down ( 1 ); Some ( EventResult :: Consumed ( Some ( cb ))) }); siv . add_layer ( Dialog :: around ( select . scrollable () // . fixed_size (( 20 , 10 )) // ). title ( \"Choose a ship\" ), ); } fn show_ship ( siv : & mut cursive :: Cursive , starship : & str ) { siv . pop_layer (); let starship = String :: from ( starship ). replace ( \"USS \" , \"\" ); let text = format! ( \"The {} is a fine ship!\" , starship ); siv . add_layer ( Dialog :: around ( TextView :: new ( text )) . button ( \"OK\" , | s | list_ships ( s )) . button ( \"Quit\" , cursive :: Cursive :: quit ), ); } use cursive :: traits :: { Scrollable , Resizable }; use cursive :: views :: { Dialog , SelectView , TextView , OnEventView }; use cursive :: event :: EventResult ; fn main () { let mut siv = cursive :: default (); list_ships ( & mut siv ); siv . run (); } fn list_ships ( siv : & mut cursive :: Cursive ) { siv . pop_layer (); let mut select = SelectView :: new (); select . add_all_str ( vec! [ \"USS Enterprise\" , \"USS Voyager\" , \"USS Reliant\" ]); select . set_on_submit ( show_ship ); let select = OnEventView :: new ( select ) . on_pre_event_inner ( 'k' , | s , _ | { let cb = s . select_up ( 1 ); Some ( EventResult :: Consumed ( Some ( cb ))) }) . on_pre_event_inner ( 'j' , | s , _ | { let cb = s . select_down ( 1 ); Some ( EventResult :: Consumed ( Some ( cb ))) }); siv . add_layer ( Dialog :: around ( select . scrollable () // . fixed_size (( 20 , 10 )) // ). title ( \"Choose a ship\" ), ); } fn show_ship ( siv : & mut cursive :: Cursive , starship : & str ) { siv . pop_layer (); let starship = String :: from ( starship ). replace ( \"USS \" , \"\" ); let text = format! ( \"The {} is a fine ship!\" , starship ); siv . add_layer ( Dialog :: around ( TextView :: new ( text )) . button ( \"OK\" , | s | list_ships ( s )) . button ( \"Quit\" , cursive :: Cursive :: quit ), ); } PaddedView PaddedView wraps a single View and applies a margin specified in terminal cells. It can be instantiated with new() or the somewhat abbreviated lrtb() help method. lrtb() new() use cursive :: { Cursive , CursiveExt }; use cursive :: views :: { TextView , PaddedView }; fn main () { let mut siv = Cursive :: new (); siv . add_global_callback ( 'q' , | s | s . quit ()); siv . add_layer ( PaddedView :: lrtb ( 2 , 2 , 1 , 1 , TextView :: new ( \"Hello, World!\" )) ); siv . run (); } use cursive :: { Cursive , CursiveExt }; use cursive :: views :: { TextView , PaddedView }; use cursive :: view :: Margins ; fn main () { let mut siv = Cursive :: new (); siv . add_global_callback ( 'q' , | s | s . quit ()); siv . add_layer ( PaddedView :: new ( Margins :: lrtb ( 2 , 2 , 1 , 1 ), TextView :: new ( \"Hello, World!\" )) ); siv . run (); } RadioGroup RadioGroup<T> is used to coordinate multiple radio buttons. Because RadioGroup is a View which does not implement Nameable, a closure will always be used to retrieve the user's choice with selection() . It is possible to abstract some of the logic of creating a new View using the user's selection, with some bizarre syntax. Closure Closure and function use cursive :: views :: { Dialog , LinearLayout , RadioGroup , TextView }; fn main () { let mut siv = cursive :: default (); let mut options : RadioGroup < String > = RadioGroup :: new (); // (1) siv . add_layer ( Dialog :: around ( LinearLayout :: vertical () . child ( options . button_str ( \"Plato\" )) . child ( options . button_str ( \"Aristotle\" )) // (2) . child ( options . button_str ( \"Socrates\" )), ). button ( \"Ok\" , move | s | { // (3) s . pop_layer (); s . add_layer ( Dialog :: around ( TextView :: new ( format! ( \"You chose {}!\" , options . selection ())) ). button ( \"Ok\" , | s | s . quit ()) ); }) ); siv . run (); } RadioGroup is a generic and must be typed (according to its values?) The button_str() method adds a button where the value equals its label. A button() method is also available which takes an additional argument, the value to be passed back upon selection. The move keyword must be used because the closure may outlive the current function but it captures options from the environment of the current function's scope. use cursive :: views :: { Dialog , LinearLayout , RadioGroup , TextView }; fn main () { let mut siv = cursive :: default (); let mut options : RadioGroup < String > = RadioGroup :: new (); siv . add_layer ( Dialog :: around ( LinearLayout :: vertical () . child ( options . button_str ( \"Plato\" )) . child ( options . button_str ( \"Aristotle\" )) . child ( options . button_str ( \"Socrates\" )), ). button ( \"Ok\" , move | s | { let selection = &* options . selection (); // (1) display_selection ( s , selection ); }) ); siv . run (); } fn display_selection ( siv : & mut cursive :: Cursive , selection : & String ) { siv . pop_layer (); siv . add_layer ( Dialog :: around ( TextView :: new ( format! ( \"You chose {}!\" , selection )) ). button ( \"Ok\" , | s | s . quit ()) ); } This is apparently needed to dereference the Rc returned by selection() ResizedView A ResizedView functions as a container around a single view, similar to a Box in GTK. ResizedView can be initiated with a variety of methods that define its appearance. Helper methods like with_fixed_size() take a tuple of two integers defining the size of the view in columns and rows as well as an inner View. Helper methods like with_fixed_height() take a single integer and an inner View. Finally, methods like with_full_screen() take only the contained View. SelectView SelectView allows an item to be selected from a list and so is equivalent to widgets like ListView in WinUI. use cursive :: traits :: { Scrollable , Resizable }; use cursive :: views :: { Dialog , SelectView , TextView }; use cursive :: Cursive ; fn main () { let mut siv = cursive :: default (); let mut select = SelectView :: new (); select . add_all_str ( vec! [ \"USS Enterprise\" , \"USS Voyager\" , \"USS Reliant\" ]); select . set_on_submit ( show_next_window ); // (1) let select = OnEventView :: new ( select ) . on_pre_event_inner ( 'k' , | s , _ | { let cb = s . select_up ( 1 ); Some ( EventResult :: Consumed ( Some ( cb ))) }) siv . add_layer ( Dialog :: around ( select . scrollable () // (2) . fixed_size (( 20 , 10 )) // (3) ) . title ( \"Where are you from?\" ), ); siv . run (); } fn show_next_window ( siv : & mut Cursive , starship : & str ) { siv . pop_layer (); let starship = String :: from ( starship ). replace ( \"USS \" , \"\" ); let text = format! ( \"The {} is a fine ship!\" , starship ); siv . add_layer ( Dialog :: around ( TextView :: new ( text )). button ( \"Quit\" , | s | s . quit ()), ); } Sets the callback for when ++Enter++ is pressed. scrollable() is included in cursive::traits::Scrollable . fixed_size() is included in cursive::traits::Resizable autojump() will change the selection in response to keypresses. This will interfere with the global callback set for q . TabPanel TabPanel is an ease-of-use wrapper around TabView from the cursive-tabs crate.","title":"API"},{"location":"Rust/Crates/Cursive/API/#api","text":"","title":"API"},{"location":"Rust/Crates/Cursive/API/#dialog","text":"Dialog can be constructed with new() or with the (barely) more concise around() helper method. around() new() use cursive :: { Cursive , CursiveExt }; use cursive :: views :: { TextView , Dialog }; fn main () { let mut siv = Cursive :: new (); siv . add_global_callback ( 'q' , | s | s . quit ()); siv . add_layer ( Dialog :: around ( TextView :: new ( \"Hello, world!\" )) . button ( \"Ok\" , | s | s . quit ()) ); siv . run (); } use cursive :: { Cursive , CursiveExt }; use cursive :: views :: { TextView , Dialog }; fn main () { let mut siv = Cursive :: new (); siv . add_global_callback ( 'q' , | s | s . quit ()); siv . add_layer ( Dialog :: new () . content ( TextView :: new ( \"Hello, world!\" )) . button ( \"Ok\" , | s | s . quit ()) ); siv . run (); }","title":"Dialog"},{"location":"Rust/Crates/Cursive/API/#dummyview","text":"A DummyView is used as a spacer. use cursive :: views :: { DummyView , TextView , Dialog , LinearLayout }; use cursive :: { Cursive , CursiveExt }; fn main () { let mut siv = Cursive :: new (); siv . add_global_callback ( 'q' , | s | s . quit ()); siv . add_layer ( Dialog :: new () . title ( \"Hello, world!\" ) . content ( LinearLayout :: vertical () . child ( TextView :: new ( \"Hello, World!\" )) . child ( DummyView ) . child ( TextView :: new ( \"Hello again!\" )) ) ); siv . run (); }","title":"DummyView"},{"location":"Rust/Crates/Cursive/API/#hexview","text":"Hexview is available from the cursive_hexview crate. use cursive :: view :: { Scrollable , Resizable }; use cursive_hexview :: { DisplayState , HexView , HexViewConfig }; use std :: env ; use std :: fs :: File ; use std :: io :: { self , Read }; use std :: path :: Path ; fn read_file ( path : & Path ) -> Result < Vec < u8 > , io :: Error > { let mut file = File :: open ( path ) ? ; let mut buf = Vec :: new (); file . read_to_end ( & mut buf ) ? ; Ok ( buf ) } fn main () { let arg = env :: args () . nth ( 1 ) . expect ( \"Provide a valid filename\" ); let path = Path :: new ( & arg ); let mut cur = cursive :: default (); let view = HexView :: new_from_iter ( read_file ( path ). expect ( \"Cannot read file\" )). display_state ( DisplayState :: Enabled ) . config ( HexViewConfig { bytes_per_line : 48 , bytes_per_group : 8 , .. Default :: default () }) . scrollable (). full_screen (); cur . add_layer ( view ); cur . run (); }","title":"HexView"},{"location":"Rust/Crates/Cursive/API/#linearlayout","text":"A LinearLayout supports horizontal or vertical layout similar to a StackPanel in WinUI. use cursive :: views :: { DummyView , TextView , Dialog , LinearLayout }; use cursive :: { Cursive , CursiveExt }; fn main () { let mut siv = Cursive :: new (); siv . add_global_callback ( 'q' , | s | s . quit ()); siv . add_layer ( Dialog :: new () . title ( \"Hello, world!\" ) . content ( LinearLayout :: vertical () . child ( TextView :: new ( \"Hello, World!\" )) . child ( DummyView ) . child ( TextView :: new ( \"Hello again!\" )) ) ); siv . run (); }","title":"LinearLayout"},{"location":"Rust/Crates/Cursive/API/#oneventview","text":"OnEventView allows keybindings to take effect on wrapped Views. use cursive :: views :: { OnEventView , TextView }; use cursive :: event :: { Key , Event , EventTrigger }; use cursive :: Cursive ; fn main () { let mut siv = cursive :: default (); siv . add_layer ( OnEventView :: new ( TextView :: new ( \"Hello, World! \\n (Q to quit)\" )) . on_event ( 'q' , Cursive :: quit ) // (1) . on_event ( Event :: Key ( Key :: Enter ), | s | { // (2) s . pop_layer (); s . add_layer ( TextView :: new ( \"Enter key pressed!\" )); }) . on_event ( Event :: CtrlChar ( 'a' ), | s | { // (3) s . pop_layer (); s . add_layer ( TextView :: new ( \"Ctrl+A pressed!\" )) }) . on_event ( EventTrigger :: mouse (), | s | { // (4) s . pop_layer (); s . add_layer ( TextView :: new ( \"Mouse clicked!\" )); }) ); siv . run (); } Alphanumeric keypresses are represented by the char itself. Non-alphanumeric keypresses are represented by Event::Key wrapping a Key variant ( Enter , Esc , etc.). Single and multiple modifier key presses are represented by particular Event variants that wrap Key variants for nonalphanumeric keypresses, i.e. Event::Ctrl(Key) , Event::Alt(Key) , Event::CtrlAlt(Key) . Separate Event variants are used for alphanumeric characters (i.e. Event::CtrlChar(char) , Event::Alt(char) , Event::CtrlAltChar(char) , etc) which wrap the char value of the keypress. EventTrigger::mouse() will respond to any mouse click. More specific mouse events may be specified by a MouseEvent variant containing a MouseButton variant (TODO). In fact there are a variety of methods available that affect how Events are routed. on_event() and on_event_inner() register callbacks that are ignored by the wrapped View (?). on_pre_event() and on_pre_event_inner() register callbacks that need preprocessing and control whether the wrapped View should be given the Event. In this example, it appears not to matter which of the methods is used to register callbacks, possibly because the wrapped View ultimately receives the Event in either case. on_event_inner() on_pre_event_inner() use cursive :: traits :: { Scrollable , Resizable }; use cursive :: views :: { Dialog , SelectView , TextView , OnEventView }; use cursive :: event :: EventResult ; fn main () { let mut siv = cursive :: default (); list_ships ( & mut siv ); siv . run (); } fn list_ships ( siv : & mut cursive :: Cursive ) { siv . pop_layer (); let mut select = SelectView :: new (); select . add_all_str ( vec! [ \"USS Enterprise\" , \"USS Voyager\" , \"USS Reliant\" ]); select . set_on_submit ( show_ship ); let select = OnEventView :: new ( select ) . on_event_inner ( 'k' , | s , _ | { let cb = s . select_up ( 1 ); Some ( EventResult :: Consumed ( Some ( cb ))) }) . on_event_inner ( 'j' , | s , _ | { let cb = s . select_down ( 1 ); Some ( EventResult :: Consumed ( Some ( cb ))) }); siv . add_layer ( Dialog :: around ( select . scrollable () // . fixed_size (( 20 , 10 )) // ). title ( \"Choose a ship\" ), ); } fn show_ship ( siv : & mut cursive :: Cursive , starship : & str ) { siv . pop_layer (); let starship = String :: from ( starship ). replace ( \"USS \" , \"\" ); let text = format! ( \"The {} is a fine ship!\" , starship ); siv . add_layer ( Dialog :: around ( TextView :: new ( text )) . button ( \"OK\" , | s | list_ships ( s )) . button ( \"Quit\" , cursive :: Cursive :: quit ), ); } use cursive :: traits :: { Scrollable , Resizable }; use cursive :: views :: { Dialog , SelectView , TextView , OnEventView }; use cursive :: event :: EventResult ; fn main () { let mut siv = cursive :: default (); list_ships ( & mut siv ); siv . run (); } fn list_ships ( siv : & mut cursive :: Cursive ) { siv . pop_layer (); let mut select = SelectView :: new (); select . add_all_str ( vec! [ \"USS Enterprise\" , \"USS Voyager\" , \"USS Reliant\" ]); select . set_on_submit ( show_ship ); let select = OnEventView :: new ( select ) . on_pre_event_inner ( 'k' , | s , _ | { let cb = s . select_up ( 1 ); Some ( EventResult :: Consumed ( Some ( cb ))) }) . on_pre_event_inner ( 'j' , | s , _ | { let cb = s . select_down ( 1 ); Some ( EventResult :: Consumed ( Some ( cb ))) }); siv . add_layer ( Dialog :: around ( select . scrollable () // . fixed_size (( 20 , 10 )) // ). title ( \"Choose a ship\" ), ); } fn show_ship ( siv : & mut cursive :: Cursive , starship : & str ) { siv . pop_layer (); let starship = String :: from ( starship ). replace ( \"USS \" , \"\" ); let text = format! ( \"The {} is a fine ship!\" , starship ); siv . add_layer ( Dialog :: around ( TextView :: new ( text )) . button ( \"OK\" , | s | list_ships ( s )) . button ( \"Quit\" , cursive :: Cursive :: quit ), ); }","title":"OnEventView"},{"location":"Rust/Crates/Cursive/API/#paddedview","text":"PaddedView wraps a single View and applies a margin specified in terminal cells. It can be instantiated with new() or the somewhat abbreviated lrtb() help method. lrtb() new() use cursive :: { Cursive , CursiveExt }; use cursive :: views :: { TextView , PaddedView }; fn main () { let mut siv = Cursive :: new (); siv . add_global_callback ( 'q' , | s | s . quit ()); siv . add_layer ( PaddedView :: lrtb ( 2 , 2 , 1 , 1 , TextView :: new ( \"Hello, World!\" )) ); siv . run (); } use cursive :: { Cursive , CursiveExt }; use cursive :: views :: { TextView , PaddedView }; use cursive :: view :: Margins ; fn main () { let mut siv = Cursive :: new (); siv . add_global_callback ( 'q' , | s | s . quit ()); siv . add_layer ( PaddedView :: new ( Margins :: lrtb ( 2 , 2 , 1 , 1 ), TextView :: new ( \"Hello, World!\" )) ); siv . run (); }","title":"PaddedView"},{"location":"Rust/Crates/Cursive/API/#radiogroup","text":"RadioGroup<T> is used to coordinate multiple radio buttons. Because RadioGroup is a View which does not implement Nameable, a closure will always be used to retrieve the user's choice with selection() . It is possible to abstract some of the logic of creating a new View using the user's selection, with some bizarre syntax. Closure Closure and function use cursive :: views :: { Dialog , LinearLayout , RadioGroup , TextView }; fn main () { let mut siv = cursive :: default (); let mut options : RadioGroup < String > = RadioGroup :: new (); // (1) siv . add_layer ( Dialog :: around ( LinearLayout :: vertical () . child ( options . button_str ( \"Plato\" )) . child ( options . button_str ( \"Aristotle\" )) // (2) . child ( options . button_str ( \"Socrates\" )), ). button ( \"Ok\" , move | s | { // (3) s . pop_layer (); s . add_layer ( Dialog :: around ( TextView :: new ( format! ( \"You chose {}!\" , options . selection ())) ). button ( \"Ok\" , | s | s . quit ()) ); }) ); siv . run (); } RadioGroup is a generic and must be typed (according to its values?) The button_str() method adds a button where the value equals its label. A button() method is also available which takes an additional argument, the value to be passed back upon selection. The move keyword must be used because the closure may outlive the current function but it captures options from the environment of the current function's scope. use cursive :: views :: { Dialog , LinearLayout , RadioGroup , TextView }; fn main () { let mut siv = cursive :: default (); let mut options : RadioGroup < String > = RadioGroup :: new (); siv . add_layer ( Dialog :: around ( LinearLayout :: vertical () . child ( options . button_str ( \"Plato\" )) . child ( options . button_str ( \"Aristotle\" )) . child ( options . button_str ( \"Socrates\" )), ). button ( \"Ok\" , move | s | { let selection = &* options . selection (); // (1) display_selection ( s , selection ); }) ); siv . run (); } fn display_selection ( siv : & mut cursive :: Cursive , selection : & String ) { siv . pop_layer (); siv . add_layer ( Dialog :: around ( TextView :: new ( format! ( \"You chose {}!\" , selection )) ). button ( \"Ok\" , | s | s . quit ()) ); } This is apparently needed to dereference the Rc returned by selection()","title":"RadioGroup"},{"location":"Rust/Crates/Cursive/API/#resizedview","text":"A ResizedView functions as a container around a single view, similar to a Box in GTK. ResizedView can be initiated with a variety of methods that define its appearance. Helper methods like with_fixed_size() take a tuple of two integers defining the size of the view in columns and rows as well as an inner View. Helper methods like with_fixed_height() take a single integer and an inner View. Finally, methods like with_full_screen() take only the contained View.","title":"ResizedView"},{"location":"Rust/Crates/Cursive/API/#selectview","text":"SelectView allows an item to be selected from a list and so is equivalent to widgets like ListView in WinUI. use cursive :: traits :: { Scrollable , Resizable }; use cursive :: views :: { Dialog , SelectView , TextView }; use cursive :: Cursive ; fn main () { let mut siv = cursive :: default (); let mut select = SelectView :: new (); select . add_all_str ( vec! [ \"USS Enterprise\" , \"USS Voyager\" , \"USS Reliant\" ]); select . set_on_submit ( show_next_window ); // (1) let select = OnEventView :: new ( select ) . on_pre_event_inner ( 'k' , | s , _ | { let cb = s . select_up ( 1 ); Some ( EventResult :: Consumed ( Some ( cb ))) }) siv . add_layer ( Dialog :: around ( select . scrollable () // (2) . fixed_size (( 20 , 10 )) // (3) ) . title ( \"Where are you from?\" ), ); siv . run (); } fn show_next_window ( siv : & mut Cursive , starship : & str ) { siv . pop_layer (); let starship = String :: from ( starship ). replace ( \"USS \" , \"\" ); let text = format! ( \"The {} is a fine ship!\" , starship ); siv . add_layer ( Dialog :: around ( TextView :: new ( text )). button ( \"Quit\" , | s | s . quit ()), ); } Sets the callback for when ++Enter++ is pressed. scrollable() is included in cursive::traits::Scrollable . fixed_size() is included in cursive::traits::Resizable autojump() will change the selection in response to keypresses. This will interfere with the global callback set for q .","title":"SelectView"},{"location":"Rust/Crates/Cursive/API/#tabpanel","text":"TabPanel is an ease-of-use wrapper around TabView from the cursive-tabs crate.","title":"TabPanel"},{"location":"Rust/Crates/Cursive/Tasks/","text":"Tasks Boilerplate Cursive apps are constructed procedurally using a succession of method calls, which require the main Cursive object (conventionally named \"siv\" in documentation) to be mutable. The default() function which sits at the root of the cursive module streamlines some of the boilerplate involved with calling Cursive::new() explicitly. default() Cursive::new() fn main () { let mut siv = cursive :: default (); siv . add_global_callback ( 'q' , | s | s . quit ()); // ... siv . run (); } use cursive :: { Cursive , CursiveExt }; // (1) fn main () { let mut app = Cursive :: new (); app . add_global_callback ( 'q' , | s | s . quit ()); // ... app . run (); } CursiveExt must be imported in order to call the run() method. Because Cursive depends on the ncurses backend, it won't work on Windows without additional modifications to Cargo.toml Cargo.toml [dependencies.cursive] version = \"0.16.3\" default-features = false [features] default = [ \"pancurses-backend\" ] pancurses-backend = [ \"cursive/pancurses-backend\" ] Hello, World! TextView Dialog EditView Choice of greeting CLI integration use cursive :: views :: TextView ; fn main () { let mut siv = cursive :: default (); siv . add_layer ( TextView :: new ( \"Hello World!\" ) ); siv . run (); } use cursive :: views :: TextView ; fn main () { let mut siv = cursive :: default (); siv . add_layer ( Dialog :: around ( TextView :: new ( \"Hello World!\" ) ) ); siv . run (); } use cursive :: view :: Nameable ; use cursive :: views :: { Dialog , EditView , TextView }; fn main () { let mut siv = cursive :: default (); get_name ( & mut siv ); siv . run (); } fn get_name ( siv : & mut cursive :: Cursive ) { // (1) siv . pop_layer (); siv . add_layer ( Dialog :: new () . title ( \"Please enter name\" ) . content ( EditView :: new (). with_name ( \"message\" )) // (2) . button ( \"OK\" , | s | { greet_user ( s ) }) . button ( \"Quit\" , cursive :: Cursive :: quit ) ); } fn greet_user ( siv : & mut cursive :: Cursive ) { let greeting = format! ( \"Hello, {}!\" , siv . call_on_name ( \"message\" , | t : & mut EditView | t . get_content ()) // (3) . unwrap ()); siv . pop_layer (); // (4) siv . add_layer ( Dialog :: around ( TextView :: new ( greeting )) . title ( \"Greetings!\" ) . button ( \"OK\" , | s | { get_name ( s ) } ), // (5) ); } In order to implement multiple pages, the Cursive object must be passed as a mutable reference from function to function. with_name() , only accessible after including the Nameable trait, implicitly wraps the the view in a NamedView. call_on_name() allows a named View to be accessed. Each function in turn runs pop_layer() on the Cursive object to remove the previous View. This callback sends the user back to the first View, to repeat the process. use cursive :: view :: Nameable ; use cursive :: views :: { Dialog , EditView , LinearLayout , ListView , RadioGroup , TextView }; fn main () { let mut siv = cursive :: default (); get_name ( & mut siv ); siv . run (); } fn get_name ( siv : & mut cursive :: Cursive ) { siv . pop_layer (); let mut options : RadioGroup < String > = RadioGroup :: new (); siv . add_layer ( Dialog :: around ( ListView :: new () . child ( \"Name\" , EditView :: new (). with_name ( \"name\" )) . child ( \"Greeting\" , LinearLayout :: vertical () . child ( options . button_str ( \"Hello\" )) . child ( options . button_str ( \"Greetings\" )), ), ) . button ( \"Ok\" , move | s | { let selection = &* options . selection (); greet_user ( s , selection ); }) . button ( \"Quit\" , cursive :: Cursive :: quit ), ); } fn greet_user ( siv : & mut cursive :: Cursive , greeting : & String ) { let name = siv . call_on_name ( \"name\" , | t : & mut EditView | t . get_content ()) . unwrap (); siv . pop_layer (); siv . add_layer ( Dialog :: around ( TextView :: new ( format! ( \"{}, {}!\" , greeting , name ))) . button ( \"Ok\" , | s | get_name ( s )), ); } use cursive :: view :: Nameable ; use cursive :: views :: { Dialog , EditView , LinearLayout , ListView , RadioGroup , TextView }; fn main () { let mut siv = cursive :: default (); get_name ( & mut siv ); siv . run (); } fn get_name ( siv : & mut cursive :: Cursive ) { siv . pop_layer (); let mut options : RadioGroup < String > = RadioGroup :: new (); let name = match std :: env :: args (). nth ( 1 ) { Some ( s ) => s , _ => String :: from ( \"World\" ) }; let editview = EditView :: new () . content ( name ) . with_name ( \"name\" ); siv . add_layer ( Dialog :: around ( ListView :: new () . child ( \"Name\" , editview ) . child ( \"Greeting\" , LinearLayout :: vertical () . child ( options . button_str ( \"Hello\" )) . child ( options . button_str ( \"Greetings\" )), ), ) . button ( \"Ok\" , move | s | { let selection = &* options . selection (); greet_user ( s , selection ); }) . button ( \"Quit\" , cursive :: Cursive :: quit ), ); } fn greet_user ( siv : & mut cursive :: Cursive , greeting : & String ) { let name = siv . call_on_name ( \"name\" , | t : & mut EditView | t . get_content ()) . unwrap (); siv . pop_layer (); siv . add_layer ( Dialog :: around ( TextView :: new ( format! ( \"{}, {}!\" , greeting , name ))) . button ( \"Ok\" , | s | get_name ( s )), ); } Installation wizard Starships SelectView Enums Structs Vim keybindings JSON PostgreSQL use cursive :: traits :: { Scrollable , Resizable }; use cursive :: views :: { Dialog , SelectView , TextView }; fn main () { let mut siv = cursive :: default (); list_ships ( & mut siv ); siv . run (); } fn list_ships ( siv : & mut cursive :: Cursive ) { siv . pop_layer (); let mut select = SelectView :: new (); select . add_all_str ( vec! [ \"USS Enterprise\" , \"USS Voyager\" , \"USS Reliant\" ]); select . set_on_submit ( show_ship ); siv . add_layer ( Dialog :: around ( select . scrollable () . fixed_size (( 20 , 10 )) ). title ( \"Choose a ship\" ), ); } fn show_ship ( siv : & mut cursive :: Cursive , starship : & str ) { siv . pop_layer (); let starship = String :: from ( starship ). replace ( \"USS \" , \"\" ); let text = format! ( \"The {} is a fine ship!\" , starship ); siv . add_layer ( Dialog :: around ( TextView :: new ( text )) . button ( \"OK\" , | s | list_ships ( s )) . button ( \"Quit\" , cursive :: Cursive :: quit ), ); } use cursive :: traits :: { Scrollable , Resizable }; use cursive :: views :: { Dialog , SelectView , TextView }; mod models ; use models :: Starships ; // (1) fn main () { let mut siv = cursive :: default (); list_ships ( & mut siv ); siv . run (); } fn list_ships ( siv : & mut cursive :: Cursive ) { siv . pop_layer (); let select = SelectView :: new () . with_all_str ( vec! [ Starships :: Enterprise , Starships :: Voyager , Starships :: Defiant ]) . on_submit ( show_ship ); siv . add_layer ( Dialog :: around ( select . scrollable () . fixed_size (( 20 , 10 )) ). title ( \"Choose a ship\" ), ); } fn show_ship ( siv : & mut cursive :: Cursive , starship : & str ) { siv . pop_layer (); let starship = String :: from ( starship ). replace ( \"USS \" , \"\" ); let text = format! ( \"The {} is a fine ship!\" , starship ); siv . add_layer ( Dialog :: around ( TextView :: new ( text )) . button ( \"OK\" , | s | list_ships ( s )) . button ( \"Quit\" , cursive :: Cursive :: quit ), ); } Implementing the list of starships as an enum is possible so long as the From<Starships> trait is implemented for String. enum Starships { Enterprise , Defiant , Voyager , } impl <' a > std :: convert :: From < Starships > for String { fn from ( item : Starships ) -> String { match item { Starships :: Enterprise => String :: from ( \"USS Enterprise\" ), Starships :: Defiant => String :: from ( \"USS Defiant\" ), Starships :: Voyager => String :: from ( \"USS Voyager\" ), } } } use cursive :: traits :: { Scrollable , Resizable }; use cursive :: views :: { Dialog , SelectView , TextView , ListView }; mod models ; // (1) use models :: Starship ; fn main () { let mut siv = cursive :: default (); list_ships ( & mut siv ); siv . run (); } fn get_ships <' a > () -> Vec < Starship <' a >> { let output = vec! [ Starship { name : \"USS Enterprise\" , registry : \"NCC-1701\" , series : Series :: TOS }, Starship { name : \"USS Defiant\" , registry : \"NX-74205\" , series : Series :: DS9 }, Starship { name : \"USS Voyager\" , registry : \"NCC-74656\" , series : Series :: VOY }, ]; output } fn list_ships ( siv : & mut cursive :: Cursive ) { let ships = get_ships (); siv . pop_layer (); let select = SelectView :: new () . with_all ( ships . into_iter (). map ( | ship | ( ship . name , ship ))) // (2) . on_submit ( show_ship ); // (3) siv . add_layer ( Dialog :: around ( select . scrollable () . fixed_size (( 20 , 10 )) ). title ( \"Choose a ship\" ), ); } fn show_ship ( siv : & mut cursive :: Cursive , starship : & Starship ) { siv . add_layer ( Dialog :: around ( ListView :: new () . child ( \"Name:\" , TextView :: new ( starship . name )) . child ( \"Registry:\" , TextView :: new ( starship . registry )) . child ( \"Series:\" , TextView :: new ( starship . series )) ) . button ( \"OK\" , list_ships ) . button ( \"Quit\" , cursive :: Cursive :: quit ), ); } The Series enum must implement the Copy trait because it may not be moved in the detailed view. This is the simpler method . #[derive(Clone, Copy)] enum Series { TOS , VOY , DS9 } impl std :: convert :: From < Series > for String { fn from ( item : Series ) -> String { String :: from ( match item { Series :: TOS => \"The Original Series\" , Series :: VOY => \"Voyager\" , Series :: DS9 => \"Deep Space Nine\" , }) } } struct Starship <' a > { name : & ' a str , registry : & ' a str , series : Series } Using with_all() or set_with_all() apparently requires a Map struct, created by the map on Iterator . The keys of this object define the text that appears in the SelectView, and the values define the data passed to the callback specified in on_submit() . Sometimes a bizarre and unhelpful compiler error will occur when this second value does not match the signature of the specified callback. For example, changing ship to a unit type () will raise a compiler error: .on_submit(show_ship), --------- ^^^^^^^^^ expected signature of `for<'r, 's> fn(&'r mut Cursive, &'s ()) -> _` | required by a bound introduced by this call use cursive :: traits :: { Scrollable , Resizable }; use cursive :: views :: { Dialog , SelectView , TextView , ListView , OnEventView }; use cursive :: event :: EventResult ; mod models ; // (1) use models :: Starship ; fn main () { let mut siv = cursive :: default (); list_ships ( & mut siv ); siv . run (); } fn get_ships <' a > () -> Vec < Starship <' a >> { let output = vec! [ Starship { name : \"USS Enterprise\" , registry : \"NCC-1701\" , series : Series :: TOS }, Starship { name : \"USS Defiant\" , registry : \"NX-74205\" , series : Series :: DS9 }, Starship { name : \"USS Voyager\" , registry : \"NCC-74656\" , series : Series :: VOY }, ]; output } fn list_ships ( siv : & mut cursive :: Cursive ) { let ships = get_ships (); siv . pop_layer (); let select = SelectView :: new () . with_all ( ships . into_iter (). map ( | ship | ( ship . name , ship ))) . on_submit ( show_ship ); let select = OnEventView :: new ( select ) . on_event_inner ( 'k' , | s , _ | { let cb = s . select_up ( 1 ); Some ( EventResult :: Consumed ( Some ( cb ))) }) . on_event_inner ( 'j' , | s , _ | { let cb = s . select_down ( 1 ); Some ( EventResult :: Consumed ( Some ( cb ))) }); siv . add_layer ( Dialog :: around ( select . scrollable () . fixed_size (( 20 , 10 )) ). title ( \"Choose a ship\" ), ); } fn show_ship ( siv : & mut cursive :: Cursive , starship : & Starship ) { siv . add_layer ( Dialog :: around ( ListView :: new () . child ( \"Name:\" , TextView :: new ( starship . name )) . child ( \"Registry:\" , TextView :: new ( starship . registry )) . child ( \"Series:\" , TextView :: new ( starship . series )) // ... ) . button ( \"OK\" , list_ships ) . button ( \"Quit\" , cursive :: Cursive :: quit ), ); } models.rs #[derive(Clone, Copy)] enum Series { TOS , TNG , VOY , } impl std :: convert :: From < Series > for String { fn from ( item : Series ) -> String { String :: from ( match item { Series :: TOS => \"The Original Series\" , Series :: TNG => \"The Next Generation\" , Series :: VOY => \"Voyager\" , }) } } struct Starship <' a > { name : & ' a str , registry : & ' a str , series : Series } use cursive :: event :: EventResult ; use cursive :: traits :: { Resizable , Scrollable }; use cursive :: views :: { Dialog , ListView , OnEventView , SelectView , TextView }; mod models ; // (1) use models :: Starship ; fn main () { let mut siv = cursive :: default (); list_ships ( & mut siv ); siv . run (); } fn get_ships () -> Vec < Starship > { let f = std :: fs :: read_to_string ( \"starships.json\" ). unwrap (); let output : Vec < Starship > = serde_json :: from_str ( & f ). unwrap (); output } fn list_ships ( siv : & mut cursive :: Cursive ) { let ships : Vec < Starship > = get_ships (); siv . pop_layer (); let select = SelectView :: new () . with_all ( ships . into_iter (). map ( | ship | ( ship . name . clone (), ship ))) // (2) . on_submit ( show_ship ); let select = OnEventView :: new ( select ) . on_event_inner ( 'k' , | s , _ | { let cb = s . select_up ( 1 ); Some ( EventResult :: Consumed ( Some ( cb ))) }) . on_event_inner ( 'j' , | s , _ | { let cb = s . select_down ( 1 ); Some ( EventResult :: Consumed ( Some ( cb ))) }); siv . add_layer ( Dialog :: around ( select . scrollable (). fixed_size (( 20 , 10 ))). title ( \"Choose a ship\" )); } fn show_ship ( siv : & mut cursive :: Cursive , starship : & Starship ) { siv . add_layer ( Dialog :: around ( ListView :: new () . child ( \"Name:\" , TextView :: new ( & starship . name )) // (3) . child ( \"Registry:\" , TextView :: new ( & starship . registry )) . child ( \"Series:\" , TextView :: new ( starship . series )), ) . button ( \"OK\" , list_ships ) . button ( \"Quit\" , cursive :: Cursive :: quit ), ); } The serde_derive::Deserialize derive is necessary for the mapped struct as well as the Series enum because it appears as a member of the Starship struct. models.rs #[derive(Clone, Copy, serde_derive::Deserialize)] enum Series { TOS , DS9 , VOY , } impl std :: convert :: From < Series > for String { fn from ( item : Series ) -> String { String :: from ( match item { Series :: TOS => \"The Original Series\" , Series :: DS9 => \"Deep Space Nine\" , Series :: VOY => \"Voyager\" , }) } } #[derive(serde_derive::Deserialize)] struct Starship { name : String , registry : String , series : Series , } A clone() is necessary here, although I'm not sure why. A reference is not accepted. References are accepted here, as is a clone(). #[macro_use] extern crate diesel ; use diesel :: prelude :: * ; use diesel :: pg :: PgConnection ; use diesel :: result :: QueryResult ; use cursive :: view :: { Nameable , Resizable }; use cursive :: views :: { Dialog , EditView , ListView , SelectView , TextView }; mod models ; // (3) use models :: Starship ; mod schema ; use schema :: starships :: dsl :: * ; fn main () { let mut siv = cursive :: default (); list_ships ( & mut siv ); siv . run (); } fn get_ships () -> QueryResult < Vec < Starship >> { dotenv :: dotenv (). expect ( \"Couldn't load .env file\" ); let url = & std :: env :: var ( \"DATABASE_URL\" ) . expect ( \"Couldn't retrieve DATABASE_URL environment variable\" ); let conn = PgConnection :: establish ( url ). unwrap (); starships . load :: < Starship > ( & conn ) } fn list_ships ( siv : & mut cursive :: Cursive ) { let ships : Vec < Starship > = get_ships (). unwrap (); siv . pop_layer (); siv . add_layer ( Dialog :: around ( SelectView :: new () . with_all ( ships . into_iter () . map ( | ship | ( format! ( \"{} ({})\" , ship . name , ship . registry ), ship )), ) . on_submit ( show_ship ) ) . button ( \"Quit\" , cursive :: Cursive :: quit ) . button ( \"Add\" , add_ship ) ); } fn show_ship ( siv : & mut cursive :: Cursive , ship : & Starship ) { siv . add_layer ( Dialog :: around ( ListView :: new () . child ( \"Name:\" , TextView :: new ( & ship . name )) . child ( \"Registry:\" , TextView :: new ( & ship . registry )) . child ( \"Crew:\" , TextView :: new ( format! ( \"{}\" , ship . crew ))), ) . button ( \"OK\" , list_ships ), ) } fn add_ship ( siv : & mut cursive :: Cursive ) { let conn = get_connection (). unwrap (); siv . add_layer ( Dialog :: around ( ListView :: new () . child ( \"Name:\" , EditView :: new (). with_name ( \"name\" )) . child ( \"Registry:\" , EditView :: new (). with_name ( \"registry\" )) . child ( \"Crew:\" , EditView :: new (). with_name ( \"crew\" )), ) . button ( \"Ok\" , move | s | { let input_name = s . call_on_name ( \"name\" , | t : & mut EditView | t . get_content ()) . unwrap (); let input_registry = s . call_on_name ( \"registry\" , | t : & mut EditView | t . get_content ()) . unwrap (); let input_crew = s . call_on_name ( \"crew\" , | t : & mut EditView | t . get_content ()) . unwrap () . parse :: < i32 > () . unwrap (); let input : Starship = Starship { name : ( * input_name ). clone (), // (2) registry : ( * input_registry ). clone (), crew : input_crew , }; input . insert_into ( starships ) // (4) . get_result :: < Starship > ( & conn ) // (5) . unwrap (); list_ships ( s ); }). min_width ( 30 ) ); } use crate :: schema :: starships ; #[derive(Debug, Queryable, Insertable)] pub struct Starship { pub name : String , pub registry : String , pub crew : i32 , } Struct intiializations move values, so the String must be deeply copied, here using String::clone use crate :: schema :: starships ; #[derive(Debug, Queryable, Insertable)] pub struct Starship { pub name : String , pub registry : String , pub crew : i32 , } insert_into is a method on the Insertable trait Equivalent to: diesel :: insert_into ( starships ) . values ( & input ) . get_result :: < Starship > ( & conn ) . unwrap (); Diesel collects various methods that execute a query into the RunQueryDsl trait . These methods include: get_result which returns the affected row get_results which returns a Vec of affected rows execute which returns a usize of affected rows. Raven A multi-page wizard can be constructed with the cursive-tabs crate. use cursive :: views :: { TextView }; use cursive :: view :: Nameable ; use cursive_tabs :: TabPanel ; fn main () { let mut siv = cursive :: default (); siv . add_global_callback ( 'q' , | s | s . quit ()); let tabs = TabPanel :: new () . with_tab ( TextView :: new ( include_str! ( \"assets/raven1\" )). with_name ( \"1\" )) . with_tab ( TextView :: new ( include_str! ( \"assets/raven2\" )). with_name ( \"2\" )) . with_tab ( TextView :: new ( include_str! ( \"assets/raven3\" )). with_name ( \"3\" )) . with_tab ( TextView :: new ( include_str! ( \"assets/raven4\" )). with_name ( \"4\" )) . with_tab ( TextView :: new ( include_str! ( \"assets/raven5\" )). with_name ( \"5\" )) ; siv . add_layer ( tabs ); siv . run (); } Key bindings are added as global callbacks. This functionality works because the TabPanel is nameable and can be retrieved by the callback by its name. use cursive :: views :: { TextView }; use cursive :: view :: Nameable ; use cursive :: event :: Key ; use cursive_tabs :: TabPanel ; fn main () { let mut siv = cursive :: default (); siv . add_global_callback ( 'q' , | s | s . quit ()); let tabs = TabPanel :: new () . with_tab ( TextView :: new ( include_str! ( \"assets/raven1\" )). with_name ( \"1\" )) . with_tab ( TextView :: new ( include_str! ( \"assets/raven2\" )). with_name ( \"2\" )) . with_tab ( TextView :: new ( include_str! ( \"assets/raven3\" )). with_name ( \"3\" )) . with_tab ( TextView :: new ( include_str! ( \"assets/raven4\" )). with_name ( \"4\" )) . with_tab ( TextView :: new ( include_str! ( \"assets/raven5\" )). with_name ( \"5\" )) ; siv . add_layer ( tabs . with_name ( \"Tabs\" )); siv . add_global_callback ( Key :: PageUp , | s | { let mut tabs : cursive :: views :: ViewRef < TabPanel > = s . find_name ( \"Tabs\" ). unwrap (); tabs . prev (); }); siv . add_global_callback ( Key :: PageDown , | s | { let mut tabs : cursive :: views :: ViewRef < TabPanel > = s . find_name ( \"Tabs\" ). unwrap (); tabs . next (); }); siv . run (); }","title":"Tasks"},{"location":"Rust/Crates/Cursive/Tasks/#tasks","text":"","title":"Tasks"},{"location":"Rust/Crates/Cursive/Tasks/#boilerplate","text":"Cursive apps are constructed procedurally using a succession of method calls, which require the main Cursive object (conventionally named \"siv\" in documentation) to be mutable. The default() function which sits at the root of the cursive module streamlines some of the boilerplate involved with calling Cursive::new() explicitly. default() Cursive::new() fn main () { let mut siv = cursive :: default (); siv . add_global_callback ( 'q' , | s | s . quit ()); // ... siv . run (); } use cursive :: { Cursive , CursiveExt }; // (1) fn main () { let mut app = Cursive :: new (); app . add_global_callback ( 'q' , | s | s . quit ()); // ... app . run (); } CursiveExt must be imported in order to call the run() method. Because Cursive depends on the ncurses backend, it won't work on Windows without additional modifications to Cargo.toml Cargo.toml [dependencies.cursive] version = \"0.16.3\" default-features = false [features] default = [ \"pancurses-backend\" ] pancurses-backend = [ \"cursive/pancurses-backend\" ]","title":"Boilerplate"},{"location":"Rust/Crates/Cursive/Tasks/#hello-world","text":"TextView Dialog EditView Choice of greeting CLI integration use cursive :: views :: TextView ; fn main () { let mut siv = cursive :: default (); siv . add_layer ( TextView :: new ( \"Hello World!\" ) ); siv . run (); } use cursive :: views :: TextView ; fn main () { let mut siv = cursive :: default (); siv . add_layer ( Dialog :: around ( TextView :: new ( \"Hello World!\" ) ) ); siv . run (); } use cursive :: view :: Nameable ; use cursive :: views :: { Dialog , EditView , TextView }; fn main () { let mut siv = cursive :: default (); get_name ( & mut siv ); siv . run (); } fn get_name ( siv : & mut cursive :: Cursive ) { // (1) siv . pop_layer (); siv . add_layer ( Dialog :: new () . title ( \"Please enter name\" ) . content ( EditView :: new (). with_name ( \"message\" )) // (2) . button ( \"OK\" , | s | { greet_user ( s ) }) . button ( \"Quit\" , cursive :: Cursive :: quit ) ); } fn greet_user ( siv : & mut cursive :: Cursive ) { let greeting = format! ( \"Hello, {}!\" , siv . call_on_name ( \"message\" , | t : & mut EditView | t . get_content ()) // (3) . unwrap ()); siv . pop_layer (); // (4) siv . add_layer ( Dialog :: around ( TextView :: new ( greeting )) . title ( \"Greetings!\" ) . button ( \"OK\" , | s | { get_name ( s ) } ), // (5) ); } In order to implement multiple pages, the Cursive object must be passed as a mutable reference from function to function. with_name() , only accessible after including the Nameable trait, implicitly wraps the the view in a NamedView. call_on_name() allows a named View to be accessed. Each function in turn runs pop_layer() on the Cursive object to remove the previous View. This callback sends the user back to the first View, to repeat the process. use cursive :: view :: Nameable ; use cursive :: views :: { Dialog , EditView , LinearLayout , ListView , RadioGroup , TextView }; fn main () { let mut siv = cursive :: default (); get_name ( & mut siv ); siv . run (); } fn get_name ( siv : & mut cursive :: Cursive ) { siv . pop_layer (); let mut options : RadioGroup < String > = RadioGroup :: new (); siv . add_layer ( Dialog :: around ( ListView :: new () . child ( \"Name\" , EditView :: new (). with_name ( \"name\" )) . child ( \"Greeting\" , LinearLayout :: vertical () . child ( options . button_str ( \"Hello\" )) . child ( options . button_str ( \"Greetings\" )), ), ) . button ( \"Ok\" , move | s | { let selection = &* options . selection (); greet_user ( s , selection ); }) . button ( \"Quit\" , cursive :: Cursive :: quit ), ); } fn greet_user ( siv : & mut cursive :: Cursive , greeting : & String ) { let name = siv . call_on_name ( \"name\" , | t : & mut EditView | t . get_content ()) . unwrap (); siv . pop_layer (); siv . add_layer ( Dialog :: around ( TextView :: new ( format! ( \"{}, {}!\" , greeting , name ))) . button ( \"Ok\" , | s | get_name ( s )), ); } use cursive :: view :: Nameable ; use cursive :: views :: { Dialog , EditView , LinearLayout , ListView , RadioGroup , TextView }; fn main () { let mut siv = cursive :: default (); get_name ( & mut siv ); siv . run (); } fn get_name ( siv : & mut cursive :: Cursive ) { siv . pop_layer (); let mut options : RadioGroup < String > = RadioGroup :: new (); let name = match std :: env :: args (). nth ( 1 ) { Some ( s ) => s , _ => String :: from ( \"World\" ) }; let editview = EditView :: new () . content ( name ) . with_name ( \"name\" ); siv . add_layer ( Dialog :: around ( ListView :: new () . child ( \"Name\" , editview ) . child ( \"Greeting\" , LinearLayout :: vertical () . child ( options . button_str ( \"Hello\" )) . child ( options . button_str ( \"Greetings\" )), ), ) . button ( \"Ok\" , move | s | { let selection = &* options . selection (); greet_user ( s , selection ); }) . button ( \"Quit\" , cursive :: Cursive :: quit ), ); } fn greet_user ( siv : & mut cursive :: Cursive , greeting : & String ) { let name = siv . call_on_name ( \"name\" , | t : & mut EditView | t . get_content ()) . unwrap (); siv . pop_layer (); siv . add_layer ( Dialog :: around ( TextView :: new ( format! ( \"{}, {}!\" , greeting , name ))) . button ( \"Ok\" , | s | get_name ( s )), ); }","title":"Hello, World!"},{"location":"Rust/Crates/Cursive/Tasks/#installation-wizard","text":"","title":"Installation wizard"},{"location":"Rust/Crates/Cursive/Tasks/#starships","text":"SelectView Enums Structs Vim keybindings JSON PostgreSQL use cursive :: traits :: { Scrollable , Resizable }; use cursive :: views :: { Dialog , SelectView , TextView }; fn main () { let mut siv = cursive :: default (); list_ships ( & mut siv ); siv . run (); } fn list_ships ( siv : & mut cursive :: Cursive ) { siv . pop_layer (); let mut select = SelectView :: new (); select . add_all_str ( vec! [ \"USS Enterprise\" , \"USS Voyager\" , \"USS Reliant\" ]); select . set_on_submit ( show_ship ); siv . add_layer ( Dialog :: around ( select . scrollable () . fixed_size (( 20 , 10 )) ). title ( \"Choose a ship\" ), ); } fn show_ship ( siv : & mut cursive :: Cursive , starship : & str ) { siv . pop_layer (); let starship = String :: from ( starship ). replace ( \"USS \" , \"\" ); let text = format! ( \"The {} is a fine ship!\" , starship ); siv . add_layer ( Dialog :: around ( TextView :: new ( text )) . button ( \"OK\" , | s | list_ships ( s )) . button ( \"Quit\" , cursive :: Cursive :: quit ), ); } use cursive :: traits :: { Scrollable , Resizable }; use cursive :: views :: { Dialog , SelectView , TextView }; mod models ; use models :: Starships ; // (1) fn main () { let mut siv = cursive :: default (); list_ships ( & mut siv ); siv . run (); } fn list_ships ( siv : & mut cursive :: Cursive ) { siv . pop_layer (); let select = SelectView :: new () . with_all_str ( vec! [ Starships :: Enterprise , Starships :: Voyager , Starships :: Defiant ]) . on_submit ( show_ship ); siv . add_layer ( Dialog :: around ( select . scrollable () . fixed_size (( 20 , 10 )) ). title ( \"Choose a ship\" ), ); } fn show_ship ( siv : & mut cursive :: Cursive , starship : & str ) { siv . pop_layer (); let starship = String :: from ( starship ). replace ( \"USS \" , \"\" ); let text = format! ( \"The {} is a fine ship!\" , starship ); siv . add_layer ( Dialog :: around ( TextView :: new ( text )) . button ( \"OK\" , | s | list_ships ( s )) . button ( \"Quit\" , cursive :: Cursive :: quit ), ); } Implementing the list of starships as an enum is possible so long as the From<Starships> trait is implemented for String. enum Starships { Enterprise , Defiant , Voyager , } impl <' a > std :: convert :: From < Starships > for String { fn from ( item : Starships ) -> String { match item { Starships :: Enterprise => String :: from ( \"USS Enterprise\" ), Starships :: Defiant => String :: from ( \"USS Defiant\" ), Starships :: Voyager => String :: from ( \"USS Voyager\" ), } } } use cursive :: traits :: { Scrollable , Resizable }; use cursive :: views :: { Dialog , SelectView , TextView , ListView }; mod models ; // (1) use models :: Starship ; fn main () { let mut siv = cursive :: default (); list_ships ( & mut siv ); siv . run (); } fn get_ships <' a > () -> Vec < Starship <' a >> { let output = vec! [ Starship { name : \"USS Enterprise\" , registry : \"NCC-1701\" , series : Series :: TOS }, Starship { name : \"USS Defiant\" , registry : \"NX-74205\" , series : Series :: DS9 }, Starship { name : \"USS Voyager\" , registry : \"NCC-74656\" , series : Series :: VOY }, ]; output } fn list_ships ( siv : & mut cursive :: Cursive ) { let ships = get_ships (); siv . pop_layer (); let select = SelectView :: new () . with_all ( ships . into_iter (). map ( | ship | ( ship . name , ship ))) // (2) . on_submit ( show_ship ); // (3) siv . add_layer ( Dialog :: around ( select . scrollable () . fixed_size (( 20 , 10 )) ). title ( \"Choose a ship\" ), ); } fn show_ship ( siv : & mut cursive :: Cursive , starship : & Starship ) { siv . add_layer ( Dialog :: around ( ListView :: new () . child ( \"Name:\" , TextView :: new ( starship . name )) . child ( \"Registry:\" , TextView :: new ( starship . registry )) . child ( \"Series:\" , TextView :: new ( starship . series )) ) . button ( \"OK\" , list_ships ) . button ( \"Quit\" , cursive :: Cursive :: quit ), ); } The Series enum must implement the Copy trait because it may not be moved in the detailed view. This is the simpler method . #[derive(Clone, Copy)] enum Series { TOS , VOY , DS9 } impl std :: convert :: From < Series > for String { fn from ( item : Series ) -> String { String :: from ( match item { Series :: TOS => \"The Original Series\" , Series :: VOY => \"Voyager\" , Series :: DS9 => \"Deep Space Nine\" , }) } } struct Starship <' a > { name : & ' a str , registry : & ' a str , series : Series } Using with_all() or set_with_all() apparently requires a Map struct, created by the map on Iterator . The keys of this object define the text that appears in the SelectView, and the values define the data passed to the callback specified in on_submit() . Sometimes a bizarre and unhelpful compiler error will occur when this second value does not match the signature of the specified callback. For example, changing ship to a unit type () will raise a compiler error: .on_submit(show_ship), --------- ^^^^^^^^^ expected signature of `for<'r, 's> fn(&'r mut Cursive, &'s ()) -> _` | required by a bound introduced by this call use cursive :: traits :: { Scrollable , Resizable }; use cursive :: views :: { Dialog , SelectView , TextView , ListView , OnEventView }; use cursive :: event :: EventResult ; mod models ; // (1) use models :: Starship ; fn main () { let mut siv = cursive :: default (); list_ships ( & mut siv ); siv . run (); } fn get_ships <' a > () -> Vec < Starship <' a >> { let output = vec! [ Starship { name : \"USS Enterprise\" , registry : \"NCC-1701\" , series : Series :: TOS }, Starship { name : \"USS Defiant\" , registry : \"NX-74205\" , series : Series :: DS9 }, Starship { name : \"USS Voyager\" , registry : \"NCC-74656\" , series : Series :: VOY }, ]; output } fn list_ships ( siv : & mut cursive :: Cursive ) { let ships = get_ships (); siv . pop_layer (); let select = SelectView :: new () . with_all ( ships . into_iter (). map ( | ship | ( ship . name , ship ))) . on_submit ( show_ship ); let select = OnEventView :: new ( select ) . on_event_inner ( 'k' , | s , _ | { let cb = s . select_up ( 1 ); Some ( EventResult :: Consumed ( Some ( cb ))) }) . on_event_inner ( 'j' , | s , _ | { let cb = s . select_down ( 1 ); Some ( EventResult :: Consumed ( Some ( cb ))) }); siv . add_layer ( Dialog :: around ( select . scrollable () . fixed_size (( 20 , 10 )) ). title ( \"Choose a ship\" ), ); } fn show_ship ( siv : & mut cursive :: Cursive , starship : & Starship ) { siv . add_layer ( Dialog :: around ( ListView :: new () . child ( \"Name:\" , TextView :: new ( starship . name )) . child ( \"Registry:\" , TextView :: new ( starship . registry )) . child ( \"Series:\" , TextView :: new ( starship . series )) // ... ) . button ( \"OK\" , list_ships ) . button ( \"Quit\" , cursive :: Cursive :: quit ), ); } models.rs #[derive(Clone, Copy)] enum Series { TOS , TNG , VOY , } impl std :: convert :: From < Series > for String { fn from ( item : Series ) -> String { String :: from ( match item { Series :: TOS => \"The Original Series\" , Series :: TNG => \"The Next Generation\" , Series :: VOY => \"Voyager\" , }) } } struct Starship <' a > { name : & ' a str , registry : & ' a str , series : Series } use cursive :: event :: EventResult ; use cursive :: traits :: { Resizable , Scrollable }; use cursive :: views :: { Dialog , ListView , OnEventView , SelectView , TextView }; mod models ; // (1) use models :: Starship ; fn main () { let mut siv = cursive :: default (); list_ships ( & mut siv ); siv . run (); } fn get_ships () -> Vec < Starship > { let f = std :: fs :: read_to_string ( \"starships.json\" ). unwrap (); let output : Vec < Starship > = serde_json :: from_str ( & f ). unwrap (); output } fn list_ships ( siv : & mut cursive :: Cursive ) { let ships : Vec < Starship > = get_ships (); siv . pop_layer (); let select = SelectView :: new () . with_all ( ships . into_iter (). map ( | ship | ( ship . name . clone (), ship ))) // (2) . on_submit ( show_ship ); let select = OnEventView :: new ( select ) . on_event_inner ( 'k' , | s , _ | { let cb = s . select_up ( 1 ); Some ( EventResult :: Consumed ( Some ( cb ))) }) . on_event_inner ( 'j' , | s , _ | { let cb = s . select_down ( 1 ); Some ( EventResult :: Consumed ( Some ( cb ))) }); siv . add_layer ( Dialog :: around ( select . scrollable (). fixed_size (( 20 , 10 ))). title ( \"Choose a ship\" )); } fn show_ship ( siv : & mut cursive :: Cursive , starship : & Starship ) { siv . add_layer ( Dialog :: around ( ListView :: new () . child ( \"Name:\" , TextView :: new ( & starship . name )) // (3) . child ( \"Registry:\" , TextView :: new ( & starship . registry )) . child ( \"Series:\" , TextView :: new ( starship . series )), ) . button ( \"OK\" , list_ships ) . button ( \"Quit\" , cursive :: Cursive :: quit ), ); } The serde_derive::Deserialize derive is necessary for the mapped struct as well as the Series enum because it appears as a member of the Starship struct. models.rs #[derive(Clone, Copy, serde_derive::Deserialize)] enum Series { TOS , DS9 , VOY , } impl std :: convert :: From < Series > for String { fn from ( item : Series ) -> String { String :: from ( match item { Series :: TOS => \"The Original Series\" , Series :: DS9 => \"Deep Space Nine\" , Series :: VOY => \"Voyager\" , }) } } #[derive(serde_derive::Deserialize)] struct Starship { name : String , registry : String , series : Series , } A clone() is necessary here, although I'm not sure why. A reference is not accepted. References are accepted here, as is a clone(). #[macro_use] extern crate diesel ; use diesel :: prelude :: * ; use diesel :: pg :: PgConnection ; use diesel :: result :: QueryResult ; use cursive :: view :: { Nameable , Resizable }; use cursive :: views :: { Dialog , EditView , ListView , SelectView , TextView }; mod models ; // (3) use models :: Starship ; mod schema ; use schema :: starships :: dsl :: * ; fn main () { let mut siv = cursive :: default (); list_ships ( & mut siv ); siv . run (); } fn get_ships () -> QueryResult < Vec < Starship >> { dotenv :: dotenv (). expect ( \"Couldn't load .env file\" ); let url = & std :: env :: var ( \"DATABASE_URL\" ) . expect ( \"Couldn't retrieve DATABASE_URL environment variable\" ); let conn = PgConnection :: establish ( url ). unwrap (); starships . load :: < Starship > ( & conn ) } fn list_ships ( siv : & mut cursive :: Cursive ) { let ships : Vec < Starship > = get_ships (). unwrap (); siv . pop_layer (); siv . add_layer ( Dialog :: around ( SelectView :: new () . with_all ( ships . into_iter () . map ( | ship | ( format! ( \"{} ({})\" , ship . name , ship . registry ), ship )), ) . on_submit ( show_ship ) ) . button ( \"Quit\" , cursive :: Cursive :: quit ) . button ( \"Add\" , add_ship ) ); } fn show_ship ( siv : & mut cursive :: Cursive , ship : & Starship ) { siv . add_layer ( Dialog :: around ( ListView :: new () . child ( \"Name:\" , TextView :: new ( & ship . name )) . child ( \"Registry:\" , TextView :: new ( & ship . registry )) . child ( \"Crew:\" , TextView :: new ( format! ( \"{}\" , ship . crew ))), ) . button ( \"OK\" , list_ships ), ) } fn add_ship ( siv : & mut cursive :: Cursive ) { let conn = get_connection (). unwrap (); siv . add_layer ( Dialog :: around ( ListView :: new () . child ( \"Name:\" , EditView :: new (). with_name ( \"name\" )) . child ( \"Registry:\" , EditView :: new (). with_name ( \"registry\" )) . child ( \"Crew:\" , EditView :: new (). with_name ( \"crew\" )), ) . button ( \"Ok\" , move | s | { let input_name = s . call_on_name ( \"name\" , | t : & mut EditView | t . get_content ()) . unwrap (); let input_registry = s . call_on_name ( \"registry\" , | t : & mut EditView | t . get_content ()) . unwrap (); let input_crew = s . call_on_name ( \"crew\" , | t : & mut EditView | t . get_content ()) . unwrap () . parse :: < i32 > () . unwrap (); let input : Starship = Starship { name : ( * input_name ). clone (), // (2) registry : ( * input_registry ). clone (), crew : input_crew , }; input . insert_into ( starships ) // (4) . get_result :: < Starship > ( & conn ) // (5) . unwrap (); list_ships ( s ); }). min_width ( 30 ) ); } use crate :: schema :: starships ; #[derive(Debug, Queryable, Insertable)] pub struct Starship { pub name : String , pub registry : String , pub crew : i32 , } Struct intiializations move values, so the String must be deeply copied, here using String::clone use crate :: schema :: starships ; #[derive(Debug, Queryable, Insertable)] pub struct Starship { pub name : String , pub registry : String , pub crew : i32 , } insert_into is a method on the Insertable trait Equivalent to: diesel :: insert_into ( starships ) . values ( & input ) . get_result :: < Starship > ( & conn ) . unwrap (); Diesel collects various methods that execute a query into the RunQueryDsl trait . These methods include: get_result which returns the affected row get_results which returns a Vec of affected rows execute which returns a usize of affected rows.","title":"Starships"},{"location":"Rust/Crates/Cursive/Tasks/#raven","text":"A multi-page wizard can be constructed with the cursive-tabs crate. use cursive :: views :: { TextView }; use cursive :: view :: Nameable ; use cursive_tabs :: TabPanel ; fn main () { let mut siv = cursive :: default (); siv . add_global_callback ( 'q' , | s | s . quit ()); let tabs = TabPanel :: new () . with_tab ( TextView :: new ( include_str! ( \"assets/raven1\" )). with_name ( \"1\" )) . with_tab ( TextView :: new ( include_str! ( \"assets/raven2\" )). with_name ( \"2\" )) . with_tab ( TextView :: new ( include_str! ( \"assets/raven3\" )). with_name ( \"3\" )) . with_tab ( TextView :: new ( include_str! ( \"assets/raven4\" )). with_name ( \"4\" )) . with_tab ( TextView :: new ( include_str! ( \"assets/raven5\" )). with_name ( \"5\" )) ; siv . add_layer ( tabs ); siv . run (); } Key bindings are added as global callbacks. This functionality works because the TabPanel is nameable and can be retrieved by the callback by its name. use cursive :: views :: { TextView }; use cursive :: view :: Nameable ; use cursive :: event :: Key ; use cursive_tabs :: TabPanel ; fn main () { let mut siv = cursive :: default (); siv . add_global_callback ( 'q' , | s | s . quit ()); let tabs = TabPanel :: new () . with_tab ( TextView :: new ( include_str! ( \"assets/raven1\" )). with_name ( \"1\" )) . with_tab ( TextView :: new ( include_str! ( \"assets/raven2\" )). with_name ( \"2\" )) . with_tab ( TextView :: new ( include_str! ( \"assets/raven3\" )). with_name ( \"3\" )) . with_tab ( TextView :: new ( include_str! ( \"assets/raven4\" )). with_name ( \"4\" )) . with_tab ( TextView :: new ( include_str! ( \"assets/raven5\" )). with_name ( \"5\" )) ; siv . add_layer ( tabs . with_name ( \"Tabs\" )); siv . add_global_callback ( Key :: PageUp , | s | { let mut tabs : cursive :: views :: ViewRef < TabPanel > = s . find_name ( \"Tabs\" ). unwrap (); tabs . prev (); }); siv . add_global_callback ( Key :: PageDown , | s | { let mut tabs : cursive :: views :: ViewRef < TabPanel > = s . find_name ( \"Tabs\" ). unwrap (); tabs . next (); }); siv . run (); }","title":"Raven"},{"location":"Rust/Crates/Diesel/","text":"Overview Learning Diesel is harder than other crates because of the amount of setup that is required. Reliant on a specific workflow dependent on the Diesel CLI utility A lot of boilerplate code Multiple APIs available for doing the same thing Diesel supports procedural functions for CRUD functions as well as (mostly) equivalent methods on structs that implement Dsl traits. The Dsl API is more fluent CRUD verb function method Create diesel::insert_into diesel::dsl::insert_into Delete diesel::delete diesel::dsl::delete ? Prerequisites Diesel has drivers for various DBMSes. Prerequisites for these have to be installed before attempting to install the corresponding features in Diesel CLI or compiling a Rust project using it. Arch Red Hat Ubuntu pacman -S postgresql su - postgres -c 'initdb --pgdata /var/lib/postgres/data' # (1) systemctl enable postgresql --now On Arch, this step appears to be necessary before the postgresql service can be enabled. initdb requires a directory to be explicitly specified using --pgdata or alternatively the PGDATA environment variable. dnf install libpq-devel mariadb-devel postgresql postgresql-server postgresql-setup --initdb # (1) systemctl enable postgresql --now This command facilitates initialization of the database cluster, which defaults to /var/lib/pgsql/data , similar to using initdb . apt install libpq-dev Diesel CLI Install Diesel CLI cargo install diesel_cli --no-default-features --features postgres sqlite # (1) Without installing dependencies, the installation will fail upon calling /usr/bin/ld because it is unable to find \"-lpq\" (in the case of PostgreSQL). .env DATABASE_URL = database.db # (1) Because this is a relative path, Diesel will create the database in the working directory, wherever it may be. So absolute paths are recommended. Create migrations directory and database specified in .env diesel setup diesel.toml [print_schema] file = \"src/schema.rs\" schema.rs table ! { starships ( registry ) { registry -> Text , name -> Text , crew -> Integer , } } The schema can also be printed from the command-line diesel print-schema Migrations Diesel facilitates the creation of migration scripts, reversible changes to the database's schema. Named up.sql and down.sql , they must be manually edited by the user. Migration commands diesel migration generate setup_tables # (1) diesel migration list diesel migration run diesel migration revert # (2) diesel migration redo Example scripts: up.sql CREATE TABLE starships ( registry TEXT PRIMARY KEY NOT NULL , name TEXT NOT NULL , crew INTEGER NOT NULL ); down.sql DROP TABLE starships ; Because the migrations are reversible (via the down.sql script), the migration can be rolled back. When a migration is performed, the Diesel CLI will generate a schema.rs file in a directory set by diesel.toml .","title":"Index"},{"location":"Rust/Crates/Diesel/#overview","text":"Learning Diesel is harder than other crates because of the amount of setup that is required. Reliant on a specific workflow dependent on the Diesel CLI utility A lot of boilerplate code Multiple APIs available for doing the same thing Diesel supports procedural functions for CRUD functions as well as (mostly) equivalent methods on structs that implement Dsl traits. The Dsl API is more fluent CRUD verb function method Create diesel::insert_into diesel::dsl::insert_into Delete diesel::delete diesel::dsl::delete ?","title":"Overview"},{"location":"Rust/Crates/Diesel/#prerequisites","text":"Diesel has drivers for various DBMSes. Prerequisites for these have to be installed before attempting to install the corresponding features in Diesel CLI or compiling a Rust project using it. Arch Red Hat Ubuntu pacman -S postgresql su - postgres -c 'initdb --pgdata /var/lib/postgres/data' # (1) systemctl enable postgresql --now On Arch, this step appears to be necessary before the postgresql service can be enabled. initdb requires a directory to be explicitly specified using --pgdata or alternatively the PGDATA environment variable. dnf install libpq-devel mariadb-devel postgresql postgresql-server postgresql-setup --initdb # (1) systemctl enable postgresql --now This command facilitates initialization of the database cluster, which defaults to /var/lib/pgsql/data , similar to using initdb . apt install libpq-dev","title":"Prerequisites"},{"location":"Rust/Crates/Diesel/#diesel-cli","text":"Install Diesel CLI cargo install diesel_cli --no-default-features --features postgres sqlite # (1) Without installing dependencies, the installation will fail upon calling /usr/bin/ld because it is unable to find \"-lpq\" (in the case of PostgreSQL). .env DATABASE_URL = database.db # (1) Because this is a relative path, Diesel will create the database in the working directory, wherever it may be. So absolute paths are recommended. Create migrations directory and database specified in .env diesel setup diesel.toml [print_schema] file = \"src/schema.rs\" schema.rs table ! { starships ( registry ) { registry -> Text , name -> Text , crew -> Integer , } } The schema can also be printed from the command-line diesel print-schema","title":"Diesel CLI"},{"location":"Rust/Crates/Diesel/#migrations","text":"Diesel facilitates the creation of migration scripts, reversible changes to the database's schema. Named up.sql and down.sql , they must be manually edited by the user. Migration commands diesel migration generate setup_tables # (1) diesel migration list diesel migration run diesel migration revert # (2) diesel migration redo Example scripts: up.sql CREATE TABLE starships ( registry TEXT PRIMARY KEY NOT NULL , name TEXT NOT NULL , crew INTEGER NOT NULL ); down.sql DROP TABLE starships ; Because the migrations are reversible (via the down.sql script), the migration can be rolled back. When a migration is performed, the Diesel CLI will generate a schema.rs file in a directory set by diesel.toml .","title":"Migrations"},{"location":"Rust/Crates/Diesel/Tasks/","text":"Tasks To-do app #[macro_use] extern crate diesel ; // (1) use diesel :: pg :: PgConnection ; use diesel :: prelude :: * ; use schema :: tasks :: dsl :: * ; mod schema { table ! { // (3) tasks ( description ) { description -> Text , } } } #[derive(Queryable, Insertable, Debug)] // (4) pub struct Task { // (6) pub description : String , } fn main () { let conn = get_connection (). unwrap (); let todo = Task { description : String :: from ( \"Buy milk\" ), }; let result = add_task ( & conn , & todo ). unwrap (); println! ( \"Added {:?}\" , result ); println! ( \"All tasks: {:?}\" , get_tasks ( & conn ). unwrap ()); } fn get_connection () -> ConnectionResult < PgConnection > { // (7) dotenv :: dotenv (). ok (); PgConnection :: establish ( & std :: env :: var ( \"DATABASE_URL\" ). unwrap ()) } fn add_task ( conn : & PgConnection , todo : & Task ) -> QueryResult < Task > { // (2) diesel :: insert_into ( tasks ) // (8) . values ( todo ). get_result ( conn ) // (5) } fn get_tasks ( conn : & PgConnection ) -> QueryResult < Vec < Task >> { // (9) tasks . load :: < Task > ( conn ) } For some reason, Diesel appears to require use of outdated syntax at the crate root (main.rs or lib.rs), without which the table! macro import in the schema is not resolved. Like ConnectionError , QueryResult is a convenience wrapper around Result : use diesel :: result :: Error ; fn add_task ( conn : & PgConnection , todo : & Task ) -> Result < Task , Error > { /* ... */ } Normally found in src/schema.rs (which can be changed by editing diesel.toml), this is the output of the Diesel CLI when running migrations. This macro actually exposes a module with the table name ( tasks in this case). This struct represents the model , or the language-native data structure into which the database's query results will be cast. If the model is in a separate module (as it normally is), the Queryable and Insertable derives are made accessible not by including from the Diesel crate directly but by including the module generated by table! . models.rs use schema :: tasks ; #[derive(Queryable, Insertable)] pub struct Task { pub description : String , } Thanks to the dsl import, the table field doesn't have to be accessed: fn get_tasks ( conn : & PgConnection ) -> QueryResult < Vec < Task >> { tasks . table . load :: < Task > ( conn ) } There is also a get_results() function that returns a Vec in case of success. fn add_task ( conn : & PgConnection , todo : & Task ) -> QueryResult < Vec < Task >> { diesel :: insert_into ( tasks :: table ) . values ( todo ) . get_results ( conn ) } The compiler will infer that the annotated struct is a record belonging to a table named after the plural of the struct's identifier (i.e. it will look for \"Tasks\", case insensitive). If the table does not have such a name it must be explicitly annotated with table_name . #[derive(Queryable, Insertable )] #[table_name = \"tasks\" ] pub struct Todo { pub description : String , } Like QueryResult , ConnectionResult is simply a convenience wrapper around Result : use diesel :: result :: ConnectionError ; fn get_connection () -> Result < PgConnection , ConnectionError > { /* ... */ } Using the Dsl import exposes some minor syntactic sugars, otherwise: fn add_task ( conn : & PgConnection , todo : & Task ) -> QueryResult < Task > { diesel :: insert_into ( tasks :: table ) . values ( todo ). get_result ( conn ) } Using the Dsl import exposes some minor syntactic sugars, otherwise: fn get_tasks ( conn : & PgConnection ) -> QueryResult < Vec < Task >> { tasks :: table . load :: < Task > ( conn ) } Starships main.rs #[macro_use] extern crate diesel ; use diesel :: pg :: PgConnection ; use diesel :: prelude :: * ; mod models ; // (1) use models :: Starship ; mod schema ; // (2) use schema :: starships ; fn main () { let conn = create_connection (). unwrap (); add_item ( & conn , & get_data ()). unwrap (); } fn add_item ( conn : & PgConnection , ship : & Starship ) -> Result < Starship , diesel :: result :: Error > { diesel :: insert_into ( starships :: table ) . values ( ship ) . get_result ( conn ) } fn create_connection () -> Result < PgConnection , diesel :: result :: ConnectionError > { dotenv :: dotenv (). ok (); let url = & std :: env :: var ( \"DATABASE_URL\" ). unwrap (); Ok ( PgConnection :: establish ( url ) ? ) } fn get_data () -> Starship { Starship { registry : \"NCC-1700\" . to_string (), name : \"USS Constitution\" . to_string (), crew : 204 , } } models.rs use crate :: schema :: starships ; #[derive(Queryable, Insertable)] pub struct Starship { pub registry : String , pub name : String , pub crew : i32 , } schema.rs table ! { starships ( registry ) { registry -> Text , name -> Text , crew -> Int4 , } } #[macro_use] extern crate diesel ; use diesel :: pg :: PgConnection ; use diesel :: prelude :: * ; use diesel :: result :: QueryResult ; mod models ; use models :: Starship ; mod schema ; // (2) use schema :: starships :: dsl :: * ; use clap :: { Args , Parser , Subcommand }; #[derive(Parser)] struct Cli { #[clap(subcommand)] command : Commands , } #[derive(Subcommand)] enum Commands { Add ( Starship ), Update ( StarshipFilter ), Remove ( StarshipFilter ), List , } #[derive(Args)] struct StarshipFilter { #[clap(short, long)] filter : String } fn main () { let app = Cli :: parse (); match app . command { Commands :: Add ( s ) => add_ship ( & s ), Commands :: List => list_ships (), Commands :: Remove ( s ) => remove_ship ( & s ), Commands :: Update ( s ) => update_ship ( & s ), } } fn update_ship ( s : & StarshipFilter ) { let s = & s . filter ; let conn = get_connection (). unwrap (); } fn remove_ship ( s : & StarshipFilter ) { let s = & s . filter ; let conn = get_connection (). unwrap (); let result = diesel :: delete ( starships . filter ( registry . ilike ( s )) ). get_result :: < Starship > ( & conn ) . expect ( \"Record not found!\" ); println! ( \"Removing {:?}\" , result ); } fn add_ship ( s : & Starship ) { let conn = get_connection (). unwrap (); println! ( \"Adding {:?}\" , s ); s . insert_into ( starships ) . execute ( & conn ) . unwrap (); } fn list_ships () { println! ( \"{:?}\" , get_ships (). unwrap ()); } fn get_connection () -> ConnectionResult < PgConnection > { dotenv :: dotenv (). expect ( \"Couldn't load .env file\" ); let url = & std :: env :: var ( \"DATABASE_URL\" ). unwrap (); PgConnection :: establish ( url ) } fn get_ships () -> QueryResult < Vec < Starship >> { let conn = get_connection (). unwrap (); starships . load :: < Starship > ( & conn ) } use crate :: schema :: starships ; use clap :: Args ; #[derive(Args,Debug, Queryable, Insertable, Identifiable, Clone)] #[primary_key(registry)] pub struct Starship { #[clap(long, short)] pub registry : String , #[clap(long, short)] pub name : String , #[clap(long, short)] pub crew : i32 , } table ! { starships ( registry ) { registry -> Text , name -> Text , crew -> Integer , } } #[macro_use] extern crate diesel ; use diesel :: pg :: PgConnection ; use diesel :: prelude :: * ; use diesel :: result :: QueryResult ; mod models ; use models :: Starship ; mod schema ; use schema :: starships :: dsl :: * ; use clap :: { Args , Parser , Subcommand }; #[derive(Parser)] struct Cli { #[clap(subcommand)] command : Commands , } #[derive(Subcommand)] enum Commands { Add ( Starship ), Update ( StarshipFilter ), Remove ( StarshipFilter ), List ( OptionalStarshipFilter ), } #[derive(Args)] struct StarshipFilter { #[clap(short, long)] filter : String } #[derive(Args)] struct OptionalStarshipFilter { #[clap(short,long)] filter : Option < String > } fn main () { let app = Cli :: parse (); match app . command { Commands :: Add ( arg ) => add_ship ( & arg ), Commands :: List ( arg ) => list_ships ( & arg ), Commands :: Remove ( arg ) => remove_ship ( & arg ), Commands :: Update ( arg ) => update_ship ( & arg ), } } fn update_ship ( s : & StarshipFilter ) { let s = & s . filter ; let conn = get_connection (). unwrap (); } fn remove_ship ( s : & StarshipFilter ) { let s = & s . filter ; let conn = get_connection (). unwrap (); let result = diesel :: delete ( starships . filter ( registry . ilike ( s )) ). get_result :: < Starship > ( & conn ) . expect ( \"Record not found!\" ); println! ( \"Removing {:?}\" , result ); } fn add_ship ( s : & Starship ) { let conn = get_connection (). unwrap (); println! ( \"Adding {:?}\" , s ); s . insert_into ( starships ) . execute ( & conn ) . unwrap (); } fn list_ships ( arg : & OptionalStarshipFilter ) { let conn = get_connection (). unwrap (); match & arg . filter { Some ( s ) => println! ( \"{:?}\" , starships . filter ( registry . ilike ( s )). load :: < Starship > ( & conn ). unwrap ()), None => println! ( \"{:?}\" , starships . load :: < Starship > ( & conn ). unwrap ()), } } fn get_connection () -> ConnectionResult < PgConnection > { dotenv :: dotenv (). expect ( \"Couldn't load .env file\" ); let url = & std :: env :: var ( \"DATABASE_URL\" ). unwrap (); PgConnection :: establish ( url ) }","title":"Tasks"},{"location":"Rust/Crates/Diesel/Tasks/#tasks","text":"","title":"Tasks"},{"location":"Rust/Crates/Diesel/Tasks/#to-do-app","text":"#[macro_use] extern crate diesel ; // (1) use diesel :: pg :: PgConnection ; use diesel :: prelude :: * ; use schema :: tasks :: dsl :: * ; mod schema { table ! { // (3) tasks ( description ) { description -> Text , } } } #[derive(Queryable, Insertable, Debug)] // (4) pub struct Task { // (6) pub description : String , } fn main () { let conn = get_connection (). unwrap (); let todo = Task { description : String :: from ( \"Buy milk\" ), }; let result = add_task ( & conn , & todo ). unwrap (); println! ( \"Added {:?}\" , result ); println! ( \"All tasks: {:?}\" , get_tasks ( & conn ). unwrap ()); } fn get_connection () -> ConnectionResult < PgConnection > { // (7) dotenv :: dotenv (). ok (); PgConnection :: establish ( & std :: env :: var ( \"DATABASE_URL\" ). unwrap ()) } fn add_task ( conn : & PgConnection , todo : & Task ) -> QueryResult < Task > { // (2) diesel :: insert_into ( tasks ) // (8) . values ( todo ). get_result ( conn ) // (5) } fn get_tasks ( conn : & PgConnection ) -> QueryResult < Vec < Task >> { // (9) tasks . load :: < Task > ( conn ) } For some reason, Diesel appears to require use of outdated syntax at the crate root (main.rs or lib.rs), without which the table! macro import in the schema is not resolved. Like ConnectionError , QueryResult is a convenience wrapper around Result : use diesel :: result :: Error ; fn add_task ( conn : & PgConnection , todo : & Task ) -> Result < Task , Error > { /* ... */ } Normally found in src/schema.rs (which can be changed by editing diesel.toml), this is the output of the Diesel CLI when running migrations. This macro actually exposes a module with the table name ( tasks in this case). This struct represents the model , or the language-native data structure into which the database's query results will be cast. If the model is in a separate module (as it normally is), the Queryable and Insertable derives are made accessible not by including from the Diesel crate directly but by including the module generated by table! . models.rs use schema :: tasks ; #[derive(Queryable, Insertable)] pub struct Task { pub description : String , } Thanks to the dsl import, the table field doesn't have to be accessed: fn get_tasks ( conn : & PgConnection ) -> QueryResult < Vec < Task >> { tasks . table . load :: < Task > ( conn ) } There is also a get_results() function that returns a Vec in case of success. fn add_task ( conn : & PgConnection , todo : & Task ) -> QueryResult < Vec < Task >> { diesel :: insert_into ( tasks :: table ) . values ( todo ) . get_results ( conn ) } The compiler will infer that the annotated struct is a record belonging to a table named after the plural of the struct's identifier (i.e. it will look for \"Tasks\", case insensitive). If the table does not have such a name it must be explicitly annotated with table_name . #[derive(Queryable, Insertable )] #[table_name = \"tasks\" ] pub struct Todo { pub description : String , } Like QueryResult , ConnectionResult is simply a convenience wrapper around Result : use diesel :: result :: ConnectionError ; fn get_connection () -> Result < PgConnection , ConnectionError > { /* ... */ } Using the Dsl import exposes some minor syntactic sugars, otherwise: fn add_task ( conn : & PgConnection , todo : & Task ) -> QueryResult < Task > { diesel :: insert_into ( tasks :: table ) . values ( todo ). get_result ( conn ) } Using the Dsl import exposes some minor syntactic sugars, otherwise: fn get_tasks ( conn : & PgConnection ) -> QueryResult < Vec < Task >> { tasks :: table . load :: < Task > ( conn ) }","title":"To-do app"},{"location":"Rust/Crates/Diesel/Tasks/#starships","text":"main.rs #[macro_use] extern crate diesel ; use diesel :: pg :: PgConnection ; use diesel :: prelude :: * ; mod models ; // (1) use models :: Starship ; mod schema ; // (2) use schema :: starships ; fn main () { let conn = create_connection (). unwrap (); add_item ( & conn , & get_data ()). unwrap (); } fn add_item ( conn : & PgConnection , ship : & Starship ) -> Result < Starship , diesel :: result :: Error > { diesel :: insert_into ( starships :: table ) . values ( ship ) . get_result ( conn ) } fn create_connection () -> Result < PgConnection , diesel :: result :: ConnectionError > { dotenv :: dotenv (). ok (); let url = & std :: env :: var ( \"DATABASE_URL\" ). unwrap (); Ok ( PgConnection :: establish ( url ) ? ) } fn get_data () -> Starship { Starship { registry : \"NCC-1700\" . to_string (), name : \"USS Constitution\" . to_string (), crew : 204 , } } models.rs use crate :: schema :: starships ; #[derive(Queryable, Insertable)] pub struct Starship { pub registry : String , pub name : String , pub crew : i32 , } schema.rs table ! { starships ( registry ) { registry -> Text , name -> Text , crew -> Int4 , } } #[macro_use] extern crate diesel ; use diesel :: pg :: PgConnection ; use diesel :: prelude :: * ; use diesel :: result :: QueryResult ; mod models ; use models :: Starship ; mod schema ; // (2) use schema :: starships :: dsl :: * ; use clap :: { Args , Parser , Subcommand }; #[derive(Parser)] struct Cli { #[clap(subcommand)] command : Commands , } #[derive(Subcommand)] enum Commands { Add ( Starship ), Update ( StarshipFilter ), Remove ( StarshipFilter ), List , } #[derive(Args)] struct StarshipFilter { #[clap(short, long)] filter : String } fn main () { let app = Cli :: parse (); match app . command { Commands :: Add ( s ) => add_ship ( & s ), Commands :: List => list_ships (), Commands :: Remove ( s ) => remove_ship ( & s ), Commands :: Update ( s ) => update_ship ( & s ), } } fn update_ship ( s : & StarshipFilter ) { let s = & s . filter ; let conn = get_connection (). unwrap (); } fn remove_ship ( s : & StarshipFilter ) { let s = & s . filter ; let conn = get_connection (). unwrap (); let result = diesel :: delete ( starships . filter ( registry . ilike ( s )) ). get_result :: < Starship > ( & conn ) . expect ( \"Record not found!\" ); println! ( \"Removing {:?}\" , result ); } fn add_ship ( s : & Starship ) { let conn = get_connection (). unwrap (); println! ( \"Adding {:?}\" , s ); s . insert_into ( starships ) . execute ( & conn ) . unwrap (); } fn list_ships () { println! ( \"{:?}\" , get_ships (). unwrap ()); } fn get_connection () -> ConnectionResult < PgConnection > { dotenv :: dotenv (). expect ( \"Couldn't load .env file\" ); let url = & std :: env :: var ( \"DATABASE_URL\" ). unwrap (); PgConnection :: establish ( url ) } fn get_ships () -> QueryResult < Vec < Starship >> { let conn = get_connection (). unwrap (); starships . load :: < Starship > ( & conn ) } use crate :: schema :: starships ; use clap :: Args ; #[derive(Args,Debug, Queryable, Insertable, Identifiable, Clone)] #[primary_key(registry)] pub struct Starship { #[clap(long, short)] pub registry : String , #[clap(long, short)] pub name : String , #[clap(long, short)] pub crew : i32 , } table ! { starships ( registry ) { registry -> Text , name -> Text , crew -> Integer , } } #[macro_use] extern crate diesel ; use diesel :: pg :: PgConnection ; use diesel :: prelude :: * ; use diesel :: result :: QueryResult ; mod models ; use models :: Starship ; mod schema ; use schema :: starships :: dsl :: * ; use clap :: { Args , Parser , Subcommand }; #[derive(Parser)] struct Cli { #[clap(subcommand)] command : Commands , } #[derive(Subcommand)] enum Commands { Add ( Starship ), Update ( StarshipFilter ), Remove ( StarshipFilter ), List ( OptionalStarshipFilter ), } #[derive(Args)] struct StarshipFilter { #[clap(short, long)] filter : String } #[derive(Args)] struct OptionalStarshipFilter { #[clap(short,long)] filter : Option < String > } fn main () { let app = Cli :: parse (); match app . command { Commands :: Add ( arg ) => add_ship ( & arg ), Commands :: List ( arg ) => list_ships ( & arg ), Commands :: Remove ( arg ) => remove_ship ( & arg ), Commands :: Update ( arg ) => update_ship ( & arg ), } } fn update_ship ( s : & StarshipFilter ) { let s = & s . filter ; let conn = get_connection (). unwrap (); } fn remove_ship ( s : & StarshipFilter ) { let s = & s . filter ; let conn = get_connection (). unwrap (); let result = diesel :: delete ( starships . filter ( registry . ilike ( s )) ). get_result :: < Starship > ( & conn ) . expect ( \"Record not found!\" ); println! ( \"Removing {:?}\" , result ); } fn add_ship ( s : & Starship ) { let conn = get_connection (). unwrap (); println! ( \"Adding {:?}\" , s ); s . insert_into ( starships ) . execute ( & conn ) . unwrap (); } fn list_ships ( arg : & OptionalStarshipFilter ) { let conn = get_connection (). unwrap (); match & arg . filter { Some ( s ) => println! ( \"{:?}\" , starships . filter ( registry . ilike ( s )). load :: < Starship > ( & conn ). unwrap ()), None => println! ( \"{:?}\" , starships . load :: < Starship > ( & conn ). unwrap ()), } } fn get_connection () -> ConnectionResult < PgConnection > { dotenv :: dotenv (). expect ( \"Couldn't load .env file\" ); let url = & std :: env :: var ( \"DATABASE_URL\" ). unwrap (); PgConnection :: establish ( url ) }","title":"Starships"},{"location":"Rust/Tasks/","text":"Boilerplate Nightly build rustup install nightly The nightly build can be specified ad hoc or permanently for the crate Ad hoc Permanently cargo +nightly run rustup override set nightly Publishing Some additional fields of the Cargo.toml are required before publishing: [package] name = \"mdrend\" version = \"0.1.0\" edition = \"2018\" authors = [ \"Johnny Appleseed <johnny@apple.com>\" ] license = \"MIT\" keywords = [ \"Parse\" , \"markdown\" ] repository = \"https://github.com/...\" description = \"Read a markdown file and return parsed HTML\" [dependencies] clap = \"2.34.0\" maud = \"0.23.0\" pulldown-cmark = \"0.8.0\" cargo login cargo publish","title":"Boilerplate"},{"location":"Rust/Tasks/#boilerplate","text":"","title":"Boilerplate"},{"location":"Rust/Tasks/#nightly-build","text":"rustup install nightly The nightly build can be specified ad hoc or permanently for the crate Ad hoc Permanently cargo +nightly run rustup override set nightly","title":"Nightly build"},{"location":"Rust/Tasks/#publishing","text":"Some additional fields of the Cargo.toml are required before publishing: [package] name = \"mdrend\" version = \"0.1.0\" edition = \"2018\" authors = [ \"Johnny Appleseed <johnny@apple.com>\" ] license = \"MIT\" keywords = [ \"Parse\" , \"markdown\" ] repository = \"https://github.com/...\" description = \"Read a markdown file and return parsed HTML\" [dependencies] clap = \"2.34.0\" maud = \"0.23.0\" pulldown-cmark = \"0.8.0\" cargo login cargo publish","title":"Publishing"},{"location":"Rust/Tasks/Hello-World/","text":"Hello, World! Parameterized fn main () { let name = String :: from ( match std :: env :: args (). nth ( 1 ) { // (1) Some ( s ) => s , _ => \"World\" ; }); println! ( \"Hello, {}!\" , name ); } if let is also possible, if more verbose. let mut name = String :: new (); if let Some ( s ) = std :: env :: args (). nth ( 1 ) { name = s ; } else { name = String :: from ( \"World\" ); } Interactive The interactive implementation of Hello, World! allows us to apply the and_then() combinator to great effect. fn main () { get_name (). and_then ( display_name ); } fn get_name () -> Result < String , std :: io :: Error > { let mut name = String :: new (); std :: io :: stdin (). read_line ( & mut name ) ? ; Ok ( name . trim (). to_string ()) } fn display_name ( name : String ) -> Result < (), std :: io :: Error > { println! ( \"Hello, {}!\" , name ); Ok (()) } Modules use input :: get_name ; use output :: display_name ; fn main () -> Result < (), std :: io :: Error > { get_name (). and_then ( display_name ) ? ; Ok (()) } pub mod input { pub fn get_name () -> Result < String , std :: io :: Error > { let mut name = String :: new (); println! ( \"What is your name? \" ); std :: io :: stdin (). read_line ( & mut name ) ? ; Ok ( name . trim (). to_string ()) } } pub mod output { pub fn display_name ( name : String ) -> Result < (), std :: io :: Error > { println! ( \"Hello, {}!\" , name ); Ok (()) } } When separating modules into their own files, the filename of the module must match the name provided after mod . Folders can also be used, in which case the folder name must match. main.rs mod input ; mod output ; pub use input :: get_name ; pub use output :: display_name ; fn main () -> Result < (), std :: io :: Error > { get_name (). and_then ( display_name ) ? ; Ok (()) } input.rs pub fn get_name () -> Result < String , std :: io :: Error > { let mut name = String :: new (); println! ( \"What is your name? \" ); std :: io :: stdin (). read_line ( & mut name ) ? ; Ok ( name . trim (). to_string ()) } output.rs pub fn display_name ( name : String ) -> Result < (), std :: io :: Error > { println! ( \"Hello, {}!\" , name ); Ok (()) } Caesar cipher ''' rs pub mod encryptor { pub trait Encryptable { fn encrypt(&self) -> String; } pub struct Rot13(pub String); impl Encryptable for Rot13 { fn encrypt(&self) -> String { self.0 .chars() .map(|ch| match ch { 'a'..='m' | 'A'..='M' => (ch as u8 + 13) as char, 'n'..='z' | 'N'..='Z' => (ch as u8 - 13) as char, _ => ch, }) .collect() } } } use encryptor::Encryptable; fn main() { println!(\"Input the string you want to encrypt:\"); let mut user_input = String::new(); std::io::stdin() .read_line(&mut user_input) .expect(\"Cannot read input!\"); println!( \"Your encrypted string: {}\", encryptor::Rot13(user_input).encrypt() ); } '''","title":"Hello, World!"},{"location":"Rust/Tasks/Hello-World/#hello-world","text":"","title":"Hello, World!"},{"location":"Rust/Tasks/Hello-World/#parameterized","text":"fn main () { let name = String :: from ( match std :: env :: args (). nth ( 1 ) { // (1) Some ( s ) => s , _ => \"World\" ; }); println! ( \"Hello, {}!\" , name ); } if let is also possible, if more verbose. let mut name = String :: new (); if let Some ( s ) = std :: env :: args (). nth ( 1 ) { name = s ; } else { name = String :: from ( \"World\" ); }","title":"Parameterized"},{"location":"Rust/Tasks/Hello-World/#interactive","text":"The interactive implementation of Hello, World! allows us to apply the and_then() combinator to great effect. fn main () { get_name (). and_then ( display_name ); } fn get_name () -> Result < String , std :: io :: Error > { let mut name = String :: new (); std :: io :: stdin (). read_line ( & mut name ) ? ; Ok ( name . trim (). to_string ()) } fn display_name ( name : String ) -> Result < (), std :: io :: Error > { println! ( \"Hello, {}!\" , name ); Ok (()) }","title":"Interactive"},{"location":"Rust/Tasks/Hello-World/#modules","text":"use input :: get_name ; use output :: display_name ; fn main () -> Result < (), std :: io :: Error > { get_name (). and_then ( display_name ) ? ; Ok (()) } pub mod input { pub fn get_name () -> Result < String , std :: io :: Error > { let mut name = String :: new (); println! ( \"What is your name? \" ); std :: io :: stdin (). read_line ( & mut name ) ? ; Ok ( name . trim (). to_string ()) } } pub mod output { pub fn display_name ( name : String ) -> Result < (), std :: io :: Error > { println! ( \"Hello, {}!\" , name ); Ok (()) } } When separating modules into their own files, the filename of the module must match the name provided after mod . Folders can also be used, in which case the folder name must match. main.rs mod input ; mod output ; pub use input :: get_name ; pub use output :: display_name ; fn main () -> Result < (), std :: io :: Error > { get_name (). and_then ( display_name ) ? ; Ok (()) } input.rs pub fn get_name () -> Result < String , std :: io :: Error > { let mut name = String :: new (); println! ( \"What is your name? \" ); std :: io :: stdin (). read_line ( & mut name ) ? ; Ok ( name . trim (). to_string ()) } output.rs pub fn display_name ( name : String ) -> Result < (), std :: io :: Error > { println! ( \"Hello, {}!\" , name ); Ok (()) }","title":"Modules"},{"location":"Rust/Tasks/Hello-World/#caesar-cipher","text":"''' rs pub mod encryptor { pub trait Encryptable { fn encrypt(&self) -> String; } pub struct Rot13(pub String); impl Encryptable for Rot13 { fn encrypt(&self) -> String { self.0 .chars() .map(|ch| match ch { 'a'..='m' | 'A'..='M' => (ch as u8 + 13) as char, 'n'..='z' | 'N'..='Z' => (ch as u8 - 13) as char, _ => ch, }) .collect() } } } use encryptor::Encryptable; fn main() { println!(\"Input the string you want to encrypt:\"); let mut user_input = String::new(); std::io::stdin() .read_line(&mut user_input) .expect(\"Cannot read input!\"); println!( \"Your encrypted string: {}\", encryptor::Rot13(user_input).encrypt() ); } '''","title":"Caesar cipher"},{"location":"Rust/Tasks/Mathematics/","text":"Mathematics Weight on Mars fn main () { let mut input = String :: new (); println! ( \"Enter weight in kilograms: \" ); std :: io :: stdin (). read_line ( & mut input ); let input : f32 = match input . trim (). parse () { Ok ( num ) => num , Err ( _ ) => 0.0 , }; let mars_weight = calculate_weight_on_mars ( input ); println! ( \"Weight on Mars: {} kg\" , & mars_weight ); } fn calculate_weight_on_mars ( weight : f32 ) -> f32 { ( weight / 9.81 ) * 3.711 } Fibonacci sequence Recursive Memoized use std :: collections :: HashMap ; fn fib ( n : u64 ) -> u64 { match n { 0 | 1 => 1 , n => fib ( n - 1 ) + fib ( n - 2 ) } } fn main () { let n : u64 = std :: env :: args (). nth ( 1 ). unwrap (). parse (). unwrap (); for i in 1 .. n { println! ( \"{}: {}\" , i , fib ( i )); } } use std :: collections :: HashMap ; fn fib ( n : u64 , map : & mut HashMap < u64 , u64 > ) -> u64 { match n { 0 | 1 => 1 , n => { if map . contains_key ( & n ) { * map . get ( & n ). unwrap () } else { let val = fib ( n - 1 , map ) + fib ( n - 2 , map ); map . insert ( n , val ); val } } } } fn main () { let n : u64 = std :: env :: args (). nth ( 1 ). unwrap (). parse (). unwrap (); let mut map = HashMap :: new (); for i in 1 .. n { println! ( \"{}: {}\" , i , fib ( i , & mut map )); } }","title":"Mathematics"},{"location":"Rust/Tasks/Mathematics/#mathematics","text":"","title":"Mathematics"},{"location":"Rust/Tasks/Mathematics/#weight-on-mars","text":"fn main () { let mut input = String :: new (); println! ( \"Enter weight in kilograms: \" ); std :: io :: stdin (). read_line ( & mut input ); let input : f32 = match input . trim (). parse () { Ok ( num ) => num , Err ( _ ) => 0.0 , }; let mars_weight = calculate_weight_on_mars ( input ); println! ( \"Weight on Mars: {} kg\" , & mars_weight ); } fn calculate_weight_on_mars ( weight : f32 ) -> f32 { ( weight / 9.81 ) * 3.711 }","title":"Weight on Mars"},{"location":"Rust/Tasks/Mathematics/#fibonacci-sequence","text":"Recursive Memoized use std :: collections :: HashMap ; fn fib ( n : u64 ) -> u64 { match n { 0 | 1 => 1 , n => fib ( n - 1 ) + fib ( n - 2 ) } } fn main () { let n : u64 = std :: env :: args (). nth ( 1 ). unwrap (). parse (). unwrap (); for i in 1 .. n { println! ( \"{}: {}\" , i , fib ( i )); } } use std :: collections :: HashMap ; fn fib ( n : u64 , map : & mut HashMap < u64 , u64 > ) -> u64 { match n { 0 | 1 => 1 , n => { if map . contains_key ( & n ) { * map . get ( & n ). unwrap () } else { let val = fib ( n - 1 , map ) + fib ( n - 2 , map ); map . insert ( n , val ); val } } } } fn main () { let n : u64 = std :: env :: args (). nth ( 1 ). unwrap (). parse (). unwrap (); let mut map = HashMap :: new (); for i in 1 .. n { println! ( \"{}: {}\" , i , fib ( i , & mut map )); } }","title":"Fibonacci sequence"},{"location":"Rust/Tasks/Porting-grep/","text":"grep ports The success of ripgrep has inspired many ports of other GNU utilities to Rust. Porting grep to Rust provides the opportunity to explore various ways of using iterators , evolving from a naive for in loop to the filter() iterator method using a closure . Loop fn main () { let mut results = Vec :: new (); for line in contents . lines () { if line . contains ( query ) { results . push ( line ); } } results } filter fn main () { contents . lines () . filter ( | line | line . contains ( query )) . collect () } grep-lite This example begins with hard-coded strings and quickly develops the core logic of searching for a string. First regex (2), then a CLI framework is implemented (3). We take a slight digression to illustrate how file reading is done in Rust. First we produce a naive implementation (4) using a loop . A more expressive choice (5) is a for in loop iterating over the iterator returned by the BufReader's lines method. Finally, file opening is incorporated into the business logic of the application (6). The search logic itself is abstracted into a function. If the - argument is received from the command-line, STDIN is treated just as a file. Files are modeled with structs (7). read() models reading a file by cloning the File struct's data Vector, then reserve ing that clone's length and appending its values to a buffer struct. open and close functions are inert stubs (8). Refactoring read() into a method on File and implementing the new method using an impl blocks is more idiomatic and readable (9). open() , close() , and read() are refactored to use Results. Also a rand function is used to model error generation on a random basis (10). Define a FileState enum (11). Implementing the Display trait allows us to use a standard template in a println! statement (12). 1 2 3 4 5 6 7 8 9 10 11 12 // RIA 68 fn main () { let search_term = \"picture\" ; let quote = \"Once upon a midnight dreary \\n While I pondered weak and weary \\n Over many a quaint and curious volume of forgotten lore\" ; for line in quote . lines () { if line . contains ( search_term ) { println! ( \"{}\" , line ); } } } // RIA 69 use regex :: Regex ; fn main () { let re = Regex :: new ( \"picture\" ). unwrap (); let quote = \"Once upon a midnight dreary \\n While I pondered weak and weary \\n Over many a quaint and curious volume of forgotten lore\" ; for line in quote . lines () { let contains_substring = re . find ( line ); match contains_substring { Some ( _ ) => println ( \"{}\" , line ), None => (), } } } // RIA 71-72 use regex :: Regex ; use clap :: { App , Arg }; fn main () { let args = App :: new ( \"grep-lite\" ) . version ( \"0.1\" ) . about ( \"searches for patterns\" ) . arg ( Arg :: with_name ( \"pattern\" ) . help ( \"The pattern to search for\" ) . takes_value ( true ) . required ( true )) . get_matches (); let pattern = args . value_of ( \"pattern\" ). unwrap (); let re = Regex :: new ( \"picture\" ). unwrap (); let quote = \"Once upon a midnight dreary \\n While I pondered weak and weary \\n Over many a quaint and curious volume of forgotten lore\" ; for line in quote . lines () { let contains_substring = re . find ( line ); match contains_substring { Some ( _ ) => println ( \"{}\" , line ), None => (), } } } // RIA 73 use std :: fs :: File ; use std :: io :: BufReader ; use std :: io :: prelude :: * ; fn main () { let f = File :: open ( \"readme.md\" ). unwrap (); let mut reader = Bufreader :: new ( f ); let mut line = String :: new (); loop { let len = reader . read_line ( & mut line ) . unwrap (); if len == 0 { break } println! ( \"{} ({} bytes long)\" , line , len ); line . truncate ( 0 ); } } // RIA 73 use std :: fs :: File ; use std :: io :: BufReader ; use std :: io :: prelude :: * ; fn main () { let f = File :: open ( \"readme.md\" ). unwrap (); let reader = BufReader :: new ( f ); for line_ in reader . lines () { let line = line_ . unwrap (); println! ( \"{} ({} bytes long)\" , line , line . len ()); } } // RIA 74-75 use std :: fs :: File ; use std :: io ; use std :: io :: BufReader ; use std :: io :: prelude :: * ; use regex :: Regex ; use clap :: { App , Arg }; fn process_lines < T : BufRead + Sized > ( reader : T , re : Regex ) { for line_ in reader . lines () { let line = line_ . unwrap (); match re . find ( & line ) { Some ( _ ) => println! ( \"{}\" , line ), None => (),) } } } fn main () { let args = App :: new ( \"grep-lite\" ) . version ( \"0.1\" ) . about ( \"searches for patterns\" ) . arg ( Arg :: with_name ( \"pattern\" ) . help ( \"The pattern to search for\" ) . takes_value ( true ) . required ( true )) . arg ( Arg :: with_name ( \"input\" ) . help ( \"File to search\" ) . takes_value ( true ) . required ( false )) . get_matches (); let pattern = args . value_of ( \"pattern\" ). unwrap (); let re = Regex :: new ( \"picture\" ). unwrap (); let input = args . value_of ( \"input\" ). unwrap_or ( \"-\" ); if input == \"-\" { let stdin = io :: stdin (); let reader = stdin . lock (); process_lines ( reader , re ); } else { let f = File :: open ( input ). unwrap (); let reader = BufReader :: new ( f ); process_lines ( reader , re ); } } // RIA 80 #[derive(Debug)] struct File { name : String , data : Vec < u8 > , } fn main () { let f1 = File { name : String :: from ( \"f1.txt\" ), data : Vec : new (), }; let f1_name = & f1 . name ; let f1_length = & f1 . data . len (); println! ( \"{:?}\" , f1 ); println! ( \"{} is {} byes long\" , f1_name , f1_length ); } // RIA 82-83 #![allow(unused_variables)] #[derive(Debug)] struct File { name : String , data : Vec < u8 > , } fn read ( f : & File , save_to : & mut Vec < u8 > , ) -> usize { let mut tmp = f . data . clone (); let read_length = tmp . len (); save_to . reserve ( read_length ); save_to . append ( & mut tmp ); read_length } fn open ( f : & mut File ) -> bool { true } fn close ( f : & mut File ) -> bool { true } fn main () { let mut f = File { name : String :: from ( \"2.txt\" ), data : vec ! [ 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 , 29 ], }; let mut buffer : Vec < u8 > = vec! []; open ( & mut f ); let f_length = read ( & f2 , & mut buffer ); close ( & mut f ); let text = String :: from_utf8_lossy ( & buffer ); println! ( \"{:?}\" , f ); println! ( \"{} is {} bytes long\" , & f . name , f_length ); println! ( \"{}\" , text ); } // RIA 86 #![allow(unused_variables)] #[derive(Debug)] struct File { name : String , data : Vec < u8 > , } impl File { fn new ( name : & str ) -> File { File { name : String :: from ( name ), data : Vec :: new (), } } fn new_with_data ( name : & str , data : & Vec < u8 > , ) -> File { let mut f = File :: new ( name ); f . data = data . clone (); f } fn read ( self : & File , save_to : & mut Vec < u8 > , ) -> usize { let mut tmp = self . data . clone (); let read_length = tmp . len (); save_to . reserve ( read_length ); save_to . append ( & mut tmp ); read_length } } fn open ( f : & mut File ) -> bool { true } fn close ( f : & mut File ) -> bool { true } fn main () { let f_data : Vec < u8 > = vec! [ 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 , 29 ]; let mut f = File :: new_with_data ( \"2.txt\" , & f_data ); let mut buffer : Vec < u8 > = vec! []; open ( & mut f ); let f_length = f . read ( & mut buffer ); close ( & mut f ); let text = String :: from_utf8_lossy ( & buffer ); println! ( \"{:?}\" , f ); println! ( \"{} is {} bytes long\" , & f . name , f_length ); println! ( \"{}\" , text ); } // RIA 93-94 use rand :: prelude :: * ; fn one_in ( denominator : u32 ) -> bool { thread_rng (). gen_ratio ( 1 , denominator ) } #[derive(Debug)] struct File { name : String , data : Vec < u8 > , } impl File { fn new ( name : & str ) -> File { File { name : String :: from ( name ), data : Vec :: new (), } } fn new_with_data ( name : & str , data : Vec < u8 > ) -> File { let mut f = File :: new ( name ); f . data = data . clone (); f } fn read ( self : & File , save_to : & mut Vec < u8 > , ) -> Result < usize , String > { let mut tmp = self . data . clone (); let read_length = tmp . len (); save_to . reserve ( read_length ); save_to . append ( & mut tmp ); Ok ( read_length ) } } fn open ( f : File ) -> Result < File , String > { if one_in ( 10_000 ) { let err_mag = String :: from ( \"Permission denied\" ); return Err ( err_msg ); } Ok ( f ) } fn close ( f : File ) -> Result < File , String > { if one_in ( 100_000 ) { let err_mag = String :: from ( \"Interrupted by signal!\" ); return Err ( err_msg ); } Ok ( f ) } fn main () { let f_data : Vec < u8 > = vec! [ 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 , 29 ]; let mut f = File :: new_with_data ( \"4.txt\" , & f_data ); let mut buffer : Vec < u8 > = vec! []; f = open ( f ). unwrap (); let f_length = f . read ( & mut buffer ). unwrap (); f = close ( f ). unwrap (); let text = String :: from_utf8_lossy ( & buffer ); println! ( \"{:?}\" , f ); println! ( \"{} is {} bytes long\" , & f . name , f_length ); println! ( \"{}\" , text ); } // RIA 97-98 #[derive(Debug,PartialEq)] enum FileState { Open , Closed , } #[derive(Debug)] struct File { name : String , data : Vec < u8 > , state : FileState , } impl File { fn new ( name : & str ) -> File { File { name : String :: from ( name ), data : Vec :: new (), state : FileState :: Closed , } } fn read ( self : & File , save_to : & mut Vec < u8 > , ) -> Result < usize , String > { let mut tmp = self . data . clone (); let read_length = tmp . len (); save_to . reserve ( read_length ); save_to . append ( & mut tmp ); Ok ( read_length ) } } fn open ( mut f : File ) -> Result < File , String > { f . state = FileState :: Open ; Ok ( f ) } fn close ( mut f : File ) -> Result < File , String > { f . state = FileState :: Closed ; Ok ( f ) } fn main () { let f_data : Vec < u8 > = vec! [ 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 , 29 ]; let mut f = File :: new ( \"4.txt\" ); let mut buffer : Vec < u8 > = vec! []; f = open ( f ). unwrap (); let f_length = f . read ( & mut buffer ). unwrap (); f = close ( f ). unwrap (); let text = String :: from_utf8_lossy ( & buffer ); println! ( \"{:?}\" , f ); println! ( \"{} is {} bytes long\" , & f . name , f_length ); println! ( \"{}\" , text ); } // RIA 101-102 #![allow(dead_code)] use std :: fmt ; use std :: fmt :: { Display }; #[derive(Debug,PartialEq)] enum FileState { Open , Closed , } impl Display for FileState { fn fmt ( & self , f : & mut fmt :: Formatter ) -> fmt :: Result { match * self { FileState :: Open => write! ( f , \"OPEN\" ), FileState :: Closed => write! ( f , \"CLOSED\" ), } } } impl Display for File { fn fmt ( & self , f : & mut fmt :: Formatter ) -> fmt :: Result { write! ( f , \"<{} ({})>\" , self . name , self . state ) } } // --snip-- fn main () { println! ( \"{:?}\" , f ); println! ( \"{}\" , f ); } minigrep From RPL p. 233-263 1 2 3 4 5 6 7 8 9 10 11 12 13 main.rs fn main () { let args : Vec < String > = std :: env :: args (). collect (); println! ( \"{:?}\" , args ); } Read any command line arguments passed, collecting into a vector. main.rs fn main () { let args : Vec < String > = std :: env :: args (). collect (); let query = & args [ 1 ]; let filename = & args [ 2 ]; println! ( \"Searching for {}\" , query ); println! ( \"In file {}\" , filename ); } main.rs fn main () { let args : Vec < String > = std :: env :: args (). collect (); let query = & args [ 1 ]; let filename = & args [ 2 ]; println! ( \"Searching for {}\" , query ); println! ( \"In file {}\" , filename ); let contents = fs :: read_to_string ( filename ) . expect ( \"Somethign went wrong reading the file\" ); println! ( \"With text: \\n {}\" , contents ); } fs::read_to_string() takes the filename, opens it, and returns a Result<String> of the file's contents. main.rs fn main () { let args : Vec < String > = std :: env :: args (). collect (); let config = parse_config ( & args ); println! ( \"Searching for {} in file {}\" , config . query , config . filename ); let contents = std :: fs :: read_to_string ( config . filename ). expect ( \"Couldn't read file\" ); println! ( \"{}\" , contents ); } struct Config { query : String , filename : String , } fn parse_config ( args : & [ String ]) -> Config { let query = args [ 1 ]. clone (); let filename = args [ 2 ]. clone (); Config { query , filename } } Refactoring to abstract command-line argument parsing logic to its own function main.rs fn main () { let args : Vec < String > = std :: env :: args (). collect (); let config = Config :: new ( & args ); println! ( \"Searching for {} in file {}\" , config . query , config . filename ); let contents = std :: fs :: read_to_string ( config . filename ). expect ( \"Couldn't read file\" ); println! ( \"{}\" , contents ); } struct Config { query : String , filename : String , } impl Config { fn new ( args : & [ String ]) -> Config { let query = args [ 1 ]. clone (); let filename = args [ 2 ]. clone (); Config { query , filename } } } Refactor the config parser into Config's constructor main.rs fn main () { let args : Vec < String > = std :: env :: args (). collect (); let config = Config :: new ( & args ); println! ( \"Searching for {} in file {}\" , config . query , config . filename ); let contents = std :: fs :: read_to_string ( config . filename ). expect ( \"Couldn't read file\" ); println! ( \"{}\" , contents ); } struct Config { query : String , filename : String , } impl Config { fn new ( args : & [ String ]) -> Config { if args . len () < 3 { panic! ( \"Not enough args!\" ); } let query = args [ 1 ]. clone (); let filename = args [ 2 ]. clone (); Config { query , filename } } } Implement error message on invalid number of arguments main.rs fn main () { let args : Vec < String > = std :: env :: args (). collect (); let config = Config :: new ( & args ). unwrap_or_else ( | err | { println! ( \"Problem parsing arguments: {}\" , err ); std :: process :: exit ( 1 ); }); println! ( \"Searching for {} in file {}\" , config . query , config . filename ); let contents = std :: fs :: read_to_string ( config . filename ). expect ( \"Couldn't read file\" ); println! ( \"{}\" , contents ); } struct Config { query : String , filename : String , } impl Config { fn new ( args : & [ String ]) -> Result < Config , &' static str > { if args . len () < 3 { return Err ( \"not enough arguments\" ); } let query = args [ 1 ]. clone (); let filename = args [ 2 ]. clone (); Ok ( Config { query , filename }) } } Remove previous error message and incorporate error in the Err variant of a Result<T,E> . The argument passed to unwrap_or_else() is a closure . main.rs fn main () { let args : Vec < String > = std :: env :: args (). collect (); let config = Config :: new ( & args ). unwrap_or_else ( | err | { println! ( \"Problem parsing arguments: {}\" , err ); std :: process :: exit ( 1 ); }); println! ( \"Searching for {} in file {}\" , config . query , config . filename ); run ( config ); } fn run ( config : Config ) { let contents = std :: fs :: read_to_string ( config . filename ) . expect ( \"Couldn't read file\" ); println! ( \"{}\" , contents ); } struct Config { query : String , filename : String , } impl Config { fn new ( args : & [ String ]) -> Result < Config , &' static str > { if args . len () < 3 { return Err ( \"not enough arguments\" ); } let query = args [ 1 ]. clone (); let filename = args [ 2 ]. clone (); Ok ( Config { query , filename }) } } Abstract program logic into run() main.rs fn main () { let args : Vec < String > = std :: env :: args (). collect (); let config = Config :: new ( & args ). unwrap_or_else ( | err | { println! ( \"Problem parsing arguments: {}\" , err ); std :: process :: exit ( 1 ); }); println! ( \"Searching for {} in file {}\" , config . query , config . filename ); run ( config ). unwrap (); } fn run ( config : Config ) -> Result < (), Box < dyn std :: error :: Error >> { let contents = std :: fs :: read_to_string ( config . filename ) ? ; println! ( \"{}\" , contents ); Ok (()) } struct Config { query : String , filename : String , } impl Config { fn new ( args : & [ String ]) -> Result < Config , &' static str > { if args . len () < 3 { return Err ( \"not enough arguments\" ); } let query = args [ 1 ]. clone (); let filename = args [ 2 ]. clone (); Ok ( Config { query , filename }) } } Refactor run() to return a Result<T,E> in the Ok case and the trait object Box<dyn Error> for the error type. This allows various error types to be returned. Also, the expect() method is replaced by the ? operator which returns the error type for the calling function to handle, rather than a boilerplate error message. main.rs fn main () { let args : Vec < String > = std :: env :: args (). collect (); let config = Config :: new ( & args ). unwrap_or_else ( | err | { println! ( \"Problem parsing arguments: {}\" , err ); std :: process :: exit ( 1 ); }); println! ( \"Searching for {} in file {}\" , config . query , config . filename ); if let Err ( e ) = run ( config ) { println! ( \"Application error: {}\" , e ); std :: process :: exit ( 1 ); } } fn run ( config : Config ) -> Result < (), Box < dyn std :: error :: Error >> { let contents = std :: fs :: read_to_string ( config . filename ) ? ; println! ( \"{}\" , contents ); Ok (()) } struct Config { query : String , filename : String , } impl Config { fn new ( args : & [ String ]) -> Result < Config , &' static str > { if args . len () < 3 { return Err ( \"not enough arguments\" ); } let query = args [ 1 ]. clone (); let filename = args [ 2 ]. clone (); Ok ( Config { query , filename }) } } Rather than unwrap() , we use if let to check for and handle errors from run() . lib.rs main.rs use std :: fs ; use std :: error :: Error ; pub struct Config { pub query : String , pub filename : String , } impl Config { pub fn new ( args : & [ String ]) -> Result < Config , &' static str > { if args . len () < 3 { return Err ( \"not enough arguments\" ); } let query = args [ 1 ]. clone (); let filename = args [ 2 ]. clone (); Ok ( Config { query , filename }) } } pub fn run ( config : Config ) -> Result < (), Box < dyn Error >> { let contents = fs :: read_to_string ( config . filename ) ? ; println! ( \"{}\" , contents ); Ok (()) } use std :: process ; use std :: env ; use lib :: run ; use lib :: Config ; pub mod lib ; fn main () { let args : Vec < String > = env :: args (). collect (); let config = Config :: new ( & args ). unwrap_or_else ( | err | { println! ( \"Problem parsing arguments: {}\" , err ); process :: exit ( 1 ); }); println! ( \"Searching for {} in file {}\" , config . query , config . filename ); if let Err ( e ) = run ( config ) { println! ( \"Application error: {}\" , e ); process :: exit ( 1 ); } } Abstract all elements except main() into a library module lib.rs main.rs use std :: fs ; use std :: error :: Error ; pub struct Config { pub query : String , pub filename : String , } impl Config { pub fn new ( args : & [ String ]) -> Result < Config , &' static str > { if args . len () < 3 { return Err ( \"not enough arguments\" ); } let query = args [ 1 ]. clone (); let filename = args [ 2 ]. clone (); Ok ( Config { query , filename }) } } pub fn run ( config : Config ) -> Result < (), Box < dyn Error >> { let contents = fs :: read_to_string ( config . filename ) ? ; println! ( \"{}\" , contents ); Ok (()) } pub fn search <' a > ( query : & str , contents : & ' a str ) -> Vec <&' a str > { let mut results = Vec :: new (); for line in contents . lines () { if line . contains ( query ) { results . push ( line ); } } results } #[cfg(test)] mod tests { use super :: * ; #[test] fn one_result () { let query = \"duct\" ; let contents = \" \\n Rust: \\n safe, fast, productive. \\n Pick three.\" ; assert_eq! ( vec! [ \"safe, fast, productive.\" ], search ( query , contents ) ) } } use std :: process ; use std :: env ; use lib :: run ; use lib :: Config ; pub mod lib ; fn main () { let args : Vec < String > = env :: args (). collect (); let config = Config :: new ( & args ). unwrap_or_else ( | err | { println! ( \"Problem parsing arguments: {}\" , err ); process :: exit ( 1 ); }); println! ( \"Searching for {} in file {}\" , config . query , config . filename ); if let Err ( e ) = run ( config ) { println! ( \"Application error: {}\" , e ); process :: exit ( 1 ); } } Implement search() function and a unit test lib.rs main.rs use std :: fs ; use std :: error :: Error ; pub struct Config { pub query : String , pub filename : String , } impl Config { pub fn new ( mut args : std :: env :: Args ) -> Result < Config , &' static str > { args . next (); let query = match args . next () { Some ( arg ) => arg , None => return Err ( \"Didn't get a query string\" ), }; let filename = match args . next () { Some ( arg ) => arg , None => return Err ( \"Didn't get a file name\" ) }; Ok ( Config { query , filename }) } } pub fn run ( config : Config ) -> Result < (), Box < dyn Error >> { let contents = fs :: read_to_string ( config . filename ) ? ; println! ( \"{}\" , contents ); Ok (()) } pub fn search <' a > ( query : & str , contents : & ' a str ) -> Vec <&' a str > { contents . lines () . filter ( | line | line . contains ( query )) . collect () } #[cfg(test)] mod tests { use super :: * ; #[test] fn one_result () { let query = \"duct\" ; let contents = \" \\n Rust: \\n safe, fast, productive. \\n Pick three.\" ; assert_eq! ( vec! [ \"safe, fast, productive.\" ], search ( query , contents ) ) } } use std :: process ; use std :: env ; use lib :: run ; use lib :: Config ; pub mod lib ; fn main () { let args : Vec < String > = env :: args (). collect (); let config = Config :: new ( & args ). unwrap_or_else ( | err | { println! ( \"Problem parsing arguments: {}\" , err ); process :: exit ( 1 ); }); println! ( \"Searching for {} in file {}\" , config . query , config . filename ); if let Err ( e ) = run ( config ) { println! ( \"Application error: {}\" , e ); process :: exit ( 1 ); } } Implementing iterators","title":"grep ports"},{"location":"Rust/Tasks/Porting-grep/#grep-ports","text":"The success of ripgrep has inspired many ports of other GNU utilities to Rust. Porting grep to Rust provides the opportunity to explore various ways of using iterators , evolving from a naive for in loop to the filter() iterator method using a closure . Loop fn main () { let mut results = Vec :: new (); for line in contents . lines () { if line . contains ( query ) { results . push ( line ); } } results } filter fn main () { contents . lines () . filter ( | line | line . contains ( query )) . collect () }","title":"grep ports"},{"location":"Rust/Tasks/Porting-grep/#grep-lite","text":"This example begins with hard-coded strings and quickly develops the core logic of searching for a string. First regex (2), then a CLI framework is implemented (3). We take a slight digression to illustrate how file reading is done in Rust. First we produce a naive implementation (4) using a loop . A more expressive choice (5) is a for in loop iterating over the iterator returned by the BufReader's lines method. Finally, file opening is incorporated into the business logic of the application (6). The search logic itself is abstracted into a function. If the - argument is received from the command-line, STDIN is treated just as a file. Files are modeled with structs (7). read() models reading a file by cloning the File struct's data Vector, then reserve ing that clone's length and appending its values to a buffer struct. open and close functions are inert stubs (8). Refactoring read() into a method on File and implementing the new method using an impl blocks is more idiomatic and readable (9). open() , close() , and read() are refactored to use Results. Also a rand function is used to model error generation on a random basis (10). Define a FileState enum (11). Implementing the Display trait allows us to use a standard template in a println! statement (12). 1 2 3 4 5 6 7 8 9 10 11 12 // RIA 68 fn main () { let search_term = \"picture\" ; let quote = \"Once upon a midnight dreary \\n While I pondered weak and weary \\n Over many a quaint and curious volume of forgotten lore\" ; for line in quote . lines () { if line . contains ( search_term ) { println! ( \"{}\" , line ); } } } // RIA 69 use regex :: Regex ; fn main () { let re = Regex :: new ( \"picture\" ). unwrap (); let quote = \"Once upon a midnight dreary \\n While I pondered weak and weary \\n Over many a quaint and curious volume of forgotten lore\" ; for line in quote . lines () { let contains_substring = re . find ( line ); match contains_substring { Some ( _ ) => println ( \"{}\" , line ), None => (), } } } // RIA 71-72 use regex :: Regex ; use clap :: { App , Arg }; fn main () { let args = App :: new ( \"grep-lite\" ) . version ( \"0.1\" ) . about ( \"searches for patterns\" ) . arg ( Arg :: with_name ( \"pattern\" ) . help ( \"The pattern to search for\" ) . takes_value ( true ) . required ( true )) . get_matches (); let pattern = args . value_of ( \"pattern\" ). unwrap (); let re = Regex :: new ( \"picture\" ). unwrap (); let quote = \"Once upon a midnight dreary \\n While I pondered weak and weary \\n Over many a quaint and curious volume of forgotten lore\" ; for line in quote . lines () { let contains_substring = re . find ( line ); match contains_substring { Some ( _ ) => println ( \"{}\" , line ), None => (), } } } // RIA 73 use std :: fs :: File ; use std :: io :: BufReader ; use std :: io :: prelude :: * ; fn main () { let f = File :: open ( \"readme.md\" ). unwrap (); let mut reader = Bufreader :: new ( f ); let mut line = String :: new (); loop { let len = reader . read_line ( & mut line ) . unwrap (); if len == 0 { break } println! ( \"{} ({} bytes long)\" , line , len ); line . truncate ( 0 ); } } // RIA 73 use std :: fs :: File ; use std :: io :: BufReader ; use std :: io :: prelude :: * ; fn main () { let f = File :: open ( \"readme.md\" ). unwrap (); let reader = BufReader :: new ( f ); for line_ in reader . lines () { let line = line_ . unwrap (); println! ( \"{} ({} bytes long)\" , line , line . len ()); } } // RIA 74-75 use std :: fs :: File ; use std :: io ; use std :: io :: BufReader ; use std :: io :: prelude :: * ; use regex :: Regex ; use clap :: { App , Arg }; fn process_lines < T : BufRead + Sized > ( reader : T , re : Regex ) { for line_ in reader . lines () { let line = line_ . unwrap (); match re . find ( & line ) { Some ( _ ) => println! ( \"{}\" , line ), None => (),) } } } fn main () { let args = App :: new ( \"grep-lite\" ) . version ( \"0.1\" ) . about ( \"searches for patterns\" ) . arg ( Arg :: with_name ( \"pattern\" ) . help ( \"The pattern to search for\" ) . takes_value ( true ) . required ( true )) . arg ( Arg :: with_name ( \"input\" ) . help ( \"File to search\" ) . takes_value ( true ) . required ( false )) . get_matches (); let pattern = args . value_of ( \"pattern\" ). unwrap (); let re = Regex :: new ( \"picture\" ). unwrap (); let input = args . value_of ( \"input\" ). unwrap_or ( \"-\" ); if input == \"-\" { let stdin = io :: stdin (); let reader = stdin . lock (); process_lines ( reader , re ); } else { let f = File :: open ( input ). unwrap (); let reader = BufReader :: new ( f ); process_lines ( reader , re ); } } // RIA 80 #[derive(Debug)] struct File { name : String , data : Vec < u8 > , } fn main () { let f1 = File { name : String :: from ( \"f1.txt\" ), data : Vec : new (), }; let f1_name = & f1 . name ; let f1_length = & f1 . data . len (); println! ( \"{:?}\" , f1 ); println! ( \"{} is {} byes long\" , f1_name , f1_length ); } // RIA 82-83 #![allow(unused_variables)] #[derive(Debug)] struct File { name : String , data : Vec < u8 > , } fn read ( f : & File , save_to : & mut Vec < u8 > , ) -> usize { let mut tmp = f . data . clone (); let read_length = tmp . len (); save_to . reserve ( read_length ); save_to . append ( & mut tmp ); read_length } fn open ( f : & mut File ) -> bool { true } fn close ( f : & mut File ) -> bool { true } fn main () { let mut f = File { name : String :: from ( \"2.txt\" ), data : vec ! [ 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 , 29 ], }; let mut buffer : Vec < u8 > = vec! []; open ( & mut f ); let f_length = read ( & f2 , & mut buffer ); close ( & mut f ); let text = String :: from_utf8_lossy ( & buffer ); println! ( \"{:?}\" , f ); println! ( \"{} is {} bytes long\" , & f . name , f_length ); println! ( \"{}\" , text ); } // RIA 86 #![allow(unused_variables)] #[derive(Debug)] struct File { name : String , data : Vec < u8 > , } impl File { fn new ( name : & str ) -> File { File { name : String :: from ( name ), data : Vec :: new (), } } fn new_with_data ( name : & str , data : & Vec < u8 > , ) -> File { let mut f = File :: new ( name ); f . data = data . clone (); f } fn read ( self : & File , save_to : & mut Vec < u8 > , ) -> usize { let mut tmp = self . data . clone (); let read_length = tmp . len (); save_to . reserve ( read_length ); save_to . append ( & mut tmp ); read_length } } fn open ( f : & mut File ) -> bool { true } fn close ( f : & mut File ) -> bool { true } fn main () { let f_data : Vec < u8 > = vec! [ 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 , 29 ]; let mut f = File :: new_with_data ( \"2.txt\" , & f_data ); let mut buffer : Vec < u8 > = vec! []; open ( & mut f ); let f_length = f . read ( & mut buffer ); close ( & mut f ); let text = String :: from_utf8_lossy ( & buffer ); println! ( \"{:?}\" , f ); println! ( \"{} is {} bytes long\" , & f . name , f_length ); println! ( \"{}\" , text ); } // RIA 93-94 use rand :: prelude :: * ; fn one_in ( denominator : u32 ) -> bool { thread_rng (). gen_ratio ( 1 , denominator ) } #[derive(Debug)] struct File { name : String , data : Vec < u8 > , } impl File { fn new ( name : & str ) -> File { File { name : String :: from ( name ), data : Vec :: new (), } } fn new_with_data ( name : & str , data : Vec < u8 > ) -> File { let mut f = File :: new ( name ); f . data = data . clone (); f } fn read ( self : & File , save_to : & mut Vec < u8 > , ) -> Result < usize , String > { let mut tmp = self . data . clone (); let read_length = tmp . len (); save_to . reserve ( read_length ); save_to . append ( & mut tmp ); Ok ( read_length ) } } fn open ( f : File ) -> Result < File , String > { if one_in ( 10_000 ) { let err_mag = String :: from ( \"Permission denied\" ); return Err ( err_msg ); } Ok ( f ) } fn close ( f : File ) -> Result < File , String > { if one_in ( 100_000 ) { let err_mag = String :: from ( \"Interrupted by signal!\" ); return Err ( err_msg ); } Ok ( f ) } fn main () { let f_data : Vec < u8 > = vec! [ 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 , 29 ]; let mut f = File :: new_with_data ( \"4.txt\" , & f_data ); let mut buffer : Vec < u8 > = vec! []; f = open ( f ). unwrap (); let f_length = f . read ( & mut buffer ). unwrap (); f = close ( f ). unwrap (); let text = String :: from_utf8_lossy ( & buffer ); println! ( \"{:?}\" , f ); println! ( \"{} is {} bytes long\" , & f . name , f_length ); println! ( \"{}\" , text ); } // RIA 97-98 #[derive(Debug,PartialEq)] enum FileState { Open , Closed , } #[derive(Debug)] struct File { name : String , data : Vec < u8 > , state : FileState , } impl File { fn new ( name : & str ) -> File { File { name : String :: from ( name ), data : Vec :: new (), state : FileState :: Closed , } } fn read ( self : & File , save_to : & mut Vec < u8 > , ) -> Result < usize , String > { let mut tmp = self . data . clone (); let read_length = tmp . len (); save_to . reserve ( read_length ); save_to . append ( & mut tmp ); Ok ( read_length ) } } fn open ( mut f : File ) -> Result < File , String > { f . state = FileState :: Open ; Ok ( f ) } fn close ( mut f : File ) -> Result < File , String > { f . state = FileState :: Closed ; Ok ( f ) } fn main () { let f_data : Vec < u8 > = vec! [ 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 , 29 ]; let mut f = File :: new ( \"4.txt\" ); let mut buffer : Vec < u8 > = vec! []; f = open ( f ). unwrap (); let f_length = f . read ( & mut buffer ). unwrap (); f = close ( f ). unwrap (); let text = String :: from_utf8_lossy ( & buffer ); println! ( \"{:?}\" , f ); println! ( \"{} is {} bytes long\" , & f . name , f_length ); println! ( \"{}\" , text ); } // RIA 101-102 #![allow(dead_code)] use std :: fmt ; use std :: fmt :: { Display }; #[derive(Debug,PartialEq)] enum FileState { Open , Closed , } impl Display for FileState { fn fmt ( & self , f : & mut fmt :: Formatter ) -> fmt :: Result { match * self { FileState :: Open => write! ( f , \"OPEN\" ), FileState :: Closed => write! ( f , \"CLOSED\" ), } } } impl Display for File { fn fmt ( & self , f : & mut fmt :: Formatter ) -> fmt :: Result { write! ( f , \"<{} ({})>\" , self . name , self . state ) } } // --snip-- fn main () { println! ( \"{:?}\" , f ); println! ( \"{}\" , f ); }","title":"grep-lite"},{"location":"Rust/Tasks/Porting-grep/#minigrep","text":"From RPL p. 233-263 1 2 3 4 5 6 7 8 9 10 11 12 13 main.rs fn main () { let args : Vec < String > = std :: env :: args (). collect (); println! ( \"{:?}\" , args ); } Read any command line arguments passed, collecting into a vector. main.rs fn main () { let args : Vec < String > = std :: env :: args (). collect (); let query = & args [ 1 ]; let filename = & args [ 2 ]; println! ( \"Searching for {}\" , query ); println! ( \"In file {}\" , filename ); } main.rs fn main () { let args : Vec < String > = std :: env :: args (). collect (); let query = & args [ 1 ]; let filename = & args [ 2 ]; println! ( \"Searching for {}\" , query ); println! ( \"In file {}\" , filename ); let contents = fs :: read_to_string ( filename ) . expect ( \"Somethign went wrong reading the file\" ); println! ( \"With text: \\n {}\" , contents ); } fs::read_to_string() takes the filename, opens it, and returns a Result<String> of the file's contents. main.rs fn main () { let args : Vec < String > = std :: env :: args (). collect (); let config = parse_config ( & args ); println! ( \"Searching for {} in file {}\" , config . query , config . filename ); let contents = std :: fs :: read_to_string ( config . filename ). expect ( \"Couldn't read file\" ); println! ( \"{}\" , contents ); } struct Config { query : String , filename : String , } fn parse_config ( args : & [ String ]) -> Config { let query = args [ 1 ]. clone (); let filename = args [ 2 ]. clone (); Config { query , filename } } Refactoring to abstract command-line argument parsing logic to its own function main.rs fn main () { let args : Vec < String > = std :: env :: args (). collect (); let config = Config :: new ( & args ); println! ( \"Searching for {} in file {}\" , config . query , config . filename ); let contents = std :: fs :: read_to_string ( config . filename ). expect ( \"Couldn't read file\" ); println! ( \"{}\" , contents ); } struct Config { query : String , filename : String , } impl Config { fn new ( args : & [ String ]) -> Config { let query = args [ 1 ]. clone (); let filename = args [ 2 ]. clone (); Config { query , filename } } } Refactor the config parser into Config's constructor main.rs fn main () { let args : Vec < String > = std :: env :: args (). collect (); let config = Config :: new ( & args ); println! ( \"Searching for {} in file {}\" , config . query , config . filename ); let contents = std :: fs :: read_to_string ( config . filename ). expect ( \"Couldn't read file\" ); println! ( \"{}\" , contents ); } struct Config { query : String , filename : String , } impl Config { fn new ( args : & [ String ]) -> Config { if args . len () < 3 { panic! ( \"Not enough args!\" ); } let query = args [ 1 ]. clone (); let filename = args [ 2 ]. clone (); Config { query , filename } } } Implement error message on invalid number of arguments main.rs fn main () { let args : Vec < String > = std :: env :: args (). collect (); let config = Config :: new ( & args ). unwrap_or_else ( | err | { println! ( \"Problem parsing arguments: {}\" , err ); std :: process :: exit ( 1 ); }); println! ( \"Searching for {} in file {}\" , config . query , config . filename ); let contents = std :: fs :: read_to_string ( config . filename ). expect ( \"Couldn't read file\" ); println! ( \"{}\" , contents ); } struct Config { query : String , filename : String , } impl Config { fn new ( args : & [ String ]) -> Result < Config , &' static str > { if args . len () < 3 { return Err ( \"not enough arguments\" ); } let query = args [ 1 ]. clone (); let filename = args [ 2 ]. clone (); Ok ( Config { query , filename }) } } Remove previous error message and incorporate error in the Err variant of a Result<T,E> . The argument passed to unwrap_or_else() is a closure . main.rs fn main () { let args : Vec < String > = std :: env :: args (). collect (); let config = Config :: new ( & args ). unwrap_or_else ( | err | { println! ( \"Problem parsing arguments: {}\" , err ); std :: process :: exit ( 1 ); }); println! ( \"Searching for {} in file {}\" , config . query , config . filename ); run ( config ); } fn run ( config : Config ) { let contents = std :: fs :: read_to_string ( config . filename ) . expect ( \"Couldn't read file\" ); println! ( \"{}\" , contents ); } struct Config { query : String , filename : String , } impl Config { fn new ( args : & [ String ]) -> Result < Config , &' static str > { if args . len () < 3 { return Err ( \"not enough arguments\" ); } let query = args [ 1 ]. clone (); let filename = args [ 2 ]. clone (); Ok ( Config { query , filename }) } } Abstract program logic into run() main.rs fn main () { let args : Vec < String > = std :: env :: args (). collect (); let config = Config :: new ( & args ). unwrap_or_else ( | err | { println! ( \"Problem parsing arguments: {}\" , err ); std :: process :: exit ( 1 ); }); println! ( \"Searching for {} in file {}\" , config . query , config . filename ); run ( config ). unwrap (); } fn run ( config : Config ) -> Result < (), Box < dyn std :: error :: Error >> { let contents = std :: fs :: read_to_string ( config . filename ) ? ; println! ( \"{}\" , contents ); Ok (()) } struct Config { query : String , filename : String , } impl Config { fn new ( args : & [ String ]) -> Result < Config , &' static str > { if args . len () < 3 { return Err ( \"not enough arguments\" ); } let query = args [ 1 ]. clone (); let filename = args [ 2 ]. clone (); Ok ( Config { query , filename }) } } Refactor run() to return a Result<T,E> in the Ok case and the trait object Box<dyn Error> for the error type. This allows various error types to be returned. Also, the expect() method is replaced by the ? operator which returns the error type for the calling function to handle, rather than a boilerplate error message. main.rs fn main () { let args : Vec < String > = std :: env :: args (). collect (); let config = Config :: new ( & args ). unwrap_or_else ( | err | { println! ( \"Problem parsing arguments: {}\" , err ); std :: process :: exit ( 1 ); }); println! ( \"Searching for {} in file {}\" , config . query , config . filename ); if let Err ( e ) = run ( config ) { println! ( \"Application error: {}\" , e ); std :: process :: exit ( 1 ); } } fn run ( config : Config ) -> Result < (), Box < dyn std :: error :: Error >> { let contents = std :: fs :: read_to_string ( config . filename ) ? ; println! ( \"{}\" , contents ); Ok (()) } struct Config { query : String , filename : String , } impl Config { fn new ( args : & [ String ]) -> Result < Config , &' static str > { if args . len () < 3 { return Err ( \"not enough arguments\" ); } let query = args [ 1 ]. clone (); let filename = args [ 2 ]. clone (); Ok ( Config { query , filename }) } } Rather than unwrap() , we use if let to check for and handle errors from run() . lib.rs main.rs use std :: fs ; use std :: error :: Error ; pub struct Config { pub query : String , pub filename : String , } impl Config { pub fn new ( args : & [ String ]) -> Result < Config , &' static str > { if args . len () < 3 { return Err ( \"not enough arguments\" ); } let query = args [ 1 ]. clone (); let filename = args [ 2 ]. clone (); Ok ( Config { query , filename }) } } pub fn run ( config : Config ) -> Result < (), Box < dyn Error >> { let contents = fs :: read_to_string ( config . filename ) ? ; println! ( \"{}\" , contents ); Ok (()) } use std :: process ; use std :: env ; use lib :: run ; use lib :: Config ; pub mod lib ; fn main () { let args : Vec < String > = env :: args (). collect (); let config = Config :: new ( & args ). unwrap_or_else ( | err | { println! ( \"Problem parsing arguments: {}\" , err ); process :: exit ( 1 ); }); println! ( \"Searching for {} in file {}\" , config . query , config . filename ); if let Err ( e ) = run ( config ) { println! ( \"Application error: {}\" , e ); process :: exit ( 1 ); } } Abstract all elements except main() into a library module lib.rs main.rs use std :: fs ; use std :: error :: Error ; pub struct Config { pub query : String , pub filename : String , } impl Config { pub fn new ( args : & [ String ]) -> Result < Config , &' static str > { if args . len () < 3 { return Err ( \"not enough arguments\" ); } let query = args [ 1 ]. clone (); let filename = args [ 2 ]. clone (); Ok ( Config { query , filename }) } } pub fn run ( config : Config ) -> Result < (), Box < dyn Error >> { let contents = fs :: read_to_string ( config . filename ) ? ; println! ( \"{}\" , contents ); Ok (()) } pub fn search <' a > ( query : & str , contents : & ' a str ) -> Vec <&' a str > { let mut results = Vec :: new (); for line in contents . lines () { if line . contains ( query ) { results . push ( line ); } } results } #[cfg(test)] mod tests { use super :: * ; #[test] fn one_result () { let query = \"duct\" ; let contents = \" \\n Rust: \\n safe, fast, productive. \\n Pick three.\" ; assert_eq! ( vec! [ \"safe, fast, productive.\" ], search ( query , contents ) ) } } use std :: process ; use std :: env ; use lib :: run ; use lib :: Config ; pub mod lib ; fn main () { let args : Vec < String > = env :: args (). collect (); let config = Config :: new ( & args ). unwrap_or_else ( | err | { println! ( \"Problem parsing arguments: {}\" , err ); process :: exit ( 1 ); }); println! ( \"Searching for {} in file {}\" , config . query , config . filename ); if let Err ( e ) = run ( config ) { println! ( \"Application error: {}\" , e ); process :: exit ( 1 ); } } Implement search() function and a unit test lib.rs main.rs use std :: fs ; use std :: error :: Error ; pub struct Config { pub query : String , pub filename : String , } impl Config { pub fn new ( mut args : std :: env :: Args ) -> Result < Config , &' static str > { args . next (); let query = match args . next () { Some ( arg ) => arg , None => return Err ( \"Didn't get a query string\" ), }; let filename = match args . next () { Some ( arg ) => arg , None => return Err ( \"Didn't get a file name\" ) }; Ok ( Config { query , filename }) } } pub fn run ( config : Config ) -> Result < (), Box < dyn Error >> { let contents = fs :: read_to_string ( config . filename ) ? ; println! ( \"{}\" , contents ); Ok (()) } pub fn search <' a > ( query : & str , contents : & ' a str ) -> Vec <&' a str > { contents . lines () . filter ( | line | line . contains ( query )) . collect () } #[cfg(test)] mod tests { use super :: * ; #[test] fn one_result () { let query = \"duct\" ; let contents = \" \\n Rust: \\n safe, fast, productive. \\n Pick three.\" ; assert_eq! ( vec! [ \"safe, fast, productive.\" ], search ( query , contents ) ) } } use std :: process ; use std :: env ; use lib :: run ; use lib :: Config ; pub mod lib ; fn main () { let args : Vec < String > = env :: args (). collect (); let config = Config :: new ( & args ). unwrap_or_else ( | err | { println! ( \"Problem parsing arguments: {}\" , err ); process :: exit ( 1 ); }); println! ( \"Searching for {} in file {}\" , config . query , config . filename ); if let Err ( e ) = run ( config ) { println! ( \"Application error: {}\" , e ); process :: exit ( 1 ); } } Implementing iterators","title":"minigrep"},{"location":"Rust/Tasks/To-Do/","text":"To-Do Vector assembly use std :: rc :: Rc ; use std :: cell :: RefCell ; fn main () { let mut v : Vec < String > = Vec :: new (); loop { let mut input : String = String :: new (); println! ( \"Enter to-do list item ('q' to quit): \" ); std :: io :: stdin (). read_line ( & mut input ). unwrap (); match input . trim () { \"q\" => break , s => v . push ( s . to_string ()), } } for i in v { println! ( \"{}\" , i ); } }","title":"To-Do"},{"location":"Rust/Tasks/To-Do/#to-do","text":"","title":"To-Do"},{"location":"Rust/Tasks/To-Do/#vector-assembly","text":"use std :: rc :: Rc ; use std :: cell :: RefCell ; fn main () { let mut v : Vec < String > = Vec :: new (); loop { let mut input : String = String :: new (); println! ( \"Enter to-do list item ('q' to quit): \" ); std :: io :: stdin (). read_line ( & mut input ). unwrap (); match input . trim () { \"q\" => break , s => v . push ( s . to_string ()), } } for i in v { println! ( \"{}\" , i ); } }","title":"Vector assembly"},{"location":"Rust/Tasks/Transactions/","text":"Transactions Simple","title":"Transactions"},{"location":"Rust/Tasks/Transactions/#transactions","text":"","title":"Transactions"},{"location":"Rust/Tasks/Transactions/#simple","text":"","title":"Simple"},{"location":"Scripts/","text":"Video scripts Discord.py master class Anki Audio C# Django Articles Discord bot article","title":"Video scripts"},{"location":"Scripts/#video-scripts","text":"Discord.py master class Anki Audio C# Django","title":"Video scripts"},{"location":"Scripts/#articles","text":"Discord bot article","title":"Articles"},{"location":"Scripts/Anki/","text":"Tips for cramming: using cloze with Unbury Using CSS classes to simulate prompts and to visually identify CSS classes with environments, like Command Prompt or Ubuntu Use colors to set out various files in a project, i.e. Django","title":"Anki"},{"location":"Scripts/CSharp/","text":"C# solution file is analogous (?) to a manage.py file in a Django application Django : What\u2019s the difference between a project and an app? An app is a Web application that does something \u2013 e.g., a Weblog system, a database of public records or a small poll app. A project is a collection of configuration and apps for a particular website. A project can contain multiple apps. An app can be in multiple projects.","title":"CSharp"},{"location":"Scripts/Django/","text":"Django Explaining concepts F10 Django is a web application framework written in Python. Today I'm going to show you how to code a simple To-Do web application. If you've found this video, that probably means you've already seen some of the many other popular YouTube tutorials that you how to do it. What I'll show you is how to integrate it with the Bulma CSS framework! Todo Setup I'm going to assume you have a working installation of Python. Windows => Microsoft Store. If you're running Mac or Linux, you might already have a working Python installation, but if not there are many different ways of installing Python. Before we install Django we're going to set up a virtual environment. Venv Python only tolerates a single installed version of a package at any given point in time. This means that as you add packages to your installation, it's only a matter of time before you run into a conflict, where a package demands a version of a dependency that conflicts with the one you have installed. The most common way to avoid this is by creating a virtual environment. A virtual environment is like a isolated copy of Python that is only activated when you need. Virtual environments let you install packages safely, keeping your system installation clutter-free. I have PowerShell open, and if you have a Windows computer you should have it too. I'm going to cd to the venv directory. This is the directory where I like to install all my virtual environments. But you're free to manage virtual environments however you choose. I run ls here and you can see I've already set up multiple virtual environments. Let's create a new virtual environment by running python -m venv django . After a few moments, the virtual environment will be created. Now let's run ls again, and we can see that this directory was created. Let's see what a virtual environment looks like. There are a few directories and a config file. If we inspect the config file notepad pyvenv.cfg , we see that it specifies the currently installed version of Python. It also points to the system installation of Python. If I run Get-Command python I can see that the system installation of Python points to this directory. Depending on how you installed Python, the directory that appears here may be different. And if you're watching this video in the future, your version of Python will be newer than this. The business end of the virtual environment is actually here, in Scripts . Let's navigate there and run gci again. Here we see activation scripts and some executable files. One of these is python.exe, which is the Python interpreter. We also have pip.exe. At the moment, this is just a directory - if we type python.exe we will be running the system installation of Python. I'll prove it. gcm python | select source which python In order to tell the system we want to use the virtual environment, we're going to run this activation script, Activate.ps1. Now we are in the virtual environment. We can confirm this by running gcm python.exe , and we can see that running Python now will run the interpreter that we just installed. What happens when we want to run pip ? We can see that pip, too, now points to the version we just installed in the virtual environment. If we install a package now, it will be installed only to the virtual environment, and not the system installation. Let's install Django. pip install django In a few moments the installation will complete. We can confirm the installation completed by running pip list . Here is Django, version 3.xx. And if we run gci again, we can see that some new executable files have been installed to this directory. But how can we be sure that Django wasn't installed to the system installation? We can exit the virtual environment by running deactivate . Now we have returned to the system installation of Python. We can confirm this again by running gcm . And now when we check the list of installed packages using pip list , we see that Django is not present. We successfully installed Django to the virtual environment alone, leaving our system installation untouched. This is the recommended way of installing Python packages. Project skeleton Before we leave the virtual environment and start coding, let's take another look at the Scripts directory. One of the executables that has been installed is django-admin.exe . This is Django\u2019s command-line utility for administrative tasks. Other languages have similar utilities that are used to facilitate common administrative tasks, for example building out the skeleton of a new project, which you would do like this: django-admin startproject project This is all I'm going to use django-admin for in this course, but there is a lot you can do with it. Once the project has been created, let's go into the directory and peek around. As we can see, a Django project has a manage.py script and contains a nested directory with the same name as that of the project. The manage.py is actually what you will use to do a lot of administration for your web application. For example, we can run python manage.py runserver to start the web application. If we open a web browser to the address and port used by the server, we can see Django's default landing page. Let's close the server by pressing Ctrl C Let's run another command python manage.py startapp application If we get the directory contents again, we see that, similar to the django-admin command we ran earlier, this command erects a directory of content. Let's take a look and see what it created. More Python scripts.. but what are these files for? Off the bat, we notice this directory has a urls.py . Is this similar to the urls.py created in the project directory? Project structure Django web apps are structured into projects and applications . It's important to understand the distinction, because it is not immediately obvious to a novice. project describes a Django web application The project's root directory contains manage.py . The root directory contains a subdirectory with the same name as the root, but which contains other Python scripts, like urls.py and especially settings.py . This is called the project directory . An application refers to a Python package that provides some set of features. Applications are a component of projects. Application directories also contain a urls.py, but that's where the similarity ends. For a person who's new to Django, this means that much of the core functionality of your project will actually be contained in an app , which can feel awkward at first. But bear with it for now. settings.py Before we move any further, let's open the project folder and go to settings.py . If we scroll down to INSTALLED_APPS we need to make sure we add the name of the application that we just created in quotes to the list. This list is what Django uses to construct its namespace. Let's also take a look at this token value ( SECRET_KEY ) that is provided in plaintext. Because I'm guessing you're just learning Django and not about to deploy your web application to production, doing this is okay. But for production systems, this is a no-no. I'm going to show you an easy and simple it is to protect your secrest. But if you don't want to spend time on this section, feel free to use the time-codes to skip ahead. Model In order to determine the shape of a web app's data, Django uses what is called a Model . If you're familiar with object-oriented programming, this will be the easiest part. Let's go into the application's models.py and create a Model. Models form the basis of how Django organizes data. models.py from django.db import models class Task ( models . Model ): title = models . CharField ( max_length = 50 ) # done = models.BooleanField() # created = models.DateTimeField(auto_now_add=True) We're subclassing models.Model , and the model defines a series of named fields which are instances of each of these classes. Some of these fields require specific attributes to be defined. For example, CharField requires a max_length attribute to be defined. Defining a model is not permanent, so let's stick with just the one title field. We'll add to it in a moment. Let's move on to the topic of migrations. When you make even the slightest to the models used in your web application's, Django requires you to perform what's called a migration . A migration keeps the web application's database consistent with the model. This is a two-step process from the command-line. python manage.py makemigrations This command analyzes the Model classes you've defined for the web application, and then it actually codes Python scripts for you to change the database accordingly. You'll find them in the migrations directory, see? To run these scripts, run python manage.py migrate Because this is the first time I ran a migration, it created a database for me from scratch, here. We can see that it's a SQLite database, a lightweight option that is used a lot in Python development. In fact, the Python Standard Library includes a SQLite module, so every Python installation is able to create and manipulate SQLite databases. How did Django know to produce an SQLite database, and not another? Let me show you. In the settings.py there is a dictionary named DATABASES DATABASES = { 'default' : { 'ENGINE' : 'django.db.backends.sqlite3' , 'NAME' : BASE_DIR / 'db.sqlite3' , } } \"ENGINE\" specifies the driver to use with the database. Django supports four databases out of the box: PostgreSQL, MySQL, Oracle, and SQLite. The other 3 options are more involved and require a server to be set up, so if you ever want to use them, you'll have to define values for HOST , PORT , USER , and PASSWORD in this dictionary. But SQLite keeps the database in a local file - no need to mess with any of that. We don't have time for an in-depth discussion of SQLite, but what I will do is show you how we can quickly take a look at this database within VS Code. Open the Extensions Marketplace and search for sqlite. The top app, SQLite Explorer by \"alex\", is what we're looking for. Click the green install button. I already have this extension installed, so I'm going to return to the File Explorer. Now I right click on db.sqlite3 and select open database. Another section named SQlite explorer should open in the file explorer. If you installed SQLite Explorer but don't see this section, check the hamburger menu in the top right and make sure it's enabled. SQLite Explorer allows you to view the schema and content of SQLite tables. Let's expand it. Each of these is a table -- the file is the database. All of these tables comes from an application in our Django project, and all but one of them were automatically implemented. The one we created is here, at the top - application_task . When we expand the table, we see Django added an integer id field, as well as the field we defined. \" varchar \" is SQL's equivalent of a string, more or less. Let's go back to the code and make another change to the Model. We're going to add a value so we can mark completed tasks off our to-do list. # models.py from django.db import models class Task ( models . Model ): title = models . CharField ( max_length = 50 ) done = models . BooleanField ( default = False ) # created = models.DateTimeField(auto_now_add=True) BooleanField doesn't require a max_length , but it does require a default value. We changed the model, so the database will have to be updated, which means we have to run another migration. Again we run python manage.py makemigrations , and we see a new migration script has been generated. Let's run it using python manage.py migrate . Once it's complete let's return to the SQLite Explorer. Now we see that our change has taken effect in the schema of the task. Let's go back and make one final addition. # models.py from django.db import models class Task ( models . Model ): title = models . CharField ( max_length = 50 ) done = models . BooleanField () created = models . DateTimeField ( auto_now_add = True ) # def __str__(self): # return self.title As you might guess, DateTimeField stores date and time information. The auto_now_add argument means that the web application will automatically associate the current date and time when a record is created. Let's make migrations and migrate one last time. Adding data Now the foundation of our web app - the data model - is complete. Too bad we have to wait until we code the rest of the web app to know what it looks like... wouldn't it be great if we could test out some data right away? Well, we can. Actually, there are several ways. The most Pythonic way is by running the Python interpreter. In the integrated terminal, when we run python manage.py shell we are greeted by a Python REPL. Let's import the model and instantiate a new task from app.models import Task task = Task ( title = \"Shop for milk\" ) Now we can run the save method on this task object: task.save() . And that will save the task created into the database. Another way is by running the command-line client for the database itself. By running python manage.py dbshell , we're taken into the SQLite client. We can run .tables , and we'll see all the available tables in the database. The output of this command should correspond exactly with the what we see in the SQLite explorer by opening the database. A deeper discussion of SQL syntax is beyond the scope of this video, but it wouldn't hurt to learn a couple of tricks. One SQL command that everyone know is select * from . We can run select * from app_task; and we can see that the task we added is in the database, and will be served by the web server. But a more typical way to add data during development is to do it through the admin portal . Let me show you what that is. Let's run .exit to leave the SQLite client. Now let's run the server and open a web browser to \"localhost/admin\". Every Django server has an admin application at this URL, by default. But it wants a login. How do we login to our own server? Let's stop the server and create a user so that we can login. python manage.py createsuperuser I'm going to create a superuser named Jasper. The prompt asks for an email address, but you can leave it blank. I enter a password, then confirm that password. And because Django can detect that it's not as complex as it should be, it asks me if I'm sure I want to set that as the password. I confirm. Now we can use those credentials to login to the admin site. Let's stop the webserver one last time and go into the app/admin.py file. Here I'm going to register the model we just created. from .models import Task admin . website . register ( Task ) This will make that model appear in the admin app, allowing us to manipulate the data through the web browser. Now we run the web server, refresh the admin site in the web browser, and we can see that the Task model appears in the admin dashboard. If I click on it, I can see the entry we created from the command-line. Let's create a new task here. And now our database has some data in it. Let's flesh out our web application some more. View Forms The ModelForm class can be used to quickly create a form from an existing model. from django.forms import ModelForm class TaskForm ( ModelForm ): class Meta : model = Task fields = '__all__' Alternatively, this class can be defined by instantiating the modelform_factory class generator. This can be placed within the views.py outside the view function for brevity, but it's typical to place this declaration within forms.py . # forms.py from django.forms.models import modelform_factory from .models import Task from django.forms.widgets import Input TaskForm = modelform_factory ( Task , exclude = []) In either case, this subclass is instantiated in views.py and passed in with the context object views.py from .forms import TaskForm from .models import Task def index ( request ): tasks = Task . objects . all () form = TaskForm () context = { 'form' : form } return render ( request , 'website/list.html' , context ) Let's take a look at the tasks variable. This is called a QuerySet . The key of the context object where the form was placed can then simply be used in a template tag: < form method = \"POST\" > {{ form }} </ form > Without the method=\"POST\" attribute set in the tag, the form will not send a POST request. A single field of the form can be specified as well: < form method = \"POST\" > {{ form . title }} </ form > However, on the same page, the csrf_token must also be provided { % csrf_token % } If the token appears in the text of the page, that is because you have used the template tag mustaches # Wrong! {{ csrf_token }} Django starships gallery Now that we've learned the basics of Django, let's make another web app that will do something more interesting. So we'll return to PowerShell and create a new directory for a new project. We're still working with the same virtual environment, so no need to create a new one. django-admin startproject starships python manage.py startapp app ... Bulma is a popular web design framework. You've probably heard of Bootstrap, which reigns supreme in this space, but Bulma is often brought up as an alternative because it is CSS only, whereas Bootstrap does incorporate some JavaScript. Because Bulma is CSS only, incorporating it involves basically downloading the CSS file, which you can do from the website . Where do we put it? Well, CSS files are typically considered static content, so we make a new folder in our app directory named \"static\" and place it there. Now we create a base template | {% load static %} <! DOCTYPE html> html ( lang= \"en\" ) head meta ( charset= \"UTF-8\" ) meta ( name= \"viewport\" , content= \"width=device-width, initial-scale=1.0\" ) link ( rel= \"stylesheet\" , href= \"{% static 'bulma.css'%}\" ) title \ud83d\ude80\ud83d\udc0d Django Starships style //- | .is-ancestor { flex-wrap: wrap; } body section .hero.is-primary .hero-body .container h1 .title Starships section .section .container {% block content %}{% endblock content%} The content page, including | {% extends 'dist/base.html' %} | {% load static %} | {% block content %} .tile.is-ancestor {% for ship in ships %} .tile.is-parent.is-6 article .tile.is-child.notification.is-black h1 .title {{ship.Name}} h2 .subtitle {{ship.Registry}} img ( src= \"{{ ship.Image.url }}\" ) | {% endfor %} | {% endblock content %} Basic filter table Using list.js body .listy input .search ( type= \"text\" ) ul .list li p .name USS Enterprise p .reg NCC-1701 var options = { valueNames : [ 'name' , 'reg' ] }; var userList = new List ( 'listy' , options ); Bulma filter table body section .section .table-container #list input .input.search ( placeholder= \"Search\" ) button .sort ( data-sort= \"name\" ) Sort table .table.is-bordered.is-striped.is-hoverable thead tr th Name th Registry th Crew th Class tbody .list tr td .name USS Enterprise td .reg NCC-1701 td .crew 204 td .cls Constitution var options = { valueNames : [ 'name' , 'reg' , 'cls' ] }; var userList = new List ( 'list' , options ); Bulma filter tile gallery body section .section .container #foo input .search.input ( type= \"text\" ) .tile.is-ancestor.list .tile.is-parent.is-3 article .tile.is-child.notification.is-primary h1 .name.subtitle USS Enterprise p .reg NCC-1701 p .cls Constitution .tile.is-parent.is-3 article .tile.is-child.notification.is-primary h1 .name.subtitle USS Constitution p .reg NCC-1700 p .cls Constitution var options = { valueNames : [ 'name' , 'reg' , 'cls' ] }; var userList = new List ( 'foo' , options ); Bulma tiles do not wrap by default . is-ancestor { flex-wrap : wrap ; }","title":"Django"},{"location":"Scripts/Django/#django","text":"Explaining concepts F10 Django is a web application framework written in Python. Today I'm going to show you how to code a simple To-Do web application. If you've found this video, that probably means you've already seen some of the many other popular YouTube tutorials that you how to do it. What I'll show you is how to integrate it with the Bulma CSS framework!","title":"Django"},{"location":"Scripts/Django/#todo","text":"","title":"Todo"},{"location":"Scripts/Django/#setup","text":"I'm going to assume you have a working installation of Python. Windows => Microsoft Store. If you're running Mac or Linux, you might already have a working Python installation, but if not there are many different ways of installing Python. Before we install Django we're going to set up a virtual environment.","title":"Setup"},{"location":"Scripts/Django/#venv","text":"Python only tolerates a single installed version of a package at any given point in time. This means that as you add packages to your installation, it's only a matter of time before you run into a conflict, where a package demands a version of a dependency that conflicts with the one you have installed. The most common way to avoid this is by creating a virtual environment. A virtual environment is like a isolated copy of Python that is only activated when you need. Virtual environments let you install packages safely, keeping your system installation clutter-free. I have PowerShell open, and if you have a Windows computer you should have it too. I'm going to cd to the venv directory. This is the directory where I like to install all my virtual environments. But you're free to manage virtual environments however you choose. I run ls here and you can see I've already set up multiple virtual environments. Let's create a new virtual environment by running python -m venv django . After a few moments, the virtual environment will be created. Now let's run ls again, and we can see that this directory was created. Let's see what a virtual environment looks like. There are a few directories and a config file. If we inspect the config file notepad pyvenv.cfg , we see that it specifies the currently installed version of Python. It also points to the system installation of Python. If I run Get-Command python I can see that the system installation of Python points to this directory. Depending on how you installed Python, the directory that appears here may be different. And if you're watching this video in the future, your version of Python will be newer than this. The business end of the virtual environment is actually here, in Scripts . Let's navigate there and run gci again. Here we see activation scripts and some executable files. One of these is python.exe, which is the Python interpreter. We also have pip.exe. At the moment, this is just a directory - if we type python.exe we will be running the system installation of Python. I'll prove it. gcm python | select source which python In order to tell the system we want to use the virtual environment, we're going to run this activation script, Activate.ps1. Now we are in the virtual environment. We can confirm this by running gcm python.exe , and we can see that running Python now will run the interpreter that we just installed. What happens when we want to run pip ? We can see that pip, too, now points to the version we just installed in the virtual environment. If we install a package now, it will be installed only to the virtual environment, and not the system installation. Let's install Django. pip install django In a few moments the installation will complete. We can confirm the installation completed by running pip list . Here is Django, version 3.xx. And if we run gci again, we can see that some new executable files have been installed to this directory. But how can we be sure that Django wasn't installed to the system installation? We can exit the virtual environment by running deactivate . Now we have returned to the system installation of Python. We can confirm this again by running gcm . And now when we check the list of installed packages using pip list , we see that Django is not present. We successfully installed Django to the virtual environment alone, leaving our system installation untouched. This is the recommended way of installing Python packages.","title":"Venv"},{"location":"Scripts/Django/#project-skeleton","text":"Before we leave the virtual environment and start coding, let's take another look at the Scripts directory. One of the executables that has been installed is django-admin.exe . This is Django\u2019s command-line utility for administrative tasks. Other languages have similar utilities that are used to facilitate common administrative tasks, for example building out the skeleton of a new project, which you would do like this: django-admin startproject project This is all I'm going to use django-admin for in this course, but there is a lot you can do with it. Once the project has been created, let's go into the directory and peek around. As we can see, a Django project has a manage.py script and contains a nested directory with the same name as that of the project. The manage.py is actually what you will use to do a lot of administration for your web application. For example, we can run python manage.py runserver to start the web application. If we open a web browser to the address and port used by the server, we can see Django's default landing page. Let's close the server by pressing Ctrl C Let's run another command python manage.py startapp application If we get the directory contents again, we see that, similar to the django-admin command we ran earlier, this command erects a directory of content. Let's take a look and see what it created. More Python scripts.. but what are these files for? Off the bat, we notice this directory has a urls.py . Is this similar to the urls.py created in the project directory?","title":"Project skeleton"},{"location":"Scripts/Django/#project-structure","text":"Django web apps are structured into projects and applications . It's important to understand the distinction, because it is not immediately obvious to a novice. project describes a Django web application The project's root directory contains manage.py . The root directory contains a subdirectory with the same name as the root, but which contains other Python scripts, like urls.py and especially settings.py . This is called the project directory . An application refers to a Python package that provides some set of features. Applications are a component of projects. Application directories also contain a urls.py, but that's where the similarity ends. For a person who's new to Django, this means that much of the core functionality of your project will actually be contained in an app , which can feel awkward at first. But bear with it for now.","title":"Project structure"},{"location":"Scripts/Django/#settingspy","text":"Before we move any further, let's open the project folder and go to settings.py . If we scroll down to INSTALLED_APPS we need to make sure we add the name of the application that we just created in quotes to the list. This list is what Django uses to construct its namespace. Let's also take a look at this token value ( SECRET_KEY ) that is provided in plaintext. Because I'm guessing you're just learning Django and not about to deploy your web application to production, doing this is okay. But for production systems, this is a no-no. I'm going to show you an easy and simple it is to protect your secrest. But if you don't want to spend time on this section, feel free to use the time-codes to skip ahead.","title":"settings.py"},{"location":"Scripts/Django/#model","text":"In order to determine the shape of a web app's data, Django uses what is called a Model . If you're familiar with object-oriented programming, this will be the easiest part. Let's go into the application's models.py and create a Model. Models form the basis of how Django organizes data. models.py from django.db import models class Task ( models . Model ): title = models . CharField ( max_length = 50 ) # done = models.BooleanField() # created = models.DateTimeField(auto_now_add=True) We're subclassing models.Model , and the model defines a series of named fields which are instances of each of these classes. Some of these fields require specific attributes to be defined. For example, CharField requires a max_length attribute to be defined. Defining a model is not permanent, so let's stick with just the one title field. We'll add to it in a moment. Let's move on to the topic of migrations. When you make even the slightest to the models used in your web application's, Django requires you to perform what's called a migration . A migration keeps the web application's database consistent with the model. This is a two-step process from the command-line. python manage.py makemigrations This command analyzes the Model classes you've defined for the web application, and then it actually codes Python scripts for you to change the database accordingly. You'll find them in the migrations directory, see? To run these scripts, run python manage.py migrate Because this is the first time I ran a migration, it created a database for me from scratch, here. We can see that it's a SQLite database, a lightweight option that is used a lot in Python development. In fact, the Python Standard Library includes a SQLite module, so every Python installation is able to create and manipulate SQLite databases. How did Django know to produce an SQLite database, and not another? Let me show you. In the settings.py there is a dictionary named DATABASES DATABASES = { 'default' : { 'ENGINE' : 'django.db.backends.sqlite3' , 'NAME' : BASE_DIR / 'db.sqlite3' , } } \"ENGINE\" specifies the driver to use with the database. Django supports four databases out of the box: PostgreSQL, MySQL, Oracle, and SQLite. The other 3 options are more involved and require a server to be set up, so if you ever want to use them, you'll have to define values for HOST , PORT , USER , and PASSWORD in this dictionary. But SQLite keeps the database in a local file - no need to mess with any of that. We don't have time for an in-depth discussion of SQLite, but what I will do is show you how we can quickly take a look at this database within VS Code. Open the Extensions Marketplace and search for sqlite. The top app, SQLite Explorer by \"alex\", is what we're looking for. Click the green install button. I already have this extension installed, so I'm going to return to the File Explorer. Now I right click on db.sqlite3 and select open database. Another section named SQlite explorer should open in the file explorer. If you installed SQLite Explorer but don't see this section, check the hamburger menu in the top right and make sure it's enabled. SQLite Explorer allows you to view the schema and content of SQLite tables. Let's expand it. Each of these is a table -- the file is the database. All of these tables comes from an application in our Django project, and all but one of them were automatically implemented. The one we created is here, at the top - application_task . When we expand the table, we see Django added an integer id field, as well as the field we defined. \" varchar \" is SQL's equivalent of a string, more or less. Let's go back to the code and make another change to the Model. We're going to add a value so we can mark completed tasks off our to-do list. # models.py from django.db import models class Task ( models . Model ): title = models . CharField ( max_length = 50 ) done = models . BooleanField ( default = False ) # created = models.DateTimeField(auto_now_add=True) BooleanField doesn't require a max_length , but it does require a default value. We changed the model, so the database will have to be updated, which means we have to run another migration. Again we run python manage.py makemigrations , and we see a new migration script has been generated. Let's run it using python manage.py migrate . Once it's complete let's return to the SQLite Explorer. Now we see that our change has taken effect in the schema of the task. Let's go back and make one final addition. # models.py from django.db import models class Task ( models . Model ): title = models . CharField ( max_length = 50 ) done = models . BooleanField () created = models . DateTimeField ( auto_now_add = True ) # def __str__(self): # return self.title As you might guess, DateTimeField stores date and time information. The auto_now_add argument means that the web application will automatically associate the current date and time when a record is created. Let's make migrations and migrate one last time.","title":"Model"},{"location":"Scripts/Django/#adding-data","text":"Now the foundation of our web app - the data model - is complete. Too bad we have to wait until we code the rest of the web app to know what it looks like... wouldn't it be great if we could test out some data right away? Well, we can. Actually, there are several ways. The most Pythonic way is by running the Python interpreter. In the integrated terminal, when we run python manage.py shell we are greeted by a Python REPL. Let's import the model and instantiate a new task from app.models import Task task = Task ( title = \"Shop for milk\" ) Now we can run the save method on this task object: task.save() . And that will save the task created into the database. Another way is by running the command-line client for the database itself. By running python manage.py dbshell , we're taken into the SQLite client. We can run .tables , and we'll see all the available tables in the database. The output of this command should correspond exactly with the what we see in the SQLite explorer by opening the database. A deeper discussion of SQL syntax is beyond the scope of this video, but it wouldn't hurt to learn a couple of tricks. One SQL command that everyone know is select * from . We can run select * from app_task; and we can see that the task we added is in the database, and will be served by the web server. But a more typical way to add data during development is to do it through the admin portal . Let me show you what that is. Let's run .exit to leave the SQLite client. Now let's run the server and open a web browser to \"localhost/admin\". Every Django server has an admin application at this URL, by default. But it wants a login. How do we login to our own server? Let's stop the server and create a user so that we can login. python manage.py createsuperuser I'm going to create a superuser named Jasper. The prompt asks for an email address, but you can leave it blank. I enter a password, then confirm that password. And because Django can detect that it's not as complex as it should be, it asks me if I'm sure I want to set that as the password. I confirm. Now we can use those credentials to login to the admin site. Let's stop the webserver one last time and go into the app/admin.py file. Here I'm going to register the model we just created. from .models import Task admin . website . register ( Task ) This will make that model appear in the admin app, allowing us to manipulate the data through the web browser. Now we run the web server, refresh the admin site in the web browser, and we can see that the Task model appears in the admin dashboard. If I click on it, I can see the entry we created from the command-line. Let's create a new task here. And now our database has some data in it. Let's flesh out our web application some more.","title":"Adding data"},{"location":"Scripts/Django/#view","text":"","title":"View"},{"location":"Scripts/Django/#forms","text":"The ModelForm class can be used to quickly create a form from an existing model. from django.forms import ModelForm class TaskForm ( ModelForm ): class Meta : model = Task fields = '__all__' Alternatively, this class can be defined by instantiating the modelform_factory class generator. This can be placed within the views.py outside the view function for brevity, but it's typical to place this declaration within forms.py . # forms.py from django.forms.models import modelform_factory from .models import Task from django.forms.widgets import Input TaskForm = modelform_factory ( Task , exclude = []) In either case, this subclass is instantiated in views.py and passed in with the context object views.py from .forms import TaskForm from .models import Task def index ( request ): tasks = Task . objects . all () form = TaskForm () context = { 'form' : form } return render ( request , 'website/list.html' , context ) Let's take a look at the tasks variable. This is called a QuerySet . The key of the context object where the form was placed can then simply be used in a template tag: < form method = \"POST\" > {{ form }} </ form > Without the method=\"POST\" attribute set in the tag, the form will not send a POST request. A single field of the form can be specified as well: < form method = \"POST\" > {{ form . title }} </ form > However, on the same page, the csrf_token must also be provided { % csrf_token % } If the token appears in the text of the page, that is because you have used the template tag mustaches # Wrong! {{ csrf_token }}","title":"Forms"},{"location":"Scripts/Django/#django-starships-gallery","text":"Now that we've learned the basics of Django, let's make another web app that will do something more interesting. So we'll return to PowerShell and create a new directory for a new project. We're still working with the same virtual environment, so no need to create a new one. django-admin startproject starships python manage.py startapp app ... Bulma is a popular web design framework. You've probably heard of Bootstrap, which reigns supreme in this space, but Bulma is often brought up as an alternative because it is CSS only, whereas Bootstrap does incorporate some JavaScript. Because Bulma is CSS only, incorporating it involves basically downloading the CSS file, which you can do from the website . Where do we put it? Well, CSS files are typically considered static content, so we make a new folder in our app directory named \"static\" and place it there. Now we create a base template | {% load static %} <! DOCTYPE html> html ( lang= \"en\" ) head meta ( charset= \"UTF-8\" ) meta ( name= \"viewport\" , content= \"width=device-width, initial-scale=1.0\" ) link ( rel= \"stylesheet\" , href= \"{% static 'bulma.css'%}\" ) title \ud83d\ude80\ud83d\udc0d Django Starships style //- | .is-ancestor { flex-wrap: wrap; } body section .hero.is-primary .hero-body .container h1 .title Starships section .section .container {% block content %}{% endblock content%} The content page, including | {% extends 'dist/base.html' %} | {% load static %} | {% block content %} .tile.is-ancestor {% for ship in ships %} .tile.is-parent.is-6 article .tile.is-child.notification.is-black h1 .title {{ship.Name}} h2 .subtitle {{ship.Registry}} img ( src= \"{{ ship.Image.url }}\" ) | {% endfor %} | {% endblock content %}","title":"Django starships gallery"},{"location":"Scripts/Django/#basic-filter-table","text":"Using list.js body .listy input .search ( type= \"text\" ) ul .list li p .name USS Enterprise p .reg NCC-1701 var options = { valueNames : [ 'name' , 'reg' ] }; var userList = new List ( 'listy' , options );","title":"Basic filter table"},{"location":"Scripts/Django/#bulma-filter-table","text":"body section .section .table-container #list input .input.search ( placeholder= \"Search\" ) button .sort ( data-sort= \"name\" ) Sort table .table.is-bordered.is-striped.is-hoverable thead tr th Name th Registry th Crew th Class tbody .list tr td .name USS Enterprise td .reg NCC-1701 td .crew 204 td .cls Constitution var options = { valueNames : [ 'name' , 'reg' , 'cls' ] }; var userList = new List ( 'list' , options );","title":"Bulma filter table"},{"location":"Scripts/Django/#bulma-filter-tile-gallery","text":"body section .section .container #foo input .search.input ( type= \"text\" ) .tile.is-ancestor.list .tile.is-parent.is-3 article .tile.is-child.notification.is-primary h1 .name.subtitle USS Enterprise p .reg NCC-1701 p .cls Constitution .tile.is-parent.is-3 article .tile.is-child.notification.is-primary h1 .name.subtitle USS Constitution p .reg NCC-1700 p .cls Constitution var options = { valueNames : [ 'name' , 'reg' , 'cls' ] }; var userList = new List ( 'foo' , options ); Bulma tiles do not wrap by default . is-ancestor { flex-wrap : wrap ; }","title":"Bulma filter tile gallery"},{"location":"Scripts/Kusto/","text":"If you're serious about learning Azure, you've gotta learn Kusto. Kusto is Microsoft's custom-built relational database query language, combining the functionality of SQL with the syntax of a shell language. It is a deep topic, but it's worth mastering if you're planning on doing data science or security in Azure. But like a lot of things worth mastering, getting started is a pain. That's where I'm going to help you. Setup The first thing you need to get started is an Azure account. If this is your first time signing up for an account, you'll be able to receive a $200 credit that will last 30 days. One of the main complications learning Kusto is that, as far as I know, there isn't a local client like sqlite that you can just download and start playing around with. There is the Kusto Explorer which is available for Windows only, but I'm not a big fan of it. Azure Data Studio has a Kusto extension . After installing it, you can connect to an ADX cluster. Now, what they do have is a Web UI at dataexplorer.azure.com which you.. don't even need an Azure account for! It's more or less publicly accessible as long as you login with a Microsoft account. Now you would think this would be the perfect place to play around with Kusto to start learning it, but if you go straight here you won't have any available data to run queries against. This layout is modeled on similar graphical applications put out by Microsoft like SQL Server Manager Studio where your data sources or connections are listed on the left sidebar, top pane is where you would compose your queries, and then the results would be displayed on the bottom pane. As we can see, there are no data connections available at the present time. So the first thing I'm going to show you, even though I'm not going to use it, is how to add the default \"Samples\" database that is used in every other tutorial and video you're going to find on this topic. Add connection > help.kusto.windows.net, and it's available to the public so you should immediately see the connection show up and you can begin running queries, even though like I said I'm not going to use it for this video. This dataset includes tables on Covid19, US geospatial data, and of course the legendary StormEvents table which you will see everywhere somebody is trying to teach Kusto. But we're not going to go with the canned data that Microsoft provides because, frankly, it's boring and I can't learn with it. Unless you've already done a lot of data analysis, which I haven't, the results will probably seem frankly meaningless. We want to make our own data, or rather I'm going to give you some simple data to play with that you will be able to play with and engage with better. But to do that, we have to get into Azure and create our own Kusto cluster. The first thing we need to do is create a free Azure account . Now even though it says it's free, if you're signing in for the first time you will be expected to produce credit card information, so be ready for that. There are a couple of different ways that Kusto is used in Azure, for example Log Analytics workspaces. But this video is going to focus on just learning the syntax of the language itself, and for that we're going to be using Azure Data Explorer. There are many ways of going about this. Even though I typically opt for using a command-line interface, of which there are two available for Azure, Azure PowerShell and Azure CLI, today for the sake of simplicity I'm going to stick to the Azure website which is referred to as the Portal. It will take a few minutes for the cluster to deploy. But once it does, we will be able to connect to it using the Data Explorer web ui by adding it as a source just like we did. Now that the database is up let's add some tables. .create table starships (name:string, class:string, registry:string, series:string); Ingest data .ingest into table starships 'https://raw.githubusercontent.com/jasper-zanjani/kusto/main/starships.csv' with (ignoreFirstRecord=true); If you make a mistake .drop table starships; Joining other tables .create table classes (class:string, crew: int32); .ingest into table classes 'https://raw.githubusercontent.com/jasper-zanjani/kusto/main/classes.csv' with (ignoreFirstRecord=true); .create tables series (series:string, description:string); .ingest into table series 'https://raw.githubusercontent.com/jasper-zanjani/kusto/main/series.csv' with (ignoreFirstRecord=true); There are a lot of different join flavors: leftouter, rightouter, fullouter, leftsemi, rightsemi, leftanti, rightanti... Joining with classes starships | join kind = inner (classes) on $left.class==$right.class | project name, crew; Sqlbolt I love interactive tutorials, and one of my favorite tutorials has been sqlbolt.com. It is a very well-thought out site that takes you from topic to topic with interactive exercises in the browser to challenge what you learn every step of the way. Best of all, it's totally free, with no need even to sign up. In fact, I'm such a big fan when I decided to make this video I decided simply to adapt SQLbolt's lessons to Kusto. If you're interested in learning SQL, I can't recommend sqlbolt highly enough.","title":"Kusto"},{"location":"Scripts/Kusto/#setup","text":"The first thing you need to get started is an Azure account. If this is your first time signing up for an account, you'll be able to receive a $200 credit that will last 30 days. One of the main complications learning Kusto is that, as far as I know, there isn't a local client like sqlite that you can just download and start playing around with. There is the Kusto Explorer which is available for Windows only, but I'm not a big fan of it. Azure Data Studio has a Kusto extension . After installing it, you can connect to an ADX cluster. Now, what they do have is a Web UI at dataexplorer.azure.com which you.. don't even need an Azure account for! It's more or less publicly accessible as long as you login with a Microsoft account. Now you would think this would be the perfect place to play around with Kusto to start learning it, but if you go straight here you won't have any available data to run queries against. This layout is modeled on similar graphical applications put out by Microsoft like SQL Server Manager Studio where your data sources or connections are listed on the left sidebar, top pane is where you would compose your queries, and then the results would be displayed on the bottom pane. As we can see, there are no data connections available at the present time. So the first thing I'm going to show you, even though I'm not going to use it, is how to add the default \"Samples\" database that is used in every other tutorial and video you're going to find on this topic. Add connection > help.kusto.windows.net, and it's available to the public so you should immediately see the connection show up and you can begin running queries, even though like I said I'm not going to use it for this video. This dataset includes tables on Covid19, US geospatial data, and of course the legendary StormEvents table which you will see everywhere somebody is trying to teach Kusto. But we're not going to go with the canned data that Microsoft provides because, frankly, it's boring and I can't learn with it. Unless you've already done a lot of data analysis, which I haven't, the results will probably seem frankly meaningless. We want to make our own data, or rather I'm going to give you some simple data to play with that you will be able to play with and engage with better. But to do that, we have to get into Azure and create our own Kusto cluster. The first thing we need to do is create a free Azure account . Now even though it says it's free, if you're signing in for the first time you will be expected to produce credit card information, so be ready for that. There are a couple of different ways that Kusto is used in Azure, for example Log Analytics workspaces. But this video is going to focus on just learning the syntax of the language itself, and for that we're going to be using Azure Data Explorer. There are many ways of going about this. Even though I typically opt for using a command-line interface, of which there are two available for Azure, Azure PowerShell and Azure CLI, today for the sake of simplicity I'm going to stick to the Azure website which is referred to as the Portal. It will take a few minutes for the cluster to deploy. But once it does, we will be able to connect to it using the Data Explorer web ui by adding it as a source just like we did. Now that the database is up let's add some tables. .create table starships (name:string, class:string, registry:string, series:string); Ingest data .ingest into table starships 'https://raw.githubusercontent.com/jasper-zanjani/kusto/main/starships.csv' with (ignoreFirstRecord=true); If you make a mistake .drop table starships;","title":"Setup"},{"location":"Scripts/Kusto/#joining-other-tables","text":".create table classes (class:string, crew: int32); .ingest into table classes 'https://raw.githubusercontent.com/jasper-zanjani/kusto/main/classes.csv' with (ignoreFirstRecord=true); .create tables series (series:string, description:string); .ingest into table series 'https://raw.githubusercontent.com/jasper-zanjani/kusto/main/series.csv' with (ignoreFirstRecord=true); There are a lot of different join flavors: leftouter, rightouter, fullouter, leftsemi, rightsemi, leftanti, rightanti... Joining with classes starships | join kind = inner (classes) on $left.class==$right.class | project name, crew;","title":"Joining other tables"},{"location":"Scripts/Kusto/#sqlbolt","text":"I love interactive tutorials, and one of my favorite tutorials has been sqlbolt.com. It is a very well-thought out site that takes you from topic to topic with interactive exercises in the browser to challenge what you learn every step of the way. Best of all, it's totally free, with no need even to sign up. In fact, I'm such a big fan when I decided to make this video I decided simply to adapt SQLbolt's lessons to Kusto. If you're interested in learning SQL, I can't recommend sqlbolt highly enough.","title":"Sqlbolt"},{"location":"Scripts/Labbing/","text":"Labbing to Learn Linux So you've been on LinkedIn, looking for a job or just doomscrolling all the updates in everyone else's career. You happen upon people talking about Linux. What is it? Why is it useful? Or you decide to make a change and get into IT. However you came across it, you've made the decision to try to learn Linux. The","title":"Labbing to Learn Linux"},{"location":"Scripts/Labbing/#labbing-to-learn-linux","text":"So you've been on LinkedIn, looking for a job or just doomscrolling all the updates in everyone else's career. You happen upon people talking about Linux. What is it? Why is it useful? Or you decide to make a change and get into IT. However you came across it, you've made the decision to try to learn Linux. The","title":"Labbing to Learn Linux"},{"location":"Scripts/Rust/","text":"Rust tutorial Introductory monologue F1","title":"Rust"},{"location":"Scripts/Rust/#rust-tutorial","text":"Introductory monologue F1","title":"Rust tutorial"},{"location":"Scripts/SQLite/","text":"SQLite is a minimal, open-source database which supports standard relational database features. It's part of the Python Standard Library and because of that, it's used in a lot of places. And for Django web applications, it's the database of choice during development.","title":"SQLite"},{"location":"Scripts/3rd/dylan_roof/","text":"dylann roof was 21 years old and he believed in racial segregation. he also believed that the white-skinned people were superior to the dark skin ones. Dylan ran a website called the last Rhodesian where he posted images of himself burning the Asdmerican flag or holding a pistol and posing proudly at sites connected to the Confederacy. At first he wanted to go into the projects and shoot African-American drug dealers, but he then decided that they might shoot back and he didn't want to risk dying before he could change the world. he settles for the Emanuel African Methodist Episcopal Church in South Carolina. On June 17th 2015 Dylan roof pulled into that church and he sat in his car for some time. he had loaded 8 magazines of hollow point ammunition for his Glock 45 handgun. he wanted to have exactly 88 bullets because that number is the white nationalist code for Heil Hitler. Dylan then join the members of the congregation for a Bible study session that was planned for that night. he sat there for a few minutes working up the courage to attack. He then out his gun telling the others that African-Americans were taking over their country, and he then open fire. according to One Survivor, roof tried to shoot himself but he had run out of ammunition. he fled in his car he was arrested the following morning in North Carolina and brought in for interrogation Dylan roofs parents were in the middle of a divorce when he was born. his father Franklin was a carpenter and a contractor while his mother Amy serve drinks as a bartender. Dylan's birth would bring them back together but only for a short time. when Dylan was five his father would marry another woman named Paige and together they would give Dylan another sibling. Dylan would watch as his father constantly verbally and physically abused his stepmother on a daily basis at three years old Dylan received a haircut style called the bowl cut and for the remainder of his life he would make sure his hair would remain that way. He would begin to develop odd behaviors like obsessive compulsive disorder where he would fixate on any one thing and only that thing, and his family would have a hard time taking his attention away from it. he was also germaphobic, and would go out of his way to avoid any and all germs. in middle school he found that he enjoyed smoking marijuana and was even caught spending money on the drug. Dylan would attend almost a dozen different schools in his life and would even end up repeating the 9th grade before he decided to quit altogether. quitting school meant he would have a lot of free time and he would use that time to play video games and use drugs. he would live on and off between his biological mother and father maintaining this lifestyle until his father demanded he get a job as a landscaper, which Dylan did. aside from work Dylan continued to maintain his antisocial lifestyle. he would spend the majority of his free time locked in his room searching the internet or playing video games Dylan purchased the gun used in the shooting at a retail store in West Columbia using birthday money that was given to him. after showing it to his friends they would try and hide the weapon because they feared he would kill someone with it Dylan continue to post racist propaganda on the internet and was even talking to other white supremacist online. ironically it was his racist actions that would lead to him getting caught after the shooting after Dylan left the church he would drive almost 300 miles away. he was at a stoplight when a woman noticed his car with a three flag Confederate States of America bumper decoration. she then called the police, revealing his location. photos of him and his car we're all over television and the internet, so his car and him in it was like a beacon for all to see. Dylan was convicted of the murders and sentenced to death for his crimes On August 4th 2016 Dylan was assaulted by another inmate. the attacker was 25 year old African-American Dwayne Stafford who was awaiting trial for assault and robbery. Dwayne was able to sneak down to the protective custody unit where the two officers were watching Dylan just happened to be busy with other things. Dylan had bruising on his face and body but was not seriously injured strangely enough the next day Dwayne was released on bail.","title":"Dylan roof"},{"location":"Scripts/Discord/Discord.article/","text":"","title":"Discord.article"},{"location":"Scripts/Discord/Discord/","text":"Discord Discord Discord UI Creating a new Discord Creating a bot Set-up Visual Studio Code First bot Environment variable TextChannel hello-world DM hello-world Enriched hello-world Cogs Call-out to click video Inspection of ctx object Events Reaction role on_raw_reaction_add discord.utils.get Adding role API If you're watching this video, odds are you need no introduction to Discord. Discord is an instant messaging application with support for images and file sharing, custom emoji, and granular role- based access control. Just a few years ago, Discord might have been considered just one of many messaging platforms, vying for the spotlight alongside Telegram, Viber, WhatsApp, Slack, and Microsoft Teams. But today, Discord is the de facto communications platform for tech geeks, YouTubers, and especially PC gamers and Twitch streamers. With Discord it is ridiculously easy to start a free online community, populated with channels for text and voice communication, and you can even host video streams. Most interesting of all, Discord exposes an API that is supported by libraries that support the development of bots in a ton of languages . Today I'm going to take you from zero to hero in one of these libraries, Discord.py ! Let's get started! Discord UI I just created a fresh Discord account. All you need to provide is an email address for verification. The Discord logo at the top opens our private messages. We don't have any at the moment. We can discover other public servers using the compass icon. We can also create our own server by clicking on the plus button. As you can see I haven't joined any Discord servers, which would appear between the compass and the plus icons. But let's change that right now and join the Discord.py server. This is a Discord server run by the developers and maintainers of the discord.py library. I can find an invite link from their GitHub page. I scroll to the bottom, click on the link and confirm that I want to join the server. As you can see, the icon now appears on the left sidebar. Within a typical Discord server, you usually find many TextChannels for chat, and a few VoiceChannels for audio communication. These are found on the left sidebar, to the right of the list of joined servers. On the right sidebar you can find a list of all server members, organized by role. Creating a new Discord Let's create our own Discord by clicking on the plus icon. Creating a bot Let's open up a web browser and navigate to https://discordapp.com/developers token is a secret permissions Set-up Before we start coding, we're going to install a virtual environment. A virtual environment acts as a secondary installation of Python where we will install dependencies . It is considered a best practice to install packages like discord.py in virtual environments so that you don't clutter up your system installation of Python with a bunch of packages of various versions that could cause you confusion down the road. How you do this is up to you, but I like to organize all my virtual environments into a single folder that I can then reference from multiple projects that are stored elsewhere. I start PowerShell and navigate to the correct folder. I create the virtual environment by invoking Python with -m followed by \"venv\", then the name of the virtual environment I want to create. python -m venv discord Now that it's created, before I activate it I inspect where the python command points to. Get-Command python | select Source This is the system installation of Python. I invoke the Activate script, which tells the system I want to use the virtual environment that was just created and changes the appearance of the prompt. .\\ discord \\ Scripts \\ Activate . ps1 Now I inspect how the system will interpret the python command and confirm that it does indeed point to the virtual environment. Get-Command python | select Source Now when I run Python, I'll be running the Python that was installed to this directory because I activated the virtual environment. The same goes for pip , which is Python's package manager. gcm pip | select Source I install the discord.py package into the virtual environment. pip install discord.py I can confirm the package was installed by running pip list I can leave the virtual environment by typing deactivate Now I have returned to the system installation of Python. If I check installed packages pip list We see that discord.py was installed only to the virtual environment, leaving our system installation clean. Visual Studio Code Now we are ready to start up our editor. For this video I'm going to use Visual Studio Code, which is available free of charge for multiple platforms, including Windows 10. I have the Python extension installed, and you should too if you're thinking of developing in Python. I start a new project folder to contain our work, and start a new Python script. import discord As we can see, attempting to import the discord.py package produces an error. That is because VS Code is still using the default system installation of Python. We already confirmed that we installed the discord.py package only to the virtual environment and not to the system installation. How do we tell VS Code to use the virtual environment? We can select the virtual environment's interpreter by clicking here. Once we select the correct interpreter, which is found in the same directory as the activate script, the error is resolved, and we're ready to hit the ground running. First bot There are several ways of using the API exposed by the discord.py package. We will stick to the easiest one by instantiating a Bot object. If you've never used a Discord bot, they typically appear as another user, and their functionality is accessed by typing a keyword preceded by a special character within a channel or DM, like !help . command_prefix indicates what these specials characters will be. You can specify more than one prefix by specifying a list of strings. Today, we will keep things simple and stick to a period. Finally, we invoke the run method of the newly instantiated Bot object and pass it the token. from discord.ext.commands import Bot bot = Bot ( command_prefix = '.' ) bot . run ( token ) Let's run the script as-is. We don't get any feedback in the terminal, but if we check the Discord, we see our bot is now online. We haven't defined any functionality for it yet, but we can type .help and we receive a help message. This is because the help message is automatically implemented for any bot. Congratulations! You've created a completely useless Discord bot, hopefully the first of many! Environment variable If you're starting out learning coding, then it's important to learn good habits from the jump. And one of the most important habits is keeping secrets like a token out of your code. In the previous example, I placed the literal token within my code, a practice known as hardcoding . This is a security issue, because anyone who sees this token will be able to use it and abuse it as they please. Don't worry about my token here -- by the time you see this video it will have been changed long ago. One day you might decide to share your code, because you need help with it or want to show off your achievement by uploading it to a public repository like GitHub. Unfortunately, hard-coded secrets left within code remains one of the biggest sources of data leaks, and even big companies and experienced developers are guilty of this rookie mistake. So how do we protect our secret token? One of the best ways of protecting secrets is by placing them in environment variables . Environment variables are values stored in memory and which can be accessed from Python. Let's open up the integrated terminal by hitting Ctrl ` , which is the key directly to the left of the number 1 on US keyboards. As we can see, because we selected the virtual environment as our interpreter, the integrated terminal comes up with the virtual environment already loaded. Keep in mind in this video I'm using a Windows environment, so if you're using a Mac OS X or Linux computer, the terminal will look different. In PowerShell, environment variables can be viewed as a PowerShell drive, meaning it is treated like a virtual filesystem Set-Location Env :\\ Get-ChildItem Our system already has many environment variables set up. These environment variables provide information to programs on certain important locations. For example, SystemRoot points to the directory where our installation of Windows is located. My system has Windows installed to \"C:\\WINOWS\", and if you're running Windows yours probably does too. In PowerShell we can access the value of environment variable like so $Env:SystemRoot Because it's a location on the drive, we can navigate directly there by accessing the environment variable as an argument to another command. Set-Location $Env:SystemRoot Get-Location Let's start the Python interpreter and see how we can access environment variables within Python. python Now we import the os module, which is part of the Standard Library . The Standard Library is a collection of modules that are provided by default with every installation of Python. import os We can access environment variables by calling os.getenv() . Let's use this command to inspect the Windows installation directory. os . getenv ( 'SystemRoot' ) Now let's find out how to temporarily declare an environment variable and access it from the script. First we create a new file named \".env\". We open it up and we create a value TOKEN=... Let's install a new module pip install dotenv Now we place an import statement at the top of our Python script. And we can call a function import dotenv dotenv . load_dotenv () When the script is run, this function call will load the \".env\" file and add the TOKEN value to the environment. Its value will be available as an environment variable. Let's try it out: import os , dotenv dotenv . load_dotenv () token = os . getenv ( 'TOKEN' ) print ( token ) As we can see we successfully loaded the .env file, then accessed its value as an environment variable. However, once the script has completed execution and we exit the interpreter, the TOKEN variable is no longer available $Env:TOKEN This is how we can protect secrets at the measly cost of nothing more than 2 additional lines of code. This has the added benefit that we will be able to use the same .env file for any additional Python scripts we compose in this directory. Let's compose # import discord import os , dotenv from discord.ext.commands import Bot dotenv . load_dotenv () token = os . getenv ( 'TOKEN' ) client = Bot ( command_prefix = '.' ) # ... client . run ( token ) TextChannel hello-world These expressions preceded by the @ sign are called decorators . A deeper discussion of what decorators do is outside the scope of this video, but it's enough to say that they add functionality to any functions they decorate. And in discord.py in particular, they will tie together all the code you write. ctx.send equivalent to print @client . command () async def hello ( ctx ): await ctx . send ( f 'Hello world!' ) DM hello-world @client . command () async def hello ( ctx ): user = ctx . author await user . send ( 'Hey handsome' ) Enriched hello-world Embed : enriched message - URLs are automatically put into links @client . command () async def hello ( ctx ): user = ctx . message . author . name emoji = random . choice ([ ':grin:' , ':smiling_face_with_3_hearts:' , ':smirk:' ]) embed = discord . Embed ( title = f \"Hello { user } !\" , description = f \"Thank you for the attention { emoji } \" , color = discord . Color . teal ()) await ctx . send ( embed = embed ) Cogs When you're developing something from scratch or learning something new, your code can get messy very quickly. It's always a good idea to keep your code as tidy and clean as possible. You can definitely do this using the standard Python imports and modules. This shouldn't be much of an issue since we're defining procedural functions which have nothing to do with one another logically. The discord.py library offers a way to do that in what are called \"Cogs\", which not only organize and modularize your code but offer a way to group your commands into command groups. from discord.ext import commands import dotenv import os dotenv . load_dotenv () token = os . getenv ( 'TOKEN' ) client = commands . Bot ( command_prefix = '.' ) client . load_extension ( 'cogs.Example' ) if __name__ == \"__main__\" : client . run ( token ) The way the cog file itself is designated by using the directory name and filename separated by period is the same convention used in Django . from discord.ext import commands class Example ( commands . Cog ): def __init__ ( self , client ): self . client = client @commands . command () async def ping ( self , ctx ): await ctx . send ( 'pong' ) Call-out to click video If the use of these command decorators strikes you as bizarre and you feel that it would help you to see them being used in a different context, another video I recently made should help you out. In it, I go over the Click package, which allows you to create command-line utilities in Python using decorators that look and act a whole lot like these ones. Inspection of ctx object Listing channels on a Discord server @commands . command () async def channels ( self , ctx ): channels = [ c . name for c in ctx . guild . channels if type ( c ) is discord . TextChannel ] Snitch command @commands . command () async def dmowner ( self , ctx ): user_id = ctx . guild . owner_id user = self . client . get_user ( user_id ) print ( user_id ) await user . send ( f 'This guy { ctx . author . name } is creeping me out... Betta handle dat!' ) Events Event handler functions conventionally have names beginning with \"on\", i.e. on_load - @commands.Cog.listener() vs. @client.event vs. on_ready(self) (class-based) @client . event async def on_ready (): print ( f \"Logged in as { client . user . name } \" ) @commands . Cog . listener () async def on_ready ( self ): print ( f \"Logged in as { self . client . user . name } \" ) Reaction role So we've established a broad-based understanding of the features exposed in the discord.py API: - commands defined using the command() decorator - event handlers that use the event decorator and which override the API's built-in events - Context objects that are passed to commands upon invocation from the message window. Context objects, in turn, expose a - send method that we can use to send messages back to the TextChannel where the command originated - Using the author attribute of a Context object, we can send direct messages. - Embed objects for richer message content - Cogs to organize and modularize commands - Most importantly, we learned about the Bot object itself, which accepts the token we get from the Discord developer portal, and how to keep that token secure. Let's implement a practical project that will allow us to bring these lessons home. One of the most common features that Discord guild maintainers want to implement is a bot that will automatically assign roles to members based on an emoji reaction, or a \"reaction role\". In Discord these are often used to force members to indicate agreement to a code of conduct or to assign fun roles based on the individual's personal interests. First we need to adjust the permissions of the bot by assigning it the \"Manage roles\" permission. We can do this through the developer portal, but it's easier to do it in the Discord server itself We right-click on our Discord server and open server settings Let's go to the Roles section and click on Chatty Cathy We scroll down until we find the Manage Roles permission We grant Chatty Cathy the permission to manage roles We also need to actually create a role that Chatty Cathy will assign to a user. We can also do this in server settings. We also need to make sure the bot's role is placed physically above that of the role to be assigned. Discord roles are arranged like a totem pole in this way, such that roles can only be assigned by higher roles. Finally we can write some code. on_raw_reaction_add Let's go back to Visual Studio Code where we will create a new event handler in main.py for the ON RAW REACTION ADD event. This event, unlike the on_ready event will be passed a PAYLOAD argument.. @bot . event async def on_raw_reaction_add ( payload ): pass The Payload argument is similar in concept to a Context object. It is not as rich in information. A payload does expose a few useful properties that we will need if we want to implement the functionality of a reaction role. Where a Context object exposes a Guild object, a Payload object exposes only a guild_id . Where a Context object exposes a Channel object, a Payload object exposes only a channel_id . Where a Context object exposes a Message object, a Payload object exposes only a message_id . This means that we have to implement additional query logic in order to retrieve these objects from the ID numbers alone. Context RawReactionActionEvent Guild guild_id Channel channel_id Message message_id @client . event async def on_raw_reaction_add ( payload ): print ( \"Message ID:\" , payload . message_id ) print ( \"Channel ID:\" , payload . channel_id ) print ( 'Guild ID:' , payload . guild_id ) print ( 'Emoji:' , payload . emoji . name ) Let's run it in the integrated terminal and see how it responds. After a moment, the on_ready event handlers fire, and the bot is running. Let's open Discord, react to a message, then examine the output in the terminal. The event handler worked! Now let's see what the significance of this output is. Let's open Discord again, copy the message URL, and compare it to the output in the terminal. As we can see, - The first number indicates the Discord server, or Guild as it is referred to in the Discord API - The second number indicates the channel ID - The third number indicates the message ID Now let's create a new channel. This is where new Discord server members will have to go to be granted the Turtles role. Respond with :thumbs_up: And let's give it a reaction. As we can see, the event handler will be triggered when members perform emoji reactions to any message in any TextChannel on our Discord server. Now we're ready to complete the implementation of the role reaction discord.utils.get In order to retrieve the object from the object id, we're going to use the discord.utils.get function. discord.utils.get takes the sequence to be queried first, and we can pass the guild_id into the id keyword argument. guild = discord . utils . get ( client . guilds , id = payload . guild_id ) Now that we have a guild object, we can use this same get function to get the role that we want to assign. role = discord . utils . get ( guild . roles , name = 'Turtles' ) Now we can retrieve the member object for the user that actually provided the emoji reaction member = discord . utils . get ( guild . members , id = payload . user_id ) Finally, a method exposed by the member object allows us to add the role. await member . add_roles ( role ) Let's fire up our bot in the integrated terminal. The on_ready events fired, and we're up and running. Let's switch over to Discord. We can see that my username in Discord is not green. I do have a crown, because I am the Guild owner, but I do not have the Turtles role yet. I put a thumbs up on the message, and I can see that now my name is green. Our reaction role bot worked! For some objects, in particular Guild , the Client object exposes a specific method: guild = client . get_guild ( payload . guild_id ) discord.utils.find uses a syntax similar to the Python builtin filter function, where a lambda function is defined, then the sequence of items across which it will be executed. So for example, let's start a Python interpreter in the terminal. Let's make a list of numbers from 0 through 9. l = range ( 10 ) We can use the filter command to find only even numbers: list ( filter ( lambda x : x % 2 == 0 , l )) # [0,2,4,6,8] What we're doing here is that we're iterating over this sequence of elements and executing this lambda function by passing each element through this expression. If the expression returns true, then that element makes it through. If the expression evaluates as false, that value is filtered. discord.utils.find uses an identical syntax, but instead of returning multiple values, it returns only one. So we can use it to retrieve the guild object from the guild_id guild = discord . utils . find ( lambda g : g . id == payload . guild_id , client . guilds ) A deeper discussion of lambdas is outside the scope of this video, and because the syntax can be confusing to someone who's not familiar with the topic, I'm actually going to comment out this line. Adding role Member.add_roles method takes a role object @client . event async def on_raw_reaction_add ( payload ): if payload . message_id == 751854496748929065 and payload . emoji . name == u \" \\U0001F44D \" : guild = get ( client . guilds , id = payload . guild_id ) API Context Guild Message Author Client guilds Guild channels roles members id Payload Member Emoji message_id channel_id guild_id","title":"Discord"},{"location":"Scripts/Discord/Discord/#discord","text":"Discord Discord UI Creating a new Discord Creating a bot Set-up Visual Studio Code First bot Environment variable TextChannel hello-world DM hello-world Enriched hello-world Cogs Call-out to click video Inspection of ctx object Events Reaction role on_raw_reaction_add discord.utils.get Adding role API If you're watching this video, odds are you need no introduction to Discord. Discord is an instant messaging application with support for images and file sharing, custom emoji, and granular role- based access control. Just a few years ago, Discord might have been considered just one of many messaging platforms, vying for the spotlight alongside Telegram, Viber, WhatsApp, Slack, and Microsoft Teams. But today, Discord is the de facto communications platform for tech geeks, YouTubers, and especially PC gamers and Twitch streamers. With Discord it is ridiculously easy to start a free online community, populated with channels for text and voice communication, and you can even host video streams. Most interesting of all, Discord exposes an API that is supported by libraries that support the development of bots in a ton of languages . Today I'm going to take you from zero to hero in one of these libraries, Discord.py ! Let's get started!","title":"Discord"},{"location":"Scripts/Discord/Discord/#discord-ui","text":"I just created a fresh Discord account. All you need to provide is an email address for verification. The Discord logo at the top opens our private messages. We don't have any at the moment. We can discover other public servers using the compass icon. We can also create our own server by clicking on the plus button. As you can see I haven't joined any Discord servers, which would appear between the compass and the plus icons. But let's change that right now and join the Discord.py server. This is a Discord server run by the developers and maintainers of the discord.py library. I can find an invite link from their GitHub page. I scroll to the bottom, click on the link and confirm that I want to join the server. As you can see, the icon now appears on the left sidebar. Within a typical Discord server, you usually find many TextChannels for chat, and a few VoiceChannels for audio communication. These are found on the left sidebar, to the right of the list of joined servers. On the right sidebar you can find a list of all server members, organized by role.","title":"Discord UI"},{"location":"Scripts/Discord/Discord/#creating-a-new-discord","text":"Let's create our own Discord by clicking on the plus icon.","title":"Creating a new Discord"},{"location":"Scripts/Discord/Discord/#creating-a-bot","text":"Let's open up a web browser and navigate to https://discordapp.com/developers token is a secret permissions","title":"Creating a bot"},{"location":"Scripts/Discord/Discord/#set-up","text":"Before we start coding, we're going to install a virtual environment. A virtual environment acts as a secondary installation of Python where we will install dependencies . It is considered a best practice to install packages like discord.py in virtual environments so that you don't clutter up your system installation of Python with a bunch of packages of various versions that could cause you confusion down the road. How you do this is up to you, but I like to organize all my virtual environments into a single folder that I can then reference from multiple projects that are stored elsewhere. I start PowerShell and navigate to the correct folder. I create the virtual environment by invoking Python with -m followed by \"venv\", then the name of the virtual environment I want to create. python -m venv discord Now that it's created, before I activate it I inspect where the python command points to. Get-Command python | select Source This is the system installation of Python. I invoke the Activate script, which tells the system I want to use the virtual environment that was just created and changes the appearance of the prompt. .\\ discord \\ Scripts \\ Activate . ps1 Now I inspect how the system will interpret the python command and confirm that it does indeed point to the virtual environment. Get-Command python | select Source Now when I run Python, I'll be running the Python that was installed to this directory because I activated the virtual environment. The same goes for pip , which is Python's package manager. gcm pip | select Source I install the discord.py package into the virtual environment. pip install discord.py I can confirm the package was installed by running pip list I can leave the virtual environment by typing deactivate Now I have returned to the system installation of Python. If I check installed packages pip list We see that discord.py was installed only to the virtual environment, leaving our system installation clean.","title":"Set-up"},{"location":"Scripts/Discord/Discord/#visual-studio-code","text":"Now we are ready to start up our editor. For this video I'm going to use Visual Studio Code, which is available free of charge for multiple platforms, including Windows 10. I have the Python extension installed, and you should too if you're thinking of developing in Python. I start a new project folder to contain our work, and start a new Python script. import discord As we can see, attempting to import the discord.py package produces an error. That is because VS Code is still using the default system installation of Python. We already confirmed that we installed the discord.py package only to the virtual environment and not to the system installation. How do we tell VS Code to use the virtual environment? We can select the virtual environment's interpreter by clicking here. Once we select the correct interpreter, which is found in the same directory as the activate script, the error is resolved, and we're ready to hit the ground running.","title":"Visual Studio Code"},{"location":"Scripts/Discord/Discord/#first-bot","text":"There are several ways of using the API exposed by the discord.py package. We will stick to the easiest one by instantiating a Bot object. If you've never used a Discord bot, they typically appear as another user, and their functionality is accessed by typing a keyword preceded by a special character within a channel or DM, like !help . command_prefix indicates what these specials characters will be. You can specify more than one prefix by specifying a list of strings. Today, we will keep things simple and stick to a period. Finally, we invoke the run method of the newly instantiated Bot object and pass it the token. from discord.ext.commands import Bot bot = Bot ( command_prefix = '.' ) bot . run ( token ) Let's run the script as-is. We don't get any feedback in the terminal, but if we check the Discord, we see our bot is now online. We haven't defined any functionality for it yet, but we can type .help and we receive a help message. This is because the help message is automatically implemented for any bot. Congratulations! You've created a completely useless Discord bot, hopefully the first of many!","title":"First bot"},{"location":"Scripts/Discord/Discord/#environment-variable","text":"If you're starting out learning coding, then it's important to learn good habits from the jump. And one of the most important habits is keeping secrets like a token out of your code. In the previous example, I placed the literal token within my code, a practice known as hardcoding . This is a security issue, because anyone who sees this token will be able to use it and abuse it as they please. Don't worry about my token here -- by the time you see this video it will have been changed long ago. One day you might decide to share your code, because you need help with it or want to show off your achievement by uploading it to a public repository like GitHub. Unfortunately, hard-coded secrets left within code remains one of the biggest sources of data leaks, and even big companies and experienced developers are guilty of this rookie mistake. So how do we protect our secret token? One of the best ways of protecting secrets is by placing them in environment variables . Environment variables are values stored in memory and which can be accessed from Python. Let's open up the integrated terminal by hitting Ctrl ` , which is the key directly to the left of the number 1 on US keyboards. As we can see, because we selected the virtual environment as our interpreter, the integrated terminal comes up with the virtual environment already loaded. Keep in mind in this video I'm using a Windows environment, so if you're using a Mac OS X or Linux computer, the terminal will look different. In PowerShell, environment variables can be viewed as a PowerShell drive, meaning it is treated like a virtual filesystem Set-Location Env :\\ Get-ChildItem Our system already has many environment variables set up. These environment variables provide information to programs on certain important locations. For example, SystemRoot points to the directory where our installation of Windows is located. My system has Windows installed to \"C:\\WINOWS\", and if you're running Windows yours probably does too. In PowerShell we can access the value of environment variable like so $Env:SystemRoot Because it's a location on the drive, we can navigate directly there by accessing the environment variable as an argument to another command. Set-Location $Env:SystemRoot Get-Location Let's start the Python interpreter and see how we can access environment variables within Python. python Now we import the os module, which is part of the Standard Library . The Standard Library is a collection of modules that are provided by default with every installation of Python. import os We can access environment variables by calling os.getenv() . Let's use this command to inspect the Windows installation directory. os . getenv ( 'SystemRoot' ) Now let's find out how to temporarily declare an environment variable and access it from the script. First we create a new file named \".env\". We open it up and we create a value TOKEN=... Let's install a new module pip install dotenv Now we place an import statement at the top of our Python script. And we can call a function import dotenv dotenv . load_dotenv () When the script is run, this function call will load the \".env\" file and add the TOKEN value to the environment. Its value will be available as an environment variable. Let's try it out: import os , dotenv dotenv . load_dotenv () token = os . getenv ( 'TOKEN' ) print ( token ) As we can see we successfully loaded the .env file, then accessed its value as an environment variable. However, once the script has completed execution and we exit the interpreter, the TOKEN variable is no longer available $Env:TOKEN This is how we can protect secrets at the measly cost of nothing more than 2 additional lines of code. This has the added benefit that we will be able to use the same .env file for any additional Python scripts we compose in this directory. Let's compose # import discord import os , dotenv from discord.ext.commands import Bot dotenv . load_dotenv () token = os . getenv ( 'TOKEN' ) client = Bot ( command_prefix = '.' ) # ... client . run ( token )","title":"Environment variable"},{"location":"Scripts/Discord/Discord/#textchannel-hello-world","text":"These expressions preceded by the @ sign are called decorators . A deeper discussion of what decorators do is outside the scope of this video, but it's enough to say that they add functionality to any functions they decorate. And in discord.py in particular, they will tie together all the code you write. ctx.send equivalent to print @client . command () async def hello ( ctx ): await ctx . send ( f 'Hello world!' )","title":"TextChannel hello-world"},{"location":"Scripts/Discord/Discord/#dm-hello-world","text":"@client . command () async def hello ( ctx ): user = ctx . author await user . send ( 'Hey handsome' )","title":"DM hello-world"},{"location":"Scripts/Discord/Discord/#enriched-hello-world","text":"Embed : enriched message - URLs are automatically put into links @client . command () async def hello ( ctx ): user = ctx . message . author . name emoji = random . choice ([ ':grin:' , ':smiling_face_with_3_hearts:' , ':smirk:' ]) embed = discord . Embed ( title = f \"Hello { user } !\" , description = f \"Thank you for the attention { emoji } \" , color = discord . Color . teal ()) await ctx . send ( embed = embed )","title":"Enriched hello-world"},{"location":"Scripts/Discord/Discord/#cogs","text":"When you're developing something from scratch or learning something new, your code can get messy very quickly. It's always a good idea to keep your code as tidy and clean as possible. You can definitely do this using the standard Python imports and modules. This shouldn't be much of an issue since we're defining procedural functions which have nothing to do with one another logically. The discord.py library offers a way to do that in what are called \"Cogs\", which not only organize and modularize your code but offer a way to group your commands into command groups. from discord.ext import commands import dotenv import os dotenv . load_dotenv () token = os . getenv ( 'TOKEN' ) client = commands . Bot ( command_prefix = '.' ) client . load_extension ( 'cogs.Example' ) if __name__ == \"__main__\" : client . run ( token ) The way the cog file itself is designated by using the directory name and filename separated by period is the same convention used in Django . from discord.ext import commands class Example ( commands . Cog ): def __init__ ( self , client ): self . client = client @commands . command () async def ping ( self , ctx ): await ctx . send ( 'pong' )","title":"Cogs"},{"location":"Scripts/Discord/Discord/#call-out-to-click-video","text":"If the use of these command decorators strikes you as bizarre and you feel that it would help you to see them being used in a different context, another video I recently made should help you out. In it, I go over the Click package, which allows you to create command-line utilities in Python using decorators that look and act a whole lot like these ones.","title":"Call-out to click video"},{"location":"Scripts/Discord/Discord/#inspection-of-ctx-object","text":"Listing channels on a Discord server @commands . command () async def channels ( self , ctx ): channels = [ c . name for c in ctx . guild . channels if type ( c ) is discord . TextChannel ] Snitch command @commands . command () async def dmowner ( self , ctx ): user_id = ctx . guild . owner_id user = self . client . get_user ( user_id ) print ( user_id ) await user . send ( f 'This guy { ctx . author . name } is creeping me out... Betta handle dat!' )","title":"Inspection of ctx object"},{"location":"Scripts/Discord/Discord/#events","text":"Event handler functions conventionally have names beginning with \"on\", i.e. on_load - @commands.Cog.listener() vs. @client.event vs. on_ready(self) (class-based) @client . event async def on_ready (): print ( f \"Logged in as { client . user . name } \" ) @commands . Cog . listener () async def on_ready ( self ): print ( f \"Logged in as { self . client . user . name } \" )","title":"Events"},{"location":"Scripts/Discord/Discord/#reaction-role","text":"So we've established a broad-based understanding of the features exposed in the discord.py API: - commands defined using the command() decorator - event handlers that use the event decorator and which override the API's built-in events - Context objects that are passed to commands upon invocation from the message window. Context objects, in turn, expose a - send method that we can use to send messages back to the TextChannel where the command originated - Using the author attribute of a Context object, we can send direct messages. - Embed objects for richer message content - Cogs to organize and modularize commands - Most importantly, we learned about the Bot object itself, which accepts the token we get from the Discord developer portal, and how to keep that token secure. Let's implement a practical project that will allow us to bring these lessons home. One of the most common features that Discord guild maintainers want to implement is a bot that will automatically assign roles to members based on an emoji reaction, or a \"reaction role\". In Discord these are often used to force members to indicate agreement to a code of conduct or to assign fun roles based on the individual's personal interests. First we need to adjust the permissions of the bot by assigning it the \"Manage roles\" permission. We can do this through the developer portal, but it's easier to do it in the Discord server itself We right-click on our Discord server and open server settings Let's go to the Roles section and click on Chatty Cathy We scroll down until we find the Manage Roles permission We grant Chatty Cathy the permission to manage roles We also need to actually create a role that Chatty Cathy will assign to a user. We can also do this in server settings. We also need to make sure the bot's role is placed physically above that of the role to be assigned. Discord roles are arranged like a totem pole in this way, such that roles can only be assigned by higher roles. Finally we can write some code.","title":"Reaction role"},{"location":"Scripts/Discord/Discord/#on_raw_reaction_add","text":"Let's go back to Visual Studio Code where we will create a new event handler in main.py for the ON RAW REACTION ADD event. This event, unlike the on_ready event will be passed a PAYLOAD argument.. @bot . event async def on_raw_reaction_add ( payload ): pass The Payload argument is similar in concept to a Context object. It is not as rich in information. A payload does expose a few useful properties that we will need if we want to implement the functionality of a reaction role. Where a Context object exposes a Guild object, a Payload object exposes only a guild_id . Where a Context object exposes a Channel object, a Payload object exposes only a channel_id . Where a Context object exposes a Message object, a Payload object exposes only a message_id . This means that we have to implement additional query logic in order to retrieve these objects from the ID numbers alone. Context RawReactionActionEvent Guild guild_id Channel channel_id Message message_id @client . event async def on_raw_reaction_add ( payload ): print ( \"Message ID:\" , payload . message_id ) print ( \"Channel ID:\" , payload . channel_id ) print ( 'Guild ID:' , payload . guild_id ) print ( 'Emoji:' , payload . emoji . name ) Let's run it in the integrated terminal and see how it responds. After a moment, the on_ready event handlers fire, and the bot is running. Let's open Discord, react to a message, then examine the output in the terminal. The event handler worked! Now let's see what the significance of this output is. Let's open Discord again, copy the message URL, and compare it to the output in the terminal. As we can see, - The first number indicates the Discord server, or Guild as it is referred to in the Discord API - The second number indicates the channel ID - The third number indicates the message ID Now let's create a new channel. This is where new Discord server members will have to go to be granted the Turtles role. Respond with :thumbs_up: And let's give it a reaction. As we can see, the event handler will be triggered when members perform emoji reactions to any message in any TextChannel on our Discord server. Now we're ready to complete the implementation of the role reaction","title":"on_raw_reaction_add"},{"location":"Scripts/Discord/Discord/#discordutilsget","text":"In order to retrieve the object from the object id, we're going to use the discord.utils.get function. discord.utils.get takes the sequence to be queried first, and we can pass the guild_id into the id keyword argument. guild = discord . utils . get ( client . guilds , id = payload . guild_id ) Now that we have a guild object, we can use this same get function to get the role that we want to assign. role = discord . utils . get ( guild . roles , name = 'Turtles' ) Now we can retrieve the member object for the user that actually provided the emoji reaction member = discord . utils . get ( guild . members , id = payload . user_id ) Finally, a method exposed by the member object allows us to add the role. await member . add_roles ( role ) Let's fire up our bot in the integrated terminal. The on_ready events fired, and we're up and running. Let's switch over to Discord. We can see that my username in Discord is not green. I do have a crown, because I am the Guild owner, but I do not have the Turtles role yet. I put a thumbs up on the message, and I can see that now my name is green. Our reaction role bot worked! For some objects, in particular Guild , the Client object exposes a specific method: guild = client . get_guild ( payload . guild_id ) discord.utils.find uses a syntax similar to the Python builtin filter function, where a lambda function is defined, then the sequence of items across which it will be executed. So for example, let's start a Python interpreter in the terminal. Let's make a list of numbers from 0 through 9. l = range ( 10 ) We can use the filter command to find only even numbers: list ( filter ( lambda x : x % 2 == 0 , l )) # [0,2,4,6,8] What we're doing here is that we're iterating over this sequence of elements and executing this lambda function by passing each element through this expression. If the expression returns true, then that element makes it through. If the expression evaluates as false, that value is filtered. discord.utils.find uses an identical syntax, but instead of returning multiple values, it returns only one. So we can use it to retrieve the guild object from the guild_id guild = discord . utils . find ( lambda g : g . id == payload . guild_id , client . guilds ) A deeper discussion of lambdas is outside the scope of this video, and because the syntax can be confusing to someone who's not familiar with the topic, I'm actually going to comment out this line.","title":"discord.utils.get"},{"location":"Scripts/Discord/Discord/#adding-role","text":"Member.add_roles method takes a role object @client . event async def on_raw_reaction_add ( payload ): if payload . message_id == 751854496748929065 and payload . emoji . name == u \" \\U0001F44D \" : guild = get ( client . guilds , id = payload . guild_id )","title":"Adding role"},{"location":"Scripts/Discord/Discord/#api","text":"Context Guild Message Author Client guilds Guild channels roles members id Payload Member Emoji message_id channel_id guild_id","title":"API"}]}